{
    "status": "Opinion", 
    "last_updated": "2014-09-18 23:16:46.287164+00:00", 
    "description": "Due to the way nova currently handles rescheduling there is a tendency, when a large number of compute nodes need to reschedule (for whatever error) that they then swamp the message queue (and nova scheduler) with rescheduling messages. This can cascade to where further rescheduling messages will occur (and repeat...) or until the MQ piles up and/or the scheduler falls over.\n\nEven with a reschedule 'count' under situations when rescheduling is happening on mass the rescheduling itself can cause more problems for your system than it helps solve (aka, just leave the scheduled instance in error state). Likely the way to do this in a more centralized manner (aka with a orchestration unit that can do this rescheduling on behalf of the request) can help rate limit itself and its requests to the scheduler for new locations to schedule to. Having each compute node perform this same operation means rate limiting is not possible (and allows for your own system to DDOS itself).", 
    "tags": [
        "ops", 
        "scheduler"
    ], 
    "importance": "Wishlist", 
    "heat": 14, 
    "link": "https://bugs.launchpad.net/nova/+bug/1161664", 
    "owner": "None", 
    "id": 1161664, 
    "index": 1339, 
    "created": "2013-03-28 23:10:41.358212+00:00", 
    "title": "Rescheduling can DDOS itself", 
    "comments": [
        {
            "content": "Due to the way nova currently handles rescheduling there is a tendency, when a large number of compute nodes need to reschedule (for whatever error) that they then swamp the message queue (and nova scheduler) with rescheduling messages. This can cascade to where further rescheduling messages will occur (and repeat...) or until the MQ piles up and/or the scheduler falls over. \n\nEven with a reschedule 'count' under situations when rescheduling is happening on mass the rescheduling itself can cause more problems for your system than it helps solve (aka, just leave the scheduled instance in error state). Likely the way to do this in a more centralized manner (aka with a orchestration unit that can do this rescheduling on behalf of the request) can help rate limit itself and its requests to the scheduler for new locations to schedule to. Having each compute node perform this same operation means rate limiting is not possible (and allows for your own system to DOS itself).", 
            "date_created": "2013-03-28 23:10:41.358212+00:00", 
            "author": "https://api.launchpad.net/1.0/~harlowja"
        }, 
        {
            "content": "More a performance issue at scale than incorrect behavior.  Additionally it seems like this could be addressed indirectly by some of the orchestration work being discussed rather than needing a specific fix.", 
            "date_created": "2013-03-29 18:23:08.994246+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "@Josh -- what scheduler are you using?", 
            "date_created": "2013-03-31 23:28:46.624925+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "@Michael: this was just with a simple scheduler on a 5 nodeish setup. Basically you can just cause the whole system to backup with rescheduling. Should changing the scheduler help?", 
            "date_created": "2013-04-01 17:14:14.019139+00:00", 
            "author": "https://api.launchpad.net/1.0/~harlowja"
        }, 
        {
            "content": "This isn't really a bug. If you have a reproduce with an expected behavior and actually behavior maybe we can turn it into a bug.", 
            "date_created": "2014-09-18 23:16:37.986265+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
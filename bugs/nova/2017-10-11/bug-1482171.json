{
    "status": "Invalid", 
    "last_updated": "2016-02-18 14:05:39.533447+00:00", 
    "description": "Kilo release on Centos 7.\n\nwhen nova  attach a ceph based volume to an existing instance, do not notify cinder(?) about the attaching so I cannot detach and/or delete the volume later. The volume actualy attached to the instance\n\n## Cinder volume and nova instance\n\n$ cinder list\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n$ nova list\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n| ID                                   | Name  | Status | Task State | Power State | Networks            |\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n| 890cce5e-8e5c-4871-ba80-6fd5a4045c2f | ubu 1 | ACTIVE | -          | Running     | daddy-net=10.0.0.19 |\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n\n\n\n## attach volume to instance\n$ nova volume-attach 890cce5e-8e5c-4871-ba80-6fd5a4045c2f 736ead1d-2c59-4415-a0a3-8a1e8379872d\n+----------+--------------------------------------+\n| Property | Value                                |\n+----------+--------------------------------------+\n| device   | /dev/vdb                             |\n| id       | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n| serverId | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f |\n| volumeId | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n+----------+--------------------------------------+\n\n\n\n## cinder (and nova) do not know anything about attaching ('attached to' field is empty):\n\n$ cinder list\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n\n\n## while  the instance can use the new volume:\n\nroot@ubu-1:~# mkfs.ext4 /dev/vdb \nmke2fs 1.42.12 (29-Aug-2014)\nCreating filesystem with 2621440 4k blocks and 655360 inodes\nFilesystem UUID: 450b9169-087e-4dba-aac6-4a23593a5a97\nSuperblock backups stored on blocks: \n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done \n\nroot@ubu-1:~# mount /dev/vdb /mnt/\nroot@ubu-1:~# df -h\nFilesystem                          Size  Used Avail Use% Mounted on\nudev                                997M     0  997M   0% /dev\ntmpfs                               201M  4.6M  196M   3% /run\n/dev/disk/by-label/cloudimg-rootfs   20G  858M   19G   5% /\ntmpfs                              1001M     0 1001M   0% /dev/shm\ntmpfs                               5.0M     0  5.0M   0% /run/lock\ntmpfs                              1001M     0 1001M   0% /sys/fs/cgroup\ntmpfs                               201M     0  201M   0% /run/user/1000\n/dev/vdb                            9.8G   23M  9.2G   1% /mnt\n\n\n\n\n## finally I cannot delete the volume,  cinder stuck \"deleting'  state and do not delete it from ceph  pool\n$ cinder list\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |  Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | deleting | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n\n\nlogs do not show  errors\n\nOS: Centos 7\n\ncinder and nova versions:\n\npython-cinder-2015.1.0-3.el7.noarch\nopenstack-cinder-2015.1.0-3.el7.noarch\npython-cinderclient-1.1.1-1.el7.noarch\n\npython-novaclient-2.23.0-1.el7.noarch\nopenstack-nova-conductor-2015.1.0-3.el7.noarch\nopenstack-nova-console-2015.1.0-3.el7.noarch\nopenstack-nova-common-2015.1.0-3.el7.noarch\nopenstack-nova-api-2015.1.0-3.el7.noarch\npython-nova-2015.1.0-3.el7.noarch\nopenstack-nova-cert-2015.1.0-3.el7.noarch\nopenstack-nova-novncproxy-2015.1.0-3.el7.noarch\nopenstack-nova-scheduler-2015.1.0-3.el7.noarch\n\nExpected result: able to attach a ceph based cinder volume to instance(s), and then detach and delete it\n\nActual result: nova volume-attach the volume to the instance but do not aware of the fact of the attaching therefore I cannot detach the volume or delete the volume", 
    "tags": [
        "ceph", 
        "cinder"
    ], 
    "importance": "Undecided", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1482171", 
    "owner": "https://api.launchpad.net/1.0/~amritgeo", 
    "id": 1482171, 
    "index": 5688, 
    "created": "2015-08-06 10:47:08.000829+00:00", 
    "title": "cinder ceph volume cannot attach to instance in Kilo", 
    "comments": [
        {
            "content": "Kilo release on Centos 7.\n\nwhen nova  attach a ceph based volume to an existing instance, do not notify cinder(?) about the attaching so I cannot detach and/or delete the volume later. The volume actualy attached to the instance\n\n## Cinder volume and nova instance\n\n$ cinder list\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n$ nova list\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n| ID                                   | Name  | Status | Task State | Power State | Networks            |\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n| 890cce5e-8e5c-4871-ba80-6fd5a4045c2f | ubu 1 | ACTIVE | -          | Running     | daddy-net=10.0.0.19 |\n+--------------------------------------+-------+--------+------------+-------------+---------------------+\n\n\n\n## attach volume to instance\n$ nova volume-attach 890cce5e-8e5c-4871-ba80-6fd5a4045c2f 736ead1d-2c59-4415-a0a3-8a1e8379872d\n+----------+--------------------------------------+\n| Property | Value                                |\n+----------+--------------------------------------+\n| device   | /dev/vdb                             |\n| id       | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n| serverId | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f |\n| volumeId | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n+----------+--------------------------------------+\n\n\n\n## cinder (and nova) do not know anything about attaching ('attached to' field is empty):\n\n$ cinder list\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n\n\n## while  the instance can use the new volume:\n\nroot@ubu-1:~# mkfs.ext4 /dev/vdb \nmke2fs 1.42.12 (29-Aug-2014)\nCreating filesystem with 2621440 4k blocks and 655360 inodes\nFilesystem UUID: 450b9169-087e-4dba-aac6-4a23593a5a97\nSuperblock backups stored on blocks: \n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done \n\nroot@ubu-1:~# mount /dev/vdb /mnt/\nroot@ubu-1:~# df -h\nFilesystem                          Size  Used Avail Use% Mounted on\nudev                                997M     0  997M   0% /dev\ntmpfs                               201M  4.6M  196M   3% /run\n/dev/disk/by-label/cloudimg-rootfs   20G  858M   19G   5% /\ntmpfs                              1001M     0 1001M   0% /dev/shm\ntmpfs                               5.0M     0  5.0M   0% /run/lock\ntmpfs                              1001M     0 1001M   0% /sys/fs/cgroup\ntmpfs                               201M     0  201M   0% /run/user/1000\n/dev/vdb                            9.8G   23M  9.2G   1% /mnt\n\n\n\n\n## finally I cannot delete the volume,  cinder stuck \"deleting'  state and do not delete it from ceph  pool\n$ cinder list\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n|                  ID                  |  Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n| 736ead1d-2c59-4415-a0a3-8a1e8379872d | deleting | test volume |  10  | ceph_cluster |  false   |    False    |             |\n+--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n\n\nlogs do not show  errors\n\nOS: Centos 7\n\ncinder and nova versions:\n\npython-cinder-2015.1.0-3.el7.noarch\nopenstack-cinder-2015.1.0-3.el7.noarch\npython-cinderclient-1.1.1-1.el7.noarch\n\npython-novaclient-2.23.0-1.el7.noarch\nopenstack-nova-conductor-2015.1.0-3.el7.noarch\nopenstack-nova-console-2015.1.0-3.el7.noarch\nopenstack-nova-common-2015.1.0-3.el7.noarch\nopenstack-nova-api-2015.1.0-3.el7.noarch\npython-nova-2015.1.0-3.el7.noarch\nopenstack-nova-cert-2015.1.0-3.el7.noarch\nopenstack-nova-novncproxy-2015.1.0-3.el7.noarch\nopenstack-nova-scheduler-2015.1.0-3.el7.noarch\n\nExpected result: able to attach a ceph based cinder volume to instance(s), and then detach and delete it\n\nActual result: nova volume-attach the volume to the instance but do not aware of the fact of the attaching therefore I cannot detach the volume or delete the volume", 
            "date_created": "2015-08-06 10:47:08.000829+00:00", 
            "author": "https://api.launchpad.net/1.0/~kollarl-f"
        }, 
        {
            "content": "Could you please place the info related to ceph version which you have installed ", 
            "date_created": "2015-08-24 09:36:28.648062+00:00", 
            "author": "https://api.launchpad.net/1.0/~narasimha18sv"
        }, 
        {
            "content": "\nDear developer, \n\n\nThe ceph version is 'hammer'. \n\nThe things automagically started working. I left our system alone for three days because of holiday. Then it starts work when I tested it again. \nI do not understand why it failed, and do not understand why it started working as expected. \n\nThank you for the help! \n\nBests \n\nLaszlo Kollar\n\n>>> narasimha18sv <email address hidden> 8/24/2015 11:36 AM >>>\nCould you please place the info related to ceph version which you have\ninstalled\n\n--\nYou received this bug notification because you are subscribed to the bug\nreport.\nhttps://bugs.launchpad.net/bugs/1482171\n\nTitle:\n  cinder ceph volume cannot attach to instance in Kilo\n\nStatus in OpenStack Compute (nova):\n  New\n\nBug description:\n  Kilo release on Centos 7.\n\n  when nova  attach a ceph based volume to an existing instance, do not\n  notify cinder(?) about the attaching so I cannot detach and/or delete\n  the volume later. The volume actualy attached to the instance\n\n  ## Cinder volume and nova instance\n\n  $ cinder list\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n  $ nova list\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n  | ID                                   | Name  | Status | Task State | Power State | Networks            |\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n  | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f | ubu 1 | ACTIVE | -          | Running     | daddy-net=10.0.0.19 |\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n\n\n  ## attach volume to instance\n  $ nova volume-attach 890cce5e-8e5c-4871-ba80-6fd5a4045c2f 736ead1d-2c59-4415-a0a3-8a1e8379872d\n  +----------+--------------------------------------+\n  | Property | Value                                |\n  +----------+--------------------------------------+\n  | device   | /dev/vdb                             |\n  | id       | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n  | serverId | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f |\n  | volumeId | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n  +----------+--------------------------------------+\n\n\n  ## cinder (and nova) do not know anything about attaching ('attached\n  to' field is empty):\n\n  $ cinder list\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n\n  ## while  the instance can use the new volume:\n\n  root@ubu-1:~# mkfs.ext4 /dev/vdb\n  mke2fs 1.42.12 (29-Aug-2014)\n  Creating filesystem with 2621440 4k blocks and 655360 inodes\n  Filesystem UUID: 450b9169-087e-4dba-aac6-4a23593a5a97\n  Superblock backups stored on blocks:\n  32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\n  Allocating group tables: done                           \n  Writing inode tables: done                           \n  Creating journal (32768 blocks): done\n  Writing superblocks and filesystem accounting information: done\n\n  root@ubu-1:~# mount /dev/vdb /mnt/\n  root@ubu-1:~# df -h\n  Filesystem                          Size  Used Avail Use% Mounted on\n  udev                                997M     0  997M   0% /dev\n  tmpfs                               201M  4.6M  196M   3% /run\n  /dev/disk/by-label/cloudimg-rootfs   20G  858M   19G   5% /\n  tmpfs                              1001M     0 1001M   0% /dev/shm\n  tmpfs                               5.0M     0  5.0M   0% /run/lock\n  tmpfs                              1001M     0 1001M   0% /sys/fs/cgroup\n  tmpfs                               201M     0  201M   0% /run/user/1000\n  /dev/vdb                            9.8G   23M  9.2G   1% /mnt\n\n\n \n  ## finally I cannot delete the volume,  cinder stuck \"deleting'  state and do not delete it from ceph  pool\n  $ cinder list\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |  Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | deleting | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n\n \n  logs do not show  errors\n\n  OS: Centos 7\n\n  cinder and nova versions:\n\n  python-cinder-2015.1.0-3.el7.noarch\n  openstack-cinder-2015.1.0-3.el7.noarch\n  python-cinderclient-1.1.1-1.el7.noarch\n\n  python-novaclient-2.23.0-1.el7.noarch\n  openstack-nova-conductor-2015.1.0-3.el7.noarch\n  openstack-nova-console-2015.1.0-3.el7.noarch\n  openstack-nova-common-2015.1.0-3.el7.noarch\n  openstack-nova-api-2015.1.0-3.el7.noarch\n  python-nova-2015.1.0-3.el7.noarch\n  openstack-nova-cert-2015.1.0-3.el7.noarch\n  openstack-nova-novncproxy-2015.1.0-3.el7.noarch\n  openstack-nova-scheduler-2015.1.0-3.el7.noarch\n\n  Expected result: able to attach a ceph based cinder volume to\n  instance(s), and then detach and delete it\n\n  Actual result: nova volume-attach the volume to the instance but do\n  not aware of the fact of the attaching therefore I cannot detach the\n  volume or delete the volume\n\nTo manage notifications about this bug go to:\nhttps://bugs.launchpad.net/nova/+bug/1482171/+subscriptions\n", 
            "date_created": "2015-08-24 11:20:46+00:00", 
            "author": "https://api.launchpad.net/1.0/~kollarl-f"
        }, 
        {
            "content": "Please specify which openstack version is used for this deployment", 
            "date_created": "2015-09-01 12:19:47.401128+00:00", 
            "author": "https://api.launchpad.net/1.0/~amritgeo"
        }, 
        {
            "content": "\nIt was openstack kilo release, and ceph hammer release. \n\nInfortunately I cannot reproduce th error. I left alone my openstack for three days then the cinder works as expected with ceph. I was not able to figure out the root of the phenomen. \n\nCase can be closed. \n\nThank you anyway!\n\n>>> AMRITANSHU <email address hidden> 9/1/2015 02:19 PM >>>\nPlease specify which openstack version is used for this deployment\n\n--\nYou received this bug notification because you are subscribed to the bug\nreport.\nhttps://bugs.launchpad.net/bugs/1482171\n\nTitle:\n  cinder ceph volume cannot attach to instance in Kilo\n\nStatus in OpenStack Compute (nova):\n  New\n\nBug description:\n  Kilo release on Centos 7.\n\n  when nova  attach a ceph based volume to an existing instance, do not\n  notify cinder(?) about the attaching so I cannot detach and/or delete\n  the volume later. The volume actualy attached to the instance\n\n  ## Cinder volume and nova instance\n\n  $ cinder list\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n  $ nova list\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n  | ID                                   | Name  | Status | Task State | Power State | Networks            |\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n  | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f | ubu 1 | ACTIVE | -          | Running     | daddy-net=10.0.0.19 |\n  +--------------------------------------+-------+--------+------------+-------------+---------------------+\n\n\n  ## attach volume to instance\n  $ nova volume-attach 890cce5e-8e5c-4871-ba80-6fd5a4045c2f 736ead1d-2c59-4415-a0a3-8a1e8379872d\n  +----------+--------------------------------------+\n  | Property | Value                                |\n  +----------+--------------------------------------+\n  | device   | /dev/vdb                             |\n  | id       | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n  | serverId | 890cce5e-8e5c-4871-ba80-6fd5a4045c2f |\n  | volumeId | 736ead1d-2c59-4415-a0a3-8a1e8379872d |\n  +----------+--------------------------------------+\n\n\n  ## cinder (and nova) do not know anything about attaching ('attached\n  to' field is empty):\n\n  $ cinder list\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |   Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | available | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+-----------+-------------+------+--------------+----------+-------------+-------------+\n\n\n  ## while  the instance can use the new volume:\n\n  root@ubu-1:~# mkfs.ext4 /dev/vdb\n  mke2fs 1.42.12 (29-Aug-2014)\n  Creating filesystem with 2621440 4k blocks and 655360 inodes\n  Filesystem UUID: 450b9169-087e-4dba-aac6-4a23593a5a97\n  Superblock backups stored on blocks:\n  32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\n  Allocating group tables: done                           \n  Writing inode tables: done                           \n  Creating journal (32768 blocks): done\n  Writing superblocks and filesystem accounting information: done\n\n  root@ubu-1:~# mount /dev/vdb /mnt/\n  root@ubu-1:~# df -h\n  Filesystem                          Size  Used Avail Use% Mounted on\n  udev                                997M     0  997M   0% /dev\n  tmpfs                               201M  4.6M  196M   3% /run\n  /dev/disk/by-label/cloudimg-rootfs   20G  858M   19G   5% /\n  tmpfs                              1001M     0 1001M   0% /dev/shm\n  tmpfs                               5.0M     0  5.0M   0% /run/lock\n  tmpfs                              1001M     0 1001M   0% /sys/fs/cgroup\n  tmpfs                               201M     0  201M   0% /run/user/1000\n  /dev/vdb                            9.8G   23M  9.2G   1% /mnt\n\n\n \n  ## finally I cannot delete the volume,  cinder stuck \"deleting'  state and do not delete it from ceph  pool\n  $ cinder list\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n  |                  ID                  |  Status  |     Name    | Size | Volume Type  | Bootable | Multiattach | Attached to |\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n  | 736ead1d-2c59-4415-a0a3-8a1e8379872d | deleting | test volume |  10  | ceph_cluster |  false   |    False    |             |\n  +--------------------------------------+----------+-------------+------+--------------+----------+-------------+-------------+\n\n \n  logs do not show  errors\n\n  OS: Centos 7\n\n  cinder and nova versions:\n\n  python-cinder-2015.1.0-3.el7.noarch\n  openstack-cinder-2015.1.0-3.el7.noarch\n  python-cinderclient-1.1.1-1.el7.noarch\n\n  python-novaclient-2.23.0-1.el7.noarch\n  openstack-nova-conductor-2015.1.0-3.el7.noarch\n  openstack-nova-console-2015.1.0-3.el7.noarch\n  openstack-nova-common-2015.1.0-3.el7.noarch\n  openstack-nova-api-2015.1.0-3.el7.noarch\n  python-nova-2015.1.0-3.el7.noarch\n  openstack-nova-cert-2015.1.0-3.el7.noarch\n  openstack-nova-novncproxy-2015.1.0-3.el7.noarch\n  openstack-nova-scheduler-2015.1.0-3.el7.noarch\n\n  Expected result: able to attach a ceph based cinder volume to\n  instance(s), and then detach and delete it\n\n  Actual result: nova volume-attach the volume to the instance but do\n  not aware of the fact of the attaching therefore I cannot detach the\n  volume or delete the volume\n\nTo manage notifications about this bug go to:\nhttps://bugs.launchpad.net/nova/+bug/1482171/+subscriptions\n", 
            "date_created": "2015-09-01 12:29:25+00:00", 
            "author": "https://api.launchpad.net/1.0/~kollarl-f"
        }, 
        {
            "content": "I have removed the \"nova\" tag because there is no subteam which watches this tag.", 
            "date_created": "2015-11-26 14:07:20.341610+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Original reported said they could no longer reproduce the bug. ", 
            "date_created": "2016-02-18 14:05:38.779228+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
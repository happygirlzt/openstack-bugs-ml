{
    "status": "Confirmed", 
    "last_updated": "2017-06-23 16:32:04.375857+00:00", 
    "description": "We don't have a consistent behaviour when we're using the AZ hack for forcing a destination.\n\nTo be clear, when calling the scheduler by adding a forced destination, we're not verifying the filters but we still return the limits from the HostState. Unfortunately, given those limits are only set by the corresponding filter, we return what we have in memory that doesn't really correspond to the usage request.\n\nExample : \nSay I'm booting an instance first and then booting another instance with the AZ flag. In that case, the tuple returned by the scheduler will include the exisiting limits. For example, say that I've an allocation ratio for VCPUs of 1.0, then I got :\n\n[{'host': u'foo', 'nodename': u'bar', 'limits': {'vcpu': 1.0}}]\n\nNow, restart the scheduler service (so the corresponding HostState is recreated) and just boot an instance using the AZ hack, then we'll return :\n\n[{'host': u'foo', 'nodename': u'bar', 'limits': {}]\n\nThat's very inconsistent as two requests can have very different behaviour. For example, say I'm running a compute with only one pCPU and an CPU allocation ratio of 1.0 :\n - in the first case above, the forced instance will ERROR\n - in the second case (ie. restart the scheduler and issue the exact same command), it will boot.\n\nHonestly, I just think we should just not return limits if the instance is having the AZ hack.", 
    "tags": [
        "availability-zones", 
        "low-hanging-fruit", 
        "scheduler"
    ], 
    "importance": "Medium", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1652335", 
    "owner": "None", 
    "id": 1652335, 
    "index": 4715, 
    "created": "2016-12-23 15:36:30.587848+00:00", 
    "title": "The scheduler returns limits even if the instance was boot using AZ forced flag", 
    "comments": [
        {
            "content": "We don't have a consistent behaviour when we're using the AZ hack for forcing a destination.\n\nTo be clear, when calling the scheduler by adding a forced destination, we're not verifying the filters but we still return the limits from the HostState. Unfortunately, given those limits are only set by the corresponding filter, we return what we have in memory that doesn't really correspond to the usage request.\n\nExample : \nSay I'm booting an instance first and then booting another instance with the AZ flag. In that case, the tuple returned by the scheduler will include the exisiting limits. For example, say that I've an allocation ratio for VCPUs of 1.0, then I got :\n\n[{'host': u'foo', 'nodename': u'bar', 'limits': {'vcpu': 1.0}}]\n\nNow, restart the scheduler service (so the corresponding HostState is recreated) and just boot an instance using the AZ hack, then we'll return :\n\n[{'host': u'foo', 'nodename': u'bar', 'limits': {}]\n\nThat's very inconsistent as two requests can have very different behaviour. For example, say I'm running a compute with only one pCPU and an CPU allocation ratio of 1.0 :\n - in the first case above, the forced instance will ERROR\n - in the second case (ie. restart the scheduler and issue the exact same command), it will boot.\n\nHonestly, I just think we should just not return limits if the instance is having the AZ hack.", 
            "date_created": "2016-12-23 15:36:30.587848+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "Bugs with an assignee should be In Progress, so I am tagging this one with such label. If you don't feel like working on this bug please un-assign yourself :)", 
            "date_created": "2017-03-21 16:12:46.525186+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }, 
        {
            "content": "Setting this to unassigned and New since I don't see any patches from Sylvain and this needs a reproducible test case to be Confirmed.", 
            "date_created": "2017-05-17 02:41:25.567846+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "@Sylvain Bauza\uff0cI think we cannot just ignore host state limits. Especially in NUMATopologyLimits situation, it is possible the request NUMATopology is not host NUMATopology.\nI believe we should load all filter when scheduler starts.", 
            "date_created": "2017-05-22 07:51:40.654050+00:00", 
            "author": "https://api.launchpad.net/1.0/~luogangyi"
        }
    ]
}
{
    "status": "Won't Fix", 
    "last_updated": "2016-05-17 06:30:47.988737+00:00", 
    "description": "Observed in the tempest-dsvm-cells job during tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_lock_unlock_server\n\nThe test locks an instance, attempts to stop it, makes sure that fails, unlocks it, attempts to stop it, and makes sure that succeeds.\n\nThe problem happens during the succession of actions \"unlock\" and \"stop\". The \"unlock\" does an instance.save() of the locked state at the top cell which will sync to the child. If the \"stop\" request reaches the child cell before the instance.save() state locked = False syncs to the child cell, the \"stop\" will fail with the following trace in screen-n-cell-child.txt:\n\n2015-06-18 19:09:23.852 ERROR nova.cells.messaging [req-6b3584bb-b52a-41ba-8e88-000e14ba6ec6 ServerActionsTestJSON-1466672685 ServerActionsTestJSON-639331338] Error processing message locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging Traceback (most recent call last):\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 200, in _process_locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 1256, in _process_message_locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     return fn(message, **message.method_kwargs)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 850, in stop_instance\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     clean_shutdown=clean_shutdown)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 839, in _call_compute_api_with_obj\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     return fn(ctxt, instance, *args, **kwargs)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/compute/api.py\", line 214, in inner\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     raise exception.InstanceIsLocked(instance_uuid=instance.uuid)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging InstanceIsLocked: Instance cb2485ba-e3e5-4668-869b-f145e5f28a1a is locked\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging\n\nLogstash query: message:\"InstanceIsLocked\" AND tags:\"screen-n-cell-child.txt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiSW5zdGFuY2VJc0xvY2tlZFwiIEFORCB0YWdzOlwic2NyZWVuLW4tY2VsbC1jaGlsZC50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQzNDY3NDMyOTQ0Mn0=", 
    "tags": [
        "cells", 
        "in-stable-liberty"
    ], 
    "importance": "Low", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1466696", 
    "owner": "None", 
    "id": 1466696, 
    "index": 494, 
    "created": "2015-06-19 00:41:58.209339+00:00", 
    "title": "Cells: Race between instance 'unlock' and 'stop' can cause 'stop' to fail", 
    "comments": [
        {
            "content": "Observed in the tempest-dsvm-cells job during tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_lock_unlock_server\n\nThe test locks an instance, attempts to stop it, makes sure that fails, unlocks it, attempts to stop it, and makes sure that succeeds.\n\nThe problem happens during the succession of actions \"unlock\" and \"stop\". The \"unlock\" does an instance.save() of the locked state at the top cell which will sync to the child. If the \"stop\" request reaches the child cell before the instance.save() state locked = False syncs to the child cell, the \"stop\" will fail with the following trace in screen-n-cell-child.txt:\n\n2015-06-18 19:09:23.852 ERROR nova.cells.messaging [req-6b3584bb-b52a-41ba-8e88-000e14ba6ec6 ServerActionsTestJSON-1466672685 ServerActionsTestJSON-639331338] Error processing message locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging Traceback (most recent call last):\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 200, in _process_locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 1256, in _process_message_locally\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     return fn(message, **message.method_kwargs)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 850, in stop_instance\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     clean_shutdown=clean_shutdown)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/cells/messaging.py\", line 839, in _call_compute_api_with_obj\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     return fn(ctxt, instance, *args, **kwargs)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging   File \"/opt/stack/new/nova/nova/compute/api.py\", line 214, in inner\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging     raise exception.InstanceIsLocked(instance_uuid=instance.uuid)\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging InstanceIsLocked: Instance cb2485ba-e3e5-4668-869b-f145e5f28a1a is locked\n2015-06-18 19:09:23.852 16161 ERROR nova.cells.messaging\n\nLogstash query: message:\"InstanceIsLocked\" AND tags:\"screen-n-cell-child.txt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiSW5zdGFuY2VJc0xvY2tlZFwiIEFORCB0YWdzOlwic2NyZWVuLW4tY2VsbC1jaGlsZC50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQzNDY3NDMyOTQ0Mn0=", 
            "date_created": "2015-06-19 00:41:58.209339+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Could it be related to https://bugs.launchpad.net/tempest/+bug/1468623 ?", 
            "date_created": "2015-07-10 15:26:11.677652+00:00", 
            "author": "https://api.launchpad.net/1.0/~jordan-pittier"
        }, 
        {
            "content": "I don't think so. The race makes it so stop reaches the child cells first and fails, then the instance.save() of locked=False hits the child cell and unlocks it, so the teardown ends up succeeding.", 
            "date_created": "2015-07-16 19:19:55.700209+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/243327", 
            "date_created": "2015-11-09 21:58:46.252576+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/243327\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=02de17c72ac0f0824df2b3d460739a58cb6b5285\nSubmitter: Jenkins\nBranch:    master\n\ncommit 02de17c72ac0f0824df2b3d460739a58cb6b5285\nAuthor: Sean Dague <email address hidden>\nDate:   Mon Nov 9 16:54:19 2015 -0500\n\n    skip lock_unlock_server test for cells\n    \n    The cells architecture of doing db replay makes this call\n    fundamentally racey for cells. The contract is that if you request an\n    action on a server and it is locked, you get a 409 at the API, and are\n    done. If you get a 2xx, the resource is not locked, and life is good.\n    \n    Except in cells, the API db will be updated, and the API will return\n    all is well. Except the db replay hasn't happened for the cell, so the\n    additional check in the nova compute for locked blows up.\n    \n    The fix probably involves the nova-compute check doing a timed retry\n    under cells because of replication lags.\n    \n    Change-Id: Ic18407b8372c286d8e258860f3e2bf76eceef2aa\n    Related-Bug: #1466696\n", 
            "date_created": "2015-11-11 19:38:42.143485+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/316150", 
            "date_created": "2016-05-13 15:04:44.338153+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/316150\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d836c7172d5662207ea89a4489d41fc9b28e34f5\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit d836c7172d5662207ea89a4489d41fc9b28e34f5\nAuthor: Sean Dague <email address hidden>\nDate:   Mon Nov 9 16:54:19 2015 -0500\n\n    skip lock_unlock_server test for cells\n    \n    The cells architecture of doing db replay makes this call\n    fundamentally racey for cells. The contract is that if you request an\n    action on a server and it is locked, you get a 409 at the API, and are\n    done. If you get a 2xx, the resource is not locked, and life is good.\n    \n    Except in cells, the API db will be updated, and the API will return\n    all is well. Except the db replay hasn't happened for the cell, so the\n    additional check in the nova compute for locked blows up.\n    \n    The fix probably involves the nova-compute check doing a timed retry\n    under cells because of replication lags.\n    \n    Conflicts:\n            devstack/tempest-dsvm-cells-rc\n    \n    NOTE(mriedem): The conflict is due to an additional test being\n    skipped in liberty that wasn't skipped in mitaka.\n    \n    Change-Id: Ic18407b8372c286d8e258860f3e2bf76eceef2aa\n    Related-Bug: #1466696\n    (cherry picked from commit 02de17c72ac0f0824df2b3d460739a58cb6b5285)\n", 
            "date_created": "2016-05-17 06:30:47.483137+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
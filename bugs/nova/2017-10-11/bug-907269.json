{
    "status": "Invalid", 
    "last_updated": "2012-07-12 13:11:11.437486+00:00", 
    "description": "Summary: can't launch instances since fresh install of Precise and Essex2 - Show stopper.\n\nPrecise Alpha-1 AMD64\nEssex-2 (Ubuntu repo)\n\nFresh install (this is a compute node, but instances fail to spawn on controller where nova-compute also runs)\n\nsudo apt-get install qemu unzip nova-compute python-support\n\n/etc/nova/nova.conf\n--daemonize=1\n--verbose\n--dhcpbridge_flagfile=/etc/nova/nova.conf\n--dhcpbridge=/usr/bin/nova-dhcpbridge\n--force_dhcp_release\n--logdir=/var/log/nova\n--state_path=/var/lib/nova\n--libvirt_type=kvm\n--libvirt_use_virtio_for_bridges\n--sql_connection=mysql://root:nova@172.15.0.1/nova\n--s3_host=172.15.0.1\n--rabbit_host=172.15.0.1\n--ec2_host=172.15.0.1\n--ec2_url=http://172.15.0.1:8773/services/Cloud\n--fixed_range=192.168.0.0/16\n--network_size=256\n--num_networks=1\n--FAKE_subdomain=ec2\n--public_interface=eth1\n--state_path=/var/lib/nova\n--lock_path=/var/lock/nova\n--image_service=nova.image.glance.GlanceImageService\n--glance_api_servers=172.15.0.1:9292\n--iscsi_helper=tgtadm\n--vlan_start=4025\n--vlan_interface=eth0\n--auto_assign_floating_ip\n\nProblem: fail to launch Ubuntu image.  Nova says successful, but Warning error suggests problem with disk image.  Tried Oneiric cloud image and Precise cloud image. I've tried m1.tiny and m1.small to see if there's a difference and still the same outcome.  These same steps and config used to work with Oneiric and Diablo.\n\nwget http://uec-images.ubuntu.com/releases/oneiric/release/ubuntu-11.10-server-cloudimg-i386.tar.gz\ncloud-publish-tarball ubuntu-11.10-server-cloudimg-i386.tar.gz images i386\neuca-run-instances ami-00000002 -k openstack -t m1.tiny\n\neuca-describe-instances\nINSTANCE\ti-00000006\tami-00000002\t172.15.1.5\tserver-6\trunning\topenstack (devops, openstack2)\t1\t\tm1.tiny\t2011-12-21T10:19:35Z\tnova\n\neuca-get-console-output i-0000006\ni-00000006\n2011-12-21T10:35:42Z\n\n\n/var/log/nova/nova-compute.log\n\n2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: starting toXML method from (pid=6760) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1183\n2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt.vif [-] Ensuring vlan 4025 and bridge br4025 from (pid=6760) plug /usr/lib/python2.7/dist-packages/nova/virt/libvirt/vif.py:82\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Got semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,279 DEBUG nova.utils [-] Got file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,279 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev vlan4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,300 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,301 DEBUG nova.utils [-] Got semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,301 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,302 DEBUG nova.utils [-] Got file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,302 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev br4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,322 DEBUG nova.utils [-] Running cmd (subprocess): sudo brctl addif br4025 vlan4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,335 DEBUG nova.utils [-] Result was 1 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:214\n2011-12-21 10:19:33,335 DEBUG nova.utils [-] Running cmd (subprocess): sudo route -n from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,349 DEBUG nova.utils [-] Running cmd (subprocess): sudo ip addr show dev vlan4025 scope global from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,486 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: finished toXML method from (pid=6760) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1187\n2011-12-21 10:19:33,486 INFO nova [-] called setup_basic_filtering in nwfilter\n2011-12-21 10:19:33,487 INFO nova [-] ensuring static filters\n2011-12-21 10:19:33,835 DEBUG nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x53fd650> from (pid=6760) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2011-12-21 10:19:33,835 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n2011-12-21 10:19:33,836 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0']\n2011-12-21 10:19:33,836 DEBUG nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x53fd110> from (pid=6760) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0', '-j ACCEPT -p icmp -s 0.0.0.0/0']\n2011-12-21 10:19:33,837 DEBUG nova.utils [-] Attempting to grab semaphore \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got semaphore \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Attempting to grab file lock \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got file lock \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,839 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t filter from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,855 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,870 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t nat from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,886 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,901 DEBUG nova.utils [-] Running cmd (subprocess): mkdir -p /var/lib/nova/instances/instance-00000006/ from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,921 INFO nova.virt.libvirt_conn [-] instance instance-00000006: Creating image\n2011-12-21 10:19:33,934 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,935 DEBUG nova.utils [-] Attempting to grab semaphore \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method \"call_if_not_exists\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,935 DEBUG nova.utils [-] Got semaphore \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method \"call_if_not_exists\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:20:07,236 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,237 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Notifying Schedulers of capabilities ... from (pid=6760) _publish_service_capabilities /usr/lib/python2.7/dist-packages/nova/manager.py:193\n2011-12-21 10:20:07,238 DEBUG nova.rpc [a66206a2-7745-423a-9cab-c14ade429be4 None None] Making asynchronous fanout cast... from (pid=6760) fanout_cast /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n2011-12-21 10:20:07,493 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,494 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._sync_power_states from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,854 INFO nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Found 1 in the database and 0 on the hypervisor.\n2011-12-21 10:20:07,854 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760) _reclaim_queued_deletes /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n2011-12-21 10:20:07,856 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._report_driver_status from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,856 INFO nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host status\n2011-12-21 10:20:07,856 DEBUG nova.virt.libvirt_conn [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host stats from (pid=6760) update_status /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1918\n2011-12-21 10:20:08,969 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:08,970 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:08,970 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Notifying Schedulers of capabilities ... from (pid=6760) _publish_service_capabilities /usr/lib/python2.7/dist-packages/nova/manager.py:193\n2011-12-21 10:21:08,971 DEBUG nova.rpc [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Making asynchronous fanout cast... from (pid=6760) fanout_cast /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n2011-12-21 10:21:09,289 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,289 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._sync_power_states from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,885 INFO nova.compute.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Found 1 in the database and 0 on the hypervisor.\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,887 DEBUG nova.compute.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760) _reclaim_queued_deletes /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n2011-12-21 10:21:09,887 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._report_driver_status from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,888 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:41,718 DEBUG nova.utils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/cb7d6887f61381feaa684f7b203baa586f53c02c_sm /var/lib/nova/instances/instance-00000006/disk from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,869 INFO nova.virt.libvirt_conn [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance instance-00000006: injecting key into image 97bb58ab-a17e-4611-a8de-875064ebeef0\n2011-12-21 10:21:41,870 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-00000006/disk from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,907 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo kpartx -a /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,925 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo kpartx -d /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,940 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo qemu-nbd -d /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,969 WARNING nova.virt.libvirt_conn [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance instance-00000006: ignoring error injecting data into image 97bb58ab-a17e-4611-a8de-875064ebeef0 (Mapped device was not found (we can only inject raw disk images): /dev/mapper/nbd15p1)\n2011-12-21 10:21:43,391 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: is running from (pid=6760) spawn /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:670\n2011-12-21 10:21:43,392 DEBUG nova.compute.manager [-] Checking state of instance-00000006 from (pid=6760) _get_power_state /usr/lib/python2.7/dist-packages/nova/compute/manager.py:192\n2011-12-21 10:21:44,043 INFO nova.virt.libvirt_conn [-] Instance instance-00000006 spawned successfully.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/907269", 
    "owner": "None", 
    "id": 907269, 
    "index": 2447, 
    "created": "2011-12-21 10:49:25.398603+00:00", 
    "title": "Mapped device was not found (we can only inject raw disk images): /dev/mapper/nbd15p1) : Precise A1 and E2", 
    "comments": [
        {
            "content": "Summary: can't launch instances since fresh install of Precise and Essex2 - Show stopper.\n\nPrecise Alpha-1 AMD64\nEssex-2 (Ubuntu repo)\n\nFresh install (this is a compute node, but instances fail to spawn on controller where nova-compute also runs)\n\nsudo apt-get install qemu unzip nova-compute python-support\n\n/etc/nova/nova.conf\n--daemonize=1\n--verbose\n--dhcpbridge_flagfile=/etc/nova/nova.conf\n--dhcpbridge=/usr/bin/nova-dhcpbridge\n--force_dhcp_release\n--logdir=/var/log/nova\n--state_path=/var/lib/nova\n--libvirt_type=kvm\n--libvirt_use_virtio_for_bridges\n--sql_connection=mysql://root:nova@172.15.0.1/nova\n--s3_host=172.15.0.1\n--rabbit_host=172.15.0.1\n--ec2_host=172.15.0.1\n--ec2_url=http://172.15.0.1:8773/services/Cloud\n--fixed_range=192.168.0.0/16\n--network_size=256\n--num_networks=1\n--FAKE_subdomain=ec2\n--public_interface=eth1\n--state_path=/var/lib/nova\n--lock_path=/var/lock/nova\n--image_service=nova.image.glance.GlanceImageService\n--glance_api_servers=172.15.0.1:9292\n--iscsi_helper=tgtadm\n--vlan_start=4025\n--vlan_interface=eth0\n--auto_assign_floating_ip\n\nProblem: fail to launch Ubuntu image.  Nova says successful, but Warning error suggests problem with disk image.  Tried Oneiric cloud image and Precise cloud image. I've tried m1.tiny and m1.small to see if there's a difference and still the same outcome.  These same steps and config used to work with Oneiric and Diablo.\n\nwget http://uec-images.ubuntu.com/releases/oneiric/release/ubuntu-11.10-server-cloudimg-i386.tar.gz\ncloud-publish-tarball ubuntu-11.10-server-cloudimg-i386.tar.gz images i386\neuca-run-instances ami-00000002 -k openstack -t m1.tiny\n\neuca-describe-instances\nINSTANCE\ti-00000006\tami-00000002\t172.15.1.5\tserver-6\trunning\topenstack (devops, openstack2)\t1\t\tm1.tiny\t2011-12-21T10:19:35Z\tnova\n\neuca-get-console-output i-0000006\ni-00000006\n2011-12-21T10:35:42Z\n\n\n/var/log/nova/nova-compute.log\n\n2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: starting toXML method from (pid=6760) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1183\n2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt.vif [-] Ensuring vlan 4025 and bridge br4025 from (pid=6760) plug /usr/lib/python2.7/dist-packages/nova/virt/libvirt/vif.py:82\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Got semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,279 DEBUG nova.utils [-] Got file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,279 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev vlan4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,300 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,301 DEBUG nova.utils [-] Got semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,301 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,302 DEBUG nova.utils [-] Got file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,302 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev br4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,322 DEBUG nova.utils [-] Running cmd (subprocess): sudo brctl addif br4025 vlan4025 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,335 DEBUG nova.utils [-] Result was 1 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:214\n2011-12-21 10:19:33,335 DEBUG nova.utils [-] Running cmd (subprocess): sudo route -n from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,349 DEBUG nova.utils [-] Running cmd (subprocess): sudo ip addr show dev vlan4025 scope global from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,486 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: finished toXML method from (pid=6760) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1187\n2011-12-21 10:19:33,486 INFO nova [-] called setup_basic_filtering in nwfilter\n2011-12-21 10:19:33,487 INFO nova [-] ensuring static filters\n2011-12-21 10:19:33,835 DEBUG nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x53fd650> from (pid=6760) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2011-12-21 10:19:33,835 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n2011-12-21 10:19:33,836 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0']\n2011-12-21 10:19:33,836 DEBUG nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x53fd110> from (pid=6760) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0', '-j ACCEPT -p icmp -s 0.0.0.0/0']\n2011-12-21 10:19:33,837 DEBUG nova.utils [-] Attempting to grab semaphore \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got semaphore \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Attempting to grab file lock \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got file lock \"iptables\" for method \"apply\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2011-12-21 10:19:33,839 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t filter from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,855 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,870 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t nat from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,886 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,901 DEBUG nova.utils [-] Running cmd (subprocess): mkdir -p /var/lib/nova/instances/instance-00000006/ from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:19:33,921 INFO nova.virt.libvirt_conn [-] instance instance-00000006: Creating image\n2011-12-21 10:19:33,934 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=6760) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2011-12-21 10:19:33,935 DEBUG nova.utils [-] Attempting to grab semaphore \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method \"call_if_not_exists\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2011-12-21 10:19:33,935 DEBUG nova.utils [-] Got semaphore \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method \"call_if_not_exists\"... from (pid=6760) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2011-12-21 10:20:07,236 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,237 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Notifying Schedulers of capabilities ... from (pid=6760) _publish_service_capabilities /usr/lib/python2.7/dist-packages/nova/manager.py:193\n2011-12-21 10:20:07,238 DEBUG nova.rpc [a66206a2-7745-423a-9cab-c14ade429be4 None None] Making asynchronous fanout cast... from (pid=6760) fanout_cast /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n2011-12-21 10:20:07,493 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,494 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._sync_power_states from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,854 INFO nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Found 1 in the database and 0 on the hypervisor.\n2011-12-21 10:20:07,854 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,855 DEBUG nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760) _reclaim_queued_deletes /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n2011-12-21 10:20:07,856 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._report_driver_status from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:20:07,856 INFO nova.compute.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host status\n2011-12-21 10:20:07,856 DEBUG nova.virt.libvirt_conn [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host stats from (pid=6760) update_status /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1918\n2011-12-21 10:20:08,969 DEBUG nova.manager [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:08,970 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:08,970 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Notifying Schedulers of capabilities ... from (pid=6760) _publish_service_capabilities /usr/lib/python2.7/dist-packages/nova/manager.py:193\n2011-12-21 10:21:08,971 DEBUG nova.rpc [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Making asynchronous fanout cast... from (pid=6760) fanout_cast /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n2011-12-21 10:21:09,289 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,289 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._sync_power_states from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,885 INFO nova.compute.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Found 1 in the database and 0 on the hypervisor.\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,886 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,887 DEBUG nova.compute.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760) _reclaim_queued_deletes /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n2011-12-21 10:21:09,887 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._report_driver_status from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:09,888 DEBUG nova.manager [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2011-12-21 10:21:41,718 DEBUG nova.utils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/cb7d6887f61381feaa684f7b203baa586f53c02c_sm /var/lib/nova/instances/instance-00000006/disk from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,869 INFO nova.virt.libvirt_conn [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance instance-00000006: injecting key into image 97bb58ab-a17e-4611-a8de-875064ebeef0\n2011-12-21 10:21:41,870 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-00000006/disk from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,907 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo kpartx -a /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,925 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo kpartx -d /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,940 DEBUG nova.utils [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess): sudo qemu-nbd -d /dev/nbd15 from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 10:21:41,969 WARNING nova.virt.libvirt_conn [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance instance-00000006: ignoring error injecting data into image 97bb58ab-a17e-4611-a8de-875064ebeef0 (Mapped device was not found (we can only inject raw disk images): /dev/mapper/nbd15p1)\n2011-12-21 10:21:43,391 DEBUG nova.virt.libvirt_conn [-] instance instance-00000006: is running from (pid=6760) spawn /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:670\n2011-12-21 10:21:43,392 DEBUG nova.compute.manager [-] Checking state of instance-00000006 from (pid=6760) _get_power_state /usr/lib/python2.7/dist-packages/nova/compute/manager.py:192\n2011-12-21 10:21:44,043 INFO nova.virt.libvirt_conn [-] Instance instance-00000006 spawned successfully.", 
            "date_created": "2011-12-21 10:49:25.398603+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Given the error message, could be a Precise issue. What was your last known-good config ?", 
            "date_created": "2011-12-21 11:11:45.453963+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "My last known good config was\n\nUbuntu 11.10\nDiablo 2011.3~3 (oneiric-proposed)\n\nI'll reinstall 11.10 and install E2 and report back.", 
            "date_created": "2011-12-21 11:26:48.034101+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Hmm - wasn't expecting this - fails on Ubuntu 11.10 with nova-core from milestone PPA.\n\nUbuntu 11.10 Fresh Install with latest updates.\n\napt-get install -y python-software-properties\nadd-apt-repository ppa:nova-core/milestone\napt-get install -y rabbitmq-server nova-api nova-objectstore nova-scheduler nova-network nova-compute glance qemu unzip python-support\n\nSame nova.conf as before\nCreated network configs (private and floating)\nUsed Oneiric cloud image\ncloud-publish-tarball ...\n\n\neuca-run-instances ami-00000002 -k openstack -t m1.tiny\n\nSays spawned successful - but instance hasn't been so successful.\neuca-get-console-output empty (apart from beginning identifier)\nSame warning (I'm presuming the warning is the crux - I've not noticed it before)\n\n011-12-21 12:41:47,059 DEBUG nova.utils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/da4b9237bacccdf19c0760cab7aec4a8359010b0_sm /var/lib/nova/instances/instance-00000001/disk from (pid=9308) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 12:41:47,553 INFO nova.virt.libvirt_conn [db1901eb-7181-489e-9051-0f303a1e5a5f None None] instance instance-00000001: injecting key into image 2\n2011-12-21 12:41:47,554 DEBUG nova.utils [db1901eb-7181-489e-9051-0f303a1e5a5f None None] Running cmd (subprocess): sudo qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-00000001/disk from (pid=9308) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 12:41:48,809 DEBUG nova.utils [db1901eb-7181-489e-9051-0f303a1e5a5f None None] Running cmd (subprocess): sudo kpartx -a /dev/nbd15 from (pid=9308) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 12:41:48,916 DEBUG nova.utils [db1901eb-7181-489e-9051-0f303a1e5a5f None None] Running cmd (subprocess): sudo kpartx -d /dev/nbd15 from (pid=9308) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 12:41:48,929 DEBUG nova.utils [db1901eb-7181-489e-9051-0f303a1e5a5f None None] Running cmd (subprocess): sudo qemu-nbd -d /dev/nbd15 from (pid=9308) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2011-12-21 12:41:48,954 WARNING nova.virt.libvirt_conn [db1901eb-7181-489e-9051-0f303a1e5a5f None None] instance instance-00000001: ignoring error injecting data into image 2 (Mapped device was not found (we can only inject raw disk images): /dev/mapper/nbd15p1)\n2011-12-21 12:41:51,260 DEBUG nova.virt.libvirt_conn [-] instance instance-00000001: is running from (pid=9308) spawn /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:670\n", 
            "date_created": "2011-12-21 12:55:29.730509+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "(there was an apt-get update after the add-apt...)", 
            "date_created": "2011-12-21 12:56:06.185514+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Noticed this in dmesg\n\n[ 2221.217072]  nbd15: unknown partition table\n[ 2222.262990] nbd15: NBD_DISCONNECT\n[ 2222.263089] nbd15: Receive control failed (result -32)\n[ 2222.267259] nbd15: queue cleared\n[ 2223.122312] type=1400 audit(1324471309.796:15): apparmor=\"DENIED\" operation=\"open\" parent=5629 profile=\"/usr/lib/libvirt/virt-aa-helper\" name=\"/var/lib/nova/instances/_base/da4b9237bacccdf19c0760cab7aec4a8359010b0_sm\" pid=10338 comm=\"virt-aa-helper\" requested_mask=\"r\" denied_mask=\"r\" fsuid=0 ouid=108\n[ 2223.536709] type=1400 audit(1324471310.212:16): apparmor=\"STATUS\" operation=\"profile_load\" name=\"libvirt-fcf9aaa0-2f5b-122a-63de-8766a9f57f79\" pid=10339 comm=\"apparmor_parser\"\n", 
            "date_created": "2011-12-21 13:07:25.713436+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Attaching to VNC says:\n\nBooting from Hard DIsk...\nBoot failed: not a bootable disk\n\nI'm trying an alternative home-grown image, as opposed to the Ubuntu ones to see if that's the issue (big assumption its not that) then I'll go back to a good known config to rule out hardware/my process as being the issue.", 
            "date_created": "2011-12-21 13:42:26.587409+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "You can also try registering the .img (from the cloud-images website) directly through Glance and see if it yields better results.", 
            "date_created": "2011-12-21 14:11:53.993857+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I've tried going back to good known config:\n\nUbuntu 11.10\nDiablo (with packages from proposed)\n\nAnd all is well.  Will re-install 11.10 with E2 and try again.", 
            "date_created": "2011-12-22 14:03:09.423413+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Same issue running Ubuntu 11.10 and nova and glance from milestone ppa (2012.1~e2-0ubuntu0~ppa1~oneiric1).\n\n/var/log/nova/nova-compute.log\n\n2012-01-04 09:39:52,453 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Notifying Schedulers of capabilities ... from (pid=27900) _publish_service_capabilities /usr/lib/python2.7/dist-packages/nova/manager.py:193\n2012-01-04 09:39:52,454 DEBUG nova.rpc [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Making asynchronous fanout cast... from (pid=27900) fanout_cast /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n2012-01-04 09:39:52,459 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._poll_rescued_instances from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:52,460 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._sync_power_states from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:53,284 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:53,284 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:53,285 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._reclaim_queued_deletes from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:53,285 DEBUG nova.compute.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=27900) _reclaim_queued_deletes /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n2012-01-04 09:39:53,285 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._report_driver_status from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:39:53,285 DEBUG nova.manager [8ab0196c-affd-4d98-bfb9-ee77b0556d0b None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=27900) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:151\n2012-01-04 09:40:13,120 DEBUG nova.rpc [-] received {u'_context_roles': [], u'_context_request_id': u'024c2b72-0920-44b5-bdf8-e08f673fad13', u'_context_read_deleted': u'no', u'args': {u'instance_uuid': u'cb3b5a38-dba5-4c8a-8bca-3e94b2b2bcaa', u'requested_networks': None, u'admin_password': None, u'injected_files': None}, u'_context_auth_token': None, u'_context_strategy': u'noauth', u'_context_is_admin': True, u'_context_project_id': u'novaproject', u'_context_timestamp': u'2012-01-04T08:40:12.510013', u'_context_user_id': u'9c2118fe-511e-4b15-bc9f-a0f51f72c38f', u'method': u'run_instance', u'_context_remote_address': u'127.0.0.1'} from (pid=27900) __call__ /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:600\n2012-01-04 09:40:13,122 DEBUG nova.rpc [-] unpacked context: {'user_id': u'9c2118fe-511e-4b15-bc9f-a0f51f72c38f', 'roles': [], 'timestamp': u'2012-01-04T08:40:12.510013', 'auth_token': None, 'msg_id': None, 'remote_address': u'127.0.0.1', 'strategy': u'noauth', 'is_admin': True, 'request_id': u'024c2b72-0920-44b5-bdf8-e08f673fad13', 'project_id': u'novaproject', 'read_deleted': u'no'} from (pid=27900) _unpack_context /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:646\n2012-01-04 09:40:14,004 AUDIT nova.compute.manager [024c2b72-0920-44b5-bdf8-e08f673fad13 9c2118fe-511e-4b15-bc9f-a0f51f72c38f novaproject] instance cb3b5a38-dba5-4c8a-8bca-3e94b2b2bcaa: starting...\n2012-01-04 09:40:14,184 DEBUG nova.rpc [-] Making asynchronous call on network ... from (pid=27900) multicall /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:730\n2012-01-04 09:40:14,185 DEBUG nova.rpc [-] MSG_ID is 8e7c5c47332f41b9bbdcbd76e71f8368 from (pid=27900) multicall /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:733\n2012-01-04 09:40:15,983 DEBUG nova.compute.manager [-] instance network_info: |[[{u'bridge': u'br100', u'multi_host': False, u'bridge_interface': u'virbr0', u'vlan': 100, u'id': 1, u'injected': False, u'cidr': u'10.10.9.0/24', u'cidr_v6': None}, {u'should_create_bridge': True, u'dns': [], u'vif_uuid': u'daee6a69-7beb-4cbf-b5ae-b7bc906f36d7', u'label': u'novanet', u'broadcast': u'10.10.9.255', u'ips': [{u'ip': u'10.10.9.3', u'netmask': u'255.255.255.0', u'enabled': u'1'}], u'mac': u'02:16:3e:76:3a:d7', u'rxtx_cap': 0, u'should_create_vlan': True, u'dhcp_server': u'10.10.9.1', u'gateway': u'10.10.9.1'}]]| from (pid=27900) _allocate_network /usr/lib/python2.7/dist-packages/nova/compute/manager.py:450\n2012-01-04 09:40:16,235 DEBUG nova.virt.libvirt_conn [-] instance instance-00000001: starting toXML method from (pid=27900) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1183\n2012-01-04 09:40:16,237 DEBUG nova.virt.libvirt.vif [-] Ensuring vlan 100 and bridge br100 from (pid=27900) plug /usr/lib/python2.7/dist-packages/nova/virt/libvirt/vif.py:82\n2012-01-04 09:40:16,238 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:16,238 DEBUG nova.utils [-] Got semaphore \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:16,239 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2012-01-04 09:40:16,241 DEBUG nova.utils [-] Got file lock \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2012-01-04 09:40:16,242 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev vlan100 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:16,256 DEBUG nova.utils [-] Attempting to grab semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:16,257 DEBUG nova.utils [-] Got semaphore \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:16,258 DEBUG nova.utils [-] Attempting to grab file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2012-01-04 09:40:16,259 DEBUG nova.utils [-] Got file lock \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2012-01-04 09:40:16,260 DEBUG nova.utils [-] Running cmd (subprocess): ip link show dev br100 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:16,274 DEBUG nova.utils [-] Running cmd (subprocess): sudo brctl addif br100 vlan100 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:16,296 DEBUG nova.utils [-] Result was 1 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:214\n2012-01-04 09:40:16,298 DEBUG nova.utils [-] Running cmd (subprocess): sudo route -n from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:16,320 DEBUG nova.utils [-] Running cmd (subprocess): sudo ip addr show dev vlan100 scope global from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:16,363 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=27900) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2012-01-04 09:40:16,363 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=27900) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2012-01-04 09:40:16,512 DEBUG nova.virt.libvirt_conn [-] instance instance-00000001: finished toXML method from (pid=27900) to_xml /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1187\n2012-01-04 09:40:16,512 INFO nova [-] called setup_basic_filtering in nwfilter\n2012-01-04 09:40:16,512 INFO nova [-] ensuring static filters\n2012-01-04 09:40:22,868 DEBUG nova.virt.libvirt.firewall [-] iptables firewall: Setup Basic Filtering from (pid=27900) setup_basic_filtering /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:524\n2012-01-04 09:40:22,869 DEBUG nova.utils [-] Attempting to grab semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:22,870 DEBUG nova.utils [-] Got semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:22,870 DEBUG nova.utils [-] Attempting to grab file lock \"iptables\" for method \"_do_refresh_provider_fw_rules\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2012-01-04 09:40:22,871 DEBUG nova.utils [-] Got file lock \"iptables\" for method \"_do_refresh_provider_fw_rules\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2012-01-04 09:40:22,877 DEBUG nova.utils [-] Attempting to grab semaphore \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:22,878 DEBUG nova.utils [-] Got semaphore \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:22,878 DEBUG nova.utils [-] Attempting to grab file lock \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2012-01-04 09:40:22,878 DEBUG nova.utils [-] Got file lock \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2012-01-04 09:40:22,878 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t filter from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:22,900 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:22,925 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t nat from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:22,950 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,051 DEBUG nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x3e04050> from (pid=27900) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2012-01-04 09:40:23,052 INFO nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Using cidr '0.0.0.0/0'\n2012-01-04 09:40:23,052 INFO nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 10.10.9.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 10.10.9.0/24 -j ACCEPT', '-j ACCEPT -p icmp -s 0.0.0.0/0']\n2012-01-04 09:40:23,052 DEBUG nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Adding security group rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0x3e04450> from (pid=27900) instance_rules /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n2012-01-04 09:40:23,053 INFO nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Using cidr '0.0.0.0/0'\n2012-01-04 09:40:23,053 INFO nova.virt.libvirt.firewall [450057d2-d542-4e24-bd84-eacef9a57d16 None None] Using fw_rules: ['-m state --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT', '-j $provider', u'-s 10.10.9.1 -p udp --sport 67 --dport 68 -j ACCEPT', u'-s 10.10.9.0/24 -j ACCEPT', '-j ACCEPT -p icmp -s 0.0.0.0/0', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0']\n2012-01-04 09:40:23,053 DEBUG nova.utils [-] Attempting to grab semaphore \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:23,053 DEBUG nova.utils [-] Got semaphore \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:23,054 DEBUG nova.utils [-] Attempting to grab file lock \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:758\n2012-01-04 09:40:23,054 DEBUG nova.utils [-] Got file lock \"iptables\" for method \"apply\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:769\n2012-01-04 09:40:23,054 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t filter from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,069 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,095 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-save -t nat from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,121 DEBUG nova.utils [-] Running cmd (subprocess): sudo iptables-restore from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,148 DEBUG nova.utils [-] Running cmd (subprocess): mkdir -p /var/lib/nova/instances/instance-00000001/ from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,163 INFO nova.virt.libvirt_conn [-] instance instance-00000001: Creating image\n2012-01-04 09:40:23,180 DEBUG nova.virt.libvirt_conn [-] block_device_list [] from (pid=27900) _volume_in_mapping /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n2012-01-04 09:40:23,181 DEBUG nova.utils [-] Running cmd (subprocess): mkdir -p /var/lib/nova/instances/_base from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:23,192 DEBUG nova.utils [-] Attempting to grab semaphore \"b141426735f0ed079deeb29d785cd743e3a0f11e_sm\" for method \"call_if_not_exists\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:750\n2012-01-04 09:40:23,193 DEBUG nova.utils [-] Got semaphore \"b141426735f0ed079deeb29d785cd743e3a0f11e_sm\" for method \"call_if_not_exists\"... from (pid=27900) inner /usr/lib/python2.7/dist-packages/nova/utils.py:754\n2012-01-04 09:40:25,501 DEBUG nova.utils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/b141426735f0ed079deeb29d785cd743e3a0f11e_sm /var/lib/nova/instances/instance-00000001/disk from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:39,093 INFO nova.virt.libvirt_conn [dddd2825-1677-4145-b8af-0870ce1acf53 None None] instance instance-00000001: injecting key into image d17a3449-edd9-4cdf-85ca-d54c02deda4f\n2012-01-04 09:40:39,094 DEBUG nova.utils [dddd2825-1677-4145-b8af-0870ce1acf53 None None] Running cmd (subprocess): sudo qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-00000001/disk from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:40,114 DEBUG nova.utils [dddd2825-1677-4145-b8af-0870ce1acf53 None None] Running cmd (subprocess): sudo kpartx -a /dev/nbd15 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:40,130 DEBUG nova.utils [dddd2825-1677-4145-b8af-0870ce1acf53 None None] Running cmd (subprocess): sudo kpartx -d /dev/nbd15 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:40,144 DEBUG nova.utils [dddd2825-1677-4145-b8af-0870ce1acf53 None None] Running cmd (subprocess): sudo qemu-nbd -d /dev/nbd15 from (pid=27900) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n2012-01-04 09:40:40,162 WARNING nova.virt.libvirt_conn [dddd2825-1677-4145-b8af-0870ce1acf53 None None] instance instance-00000001: ignoring error injecting data into image d17a3449-edd9-4cdf-85ca-d54c02deda4f (Mapped device was not found (we can only inject raw disk images): /dev/mapper/nbd15p1)\n2012-01-04 09:40:42,933 DEBUG nova.virt.libvirt_conn [-] instance instance-00000001: is running from (pid=27900) spawn /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:670\n2012-01-04 09:40:42,935 DEBUG nova.compute.manager [-] Checking state of instance-00000001 from (pid=27900) _get_power_state /usr/lib/python2.7/dist-packages/nova/compute/manager.py:192\n2012-01-04 09:40:44,803 INFO nova.virt.libvirt_conn [-] Instance instance-00000001 spawned successfully.\n\n\n/etc/nova/nova.conf\n\n--dhcpbridge_flagfile=/etc/nova/nova.conf\n--dhcpbridge=/usr/bin/nova-dhcpbridge\n--logdir=/var/log/nova\n--state_path=/var/lib/nova\n--lock_path=/var/lock/nova\n--use_deprecated_auth\n--verbose\n--sql_connection=mysql://nova:0utFinger@192.168.1.10:3306/nova\n--rabbit_host=192.168.1.10\n--fixed-range=10.10.9.0/24\n--network_size=256\n--routing_source_ip=192.168.1.10\n\n# configured using KVM, Flat, MySQL, and Glance, API is OpenStack (or EC2)\n--daemonize=1\n--osapi_host=192.168.1.10\n--ec2_host=192.168.1.10\n--image_service=nova.image.glance.GlanceImageService\n--glance_api_servers=192.168.1.10:9292\n# first 3 octets of the network your volume service is on, substitute with real numbers\n--iscsi_ip_prefix=192.168.1\n", 
            "date_created": "2012-01-04 08:47:55.047015+00:00", 
            "author": "https://api.launchpad.net/1.0/~mcirauqui"
        }, 
        {
            "content": "Note that message (which will be different in Essex-2), is not the real issue I think.\nIt's throwing that because it's looking for the first partition, and the image is probably partitionless.\nOne can see in virt/libvirt/connection.py that a partition=\"1\" is hardcoded for all kernel images.\nSo I'm guessing that the kernel image was not registered correctly", 
            "date_created": "2012-01-12 17:27:11.638160+00:00", 
            "author": "https://api.launchpad.net/1.0/~p-draigbrady"
        }, 
        {
            "content": "In comment #10 s/all kernel images/all images with no associated kernel image/\nI should also not that failure to inject seh keys etc. should not stop the instance from starting", 
            "date_created": "2012-01-12 17:29:52.378841+00:00", 
            "author": "https://api.launchpad.net/1.0/~p-draigbrady"
        }, 
        {
            "content": "I have the same problem here with the last version of nova of the openstack-trunk-testing PPA. I also have some nova-rootwrap problem but that's an other issue.\n\nI have added a raw partition in glance and then started a new lxc container with the raw image and the command:\n\n  sudo nova-rootwrap mount /dev/nbd15 /tmp/tmp0fmlKu\n\nis failing because the partition is  /dev/nbd15p1 So mayby device + 'p1' should be hard coded.\n\nthe logs:\n\n2012-02-09 16:31:29,375 DEBUG nova.utils [-] Running cmd (subprocess): qemu-img create -f qcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/ac44f323e1cbe558388b540e3d24f6490350099f_sm /var/lib/nova/instances/instance-0000000b/disk from (pid=10441) execute /usr/lib/python2.7/dist-packages/nova/utils.py:208\n2012-02-09 16:31:29,532 INFO nova.virt.libvirt_conn [-] [instance: d29a0728-9dc2-43d8-b22a-27e085d0d4bd] Injecting key into image a791bb55-7bc9-4e0d-a8d6-41bdf03c5201\n2012-02-09 16:31:29,533 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-0000000b/disk from (pid=10441) execute /usr/lib/python2.7/dist-packages/nova/utils.py:208\n2012-02-09 16:31:29,616 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap mount /dev/nbd15 /tmp/tmp0fmlKu from (pid=10441) execute /usr/lib/python2.7/dist-packages/nova/utils.py:208\n2012-02-09 16:31:29,787 DEBUG nova.utils [-] Result was 32 from (pid=10441) execute /usr/lib/python2.7/dist-packages/nova/utils.py:224\n2012-02-09 16:31:29,788 DEBUG nova.utils [-] Unexpected error while running command.\nCommand: sudo nova-rootwrap mount /dev/nbd15 /tmp/tmp0fmlKu\nExit code: 32\nStdout: ''\nStderr: 'mount\\xc2\\xa0: vous devez indiquer le type de syst\\xc3\\xa8me de fichiers\\n' from (pid=10441) trycmd /usr/lib/python2.7/dist-packages/nova/utils.py:267\n\n", 
            "date_created": "2012-02-09 22:31:48.985629+00:00", 
            "author": "https://api.launchpad.net/1.0/~patrick-hetu"
        }, 
        {
            "content": "I've test hard coding the partition number to 1 in the Mount class (nova/virt/disk/mount.py) and the instance\nboot without errors. I think it would be safe to use the first partition with a image of the type raw + use_qcow=true in\nnova config + lxc as a backend combination.", 
            "date_created": "2012-03-15 17:03:07.482675+00:00", 
            "author": "https://api.launchpad.net/1.0/~patrick-hetu"
        }, 
        {
            "content": "I'm a bit lost. Anyone could confirm this still exists in final Essex / current trunk ?", 
            "date_created": "2012-06-07 13:53:06.923895+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I've not seen this in a long while - certainly not seen this since the days\nof me raising this bug.\nI was going to close it until someone else added they had this issue and\nthen I forgot to keep an eye on it.\n\nYou get a +1 from me to close this.\n\nCheers,\n\nKev\n\nOn 7 June 2012 14:53, Thierry Carrez <email address hidden> wrote:\n\n> I'm a bit lost. Anyone could confirm this still exists in final Essex /\n> current trunk ?\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/907269\n>\n> Title:\n>  Mapped device was not found (we can only inject raw disk images):\n>  /dev/mapper/nbd15p1) : Precise A1 and E2\n>\n> Status in OpenStack Compute (Nova):\n>  Incomplete\n>\n> Bug description:\n>  Summary: can't launch instances since fresh install of Precise and\n>  Essex2 - Show stopper.\n>\n>  Precise Alpha-1 AMD64\n>  Essex-2 (Ubuntu repo)\n>\n>  Fresh install (this is a compute node, but instances fail to spawn on\n>  controller where nova-compute also runs)\n>\n>  sudo apt-get install qemu unzip nova-compute python-support\n>\n>  /etc/nova/nova.conf\n>  --daemonize=1\n>  --verbose\n>  --dhcpbridge_flagfile=/etc/nova/nova.conf\n>  --dhcpbridge=/usr/bin/nova-dhcpbridge\n>  --force_dhcp_release\n>  --logdir=/var/log/nova\n>  --state_path=/var/lib/nova\n>  --libvirt_type=kvm\n>  --libvirt_use_virtio_for_bridges\n>  --sql_connection=mysql://root:nova@172.15.0.1/nova\n>  --s3_host=172.15.0.1\n>  --rabbit_host=172.15.0.1\n>  --ec2_host=172.15.0.1\n>  --ec2_url=http://172.15.0.1:8773/services/Cloud\n>  --fixed_range=192.168.0.0/16\n>  --network_size=256\n>  --num_networks=1\n>  --FAKE_subdomain=ec2\n>  --public_interface=eth1\n>  --state_path=/var/lib/nova\n>  --lock_path=/var/lock/nova\n>  --image_service=nova.image.glance.GlanceImageService\n>  --glance_api_servers=172.15.0.1:9292\n>  --iscsi_helper=tgtadm\n>  --vlan_start=4025\n>  --vlan_interface=eth0\n>  --auto_assign_floating_ip\n>\n>  Problem: fail to launch Ubuntu image.  Nova says successful, but\n>  Warning error suggests problem with disk image.  Tried Oneiric cloud\n>  image and Precise cloud image. I've tried m1.tiny and m1.small to see\n>  if there's a difference and still the same outcome.  These same steps\n>  and config used to work with Oneiric and Diablo.\n>\n>  wget\n> http://uec-images.ubuntu.com/releases/oneiric/release/ubuntu-11.10-server-cloudimg-i386.tar.gz\n>  cloud-publish-tarball ubuntu-11.10-server-cloudimg-i386.tar.gz images i386\n>  euca-run-instances ami-00000002 -k openstack -t m1.tiny\n>\n>  euca-describe-instances\n>  INSTANCE      i-00000006      ami-00000002    172.15.1.5      server-6\n>      running openstack (devops, openstack2)  1               m1.tiny\n> 2011-12-21T10:19:35Z    nova\n>\n>  euca-get-console-output i-0000006\n>  i-00000006\n>  2011-12-21T10:35:42Z\n>\n>\n>  /var/log/nova/nova-compute.log\n>\n>  2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt_conn [-] instance\n> instance-00000006: starting toXML method from (pid=6760) to_xml\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1183\n>  2011-12-21 10:19:33,277 DEBUG nova.virt.libvirt.vif [-] Ensuring vlan\n> 4025 and bridge br4025 from (pid=6760) plug\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/vif.py:82\n>  2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab semaphore\n> \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:750\n>  2011-12-21 10:19:33,278 DEBUG nova.utils [-] Got semaphore \"ensure_vlan\"\n> for method \"ensure_vlan\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:754\n>  2011-12-21 10:19:33,278 DEBUG nova.utils [-] Attempting to grab file lock\n> \"ensure_vlan\" for method \"ensure_vlan\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:758\n>  2011-12-21 10:19:33,279 DEBUG nova.utils [-] Got file lock \"ensure_vlan\"\n> for method \"ensure_vlan\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:769\n>  2011-12-21 10:19:33,279 DEBUG nova.utils [-] Running cmd (subprocess): ip\n> link show dev vlan4025 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,300 DEBUG nova.utils [-] Attempting to grab semaphore\n> \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:750\n>  2011-12-21 10:19:33,301 DEBUG nova.utils [-] Got semaphore\n> \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:754\n>  2011-12-21 10:19:33,301 DEBUG nova.utils [-] Attempting to grab file lock\n> \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:758\n>  2011-12-21 10:19:33,302 DEBUG nova.utils [-] Got file lock\n> \"ensure_bridge\" for method \"ensure_bridge\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:769\n>  2011-12-21 10:19:33,302 DEBUG nova.utils [-] Running cmd (subprocess): ip\n> link show dev br4025 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,322 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo brctl addif br4025 vlan4025 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,335 DEBUG nova.utils [-] Result was 1 from (pid=6760)\n> execute /usr/lib/python2.7/dist-packages/nova/utils.py:214\n>  2011-12-21 10:19:33,335 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo route -n from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,349 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo ip addr show dev vlan4025 scope global from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-]\n> block_device_list [] from (pid=6760) _volume_in_mapping\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n>  2011-12-21 10:19:33,374 DEBUG nova.virt.libvirt_conn [-]\n> block_device_list [] from (pid=6760) _volume_in_mapping\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n>  2011-12-21 10:19:33,486 DEBUG nova.virt.libvirt_conn [-] instance\n> instance-00000006: finished toXML method from (pid=6760) to_xml\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1187\n>  2011-12-21 10:19:33,486 INFO nova [-] called setup_basic_filtering in\n> nwfilter\n>  2011-12-21 10:19:33,487 INFO nova [-] ensuring static filters\n>  2011-12-21 10:19:33,835 DEBUG nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group\n> rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at\n> 0x53fd650> from (pid=6760) instance_rules\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n>  2011-12-21 10:19:33,835 INFO nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n>  2011-12-21 10:19:33,836 INFO nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state\n> --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT',\n> '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT',\n> u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0\n> ']\n>  2011-12-21 10:19:33,836 DEBUG nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Adding security group\n> rule: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at\n> 0x53fd110> from (pid=6760) instance_rules\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/firewall.py:650\n>  2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Using cidr '0.0.0.0/0'\n>  2011-12-21 10:19:33,837 INFO nova.virt.libvirt.firewall\n> [ba125265-2f49-444e-a522-f20ddb079527 None None] Using fw_rules: ['-m state\n> --state INVALID -j DROP', '-m state --state ESTABLISHED,RELATED -j ACCEPT',\n> '-j $provider', u'-s 192.168.15.1 -p udp --sport 67 --dport 68 -j ACCEPT',\n> u'-s 192.168.15.0/26 -j ACCEPT', '-j ACCEPT -p tcp --dport 22 -s 0.0.0.0/0',\n> '-j ACCEPT -p icmp -s 0.0.0.0/0']\n>  2011-12-21 10:19:33,837 DEBUG nova.utils [-] Attempting to grab semaphore\n> \"iptables\" for method \"apply\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:750\n>  2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got semaphore \"iptables\" for\n> method \"apply\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:754\n>  2011-12-21 10:19:33,838 DEBUG nova.utils [-] Attempting to grab file lock\n> \"iptables\" for method \"apply\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:758\n>  2011-12-21 10:19:33,838 DEBUG nova.utils [-] Got file lock \"iptables\" for\n> method \"apply\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:769\n>  2011-12-21 10:19:33,839 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo iptables-save -t filter from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,855 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo iptables-restore from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,870 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo iptables-save -t nat from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,886 DEBUG nova.utils [-] Running cmd (subprocess):\n> sudo iptables-restore from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,901 DEBUG nova.utils [-] Running cmd (subprocess):\n> mkdir -p /var/lib/nova/instances/instance-00000006/ from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:19:33,921 INFO nova.virt.libvirt_conn [-] instance\n> instance-00000006: Creating image\n>  2011-12-21 10:19:33,934 DEBUG nova.virt.libvirt_conn [-]\n> block_device_list [] from (pid=6760) _volume_in_mapping\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1069\n>  2011-12-21 10:19:33,935 DEBUG nova.utils [-] Attempting to grab semaphore\n> \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method\n> \"call_if_not_exists\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:750\n>  2011-12-21 10:19:33,935 DEBUG nova.utils [-] Got semaphore\n> \"cb7d6887f61381feaa684f7b203baa586f53c02c_sm\" for method\n> \"call_if_not_exists\"... from (pid=6760) inner\n> /usr/lib/python2.7/dist-packages/nova/utils.py:754\n>  2011-12-21 10:20:07,236 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,237 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Notifying Schedulers of\n> capabilities ... from (pid=6760) _publish_service_capabilities\n> /usr/lib/python2.7/dist-packages/nova/manager.py:193\n>  2011-12-21 10:20:07,238 DEBUG nova.rpc\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Making asynchronous fanout\n> cast... from (pid=6760) fanout_cast\n> /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n>  2011-12-21 10:20:07,493 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,494 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._sync_power_states from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,854 INFO nova.compute.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Found 1 in the database\n> and 0 on the hypervisor.\n>  2011-12-21 10:20:07,854 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,855 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,855 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,855 DEBUG nova.compute.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None]\n> FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760)\n> _reclaim_queued_deletes\n> /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n>  2011-12-21 10:20:07,856 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._report_driver_status from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:20:07,856 INFO nova.compute.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host status\n>  2011-12-21 10:20:07,856 DEBUG nova.virt.libvirt_conn\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Updating host stats from\n> (pid=6760) update_status\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:1918\n>  2011-12-21 10:20:08,969 DEBUG nova.manager\n> [a66206a2-7745-423a-9cab-c14ade429be4 None None] Running periodic task\n> ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:08,970 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._publish_service_capabilities from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:08,970 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Notifying Schedulers of\n> capabilities ... from (pid=6760) _publish_service_capabilities\n> /usr/lib/python2.7/dist-packages/nova/manager.py:193\n>  2011-12-21 10:21:08,971 DEBUG nova.rpc\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Making asynchronous fanout\n> cast... from (pid=6760) fanout_cast\n> /usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py:763\n>  2011-12-21 10:21:09,289 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._poll_rescued_instances from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,289 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._sync_power_states from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,885 INFO nova.compute.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Found 1 in the database\n> and 0 on the hypervisor.\n>  2011-12-21 10:21:09,886 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._poll_bandwidth_usage from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,886 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._poll_rebooting_instances from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,886 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._reclaim_queued_deletes from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,887 DEBUG nova.compute.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None]\n> FLAGS.reclaim_instance_interval <= 0, skipping... from (pid=6760)\n> _reclaim_queued_deletes\n> /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1947\n>  2011-12-21 10:21:09,887 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._report_driver_status from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:09,888 DEBUG nova.manager\n> [b3348ba3-5a01-4a0f-888c-55ec298b64b9 None None] Running periodic task\n> ComputeManager._poll_unconfirmed_resizes from (pid=6760) periodic_tasks\n> /usr/lib/python2.7/dist-packages/nova/manager.py:151\n>  2011-12-21 10:21:41,718 DEBUG nova.utils [-] Running cmd (subprocess):\n> qemu-img create -f qcow2 -o\n> cluster_size=2M,backing_file=/var/lib/nova/instances/_base/cb7d6887f61381feaa684f7b203baa586f53c02c_sm\n> /var/lib/nova/instances/instance-00000006/disk from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:21:41,869 INFO nova.virt.libvirt_conn\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance\n> instance-00000006: injecting key into image\n> 97bb58ab-a17e-4611-a8de-875064ebeef0\n>  2011-12-21 10:21:41,870 DEBUG nova.utils\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess):\n> sudo qemu-nbd -c /dev/nbd15 /var/lib/nova/instances/instance-00000006/disk\n> from (pid=6760) execute /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:21:41,907 DEBUG nova.utils\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess):\n> sudo kpartx -a /dev/nbd15 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:21:41,925 DEBUG nova.utils\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess):\n> sudo kpartx -d /dev/nbd15 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:21:41,940 DEBUG nova.utils\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] Running cmd (subprocess):\n> sudo qemu-nbd -d /dev/nbd15 from (pid=6760) execute\n> /usr/lib/python2.7/dist-packages/nova/utils.py:198\n>  2011-12-21 10:21:41,969 WARNING nova.virt.libvirt_conn\n> [78db929e-72d6-47b6-a663-c435d1b5316e None None] instance\n> instance-00000006: ignoring error injecting data into image\n> 97bb58ab-a17e-4611-a8de-875064ebeef0 (Mapped device was not found (we can\n> only inject raw disk images): /dev/mapper/nbd15p1)\n>  2011-12-21 10:21:43,391 DEBUG nova.virt.libvirt_conn [-] instance\n> instance-00000006: is running from (pid=6760) spawn\n> /usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py:670\n>  2011-12-21 10:21:43,392 DEBUG nova.compute.manager [-] Checking state of\n> instance-00000006 from (pid=6760) _get_power_state\n> /usr/lib/python2.7/dist-packages/nova/compute/manager.py:192\n>  2011-12-21 10:21:44,043 INFO nova.virt.libvirt_conn [-] Instance\n> instance-00000006 spawned successfully.\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/907269/+subscriptions\n>\n\n\n\n-- \nKevin Jackson\n@itarchitectkev\n", 
            "date_created": "2012-06-07 14:14:26+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-linuxservices"
        }, 
        {
            "content": "Closing as suggested, reopen if you reproduce.", 
            "date_created": "2012-07-12 13:11:07.851744+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }
    ]
}
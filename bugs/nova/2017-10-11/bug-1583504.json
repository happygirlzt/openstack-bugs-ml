{
    "status": "Confirmed", 
    "last_updated": "2016-11-15 21:14:07.572806+00:00", 
    "description": "Description\n===========\nNormally, if you finished evacuating instance, nova-compute will destroy the existed instances whose state are \"done\" or \"accept\" in the migration table from the fault host after the nova-compute service is restarted. However, if the nova-scheduler failed to schedule the hosts, e.g. no valid host, nova-scheduler won't update the migration state of the instance to \"failed\", so that the nova-compute will destroy this instance after restarting by mistake.\n\nSteps to reproduce\n==================\n1. Create some instances in the the specific host.\n2. Make this host to fault state.\n3. Disable nova-compute service in other hosts, aim to mock that nova-scheduler fail to schedule.\n4. Recover the fault host and restart nova-compute.\n5. Check all instances are still existed in the kvm.\n\nExpected result\n===============\nAll instances should be still existed in the kvm.\n\nActual result\n=============\nAll instances are destroyed by nova-compute unfortunately.", 
    "tags": [
        "compute"
    ], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1583504", 
    "owner": "None", 
    "id": 1583504, 
    "index": 6226, 
    "created": "2016-05-19 08:58:26.862803+00:00", 
    "title": "The instances which didn't be evacuated will be destroyed when the nova-compute service is restarted.", 
    "comments": [
        {
            "content": "Description\n===========\nNormally, if you finished evacuating instance, nova-compute will destroy the existed instances whose state are \"done\" or \"accept\" in the migration table from the fault host after the nova-compute service is restarted. However, if the nova-scheduler failed to schedule the hosts, e.g. no valid host, nova-scheduler won't update the migration state of the instance to \"failed\", so that the nova-compute will destroy this instance after restarting by mistake.\n\nSteps to reproduce\n==================\n1. Create some instances in the the specific host.\n2. Make this host to fault state.\n3. Disable nova-compute service in other hosts, aim to mock that nova-scheduler fail to schedule.\n4. Recover the fault host and restart nova-compute.\n5. Check all instances are still existed in the kvm.\n\nExpected result\n===============\nAll instances should be still existed in the kvm.\n\nActual result\n=============\nAll instances are destroyed by nova-compute unfortunately.", 
            "date_created": "2016-05-19 08:58:26.862803+00:00", 
            "author": "https://api.launchpad.net/1.0/~chinabjalex"
        }, 
        {
            "content": "Can you please attach logs to this bug?", 
            "date_created": "2016-05-23 18:46:33.530508+00:00", 
            "author": "https://api.launchpad.net/1.0/~sarafraj-singh"
        }, 
        {
            "content": "nova-api.log\n\n/var/log/nova/nova-api.log:43606:2016-05-30 14:15:24.424 3987 DEBUG nova.api.openstack.wsgi [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Action: 'action', calling method: <bound method EvacuateController._evacuate of <nova.api.openstack.compute.evacuate.EvacuateController object at 0x639b310>>, body: {\"evacuate\": {\"onSharedStorage\": true}} _process_stack /usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py:789\n/var/log/nova/nova-api.log:43607:2016-05-30 14:15:24.425 3987 DEBUG nova.compute.api [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] Fetching instance by UUID get /usr/lib/python2.7/site-packages/nova/compute/api.py:2077\n/var/log/nova/nova-api.log:43608:2016-05-30 14:15:24.462 3987 DEBUG nova.compute.api [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] vm evacuation scheduled evacuate /usr/lib/python2.7/site-packages/nova/compute/api.py:3433\n/var/log/nova/nova-api.log:43609:2016-05-30 14:15:24.465 3987 DEBUG nova.servicegroup.drivers.db [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Seems service is down. Last heartbeat was 2016-05-30 06:14:08. Elapsed time is 76.465787 is_up /usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py:80\n/var/log/nova/nova-api.log:43610:2016-05-30 14:15:24.515 3987 DEBUG nova.objects.instance [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Lazy-loading `flavor' on Instance uuid eb46eb30-08ae-4faa-a62e-b5f100bcddf5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:884\n/var/log/nova/nova-api.log:43611:2016-05-30 14:15:24.570 3987 DEBUG nova.objects.instance [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Lazy-loading `flavor' on Instance uuid eb46eb30-08ae-4faa-a62e-b5f100bcddf5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:884\n/var/log/nova/nova-api.log:43612:2016-05-30 14:15:24.617 3987 INFO nova.osapi_compute.wsgi.server [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] 172.16.197.31 \"POST /v2/2c2643bad4d6489a99be97fd69c4fdcc/servers/eb46eb30-08ae-4faa-a62e-b5f100bcddf5/action HTTP/1.1\" status: 200 len: 191 time: 0.1989682\n\n\n\nnova-scheduler.log\n\n/var/log/nova/nova-scheduler.log:9490:2016-05-30 14:15:24.642 1425 WARNING nova.scheduler.host_manager [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] No compute service record found for host compute2\n/var/log/nova/nova-scheduler.log:9491:2016-05-30 14:15:24.642 1425 INFO nova.scheduler.host_manager [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Host filter ignoring hosts: compute1\n/var/log/nova/nova-scheduler.log:9492:2016-05-30 14:15:24.642 1425 DEBUG nova.scheduler.filter_scheduler [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] There are 0 hosts available but 1 instances requested to build. select_destinations /usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:87\n\n\n\nnova-conductor.log\n\n/var/log/nova/nova-conductor.log:2621:2016-05-30 14:15:24.650 3943 WARNING nova.scheduler.utils [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Failed to compute_task_rebuild_server: No valid host was found. There are not enough hosts available.\n/var/log/nova/nova-conductor.log:2635:2016-05-30 14:15:24.651 3943 WARNING nova.scheduler.utils [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] Setting instance to ACTIVE state.\n/var/log/nova/nova-conductor.log:2636:2016-05-30 14:15:24.652 3943 WARNING nova.objects.instance [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] Trying to sync the availability_zone seem to fail\n/var/log/nova/nova-conductor.log:2637:2016-05-30 14:15:24.708 3943 DEBUG nova.objects.instance [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Lazy-loading `metadata' on Instance uuid eb46eb30-08ae-4faa-a62e-b5f100bcddf5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:884\n/var/log/nova/nova-conductor.log:2638:2016-05-30 14:15:24.735 3943 DEBUG nova.objects.instance [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Lazy-loading `info_cache' on Instance uuid eb46eb30-08ae-4faa-a62e-b5f100bcddf5 obj_load_attr /usr/lib/python2.7/site-packages/nova/objects/instance.py:884\n/var/log/nova/nova-conductor.log:2639:2016-05-30 14:15:24.775 3943 WARNING nova.conductor.manager [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] No valid host found for rebuild\n/var/log/nova/nova-conductor.log:2640:2016-05-30 14:15:24.782 3943 ERROR oslo_messaging.rpc.dispatcher [req-72eab156-cfd1-483b-beab-2a164361071c 9646cdbcb7644ee79d80b0b54a6fcebe 2c2643bad4d6489a99be97fd69c4fdcc - - -] Exception during message handling: No valid host was found. There are not enough hosts available.\n\n\n\nnova-compute.log(source node. after restart nova compute service)\n\n2016-05-30 14:17:52.303 31422 INFO nova.virt.libvirt.driver [-] [instance: eb46eb30-08ae-4faa-a62e-b5f100bcddf5] Instance destroyed successfully.", 
            "date_created": "2016-05-30 08:14:21.038659+00:00", 
            "author": "https://api.launchpad.net/1.0/~chinabjalex"
        }, 
        {
            "content": "Below is the msg in migrations table from nova database.\n| created_at          | updated_at          | deleted_at | id  | source_compute | dest_compute | dest_host     | status    | instance_uuid                        | old_instance_type_id | new_instance_type_id | source_node | dest_node | deleted | migration_type | hidden |\n\nafter execute evacuate action.\n| 2016-05-30 06:15:24 | NULL                | NULL       | 127 | compute1       | NULL         | NULL          | accepted  | eb46eb30-08ae-4faa-a62e-b5f100bcddf5 |                 NULL |                 NULL | compute1    | NULL      |       0 | evacuation     |      0 |\n\n\nafter restart nova compute service on source node.\n| 2016-05-30 06:15:24 | 2016-05-30 06:17:52 | NULL       | 127 | compute1       | NULL         | NULL          | completed | eb46eb30-08ae-4faa-a62e-b5f100bcddf5 |                 NULL |                 NULL | compute1    | NULL      |       0 | evacuation     |      0 |", 
            "date_created": "2016-05-30 08:19:21.494804+00:00", 
            "author": "https://api.launchpad.net/1.0/~chinabjalex"
        }, 
        {
            "content": "Below is the instance detail. It's host is stilling compute1(the source node)\n[root@controller ~]# nova show eb46eb30-08ae-4faa-a62e-b5f100bcddf5\n+--------------------------------------+------------------------------------------------------------+\n| Property                             | Value                                                      |\n+--------------------------------------+------------------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                                     |\n| OS-EXT-AZ:availability_zone          | aaaaaaa                                                    |\n| OS-EXT-SRV-ATTR:host                 | compute1                                                   |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute1                                                   |\n| OS-EXT-SRV-ATTR:instance_name        | instance-0000003b                                          |\n| OS-EXT-STS:power_state               | 0                                                          |\n| OS-EXT-STS:task_state                | -                                                          |\n| OS-EXT-STS:vm_state                  | active                                                     |\n| OS-SRV-USG:launched_at               | 2016-05-30T06:11:41.000000                                 |\n| OS-SRV-USG:terminated_at             | -                                                          |\n| accessIPv4                           |                                                            |\n| accessIPv6                           |                                                            |\n| admin-net network                    | 192.168.0.61                                               |\n| config_drive                         | True                                                       |\n| created                              | 2016-05-30T02:53:37Z                                       |\n| flavor                               | mini (100000000)                                           |\n| hostId                               | da6fc182d7a4592413365df255c835eb99d233d8e1c479642caa7a9f   |\n| id                                   | eb46eb30-08ae-4faa-a62e-b5f100bcddf5                       |\n| image                                | cirros-0.3.1-x86_32 (6e1c83af-e57e-44eb-8a7d-0296b5f464ef) |\n| key_name                             | -                                                          |\n| metadata                             | {\"admin_pass\": \"a\", \"admin_user\": \"root\"}                  |\n| name                                 | vm1                                                        |\n| os-extended-volumes:volumes_attached | []                                                         |\n| os_type                              | linux                                                      |\n| progress                             | 0                                                          |\n| security_groups                      | default                                                    |\n| status                               | ACTIVE                                                     |\n| tenant_id                            | 2c2643bad4d6489a99be97fd69c4fdcc                           |\n| updated                              | 2016-05-30T06:18:13Z                                       |\n| user_id                              | 9646cdbcb7644ee79d80b0b54a6fcebe                           |\n+--------------------------------------+------------------------------------------------------------+", 
            "date_created": "2016-05-30 08:21:47.817048+00:00", 
            "author": "https://api.launchpad.net/1.0/~chinabjalex"
        }, 
        {
            "content": "Change abandoned by Michael Still (<email address hidden>) on branch: master\nReview: https://review.openstack.org/318731\nReason: This patch has been sitting unchanged for more than 12 weeks. I am therefore going to abandon it to keep the nova review queue sane. Please feel free to restore the change if you're still working on it.", 
            "date_created": "2016-10-25 16:12:23.372045+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Since there is no activity in this bug and the patch has been abandoned, removing the assignee and setting the status to new because this was not confirmed by anyone other than the reporter. ", 
            "date_created": "2016-11-15 19:36:04.993947+00:00", 
            "author": "https://api.launchpad.net/1.0/~anusha-unnam"
        }, 
        {
            "content": "I tried to reproduce this bug and i think this is a valid bug. I followed below steps:\n\nMultinode environment with controller and compute node,\n\n!. boot an instance vm1 on compute node.\n2. made the compute node to fault state.\n3. disabled compute service on controller node.\n4. evacaute vm1.\n5. recover the fault host and restart the compute service.\n\nAs you said vm1 is deleted on compute node. I saw this message from n-cpu log\n\"[instance: 36663d00-fad8-468e-9a13-462b2bbfbd37] Deleting instance as it has been evacuated from this host\"\n\"INFO nova.virt.libvirt.driver [-] [instance: 36663d00-fad8-468e-9a13-462b2bbfbd37] Instance destroyed successfully.\"\n\nWhen i did \"nova show vm1\", it shows wrongly that it is still on compute node and task_state is 'active' and power_state is 'NOSTATE'.\nSo the vm is not running and it was not evacuated to controller node even after I enabled the compute service on controller node.\n\nI read that nova evacuate is useful with shared storage. But in my environment i don't have shared storage. I see from your logs you have \"onSharedStorage\": true.\n\n\n\n", 
            "date_created": "2016-11-15 20:45:32.698511+00:00", 
            "author": "https://api.launchpad.net/1.0/~anusha-unnam"
        }
    ]
}
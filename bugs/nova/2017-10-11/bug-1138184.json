{
    "status": "Fix Released", 
    "last_updated": "2013-04-04 11:17:31.162365+00:00", 
    "description": "When a baremetal node is deleted, the associated compute_node record stops receiving periodic updates (but is not actually deleted). However, the scheduler's ComputeFilter seems to be unaware of this and continues to try to assign Nova instances to the deleted node.\n\nTo reproduce, start devstack with the baremetal driver, enroll a node (nova baremetal-node-create ...), wait a minute for the PeriodicTask to update compute, then delete the node (nova baremetal-node-delete ...). Then try to launch an instance (nova boot ...) and observe the failure.\n\nTo see whether this was just a timeout issue, I left devstack running for many hours after deleting the baremetal node, as can be seen from the database records below (some columns snipped for brevity).\n\nstack@ubuntu:~/devstack$ mysql nova -e 'select * from compute_nodes\\G'\n*************************** 1. row ***************************\n          created_at: 2013-02-28 18:22:38\n          updated_at: 2013-02-28 18:49:08\n          deleted_at: NULL\n                  id: 1\n          service_id: 2\n     hypervisor_type: baremetal\n  hypervisor_version: 1\n hypervisor_hostname: 653b6c79-35a1-4af8-99a5-edd62fe9625b\n             deleted: 0\n\nstack@ubuntu:~/devstack$ mysql nova_bm -e 'select * from bm_nodes where uuid=\"653b6c79-35a1-4af8-99a5-edd62fe9625b\"\\G'\n*************************** 1. row ***************************\n         created_at: 2013-02-28 18:22:04\n         updated_at: 2013-02-28 18:48:25\n         deleted_at: 2013-02-28 21:08:03\n            deleted: 1\n                 id: 1\n      instance_uuid: NULL\nregistration_status: NULL\n         task_state: deleted\n               uuid: 653b6c79-35a1-4af8-99a5-edd62fe9625b\n      instance_name: NULL\n\nstack@ubuntu:~/devstack$ mysql -e 'select now()'\n  2013-03-01 16:51:34 \n\n\nHere is a snippet from n-schd in devstack when calling \"nova boot\". What I don't understand, and what seems to be causing this issue, is why the servicegroup API believes this compute_node is still up! Note the 'updated_at' value logged by servicegroup.api is recent, whereas in the db, it is much older.\n\n2013-03-01 16:50:12.271 DEBUG nova.scheduler.filter_scheduler [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] Attempting to build 1 instance(s) from (pid=8693) schedule_run_instance /opt/stack/nova/nova/scheduler/filter_scheduler.py:75\n2013-03-01 16:50:12.280 DEBUG nova.servicegroup.api [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] Check if the given member [{'binary': u'nova-compute', 'deleted': 0L, 'created_at': datetime.datetime(2013, 2, 28, 17, 40, 45), 'updated_at': datetime.datetime(2013, 3, 1, 16, 50, 2), 'report_count': 8172L, 'topic': u'compute', 'host': u'ubuntu', 'disabled': False, 'deleted_at': None, 'id': 2L}] is part of the ServiceGroup, is up from (pid=8693) service_is_up /opt/stack/nova/nova/servicegroup/api.py:93\n2013-03-01 16:50:12.281 DEBUG nova.servicegroup.drivers.db [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] DB_Driver.is_up last_heartbeat = 2013-03-01 16:50:02 elapsed = 10.281252 from (pid=8693) is_up /opt/stack/nova/nova/servicegroup/drivers/db.py:68\n2013-03-01 16:50:12.281 DEBUG nova.scheduler.filters.compute_filter [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] ComputeFilter: Service {'binary': u'nova-compute', 'deleted': 0L, 'created_at': datetime.datetime(2013, 2, 28, 17, 40, 45), 'updated_at': datetime.datetime(2013, 3, 1, 16, 50, 2), 'report_count': 8172L, 'topic': u'compute', 'host': u'ubuntu', 'disabled': False, 'deleted_at': None, 'id': 2L} is True from (pid=8693) host_passes /opt/stack/nova/nova/scheduler/filters/compute_filter.py:39", 
    "tags": [
        "baremetal"
    ], 
    "importance": "High", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1138184", 
    "owner": "https://api.launchpad.net/1.0/~devananda", 
    "id": 1138184, 
    "index": 995, 
    "created": "2013-03-01 17:17:16.263872+00:00", 
    "title": "Scheduler selects deleted baremetal nodes", 
    "comments": [
        {
            "content": "When a baremetal node is deleted, the associated compute_node record stops receiving periodic updates (but is not actually deleted). However, the scheduler's ComputeFilter seems to be unaware of this and continues to try to assign Nova instances to the deleted node.\n\nTo reproduce, start devstack with the baremetal driver, enroll a node (nova baremetal-node-create ...), wait a minute for the PeriodicTask to update compute, then delete the node (nova baremetal-node-delete ...). Then try to launch an instance (nova boot ...) and observe the failure.\n\nTo see whether this was just a timeout issue, I left devstack running for many hours after deleting the baremetal node, as can be seen from the database records below (some columns snipped for brevity).\n\nstack@ubuntu:~/devstack$ mysql nova -e 'select * from compute_nodes\\G'\n*************************** 1. row ***************************\n          created_at: 2013-02-28 18:22:38\n          updated_at: 2013-02-28 18:49:08\n          deleted_at: NULL\n                  id: 1\n          service_id: 2\n     hypervisor_type: baremetal\n  hypervisor_version: 1\n hypervisor_hostname: 653b6c79-35a1-4af8-99a5-edd62fe9625b\n             deleted: 0\n\nstack@ubuntu:~/devstack$ mysql nova_bm -e 'select * from bm_nodes where uuid=\"653b6c79-35a1-4af8-99a5-edd62fe9625b\"\\G'\n*************************** 1. row ***************************\n         created_at: 2013-02-28 18:22:04\n         updated_at: 2013-02-28 18:48:25\n         deleted_at: 2013-02-28 21:08:03\n            deleted: 1\n                 id: 1\n      instance_uuid: NULL\nregistration_status: NULL\n         task_state: deleted\n               uuid: 653b6c79-35a1-4af8-99a5-edd62fe9625b\n      instance_name: NULL\n\nstack@ubuntu:~/devstack$ mysql -e 'select now()'\n  2013-03-01 16:51:34 \n\n\nHere is a snippet from n-schd in devstack when calling \"nova boot\". What I don't understand, and what seems to be causing this issue, is why the servicegroup API believes this compute_node is still up! Note the 'updated_at' value logged by servicegroup.api is recent, whereas in the db, it is much older.\n\n2013-03-01 16:50:12.271 DEBUG nova.scheduler.filter_scheduler [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] Attempting to build 1 instance(s) from (pid=8693) schedule_run_instance /opt/stack/nova/nova/scheduler/filter_scheduler.py:75\n2013-03-01 16:50:12.280 DEBUG nova.servicegroup.api [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] Check if the given member [{'binary': u'nova-compute', 'deleted': 0L, 'created_at': datetime.datetime(2013, 2, 28, 17, 40, 45), 'updated_at': datetime.datetime(2013, 3, 1, 16, 50, 2), 'report_count': 8172L, 'topic': u'compute', 'host': u'ubuntu', 'disabled': False, 'deleted_at': None, 'id': 2L}] is part of the ServiceGroup, is up from (pid=8693) service_is_up /opt/stack/nova/nova/servicegroup/api.py:93\n2013-03-01 16:50:12.281 DEBUG nova.servicegroup.drivers.db [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] DB_Driver.is_up last_heartbeat = 2013-03-01 16:50:02 elapsed = 10.281252 from (pid=8693) is_up /opt/stack/nova/nova/servicegroup/drivers/db.py:68\n2013-03-01 16:50:12.281 DEBUG nova.scheduler.filters.compute_filter [req-b8afe75b-dbb9-49dc-a643-eb0712cf3e5f demo demo] ComputeFilter: Service {'binary': u'nova-compute', 'deleted': 0L, 'created_at': datetime.datetime(2013, 2, 28, 17, 40, 45), 'updated_at': datetime.datetime(2013, 3, 1, 16, 50, 2), 'report_count': 8172L, 'topic': u'compute', 'host': u'ubuntu', 'disabled': False, 'deleted_at': None, 'id': 2L} is True from (pid=8693) host_passes /opt/stack/nova/nova/scheduler/filters/compute_filter.py:39", 
            "date_created": "2013-03-01 17:17:16.263872+00:00", 
            "author": "https://api.launchpad.net/1.0/~devananda"
        }, 
        {
            "content": "I did some more digging, and it looks like the issue is that:\n- the nova compute service is still running\n- servicegroup/drivers/db.py starts a FixedIntervalLoopingCall for self._report_state when the service is started\n- which continues to report its state\n- which includes a reference to compute_node_ref['compute_node'][0], as returned from service_get_by_compute_host.\n- and so that compute_node appears to be getting updated every 10 seconds, even when the compute driver knows it is dead.\n\nI've roughly validated this by observing the following:\n- adding additional baremetal nodes doesn't affect which compute_node is included in servicegroup _report_state RPC call\n- deleting baremetal nodes also has no effect on the RPC call\n- the scheduler continues to believe a compute_node is online when it is included in the _report_state RPC call, even if the compute driver knows otherwise, because its 'updated_at' value is never more than 10 seconds old.\n- deleting the oldest compute_node causes _report_state to include the next-oldest compute_node at the next update interval.\n- deleting all the compute_nodes results in the _report_state RPC call properly including no nodes.\n\nI'm convinced at this point that there needs to be a way to inform Nova that a compute_node is dead/deleted, besides merely relying on the last update_at timestamp for the associated compute service. This should allow deployers to remove baremetal nodes from production without breaking the scheduler's ability to find available nodes.\n\nAn alternative (but I think more complex) solution would be for the service group API to understand that a compute service may have any number of compute nodes (not just 1) and then to track their status' distinctly.", 
            "date_created": "2013-03-01 19:53:53.332423+00:00", 
            "author": "https://api.launchpad.net/1.0/~devananda"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/23333", 
            "date_created": "2013-03-02 00:59:03.856007+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/23333\nCommitted: http://github.com/openstack/nova/commit/ac0f6eb063fc5a5c0a9410402ecf57fae1faf594\nSubmitter: Jenkins\nBranch:    master\n\ncommit ac0f6eb063fc5a5c0a9410402ecf57fae1faf594\nAuthor: Devananda van der Veen <email address hidden>\nDate:   Fri Mar 1 14:05:35 2013 -0800\n\n    Compute manager should remove dead resources\n    \n    While most hypervisors return a single - and constant - value from\n    driver.get_available_nodes, baremetal does not. When a node is deleted\n    from the baremetal database, it is no longer returned from\n    driver.get_available_nodes. However, Nova's compute_node record is not\n    directly updated.\n    \n    This patch allows Compute Manager to detect missing nodes within\n    update_available_resources. It then invokes resource_tracker to update\n    the dead node and remove it from compute.\n    \n    This in turn allows the ServiceGroup API to properly update the\n    servicegroup when a baremetal node is no longer in service.\n    \n    Fixes bug 1138184\n    \n    Change-Id: Icfff3f8e3099668806633a6a58a152b32ec8b49b\n", 
            "date_created": "2013-03-04 17:04:51.001076+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
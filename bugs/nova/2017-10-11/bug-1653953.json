{
    "status": "In Progress", 
    "last_updated": "2017-07-28 07:33:05.256510+00:00", 
    "description": "Description\n===========\n\nI'm not entirely convinced that this is a bug but wanted to document and discuss this upstream.\n\nWhen using the rbd imagebackend, snapshots used to shelve an instance cannot be removed after unshelving as they are cloned and as a result are now the parents of the recreated instance disks.\n\nThis is in line with the behaviour of the imagebackend when initially spawning an instance from an image but has caused confusion for operators downstream who assume that the snapshot can be removed once the instance has been unshelved.\n\nWe could flatten the instance disk when spawning during an unshelve but to do so would mean extending the imagebackend to handle yet another corner case for rbd.\n\nSteps to reproduce\n==================\n\n$ nova boot --image cirros-raw --flavor 1 test-shelve\n[..]\n$ nova shelve test-shelve\n[..]\n$ nova unshelve test-shelve\n[..]\n$ sudo rbd -p vms ls -l\nNAME                                       SIZE PARENT                                           FMT PROT LOCK \n4c843671-879d-4ba6-b4e8-8eefdced5393_disk 1024M images/df96af36-5a97-4f47-a79f-f3f3c85a21d9@snap   2 \n$ glance image-delete df96af36-5a97-4f47-a79f-f3f3c85a21d9\nUnable to delete image 'df96af36-5a97-4f47-a79f-f3f3c85a21d9' because it is in use.\n\nWe can easily workaround this by manually flattening the instance disk :\n\n$ nova stop test-shelve\n$ sudo rbd -p vms flatten 4c843671-879d-4ba6-b4e8-8eefdced5393_disk\nImage flatten: 100% complete...done.\n$ nova start test-shelve\n$ glance image-delete df96af36-5a97-4f47-a79f-f3f3c85a21d9\n\nExpected result\n===============\nAble to remove the shelved snapshot from Glance after unshelve.\n\nActual result\n=============\nUnable to remove the shelved snapshot from Glance after unshelve.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running. See the following\n   list for all releases: http://docs.openstack.org/releases/\n\n   $ pwd\n   /opt/stack/nova\n   $ git rev-parse HEAD\n   d768bfa2c2fb774154a5268f58b28537f7b39f69\n   \n2. Which hypervisor did you use?\n   (For example: Libvirt + KVM, Libvirt + XEN, Hyper-V, PowerKVM, ...)\n   What's the version of that?\n\n   libvirt + kvm\n\n2. Which storage type did you use?\n   (For example: Ceph, LVM, GPFS, ...)\n   What's the version of that?\n\n   ceph\n\n3. Which networking type did you use?\n   (For example: nova-network, Neutron with OpenVSwitch, ...)\n\n   N/A\n\nLogs & Configs\n==============", 
    "tags": [
        "ceph"
    ], 
    "importance": "Medium", 
    "heat": 26, 
    "link": "https://bugs.launchpad.net/nova/+bug/1653953", 
    "owner": "https://api.launchpad.net/1.0/~rsritesh", 
    "id": 1653953, 
    "index": 4722, 
    "created": "2017-01-04 12:13:49.896332+00:00", 
    "title": "Unable to remove snapshots after an instance is unshelved when using the rbd imagebackend", 
    "comments": [
        {
            "content": "Description\n===========\n\nI'm not entirely convinced that this is a bug but wanted to document and discuss this upstream.\n\nWhen using the rbd imagebackend, snapshots used to shelve an instance cannot be removed after unshelving as they are cloned and as a result are now the parents of the recreated instance disks.\n\nThis is in line with the behaviour of the imagebackend when initially spawning an instance from an image but has caused confusion for operators downstream who assume that the snapshot can be removed once the instance has been unshelved.\n\nWe could flatten the instance disk when spawning during an unshelve but to do so would mean extending the imagebackend to handle yet another corner case for rbd.\n\nSteps to reproduce\n==================\n\n$ nova boot --image cirros-raw --flavor 1 test-shelve\n[..]\n$ nova shelve test-shelve\n[..]\n$ nova unshelve test-shelve\n[..]\n$ sudo rbd -p vms ls -l\nNAME                                       SIZE PARENT                                           FMT PROT LOCK \n4c843671-879d-4ba6-b4e8-8eefdced5393_disk 1024M images/df96af36-5a97-4f47-a79f-f3f3c85a21d9@snap   2 \n$ glance image-delete df96af36-5a97-4f47-a79f-f3f3c85a21d9\nUnable to delete image 'df96af36-5a97-4f47-a79f-f3f3c85a21d9' because it is in use.\n\nWe can easily workaround this by manually flattening the instance disk :\n\n$ nova stop test-shelve\n$ sudo rbd -p vms flatten 4c843671-879d-4ba6-b4e8-8eefdced5393_disk\nImage flatten: 100% complete...done.\n$ nova start test-shelve\n$ glance image-delete df96af36-5a97-4f47-a79f-f3f3c85a21d9\n\nExpected result\n===============\nAble to remove the shelved snapshot from Glance after unshelve.\n\nActual result\n=============\nUnable to remove the shelved snapshot from Glance after unshelve.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running. See the following\n   list for all releases: http://docs.openstack.org/releases/\n\n   $ pwd\n   /opt/stack/nova\n   $ git rev-parse HEAD\n   d768bfa2c2fb774154a5268f58b28537f7b39f69\n   \n2. Which hypervisor did you use?\n   (For example: Libvirt + KVM, Libvirt + XEN, Hyper-V, PowerKVM, ...)\n   What's the version of that?\n\n   libvirt + kvm\n\n2. Which storage type did you use?\n   (For example: Ceph, LVM, GPFS, ...)\n   What's the version of that?\n\n   ceph\n\n3. Which networking type did you use?\n   (For example: nova-network, Neutron with OpenVSwitch, ...)\n\n   N/A\n\nLogs & Configs\n==============", 
            "date_created": "2017-01-04 12:13:49.896332+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "My first thought on this is that the cloned image is flattened as part of direct_snapshot, so I'm interested in finding out why that's not sufficient for the image-delete to work. \n\nTo investigate, I tried to repro this myself in devstack and am hitting multiple other bugs. I can't get it to complete the snapshot successfully and the instance does not get shelved.\n\nThe first trace I get is:\n\n2017-01-05 01:14:50.499 ERROR nova.virt.libvirt.driver [req-a00f11f9-344f-40b7-b2ef-27b46825d9d6 admin admin] Failed to snapshot image\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver Traceback (most recent call last):\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 1559, in snapshot\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     instance.image_ref)\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/imagebackend.py\", line 990, in direct_snapshot\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     self.driver.clone(location, image_id, dest_pool=parent_pool)\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/storage/rbd_utils.py\", line 234, in clone\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     with RADOSClient(self, dest_pool) as dest_client:\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/storage/rbd_utils.py\", line 105, in __init__\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     self.cluster, self.ioctx = driver._connect_to_rados(pool)\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/storage/rbd_utils.py\", line 138, in _connect_to_rados\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     ioctx = client.open_ioctx(pool_to_open)\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver   File \"/usr/lib/python2.7/dist-packages/rados.py\", line 662, in open_ioctx\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver     raise TypeError('the name of the pool must be a string')\n2017-01-05 01:14:50.499 TRACE nova.virt.libvirt.driver TypeError: the name of the pool must be a string\n\nwhich I remedied locally by s/dest_pool/str(dest_pool)/ in the clone function in rbd_utils.py. In my environment, dest_pool is of type unicode.\n\nAfter that, I hit the next trace:\n\n2017-01-05 01:20:31.675 ERROR nova.virt.libvirt.driver [req-152c9895-39cc-464a-ae38-0e6f613278b5 admin admin] Failed to snapshot image\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver Traceback (most recent call last):\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 1559, in snapshot\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     instance.image_ref)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/imagebackend.py\", line 995, in direct_snapshot\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     self.cleanup_direct_snapshot(location)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/imagebackend.py\", line 1017, in cleanup_direct_snapshot\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     ignore_errors=ignore_errors)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/opt/stack/nova/nova/virt/libvirt/storage/rbd_utils.py\", line 404, in remove_snap\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     vol.unprotect_snap(name)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 186, in doit\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     rv = execute(f, *args, **kwargs)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     six.reraise(c, e, tb)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     rv = meth(*args, **kwargs)\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver   File \"/usr/lib/python2.7/dist-packages/rbd.py\", line 654, in unprotect_snap\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver     raise make_ex(ret, 'error unprotecting snapshot %s@%s' % (self.name, name))\n2017-01-05 01:20:31.675 TRACE nova.virt.libvirt.driver ImageBusy: error unprotecting snapshot 782834b3-f1a7-4b35-9309-796b3562ee49_disk@0a8c11db6e1349648f752dc6c0f81e88\n\nThe instance ends up in SHUTOFF state and not shelved.\n\n$ pwd\n/opt/stack/nova\n$ git rev-parse HEAD\n1bcf3b553ae8665dc6308d1bd11898efb75d3a41\n\nand local.conf:\n\n[[local|localrc]]\nDEST=/opt/stack/\nLOGDIR=$DEST/logs\nLOGFILE=$LOGDIR/stack.sh.log\nenable_plugin devstack-plugin-ceph git://git.openstack.org/openstack/devstack-plugin-ceph\nHOST_IP=127.0.0.1", 
            "date_created": "2017-01-05 01:53:18.651921+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "> My first thought on this is that the cloned image is flattened as part \n> of direct_snapshot, so I'm interested in finding out why that's not \n> sufficient for the image-delete to work.\n\nThat's the case when we shelve the instance but during unshelve we are just calling spawn and as a result we call for a clone on the imagebackend. Just the same as when we initially create an instance from an image.\n\n> To investigate, I tried to repro this myself in devstack and am hitting \n> multiple other bugs. I can't get it to complete the snapshot successfully \n> and the instance does not get shelved.\n\nOdd, I didn't see either of these in my F24 based env, I'll retry again today with the ref you used. \n", 
            "date_created": "2017-01-05 09:40:02.417395+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Just to be complete this still reproduces for me without any issues on F24 and 1bcf3b55 :\n\n$ cat /etc/redhat-release \nFedora release 24 (Twenty Four)\n$ cat local.conf \n[[local|localrc]]\nLOGDAYS=1\nLOGFILE=$DEST/logs/stack.sh.log\nSCREEN_LOGDIR=$DEST/logs/screen\nADMIN_PASSWORD=redhat\nDATABASE_PASSWORD=$ADMIN_PASSWORD\nRABBIT_PASSWORD=$ADMIN_PASSWORD\nSERVICE_PASSWORD=$ADMIN_PASSWORD\nSERVICE_TOKEN=6192fc3b-8156-4871-8979-8a8482d6c7d1\n\ndisable_service horizon\nenable_plugin devstack-plugin-ceph git://git.openstack.org/openstack/devstack-plugin-ceph\n\n$ cd /opt/stack/nova\n$ git rev-parse HEAD\n7463e1eec8f1d4b486703fbfd8d1eb755e0c5b0c\n$ git branch --contains 1bcf3b553ae8665dc6308d1bd11898efb75d3a41\n* master\n\n$ wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img\n$ qemu-img convert -f qcow2 -O raw cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.img.raw\n$ glance image-create --name cirros-raw --disk-format raw --container-format bare --file cirros-0.3.4-x86_64-disk.img.raw\n$ nova boot --image cirros-raw --flavor 1 test-shelve\n$ nova shelve test-shelve\n$ nova unshelve test-shelve\n$ glance image-delete 6bd46bd6-d85d-4f9d-b9b9-7a7a3ae7ed41\nUnable to delete image '6bd46bd6-d85d-4f9d-b9b9-7a7a3ae7ed41' because it is in use.\n$ sudo rbd -p vms ls -l\nNAME                                       SIZE PARENT                                           FMT PROT LOCK \n0cab4b03-0d53-404c-b097-892e0d35bfc1_disk 1024M images/6bd46bd6-d85d-4f9d-b9b9-7a7a3ae7ed41@snap   2   \n\n", 
            "date_created": "2017-01-05 11:56:56.180297+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Thanks for all that info. I'm using ubuntu trusty, so maybe there are some issues there. Will try again with fedora.", 
            "date_created": "2017-01-05 17:58:03.021965+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "I first tried with CentOS7 and that didn't work with stack.sh failing during install of the ceph-* packages. Tried a third time with Fedora 24 and was able to get shelve/unshelve to work. But, I wasn't able to reproduce the bug. The shelved image gets removed from glance and the backend when I unshelve.\n\n$ git rev-parse HEAD\n966446553bc8aea79692d0bc0bacae81d6a201df\n\nAfter shelving, I can see the shelved image in glance:\n\n$ glance image-list\n+--------------------------------------+---------------------------------+\n| ID                                   | Name                            |\n+--------------------------------------+---------------------------------+\n| 806c183b-80ce-4792-b19f-32e97b8b180b | cirros-0.3.4-x86_64-uec         |\n| 9d594345-c0a5-450a-9cc2-2c05a2950fcf | cirros-0.3.4-x86_64-uec-kernel  |\n| 07d825e6-b022-4eb2-8c56-73d75427541c | cirros-0.3.4-x86_64-uec-ramdisk |\n| 9b23e7b7-3262-4f4b-9da0-3185e4d85d44 | hi-shelved                      |\n+--------------------------------------+---------------------------------+\n\n$ sudo rbd -p images ls -l\nNAME                                        SIZE PARENT FMT PROT LOCK \n07d825e6-b022-4eb2-8c56-73d75427541c       3652k          2           \n07d825e6-b022-4eb2-8c56-73d75427541c@snap  3652k          2 yes       \n806c183b-80ce-4792-b19f-32e97b8b180b      24576k          2           \n806c183b-80ce-4792-b19f-32e97b8b180b@snap 24576k          2 yes       \n8fd1626b-562b-43cc-a5b3-53a3a2cfa3a9      24576k          2           \n8fd1626b-562b-43cc-a5b3-53a3a2cfa3a9@snap 24576k          2 yes       \n9d594345-c0a5-450a-9cc2-2c05a2950fcf       4862k          2           \n9d594345-c0a5-450a-9cc2-2c05a2950fcf@snap  4862k          2 yes     \n\nBut after unshelving, the shelved image is gone from glance:\n\n$ glance image-list\n+--------------------------------------+---------------------------------+\n| ID                                   | Name                            |\n+--------------------------------------+---------------------------------+\n| 806c183b-80ce-4792-b19f-32e97b8b180b | cirros-0.3.4-x86_64-uec         |\n| 9d594345-c0a5-450a-9cc2-2c05a2950fcf | cirros-0.3.4-x86_64-uec-kernel  |\n| 07d825e6-b022-4eb2-8c56-73d75427541c | cirros-0.3.4-x86_64-uec-ramdisk |\n+--------------------------------------+---------------------------------+\n\n$ sudo rbd -p images ls -l\nNAME                                        SIZE PARENT FMT PROT LOCK \n07d825e6-b022-4eb2-8c56-73d75427541c       3652k          2           \n07d825e6-b022-4eb2-8c56-73d75427541c@snap  3652k          2 yes       \n806c183b-80ce-4792-b19f-32e97b8b180b      24576k          2           \n806c183b-80ce-4792-b19f-32e97b8b180b@snap 24576k          2 yes       \n9d594345-c0a5-450a-9cc2-2c05a2950fcf       4862k          2           \n9d594345-c0a5-450a-9cc2-2c05a2950fcf@snap  4862k          2 yes\n", 
            "date_created": "2017-01-05 21:02:55.576809+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Yeah apologies, I didn't explicitly call it out in the description but this requires RAW images to be used otherwise we fallback to libvirt_utils.fetch_image.\n\nhttps://github.com/openstack/nova/blob/77b9e288a1dcd6b967b53aab818fcbac7d9105f6/nova/virt/libvirt/driver.py#L3153-L3159\n\n", 
            "date_created": "2017-01-05 21:21:11.384928+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Argh, sorry, that was my bad. Tried again with the RAW image and I see the same problem now, with this trace in n-cpu.log:\n\nSomething wrong happened when trying to delete snapshot \nfrom shelved instance.\nTraceback (most recent call last):\n  File \"/opt/stack/nova/nova/compute/manager.py\", line 2509, in _delete_snapshot_of_shelved_instance\n    self.image_api.delete(context, snapshot_id)\n  File \"/opt/stack/nova/nova/image/api.py\", line 141, in delete\n    return session.delete(context, image_id)\n  File \"/opt/stack/nova/nova/image/glance.py\", line 763, in delete\n    self._client.call(context, 2, 'delete', image_id)\n  File \"/opt/stack/nova/nova/image/glance.py\", line 168, in call\n    result = getattr(controller, method)(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/glanceclient/v2/images.py\", line 222, in delete\n    self.http_client.delete(url)\n  File \"/usr/lib/python2.7/site-packages/glanceclient/common/http.py\", line 299, in delete\n    return self._request('DELETE', url, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/glanceclient/common/http.py\", line 279, in _request\n    resp, body_iter = self._handle_response(resp)\n  File \"/usr/lib/python2.7/site-packages/glanceclient/common/http.py\", line 107, in _handle_response\n    raise exc.from_response(resp, resp.content)\nHTTPConflict: 409 Conflict\nImage 3c8ce6fe-269f-4e95-bf09-1f6173aaaf9a could not be deleted because it is in use: The image cannot be deleted because it is in use through the backend store outside of Glance.\n    (HTTP 409)\n\nI think this is a bug because nova is supposed to delete the snapshot during unshelve and fails when it tries, and thus leaks images.", 
            "date_created": "2017-01-05 22:00:55.163893+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "I wouldn't say that we leak images here, we still have references to the snapshot in Glance and can remove these later once the instance disk is flattened. We also end up in this situation when initially booting an instance from a rbd image, thus my reluctance to call this a genuine bug :\n\n$ nova boot --image cirros-raw --flavor 1 test\n[..]\n$ sudo rbd -p vms ls -l\nNAME                                       SIZE PARENT                                           FMT PROT LOCK \n1987d04b-10ce-4724-80bc-3ba699a45ded_disk 1024M images/a906bdce-47e2-4eb1-abeb-5f390fc8493d@snap   2           \n$ glance image-delete a906bdce-47e2-4eb1-abeb-5f390fc8493d\nUnable to delete image 'a906bdce-47e2-4eb1-abeb-5f390fc8493d' because it is in use.\n", 
            "date_created": "2017-01-06 09:50:34.569058+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "I have tried to reproduce this issue today but could not able to reproduce \n\nIs this issue  dependent on Ceph RBD ?  following is my system configuration, can anyone guide me how to reproduce this issue\n\nDevstack based setup\n\n$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.5 LTS\"\n\n\n$ cat local.conf \n[[local|localrc]]\nADMIN_PASSWORD=secret\nDATABASE_PASSWORD=openstack\nRABBIT_PASSWORD=openstack\nSERVICE_PASSWORD=openstack\n\n\nenable_plugin designate https://git.openstack.org/openstack/designate\n\nenable_service designate,designate-central,designate-api,designate-pool-manager,designate-zone-manager,designate-mdns\n\nenable_service tempest\n\n\n\n$ git rev-parse HEAD\nab44e75fb387847d3c679979007e7c6202dd8afd\n\n$ nova boot --image cirros-0.3.5-x86_64-disk --flavor 1 test-shelve\n+--------------------------------------+-------------+--------+------------+-------------+---------------------------------+\n| ID                                   | Name        | Status | Task State | Power State | Networks                        |\n+--------------------------------------+-------------+--------+------------+-------------+---------------------------------+\n| 1435f84a-bb87-47c1-ae94-1aa9864d4460 | test-shelve | BUILD  | spawning   | NOSTATE     | public=2001:db8::3, 172.24.4.11 |\n+--------------------------------------+-------------+--------+------------+-------------+---------------------------------+\n\n\n$ nova shelve test-shelve\n\n$ glance image-list\n+--------------------------------------+--------------------------+\n| ID                                   | Name                     |\n+--------------------------------------+--------------------------+\n| 241f16d5-42fb-4c75-8537-82a5fd5a0d83 | cirros-0.3.5-x86_64-disk |\n| adf2c16c-fb4e-4648-8959-a771bb179e6b | test-shelve-shelved      |\n+--------------------------------------+--------------------------+\n\n$ nova unshelve test-shelve\n\n$ glance image-list\n+--------------------------------------+--------------------------+\n| ID                                   | Name                     |\n+--------------------------------------+--------------------------+\n| 241f16d5-42fb-4c75-8537-82a5fd5a0d83 | cirros-0.3.5-x86_64-disk |\n+--------------------------------------+--------------------------+\n\n\n", 
            "date_created": "2017-04-13 09:55:45.967604+00:00", 
            "author": "https://api.launchpad.net/1.0/~rsritesh"
        }, 
        {
            "content": "Yes.. it is reproduce able with Ceph RBD only. ", 
            "date_created": "2017-04-19 05:19:49.006353+00:00", 
            "author": "https://api.launchpad.net/1.0/~rsritesh"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/457886", 
            "date_created": "2017-04-19 06:41:58.748116+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Changed in nova:\nstatus:\tIn Progress \u2192 Fix Committed\n\nI think you're confusing the nova bugfix with the tempest workaround here.\n", 
            "date_created": "2017-07-27 11:36:07.185281+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "There was no tempest workaround here.\nNova fix has been this : https://review.openstack.org/#/c/457886/\n", 
            "date_created": "2017-07-28 07:33:04.690091+00:00", 
            "author": "https://api.launchpad.net/1.0/~rsritesh"
        }
    ]
}
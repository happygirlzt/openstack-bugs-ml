{
    "status": "Invalid", 
    "last_updated": "2014-12-11 08:42:41.336987+00:00", 
    "description": "Setup:\n\ndevstack on master using default settings.\n\nSteps:\n\n  1) Using tempest/stress with patch https://review.openstack.org/#/c/36652/:\n  cd /opt/stack/tempest/tempest/stress\n  ./run_stress.py etc/volume-assign-delete-test.json -d 60\n  2) Test will do the following work flow:\n       - create a volume\n       - create a VM\n       - attach volume to VM\n       - delete VM\n       - delete volume\n\nProblem:\n\nDeletion of volume causes problem, since the state is still \"in-use\" even the VM is already deleted:\n\n2013-07-15 12:30:58,563 31273 tempest.stress      : INFO     creating volume: volume663095989\n2013-07-15 12:30:59,992 31273 tempest.stress      : INFO     created volume: cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60\n2013-07-15 12:30:59,993 31273 tempest.stress      : INFO     creating vm: instance331154488\n2013-07-15 12:31:11,097 31273 tempest.stress      : INFO     created vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,098 31273 tempest.stress      : INFO     attach volume (cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60) to vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,265 31273 tempest.stress      : INFO     volume (cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60) attached to vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,265 31273 tempest.stress      : INFO     deleting vm: instance331154488\n2013-07-15 12:31:13,780 31273 tempest.stress      : INFO     deleted vm: 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:13,781 31273 tempest.stress      : INFO     deleting volume: cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60\nProcess Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/stack/tempest/tempest/stress/actions/volume_attach_delete.py\", line 61, in create_delete\n    resp, _ = manager.volumes_client.delete_volume(volume['id'])\n  File \"/opt/stack/tempest/tempest/services/volume/json/volumes_client.py\", line 86, in delete_volume\n    return self.delete(\"volumes/%s\" % str(volume_id))\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 264, in delete\n    return self.request('DELETE', url, headers)\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 386, in request\n    resp, resp_body)\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 436, in _error_checker\n    raise exceptions.BadRequest(resp_body)\nBadRequest: Bad request\nDetails: {u'badRequest': {u'message': u'Invalid volume: Volume status must be available or error', u'code': 400}}\n2013-07-15 12:31:58,622 31264 tempest.stress      : INFO     cleaning up\n\nnova list:\n+----+------+--------+------------+-------------+----------+\n| ID | Name | Status | Task State | Power State | Networks |\n+----+------+--------+------------+-------------+----------+\n+----+------+--------+------------+-------------+----------+\n\ncinder list\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+\n|                  ID                  | Status |   Display Name   | Size | Volume Type | Bootable |             Attached to              |\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+\n| cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60 | in-use | volume663095989  |  1   |     None    |  False   | 4e20442b-8f72-482d-9e7c-59725748784b |\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+", 
    "tags": [
        "volumes"
    ], 
    "importance": "Undecided", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1201418", 
    "owner": "None", 
    "id": 1201418, 
    "index": 3842, 
    "created": "2013-07-15 13:07:35.403193+00:00", 
    "title": "Volume 'in-use' although VM doesn't exist", 
    "comments": [
        {
            "content": "Setup:\n\ndevstack on master using default settings.\n\nSteps:\n\n  1) Using tempest/stress with patch https://review.openstack.org/#/c/36652/:\n  cd /opt/stack/tempest/tempest/stress\n  ./run_stress.py etc/volume-assign-delete-test.json -d 60\n  2) Test will do the following work flow:\n       - create a volume\n       - create a VM\n       - attach volume to VM\n       - delete VM\n       - delete volume\n\nProblem:\n\nDeletion of volume causes problem, since the state is still \"in-use\" even the VM is already deleted:\n\n2013-07-15 12:30:58,563 31273 tempest.stress      : INFO     creating volume: volume663095989\n2013-07-15 12:30:59,992 31273 tempest.stress      : INFO     created volume: cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60\n2013-07-15 12:30:59,993 31273 tempest.stress      : INFO     creating vm: instance331154488\n2013-07-15 12:31:11,097 31273 tempest.stress      : INFO     created vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,098 31273 tempest.stress      : INFO     attach volume (cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60) to vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,265 31273 tempest.stress      : INFO     volume (cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60) attached to vm 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:11,265 31273 tempest.stress      : INFO     deleting vm: instance331154488\n2013-07-15 12:31:13,780 31273 tempest.stress      : INFO     deleted vm: 4e20442b-8f72-482d-9e7c-59725748784b\n2013-07-15 12:31:13,781 31273 tempest.stress      : INFO     deleting volume: cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60\nProcess Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/stack/tempest/tempest/stress/actions/volume_attach_delete.py\", line 61, in create_delete\n    resp, _ = manager.volumes_client.delete_volume(volume['id'])\n  File \"/opt/stack/tempest/tempest/services/volume/json/volumes_client.py\", line 86, in delete_volume\n    return self.delete(\"volumes/%s\" % str(volume_id))\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 264, in delete\n    return self.request('DELETE', url, headers)\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 386, in request\n    resp, resp_body)\n  File \"/opt/stack/tempest/tempest/common/rest_client.py\", line 436, in _error_checker\n    raise exceptions.BadRequest(resp_body)\nBadRequest: Bad request\nDetails: {u'badRequest': {u'message': u'Invalid volume: Volume status must be available or error', u'code': 400}}\n2013-07-15 12:31:58,622 31264 tempest.stress      : INFO     cleaning up\n\nnova list:\n+----+------+--------+------------+-------------+----------+\n| ID | Name | Status | Task State | Power State | Networks |\n+----+------+--------+------------+-------------+----------+\n+----+------+--------+------------+-------------+----------+\n\ncinder list\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+\n|                  ID                  | Status |   Display Name   | Size | Volume Type | Bootable |             Attached to              |\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+\n| cb4d625c-c4d8-43ee-9bdd-d4fa4e1d2c60 | in-use | volume663095989  |  1   |     None    |  False   | 4e20442b-8f72-482d-9e7c-59725748784b |\n+--------------------------------------+--------+------------------+------+-------------+----------+--------------------------------------+", 
            "date_created": "2013-07-15 13:07:35.403193+00:00", 
            "author": "https://api.launchpad.net/1.0/~m-koderer"
        }, 
        {
            "content": "In this state a force-deletion of the volume is even not possible:\n\ncinder force-delete a31d4eca-52a4-47ff-91a1-e9281addc5e9\nERROR: Volume a31d4eca-52a4-47ff-91a1-e9281addc5e9 is still attached, detach volume first.", 
            "date_created": "2013-07-15 15:13:47.595513+00:00", 
            "author": "https://api.launchpad.net/1.0/~m-koderer"
        }, 
        {
            "content": "Is this a Nova issue, where it doesn't disconnect when the VM is deleted?", 
            "date_created": "2013-07-15 19:07:58.897103+00:00", 
            "author": "https://api.launchpad.net/1.0/~avishay-il"
        }, 
        {
            "content": "Hi Marc, I can't reproduce the error with the steps you provided. I just type command in shell by hand.", 
            "date_created": "2013-07-16 01:53:02.461432+00:00", 
            "author": "https://api.launchpad.net/1.0/~haomai"
        }, 
        {
            "content": "Hi Haomai, yes sorry that I didn't mention that. From the command line it works. Seems to be a timing problem.", 
            "date_created": "2013-07-16 06:10:05.793435+00:00", 
            "author": "https://api.launchpad.net/1.0/~m-koderer"
        }, 
        {
            "content": "I also had the same thoughts as Avishay. Not sure if we want Cinder constantly verifying in-use volumes.", 
            "date_created": "2013-07-18 17:48:13.817680+00:00", 
            "author": "https://api.launchpad.net/1.0/~thingee"
        }, 
        {
            "content": "This is likely part of the cleanup on the BDM side or in the caching.   There are some other issues related to this, like failed attach never cleaning up on the compute side.", 
            "date_created": "2013-07-21 23:47:40.352344+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "I commented on that patch (https://review.openstack.org/#/c/36652) and I think the issue arrises when a user makes a call to the cinder API \"attach_volume\" directly. My understanding is that Cinder's attach_volume simply updates Cinder's database by marking the volume as being 'in-use' and associating it with the instance. It does not make a call to Nova to actually attach the volume to the instance. In fact, I believe the direction is expected to be the other way around. In other words, Nova's attach_volume will at the very end make a call to Cinder's attach_volume to inform Cinder that the volume is now attached.\n\n\n", 
            "date_created": "2013-07-22 19:08:55.558154+00:00", 
            "author": "https://api.launchpad.net/1.0/~dscannell"
        }, 
        {
            "content": "Thanks for the feedback - using nova client fixes the problem.", 
            "date_created": "2013-07-26 14:08:41.831309+00:00", 
            "author": "https://api.launchpad.net/1.0/~m-koderer"
        }, 
        {
            "content": "I actually had a similar issue with a user having volumes attached to non-existent VMs.\n\nI discovered that the user was using the not-so-well documented Cinder API call ``POST /volumes/{volume_id}/action`` through the python-cinderclient library. There are methods ``cinderclient.v{1,2}.volumes.VolumeManager.attach`` allowing a user to \"set attachment metadata\" without actually attaching volumes to instances.\n\nTo attach a volume to an instance in a Python script, one has to use the ``novaclient.v1_1.volumes.VolumeManager.create_server_volume`` method.\n\nI wrote some documentation on that issue there: http://www.florentflament.com/blog/openstack-volume-in-use-although-vm-doesnt-exist.html\n", 
            "date_created": "2014-01-04 14:47:42.039689+00:00", 
            "author": "https://api.launchpad.net/1.0/~florentflament"
        }, 
        {
            "content": "Hi all,\n\nThis bug is related to the current bug that I'm working on (https://bugs.launchpad.net/nova/+bug/1335889).\n\nThe bug can be summarized as follows. It's impossible to delete a volume attached to the VM instance that has been deleted.\n\nTo trigger or to enforce the bug, we can simply disable the Cinder's function \"detach()\" in module \"cinder\\volume\\api.py\". \nThis is because Nova's deleting of VM always triggers the execution of the \"detach()\" function of Cinder.\n\nIf so, we can have the scenario where a volume is attached to an already-deleted VM.\n", 
            "date_created": "2014-12-11 08:42:39.576050+00:00", 
            "author": "https://api.launchpad.net/1.0/~trung-t-trinh"
        }
    ]
}
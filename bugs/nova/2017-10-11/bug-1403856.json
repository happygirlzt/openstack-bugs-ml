{
    "status": "Expired", 
    "last_updated": "2016-07-05 09:55:41.346728+00:00", 
    "description": "The release: Icehouse, however the code in juno seems to same\n\nWhen a VMware node crashes, the instances will be restarted on a new node because of vSphere HA.\n\nIf _sync_power_states() is triggered just after node crash, the vms are down and Nova will update the database and print\n\"Instance shutdown by itself. Calling the stop API.\"\n\nOn next _sync_power_states() run, Nova will notice that power state is changed and will shut the instances down and print\n\"Instance is not stopped. Calling the stop API.\". This happens because vSphere HA has started instances meanwhile.\n\nTo my understanding to fix this we need either\n1. change the logic (I don't have ideas unfortunately) or\n2. add a config option that states if we force stop or not when an instance is stopped from the database point of view.", 
    "tags": [
        "vmware"
    ], 
    "importance": "Undecided", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1403856", 
    "owner": "None", 
    "id": 1403856, 
    "index": 5333, 
    "created": "2014-12-18 12:15:59.105296+00:00", 
    "title": "VMware VCDriver: A node crash, vSphere HA and badly timed  _sync_power_states() will shut instances down", 
    "comments": [
        {
            "content": "The release: Icehouse, however the code juno seems to same\n\nWhen node crashes and the instances in VMware side will be restarted on a new node because of vSphere HA.\n\nIf _sync_power_states() is triggered just after node crash, the vms are down and Nova will update the database and print\n\"Instance shutdown by itself. Calling the stop API.\"\n\nOn next _sync_power_states() run Nova will notice that power state is changed and will shut the instances down and print\n\"Instance is not stopped. Calling the stop API.\". This happens because vSphere HA has started instances meanwhile.\n\nTo my understanding to fix this we need either\n1. change the logic (I don't have ideas unfortunately) or\n2. add a config option that states if we force stop or not when instance is stopped from the database point of view.", 
            "date_created": "2014-12-18 12:15:59.105296+00:00", 
            "author": "https://api.launchpad.net/1.0/~toni-ylenius"
        }, 
        {
            "content": "This issue can be avoided by setting sync_power_state_interval=-1 in nova.con\n\nBut that stops the periodic task completely and you miss some of the goodness of this task - which is syncing the database state with the hypervisor state.  \n\nIn sphere if HA is enabled on a cluster we want to just log the discrepancy in power state and vm state, but not do the action of powering down the hosts as it is a transient state why the instances are being migrated to the new host.  Unfortunately there is no way at the compute/manager level to know this.  Another solution is yet ANOTHER config variable to control the action of either logging the warning or doing the power off", 
            "date_created": "2015-06-09 23:40:22.777674+00:00", 
            "author": "https://api.launchpad.net/1.0/~tjones-i"
        }, 
        {
            "content": "When a node crashes, the vms connection state will be \"disconnected\" and vm state will be still poweredOn.\nWhen HA restarts the VM on different host, it will be in poweredOff for a transient time.\nOne possible option could be check if HA is enabled on the cluster and if vm state is poweredOff  then retry to check if the state changed on poweredOn.", 
            "date_created": "2016-03-09 08:11:38.946370+00:00", 
            "author": "https://api.launchpad.net/1.0/~gjayavelu"
        }, 
        {
            "content": "we can detect sub state of the vm using 'recentTask' property on the vm.\n\nhttps://www.vmware.com/support/developer/vc-sdk/visdk2xpubs/ReferenceGuide/vim.VirtualMachine.PowerState.html \n\n\"\"\"clients interested in monitoring the status of a virtual machine should typically track the activeTask data object in addition to the powerState object.\"\"\"\n\nIf the recent task id is \"vim.VirtualMachine.reset\" and status=\"running\", then we can probably wait before returning the state in the driver or call _poll_rebooting_instances() at manager.py", 
            "date_created": "2016-03-10 22:46:37.798598+00:00", 
            "author": "https://api.launchpad.net/1.0/~gjayavelu"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:55:40.857557+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ]
}
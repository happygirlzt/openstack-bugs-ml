{
    "status": "Fix Released", 
    "last_updated": "2013-04-04 09:58:37.595188+00:00", 
    "description": "Running into an issue with nova-api which may be threading related. Having lots of fun trying to reproduce and debug, but not having much luck yet.  Filing here to track my progress and hopefully solicit input from others.\n\nFirst some background... We're triggering multi-node test deployments for upstream commits using Juju + MAAS.  The process of deploying services with Juju is very much event driven.  A service is deployed, and relations are added to other services.  Each new relation added fires a number of hook scripts on the machine that do various bits of configuration.  For most of the nova components, this involves making a few modifications to nova.conf or paste config and restarting the running services (using upstart).\n\nIn the case of nova-api, the service is deployed to an empty machine, the packages install and the services are stopped.  Then a number of relations are added to the other services in the environment (keystone, rabbit, mysql, glance, etc).  The nova-api configuration flow usually looks something like:\n\ndeploy -> install nova-api, stop service\nmysql relation added  -> set sql_connection, restart nova-api\nrabbit realtion added -> update nova.conf's rabbit settings , restart nova-api\nkeystone relation added -> update paste.ini with credentials, restart nova-api\netc, etc, etc\n\nThis all happens in rapid succession. The hooks themselves are not executed concurrently, but since nova doesn't fork and upstart  launches it in foreground, service restarts are basically asynchronous. Depending on how quickly the hooks fire off, the service may be stopped and started (restarted) while the previous process is still going through its initialization.  So, its likely  that every time the service is restarted, its in a different stage of init, with different configuration.\n\nIt seems that nova-api being restarted in this manor is not reliable.  It's not 100% reproducible, but calls to stop nova-api will sometimes leave an orphaned child, still bound to the port it was listening  (but unresponsive to connections, sitting in epoll_wait())  AFAICS, the parent process is mistakenly  getting killed off before its children.  At this point, nova-api is stopped as far as upstart is concerned.  The following hook will try to start nova-api and the service will fail because the port is already in use.  The only way to recover is to manually kill  the orphaned child and restart the service. \n\nI *was* seeing the following traceback in the api log whenever it hit this state, but these have disappeared from the logs over the last day or two though the problem still persists.\n\n2012-07-25 13:07:20 INFO nova.wsgi [-] WSGI server has stopped.\n2012-07-25 13:07:20 INFO nova.wsgi [-] WSGI server has stopped.\n2012-07-25 13:07:20 INFO nova.service [-] Child 6519 exited with status 0\n2012-07-25 13:07:20 INFO nova.service [-] Child 6518 exited with status 0\n2012-07-25 13:07:20 INFO nova.service [-] Child 6517 exited with status 0\n2012-07-25 13:07:20 ERROR nova.service [-] Unhandled exception\n2012-07-25 13:07:20 TRACE nova.service Traceback (most recent call last):\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 289, in _start_child\n2012-07-25 13:07:20 TRACE nova.service     self._child_process(wrap.server)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 266, in _child_process\n2012-07-25 13:07:20 TRACE nova.service     launcher.run_server(server)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 127, in run_server\n2012-07-25 13:07:20 TRACE nova.service     server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 619, in wait\n2012-07-25 13:07:20 TRACE nova.service     self.server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/wsgi.py\", line 113, in wait\n2012-07-25 13:07:20 TRACE nova.service     self._server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 166, in wait\n2012-07-25 13:07:20 TRACE nova.service     return self._exit_event.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/event.py\", line 116, in wait\n2012-07-25 13:07:20 TRACE nova.service     return hubs.get_hub().switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 177, in switch\n2012-07-25 13:07:20 TRACE nova.service     return self.greenlet.switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 226, in run\n2012-07-25 13:07:20 TRACE nova.service     self.wait(sleep_time)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/poll.py\", line 84, in wait\n2012-07-25 13:07:20 TRACE nova.service     presult = self.do_poll(seconds)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 245, in _sigterm\n2012-07-25 13:07:20 TRACE nova.service     LOG.info(_('Received SIGTERM, stopping'))\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1420, in info\n2012-07-25 13:07:20 TRACE nova.service     self.logger.info(msg, *args, **kwargs)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1140, in info\n2012-07-25 13:07:20 TRACE nova.service     self._log(INFO, msg, args, **kwargs)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1258, in _log\n2012-07-25 13:07:20 TRACE nova.service     self.handle(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1268, in handle\n2012-07-25 13:07:20 TRACE nova.service     self.callHandlers(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1308, in callHandlers\n2012-07-25 13:07:20 TRACE nova.service     hdlr.handle(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 746, in handle\n2012-07-25 13:07:20 TRACE nova.service     self.acquire()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 697, in acquire\n2012-07-25 13:07:20 TRACE nova.service     self.lock.acquire()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/threading.py\", line 127, in acquire\n2012-07-25 13:07:20 TRACE nova.service     rc = self.__block.acquire(blocking)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/semaphore.py\", line 71, in acquire\n2012-07-25 13:07:20 TRACE nova.service     hubs.get_hub().switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 162, in switch\n2012-07-25 13:07:20 TRACE nova.service     assert cur is not self.greenlet, 'Cannot switch to MAINLOOP from MAINLOOP'\n2012-07-25 13:07:20 TRACE nova.service AssertionError: Cannot switch to MAINLOOP from MAINLOOP\n2012-07-25 13:07:20 TRACE nova.service\n2012-07-25 13:07:22 INFO nova.service [-] Parent process has died unexpectedly, exiting\n\nI can't seem to reproduce manually, but I am in the process of adding debugging capabilities to the systems during provisioning, so I should be able to get gdb attached to one of the stuck process tomorrow.  Any clues in the meantime would be appreciated.\n\nI partly want to migrate to using individual API servers instead of the monolithic nova-api, but if nova-api remains it should be able to handle restarts robustly.", 
    "tags": [
        "folsom-backport-potential"
    ], 
    "importance": "Low", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1029727", 
    "owner": "https://api.launchpad.net/1.0/~johannes.erdfelt", 
    "id": 1029727, 
    "index": 5520, 
    "created": "2012-07-27 02:05:48.365823+00:00", 
    "title": "Using logging module from signal handler can wedge process", 
    "comments": [
        {
            "content": "Running into an issue with nova-api which may be threading related. Having lots of fun trying to reproduce and debug, but not having much luck yet.  Filing here to track my progress and hopefully solicit input from others.\n\nFirst some background... We're triggering multi-node test deployments for upstream commits using Juju + MAAS.  The process of deploying services with Juju is very much event driven.  A service is deployed, and relations are added to other services.  Each new relation added fires a number of hook scripts on the machine that do various bits of configuration.  For most of the nova components, this involves making a few modifications to nova.conf or paste config and restarting the running services (using upstart).\n\nIn the case of nova-api, the service is deployed to an empty machine, the packages install and the services are stopped.  Then a number of relations are added to the other services in the environment (keystone, rabbit, mysql, glance, etc).  The nova-api configuration flow usually looks something like:\n\ndeploy -> install nova-api, stop service\nmysql relation added  -> set sql_connection, restart nova-api\nrabbit realtion added -> update nova.conf's rabbit settings , restart nova-api\nkeystone relation added -> update paste.ini with credentials, restart nova-api\netc, etc, etc\n\nThis all happens in rapid succession. The hooks themselves are not executed concurrently, but since nova doesn't fork and upstart  launches it in foreground, service restarts are basically asynchronous. Depending on how quickly the hooks fire off, the service may be stopped and started (restarted) while the previous process is still going through its initialization.  So, its likely  that every time the service is restarted, its in a different stage of init, with different configuration.\n\nIt seems that nova-api being restarted in this manor is not reliable.  It's not 100% reproducible, but calls to stop nova-api will sometimes leave an orphaned child, still bound to the port it was listening  (but unresponsive to connections, sitting in epoll_wait())  AFAICS, the parent process is mistakenly  getting killed off before its children.  At this point, nova-api is stopped as far as upstart is concerned.  The following hook will try to start nova-api and the service will fail because the port is already in use.  The only way to recover is to manually kill  the orphaned child and restart the service. \n\nI *was* seeing the following traceback in the api log whenever it hit this state, but these have disappeared from the logs over the last day or two though the problem still persists.\n\n2012-07-25 13:07:20 INFO nova.wsgi [-] WSGI server has stopped.\n2012-07-25 13:07:20 INFO nova.wsgi [-] WSGI server has stopped.\n2012-07-25 13:07:20 INFO nova.service [-] Child 6519 exited with status 0\n2012-07-25 13:07:20 INFO nova.service [-] Child 6518 exited with status 0\n2012-07-25 13:07:20 INFO nova.service [-] Child 6517 exited with status 0\n2012-07-25 13:07:20 ERROR nova.service [-] Unhandled exception\n2012-07-25 13:07:20 TRACE nova.service Traceback (most recent call last):\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 289, in _start_child\n2012-07-25 13:07:20 TRACE nova.service     self._child_process(wrap.server)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 266, in _child_process\n2012-07-25 13:07:20 TRACE nova.service     launcher.run_server(server)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 127, in run_server\n2012-07-25 13:07:20 TRACE nova.service     server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 619, in wait\n2012-07-25 13:07:20 TRACE nova.service     self.server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/wsgi.py\", line 113, in wait\n2012-07-25 13:07:20 TRACE nova.service     self._server.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 166, in wait\n2012-07-25 13:07:20 TRACE nova.service     return self._exit_event.wait()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/event.py\", line 116, in wait\n2012-07-25 13:07:20 TRACE nova.service     return hubs.get_hub().switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 177, in switch\n2012-07-25 13:07:20 TRACE nova.service     return self.greenlet.switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 226, in run\n2012-07-25 13:07:20 TRACE nova.service     self.wait(sleep_time)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/poll.py\", line 84, in wait\n2012-07-25 13:07:20 TRACE nova.service     presult = self.do_poll(seconds)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/nova/service.py\", line 245, in _sigterm\n2012-07-25 13:07:20 TRACE nova.service     LOG.info(_('Received SIGTERM, stopping'))\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1420, in info\n2012-07-25 13:07:20 TRACE nova.service     self.logger.info(msg, *args, **kwargs)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1140, in info\n2012-07-25 13:07:20 TRACE nova.service     self._log(INFO, msg, args, **kwargs)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1258, in _log\n2012-07-25 13:07:20 TRACE nova.service     self.handle(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1268, in handle\n2012-07-25 13:07:20 TRACE nova.service     self.callHandlers(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 1308, in callHandlers\n2012-07-25 13:07:20 TRACE nova.service     hdlr.handle(record)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 746, in handle\n2012-07-25 13:07:20 TRACE nova.service     self.acquire()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/logging/__init__.py\", line 697, in acquire\n2012-07-25 13:07:20 TRACE nova.service     self.lock.acquire()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/threading.py\", line 127, in acquire\n2012-07-25 13:07:20 TRACE nova.service     rc = self.__block.acquire(blocking)\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/semaphore.py\", line 71, in acquire\n2012-07-25 13:07:20 TRACE nova.service     hubs.get_hub().switch()\n2012-07-25 13:07:20 TRACE nova.service   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 162, in switch\n2012-07-25 13:07:20 TRACE nova.service     assert cur is not self.greenlet, 'Cannot switch to MAINLOOP from MAINLOOP'\n2012-07-25 13:07:20 TRACE nova.service AssertionError: Cannot switch to MAINLOOP from MAINLOOP\n2012-07-25 13:07:20 TRACE nova.service\n2012-07-25 13:07:22 INFO nova.service [-] Parent process has died unexpectedly, exiting\n\nI can't seem to reproduce manually, but I am in the process of adding debugging capabilities to the systems during provisioning, so I should be able to get gdb attached to one of the stuck process tomorrow.  Any clues in the meantime would be appreciated.\n\nI partly want to migrate to using individual API servers instead of the monolithic nova-api, but if nova-api remains it should be able to handle restarts robustly.", 
            "date_created": "2012-07-27 02:05:48.365823+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "I've also observed the same thing happened to nova-compute processes.   In that case, upstart 'looses track' of the process and calls to restart it end up launching a second nova-compute process, though the first remains responsive, servicing requests from the scheduler with bad config (the defaults).\n\nI have a feeling https://review.openstack.org/#/c/8733/ has something to do with this, and also that  the disappearance of the original traceback is the result of moving to openstack-common logging functionality.\n\nThread @ https://lists.secondlife.com/pipermail/eventletdev/2011-February/000952.html seems consistent with what I'm seeing.  I've applied a small testing patch to our repository that disables the logging calls from signal handler to see if that helps. ", 
            "date_created": "2012-07-28 05:07:05.346500+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "Removing the logging calls from signal handlers helps reduce the number of times this deadlock triggers, but I'm still seeing with less frequency  in different places of the python code path, but still in epoll_wait().  Now, I'm finding the wedged child processes locked at:\n\n(gdb) pystack\n/usr/lib/python2.7/dist-packages/eventlet/hubs/epolls.py (59): do_poll\n/usr/lib/python2.7/dist-packages/eventlet/hubs/poll.py (84): wait\n/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py (226): run\n", 
            "date_created": "2012-08-01 17:38:22.804845+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "There's only two layers of processing in multi-process. There's the master process and then there's the child processes. nova-api will wait for children to exit before it exits.\n\nIf there's a process still running afterwards, then either it wasn't created by the master process (or the code that tracks the children) or it never waited for children to finish before exiting.\n\nYour traceback makes it look like it never waited, which would make sense given the behavior you saw.\n\nIt does look like using logging in a signal handler is a bad idea. I'll put a patch up for review to avoid that.", 
            "date_created": "2012-08-01 17:56:34.516563+00:00", 
            "author": "https://api.launchpad.net/1.0/~johannes.erdfelt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/10675", 
            "date_created": "2012-08-01 18:34:39.927846+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/10675\nCommitted: http://github.com/openstack/nova/commit/5de983ae390993450ac182eff2de3f358593f847\nSubmitter: Jenkins\nBranch:    master\n\ncommit 5de983ae390993450ac182eff2de3f358593f847\nAuthor: Johannes Erdfelt <email address hidden>\nDate:   Wed Aug 1 18:19:22 2012 +0000\n\n    Avoid using logging in signal handler\n    \n    Fixes bug 1029727\n    \n    In some cases, logging can grab locks and thusly attempt to reschedule,\n    which will fail in signal handlers.\n    \n    This patch simplifies the signal handlers for multi-process support to\n    only reset the signal handlers and then raise an exception. This will\n    move all logging and other cleanup out of the signal handler and into\n    safer places.\n    \n    Change-Id: I0878adf6ef5c54e577ab2ea39e2ff9845e0e9191\n", 
            "date_created": "2012-08-01 19:37:55.612394+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Adding glance since I noticed it has the same problem from perusing the code", 
            "date_created": "2012-09-21 17:01:46.013169+00:00", 
            "author": "https://api.launchpad.net/1.0/~johannes.erdfelt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/22760", 
            "date_created": "2013-02-22 23:24:35.792960+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/22760\nCommitted: http://github.com/openstack/glance/commit/42ae9dae931e2afebf916d82b39cc3481b300718\nSubmitter: Jenkins\nBranch:    master\n\ncommit 42ae9dae931e2afebf916d82b39cc3481b300718\nAuthor: Mark J. Washenberger <email address hidden>\nDate:   Fri Feb 22 14:59:02 2013 -0800\n\n    Avoid using logging in signal handlers\n    \n    Fixes bug 1029727\n    \n    It is possible for logging to cause problems during signal handlers.\n    Fortunately, the logging we have in signal handlers is minimal and does\n    not add enough value to try to keep through workarounds.\n    \n    Change-Id: I80708874c9740c906e5a49e0d9c1b45174470a85\n", 
            "date_created": "2013-02-23 20:09:09.716875+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
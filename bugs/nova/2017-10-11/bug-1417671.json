{
    "status": "Fix Released", 
    "last_updated": "2015-04-30 09:22:32.886632+00:00", 
    "description": "I'm running nova trunk, commit 752954a.\n\nI configured a flavor with two vcpus and extra specs \"hw:cpu_policy=dedicated\" in order to enable vcpu pinning.\n\nI booted up an instance with this flavor, and \"virsh dumpxml\" shows that the two vcpus were affined suitably to host cpus, but the emulator thread was left to float across the available host cores on that numa node.\n\n  <cputune>\n    <shares>2048</shares>\n        <vcpupin vcpu='0' policy='other' priority='0' cpuset='4'/>\n        <vcpupin vcpu='1' policy='other' priority='0' cpuset='5'/>\n    <emulatorpin cpuset='3-11'/>\n  </cputune>\n\n\n\nLooking at the kvm process shortly after creation, we see quite a few emulator threads running with the emulatorpin affinity:\n\ncompute-2:~$ taskset -apc 136143\npid 136143's current affinity list: 3-11\npid 136144's current affinity list: 0,3-24,27-47\npid 136146's current affinity list: 4\npid 136147's current affinity list: 5\npid 136149's current affinity list: 0\npid 136433's current affinity list: 3-11\npid 136434's current affinity list: 3-11\npid 136435's current affinity list: 3-11\npid 136436's current affinity list: 3-11\npid 136437's current affinity list: 3-11\npid 136438's current affinity list: 3-11\npid 136439's current affinity list: 3-11\npid 136440's current affinity list: 3-11\npid 136441's current affinity list: 3-11\npid 136442's current affinity list: 3-11\npid 136443's current affinity list: 3-11\npid 136444's current affinity list: 3-11\npid 136445's current affinity list: 3-11\npid 136446's current affinity list: 3-11\npid 136447's current affinity list: 3-11\npid 136448's current affinity list: 3-11\npid 136449's current affinity list: 3-11\npid 136450's current affinity list: 3-11\npid 136451's current affinity list: 3-11\npid 136452's current affinity list: 3-11\npid 136453's current affinity list: 3-11\npid 136454's current affinity list: 3-11\n\n\nSince the purpose of \"hw:cpu_policy=dedicated\" is to provide a dedicated host CPU for each guest CPU, the libvirt emulatorpin cpuset for a given guest should be set to one (or possibly more) of the CPUs specified for that guest.  Otherwise, any work done by the emulator threads could rob CPU time from another guest instance.\n\nPersonally I'd like to see the emulator thread affined the same as guest vCPU 0 (we use guest vCPU0 as a maintenance processor while doing the \"real work\" on the other vCPUs), but an argument could be made that it should be affined to the logical OR of all the guest vCPU cpusets.", 
    "tags": [
        "compute", 
        "juno-backport-potential"
    ], 
    "importance": "Medium", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1417671", 
    "owner": "https://api.launchpad.net/1.0/~berrange", 
    "id": 1417671, 
    "index": 4140, 
    "created": "2015-02-03 17:24:32.282619+00:00", 
    "title": "when using dedicated cpus, the emulator thread should be affined as well", 
    "comments": [
        {
            "content": "I'm running nova trunk, commit 752954a.\n\nI configured a flavor with two vcpus and extra specs \"hw:cpu_policy=dedicated\" in order to enable vcpu pinning.\n\nI booted up an instance with this flavor, and \"virsh dumpxml\" shows that the two vcpus were affined suitably to host cpus, but the emulator thread was left to float across the available host cores on that numa node.\n\n  <cputune>\n    <shares>2048</shares>\n        <vcpupin vcpu='0' policy='other' priority='0' cpuset='4'/>\n        <vcpupin vcpu='1' policy='other' priority='0' cpuset='5'/>\n    <emulatorpin cpuset='3-11'/>\n  </cputune>\n\n\n\nLooking at the kvm process shortly after creation, we see quite a few emulator threads running with the emulatorpin affinity:\n\ncompute-2:~$ taskset -apc 136143\npid 136143's current affinity list: 3-11\npid 136144's current affinity list: 0,3-24,27-47\npid 136146's current affinity list: 4\npid 136147's current affinity list: 5\npid 136149's current affinity list: 0\npid 136433's current affinity list: 3-11\npid 136434's current affinity list: 3-11\npid 136435's current affinity list: 3-11\npid 136436's current affinity list: 3-11\npid 136437's current affinity list: 3-11\npid 136438's current affinity list: 3-11\npid 136439's current affinity list: 3-11\npid 136440's current affinity list: 3-11\npid 136441's current affinity list: 3-11\npid 136442's current affinity list: 3-11\npid 136443's current affinity list: 3-11\npid 136444's current affinity list: 3-11\npid 136445's current affinity list: 3-11\npid 136446's current affinity list: 3-11\npid 136447's current affinity list: 3-11\npid 136448's current affinity list: 3-11\npid 136449's current affinity list: 3-11\npid 136450's current affinity list: 3-11\npid 136451's current affinity list: 3-11\npid 136452's current affinity list: 3-11\npid 136453's current affinity list: 3-11\npid 136454's current affinity list: 3-11\n\n\nSince the purpose of \"hw:cpu_policy=dedicated\" is to provide a dedicated host CPU for each guest CPU, the libvirt emulatorpin cpuset for a given guest should be set to one (or possibly more) of the CPUs specified for that guest.  Otherwise, any work done by the emulator threads could rob CPU time from another guest instance.\n\nPersonally I'd like to see the emulator thread affined the same as guest vCPU 0 (we use guest vCPU0 as a maintenance processor while doing the \"real work\" on the other vCPUs), but an argument could be made that it should be affined to the logical OR of all the guest vCPU cpusets.", 
            "date_created": "2015-02-03 17:24:32.282619+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/154580", 
            "date_created": "2015-02-10 17:54:42.092787+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "> Personally I'd like to see the emulator thread affined the same as guest vCPU 0 (we use guest vCPU0 as a maintenance\n>  processor while doing the \"real work\" on the other vCPUs), but an argument could be made that it should be affined to the\n>  logical OR of all the guest vCPU cpusets.\n\nThe design intention from the original blueprint was to use the union of the vCPU cpusets, so that's what my proposed fix does.\n\nIt is certainly valid to file a wishlist bug to support other configurable policies though. There are at least 3 credible alternative policies and no one solution will be perfect for all use cases.", 
            "date_created": "2015-02-10 17:57:59.412136+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/154845", 
            "date_created": "2015-02-11 12:47:25.062577+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/154846", 
            "date_created": "2015-02-11 12:47:34.558139+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/154845\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=6e7f916aac885615d8cef5b0df52d24919836cc3\nSubmitter: Jenkins\nBranch:    master\n\ncommit 6e7f916aac885615d8cef5b0df52d24919836cc3\nAuthor: Daniel P. Berrange <email address hidden>\nDate:   Wed Feb 11 12:19:49 2015 +0000\n\n    libvirt: Fix logically inconsistent host NUMA topology\n    \n    The _fake_caps_numa_topology() sets up a fake host NUMA topology\n    that the tests will use. The definition it has setup though is\n    logically inconsistent. The setup as 4 nodes with 2 CPUs in each\n    cell, but it then says there are 2 cores per socket and 2 thread\n    siblings per core. This implies 4 CPUs per cell, not 2. Either\n    the topology should have 2 cores with 1 thread, or 1 core with\n    2 threads. The test cases using this topology are written to\n    treat it as meaning 1 core with 2 threads, so fix it according\n    to this interpretation\n    \n    Related-bug: 1417671\n    Change-Id: I9d7928a6f9b15f8eb29ef92606e6ca1d2b688a6e\n", 
            "date_created": "2015-02-16 08:54:24.118922+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/154846\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=face0fd9251c923c9aecaa19db9779f908840b5d\nSubmitter: Jenkins\nBranch:    master\n\ncommit face0fd9251c923c9aecaa19db9779f908840b5d\nAuthor: Daniel P. Berrange <email address hidden>\nDate:   Wed Feb 11 12:25:38 2015 +0000\n\n    libvirt: rewrite NUMA topology generator to be more flexible\n    \n    The current _fake_caps_numa_topology method returns a NUMA topology\n    with 4 cells, 1 socket per cell, 1 core per socket & 2 threads per\n    core.\n    \n    Rewrite the method to it can generate topologies with arbitrary\n    values for cells, sockets, cores & threads.\n    \n    Related-bug: 1417671\n    Change-Id: I94d5b1eeb34d6681683996a5d01033f12eb7c5b8\n", 
            "date_created": "2015-02-16 09:14:44.918675+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/154580\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=353e823cc31a62464049c6abbc62a67152e64bae\nSubmitter: Jenkins\nBranch:    master\n\ncommit 353e823cc31a62464049c6abbc62a67152e64bae\nAuthor: Daniel P. Berrange <email address hidden>\nDate:   Tue Feb 10 17:46:15 2015 +0000\n\n    libvirt: fix emulator thread pinning when doing strict CPU pinning\n    \n    When guest vCPUs are confined to particular set of host pCPUs, the\n    emulator threads from QEMU are intended to be pinned to the union\n    of the host pCPUs that the vCPUs are associated with.\n    \n    The code in Nova was in fact confining the emulator threads to the\n    union of pCPUs in the host NUMA nodes that the guest was confined\n    to. When running guests in NUMA mode with shared CPUs / overcommit\n    this was functionally identical, but when running guests with\n    dedicated CPUs this was incorrect.\n    \n    The (incorrect) libvirt config being generated was\n    \n      <cputune>\n          <shares>4096</shares>\n          <vcpupin vcpu='0' cpuset='0'/>\n          <vcpupin vcpu='1' cpuset='1'/>\n          <vcpupin vcpu='2' cpuset='4'/>\n          <vcpupin vcpu='3' cpuset='5'/>\n          <emulatorpin cpuset='0-5'/>\n      </cputune>\n    \n    when it should have been\n    \n      <cputune>\n          <shares>4096</shares>\n          <vcpupin vcpu='0' cpuset='0'/>\n          <vcpupin vcpu='1' cpuset='1'/>\n          <vcpupin vcpu='2' cpuset='4'/>\n          <vcpupin vcpu='3' cpuset='5'/>\n          <emulatorpin cpuset='0-1,4-5'/>\n      </cputune>\n    \n    This was not caught because the unit tests were only checking with\n    a host numa topology that had the same number of cpus as the guest\n    topology. The test is changed to make the host topology massive\n    compared to guest topology\n    \n    Closes-bug: 1417671\n    Change-Id: I81e0fa1d9b09ec2df2af5e21c7d95b21be435f90\n", 
            "date_created": "2015-02-16 09:15:09.969208+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
{
    "status": "Fix Released", 
    "last_updated": "2011-09-22 12:57:59.975736+00:00", 
    "description": "Nova floating IP assignment has a bug. If floating IP is associated to an instance & user accidentally tries to associate the same floating IP to another instance, Nova doesn't check if the floating IP is already associated, but goes & blindly binds the floating IP to the 2nd instance as well. For Floating IP since nova sets up IPtables NAT tables in Linux, there are two sets of rules that are formed in NAT. One for the old stale entries & another one new. Here is a quick experiment I did to show this:\n\n(spawned 2 instances)\n$ euca-describe-instances\nINSTANCE i-00000330 ami-00000017 170.70.0.14 170.70.0.14 running zadarakp (zadara, ubuntu-sata-41) 0 m1.small 2011-08-24T07:13:37Z nova aki-00000015 ari-00000016\nINSTANCE i-00000331 ami-00000017 170.70.0.16 170.70.0.16 running zadarakp (zadara, ubuntu-sata-41) 1 m1.small 2011-08-24T07:13:37Z nova aki-00000015 ari-00000016\n\n(took one free floating IP & associate to instance 330)\n$ euca-associate-address --instance i-00000330 10.0.0.234\nADDRESS 10.0.0.234 i-00000330\n\n(iptables at this point has got the following DNAT & SNAT entries, which is correct)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\n\n(at this point I blindly did associate of the same floating IP to 2nd instance 331. Nova did the job without error)\n$ euca-associate-address --instance i-00000331 10.0.0.234\nADDRESS 10.0.0.234 i-00000331\n\n(iptables at this point is messed up. the old entries have become stale & new entries have been added corresponding to 2nd instance)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.16\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\nSNAT all -- 170.70.0.16 anywhere to:10.0.0.234\n\n(Now even if I disassociate the floating IP from 2nd instance, the stale entries for 1st instance remains)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\n\n(I cannot disassicate any more as nova knows the address is not associated anywhere). So, virtually the iptables is messed up.\n\nNow if user creates another instance & associate, still by nature of IPtables rules (top-down-order), the first stale entry gets privilege to process & route to a stale endpoint. This is very problematic.\n\nThe primary bug in nova is nova/network/manager.py associate_floating_ip() blindly does\nself.driver.ensure_floating_forward(floating_address, fixed_address)\n\nfor example in linux_net.py ensure_floating_forward() blindly does\n    for chain, rule in floating_forward_rules(floating_ip, fixed_ip):\n        iptables_manager.ipv4['nat'].add_rule(chain, rule)\n    iptables_manager.apply()\n\nall this without checking if entries were already setup. Fix for this could be either of:\noption-1: when user tries to associate address, see in API if its already associated & if so reject the 2nd request\noption-2: do remove_floating_forward() in all cases before doing ensure_floating_forward().", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/832598", 
    "owner": "https://api.launchpad.net/1.0/~sleepsonthefloor", 
    "id": 832598, 
    "index": 2522, 
    "created": "2011-08-24 08:37:44.197735+00:00", 
    "title": "Associate floating IP doesnt care if IP is already assigned to another instance", 
    "comments": [
        {
            "content": "Nova floating IP assignment has a bug. If floating IP is associated to an instance & user accidentally tries to associate the same floating IP to another instance, Nova doesn't check if the floating IP is already associated, but goes & blindly binds the floating IP to the 2nd instance as well. For Floating IP since nova sets up IPtables NAT tables in Linux, there are two sets of rules that are formed in NAT. One for the old stale entries & another one new. Here is a quick experiment I did to show this:\n\n(spawned 2 instances)\n$ euca-describe-instances\nINSTANCE i-00000330 ami-00000017 170.70.0.14 170.70.0.14 running zadarakp (zadara, ubuntu-sata-41) 0 m1.small 2011-08-24T07:13:37Z nova aki-00000015 ari-00000016\nINSTANCE i-00000331 ami-00000017 170.70.0.16 170.70.0.16 running zadarakp (zadara, ubuntu-sata-41) 1 m1.small 2011-08-24T07:13:37Z nova aki-00000015 ari-00000016\n\n(took one free floating IP & associate to instance 330)\n$ euca-associate-address --instance i-00000330 10.0.0.234\nADDRESS 10.0.0.234 i-00000330\n\n(iptables at this point has got the following DNAT & SNAT entries, which is correct)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\n\n(at this point I blindly did associate of the same floating IP to 2nd instance 331. Nova did the job without error)\n$ euca-associate-address --instance i-00000331 10.0.0.234\nADDRESS 10.0.0.234 i-00000331\n\n(iptables at this point is messed up. the old entries have become stale & new entries have been added corresponding to 2nd instance)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.16\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\nSNAT all -- 170.70.0.16 anywhere to:10.0.0.234\n\n(Now even if I disassociate the floating IP from 2nd instance, the stale entries for 1st instance remains)\nDNAT all -- anywhere 10.0.0.234 to:170.70.0.14\nSNAT all -- 170.70.0.14 anywhere to:10.0.0.234\n\n(I cannot disassicate any more as nova knows the address is not associated anywhere). So, virtually the iptables is messed up.\n\nNow if user creates another instance & associate, still by nature of IPtables rules (top-down-order), the first stale entry gets privilege to process & route to a stale endpoint. This is very problematic.\n\nThe primary bug in nova is nova/network/manager.py associate_floating_ip() blindly does\nself.driver.ensure_floating_forward(floating_address, fixed_address)\n\nfor example in linux_net.py ensure_floating_forward() blindly does\n    for chain, rule in floating_forward_rules(floating_ip, fixed_ip):\n        iptables_manager.ipv4['nat'].add_rule(chain, rule)\n    iptables_manager.apply()\n\nall this without checking if entries were already setup. Fix for this could be either of:\noption-1: when user tries to associate address, see in API if its already associated & if so reject the 2nd request\noption-2: do remove_floating_forward() in all cases before doing ensure_floating_forward().", 
            "date_created": "2011-08-24 08:37:44.197735+00:00", 
            "author": "https://api.launchpad.net/1.0/~shyam-zadarastorage"
        }
    ]
}
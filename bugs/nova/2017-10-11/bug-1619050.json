{
    "status": "Fix Released", 
    "last_updated": "2017-03-29 16:30:35.898169+00:00", 
    "description": "Nova scheduler has the ability to track instances per hypervisor:\nhttps://github.com/openstack/nova/commit/82cc056fb7e1b081a733797ed27550343cbaf44c\n\nThere is a bug in the logic used to load the instance infos per hypervisor. This specific line [1] found in the \"_add_instance_info\" function uses \"objects.InstanceList.get_by_host(context, host_name)\" to load instances. This means ALL instances managed by that single nova-compute host will be loaded in memory.\n\nThis logic is faulty because the \"get_all_host_states\" function loops on all compute nodes (hypervisors) and call _add_instance_info for each of them. This means the \"_add_instance_info\" function should be loading instances for a specific host AND hypervisor. \"objects.InstanceList.get_by_host_and_node(context, host_name, compute.hypervisor_hostname)\" should be used instead to limit the scope of loaded instances to the specific host/hypervisor tuple.\n\nIf you run Nova in the Ironic context or anything where a single nova-compute host can manage LOT of hypervisors, this means you could load a LOT of data in memory and causing an out-of-memory error or serious performance degradation. For example, if you have 2000 hypervisors (Ironic nodes), the \"_add_instance_info\" function will load 2000 instances per hypervisor (instead of 1) found in get_all_host_states, ending with an overall process loading 2000^2 rows from the database.\n\n[1] https://github.com/openstack/nova/blob/dd44096a04a85319481943c1b2bb2471e752b0b3/nova/scheduler/host_manager.py#L631", 
    "tags": [
        "ironic", 
        "ops", 
        "performance", 
        "scheduler"
    ], 
    "importance": "Low", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1619050", 
    "owner": "https://api.launchpad.net/1.0/~mgagne", 
    "id": 1619050, 
    "index": 799, 
    "created": "2016-08-31 21:16:49.211685+00:00", 
    "title": "Nova scheduler is too greedy when loading instances of hypervisors", 
    "comments": [
        {
            "content": "Nova scheduler has the ability to track instances per hypervisor:\nhttps://github.com/openstack/nova/commit/82cc056fb7e1b081a733797ed27550343cbaf44c\n\nThere is a bug in the logic used to load the instance infos per hypervisor. This specific line [1] found in the \"_add_instance_info\" function uses \"objects.InstanceList.get_by_host(context, host_name)\" to load instances. This means ALL instances managed by that single nova-compute host will be loaded in memory.\n\nThis logic is faulty because the \"get_all_host_states\" function loops on all compute nodes (hypervisors) and call _add_instance_info for each of them. This means the \"_add_instance_info\" function should be loading instances for a specific host AND hypervisor. \"objects.InstanceList.get_by_host_and_node(context, host_name, compute.hypervisor_hostname)\" should be used instead to limit the scope of loaded instances to the specific host/hypervisor tuple.\n\nIf you run Nova in the Ironic context or anything where a single nova-compute host can manage LOT of hypervisors, this means you could load a LOT of data in memory and causing an out-of-memory error or serious performance degradation. For example, if you have 2000 hypervisors (Ironic nodes), the \"_add_instance_info\" function will load 2000 instances per hypervisor (instead of 1) found in get_all_host_states, ending with an overall process loading 2000^2 rows from the database.\n\n[1] https://github.com/openstack/nova/blob/dd44096a04a85319481943c1b2bb2471e752b0b3/nova/scheduler/host_manager.py#L631", 
            "date_created": "2016-08-31 21:16:49.211685+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgagne"
        }, 
        {
            "content": "There is a change in progress that Gerrit didn't report to Launchpad: https://review.openstack.org/#/c/363944/", 
            "date_created": "2016-09-02 14:51:42.735740+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgagne"
        }, 
        {
            "content": "Change abandoned by Mathieu Gagn\u00e9 (<email address hidden>) on branch: master\nReview: https://review.openstack.org/363944\nReason: Fixed in:\nhttps://review.openstack.org/#/c/206736/\nhttps://review.openstack.org/#/c/347948/", 
            "date_created": "2017-02-15 22:09:54.165147+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Should this bug be closed, then, if it was fixed in those patches?", 
            "date_created": "2017-03-29 14:00:13.536763+00:00", 
            "author": "https://api.launchpad.net/1.0/~jim-rollenhagen"
        }
    ]
}
{
    "status": "Fix Released", 
    "last_updated": "2017-06-16 12:13:29.582647+00:00", 
    "description": "http://logs.openstack.org/02/173802/5/check/check-tempest-dsvm-full-ceph/a72aac1/logs/screen-n-api.txt.gz?level=TRACE#_2015-06-11_09_04_19_511\n\n2015-06-11 09:04:19.511 ERROR nova.api.ec2 [req-0ac81d78-2717-4dd2-80e2-d94363b55ac8 EC2VolumesTest-442487008 EC2VolumesTest-1066393631] Unexpected InvalidInput raised: Invalid input received: Invalid volume: Volume still has 1 dependent snapshots. (HTTP 400) (Request-ID: req-4586b5d2-7212-4ddd-af79-43ad8ba7ea58)\n2015-06-11 09:04:19.511 ERROR nova.api.ec2 [req-0ac81d78-2717-4dd2-80e2-d94363b55ac8 EC2VolumesTest-442487008 EC2VolumesTest-1066393631] Environment: {\"HTTP_AUTHORIZATION\": \"AWS4-HMAC-SHA256 Credential=a5e9253350ce4a249ddce8b7c1c798c2/20150611/0/127/aws4_request,SignedHeaders=host;x-amz-date,Signature=304830ed947f7fba3143887b08d1e47faa18d4b59782c0992727cb7593f586b4\", \"SCRIPT_NAME\": \"\", \"REQUEST_METHOD\": \"POST\", \"HTTP_X_AMZ_DATE\": \"20150611T090418Z\", \"PATH_INFO\": \"/\", \"SERVER_PROTOCOL\": \"HTTP/1.0\", \"CONTENT_LENGTH\": \"60\", \"HTTP_USER_AGENT\": \"Boto/2.38.0 Python/2.7.6 Linux/3.13.0-53-generic\", \"RAW_PATH_INFO\": \"/\", \"REMOTE_ADDR\": \"127.0.0.1\", \"wsgi.url_scheme\": \"http\", \"SERVER_PORT\": \"8773\", \"CONTENT_TYPE\": \"application/x-www-form-urlencoded; charset=UTF-8\", \"HTTP_HOST\": \"127.0.0.1:8773\", \"SERVER_NAME\": \"127.0.0.1\", \"GATEWAY_INTERFACE\": \"CGI/1.1\", \"REMOTE_PORT\": \"45819\", \"HTTP_ACCEPT_ENCODING\": \"identity\"}\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRUMyVm9sdW1lc1Rlc3RcIiBBTkQgbWVzc2FnZTpcIlVuZXhwZWN0ZWQgSW52YWxpZElucHV0IHJhaXNlZDogSW52YWxpZCBpbnB1dCByZWNlaXZlZDogSW52YWxpZCB2b2x1bWU6IFZvbHVtZSBzdGlsbCBoYXMgMSBkZXBlbmRlbnQgc25hcHNob3RzXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MzQwMzAyMTUwODd9\n\n10 hits in 7 days, check and gate, hitting on the ceph and glusterfs jobs.", 
    "tags": [
        "ceph"
    ], 
    "importance": "High", 
    "heat": 34, 
    "link": "https://bugs.launchpad.net/nova/+bug/1464259", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1464259, 
    "index": 1760, 
    "created": "2015-06-11 13:45:00.943370+00:00", 
    "title": "Volumes tests fails often with rbd backend", 
    "comments": [
        {
            "content": "http://logs.openstack.org/02/173802/5/check/check-tempest-dsvm-full-ceph/a72aac1/logs/screen-n-api.txt.gz?level=TRACE#_2015-06-11_09_04_19_511\n\n2015-06-11 09:04:19.511 ERROR nova.api.ec2 [req-0ac81d78-2717-4dd2-80e2-d94363b55ac8 EC2VolumesTest-442487008 EC2VolumesTest-1066393631] Unexpected InvalidInput raised: Invalid input received: Invalid volume: Volume still has 1 dependent snapshots. (HTTP 400) (Request-ID: req-4586b5d2-7212-4ddd-af79-43ad8ba7ea58)\n2015-06-11 09:04:19.511 ERROR nova.api.ec2 [req-0ac81d78-2717-4dd2-80e2-d94363b55ac8 EC2VolumesTest-442487008 EC2VolumesTest-1066393631] Environment: {\"HTTP_AUTHORIZATION\": \"AWS4-HMAC-SHA256 Credential=a5e9253350ce4a249ddce8b7c1c798c2/20150611/0/127/aws4_request,SignedHeaders=host;x-amz-date,Signature=304830ed947f7fba3143887b08d1e47faa18d4b59782c0992727cb7593f586b4\", \"SCRIPT_NAME\": \"\", \"REQUEST_METHOD\": \"POST\", \"HTTP_X_AMZ_DATE\": \"20150611T090418Z\", \"PATH_INFO\": \"/\", \"SERVER_PROTOCOL\": \"HTTP/1.0\", \"CONTENT_LENGTH\": \"60\", \"HTTP_USER_AGENT\": \"Boto/2.38.0 Python/2.7.6 Linux/3.13.0-53-generic\", \"RAW_PATH_INFO\": \"/\", \"REMOTE_ADDR\": \"127.0.0.1\", \"wsgi.url_scheme\": \"http\", \"SERVER_PORT\": \"8773\", \"CONTENT_TYPE\": \"application/x-www-form-urlencoded; charset=UTF-8\", \"HTTP_HOST\": \"127.0.0.1:8773\", \"SERVER_NAME\": \"127.0.0.1\", \"GATEWAY_INTERFACE\": \"CGI/1.1\", \"REMOTE_PORT\": \"45819\", \"HTTP_ACCEPT_ENCODING\": \"identity\"}\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRUMyVm9sdW1lc1Rlc3RcIiBBTkQgbWVzc2FnZTpcIlVuZXhwZWN0ZWQgSW52YWxpZElucHV0IHJhaXNlZDogSW52YWxpZCBpbnB1dCByZWNlaXZlZDogSW52YWxpZCB2b2x1bWU6IFZvbHVtZSBzdGlsbCBoYXMgMSBkZXBlbmRlbnQgc25hcHNob3RzXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MzQwMzAyMTUwODd9\n\n10 hits in 7 days, check and gate, hitting on the ceph and glusterfs jobs.", 
            "date_created": "2015-06-11 13:45:00.943370+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Here is the 400 from cinder-api:\n\nhttp://logs.openstack.org/02/173802/5/check/check-tempest-dsvm-full-ceph/a72aac1/logs/screen-c-api.txt.gz#_2015-06-11_09_04_19_506\n\n2015-06-11 09:04:19.506 INFO cinder.volume.api [req-4586b5d2-7212-4ddd-af79-43ad8ba7ea58 EC2VolumesTest-1066393631] Unable to delete volume: 1ca1133b-915c-4f13-b2e5-5acdaa61075b, volume currently has snapshots.\n\nHere is the more specific error in the cinder-volume logs:\n\nhttp://logs.openstack.org/02/173802/5/check/check-tempest-dsvm-full-ceph/a72aac1/logs/screen-c-vol.txt.gz#_2015-06-11_09_01_02_096\n\n2015-06-11 09:01:02.096 ERROR cinder.volume.manager [req-accfae1c-8732-4051-8707-c86e0ced69ee EC2VolumesTest-1066393631] Snapshot(cgsnapshot_id=None,created_at=2015-06-11T09:00:56Z,deleted=False,deleted_at=None,display_description=None,display_name=None,encryption_key_id=None,id=4465feb0-0a59-4aca-9491-36cd797a8329,metadata={},progress='100%',project_id=e3dfaf596203409784675b0aa176135e,provider_id=None,provider_location=None,status='deleting',updated_at=2015-06-11T09:00:57Z,user_id=217de41df5f54ee8b71bb4f88cb388e5,volume=Volume(1ca1133b-915c-4f13-b2e5-5acdaa61075b),volume_id=1ca1133b-915c-4f13-b2e5-5acdaa61075b,volume_size=1,volume_type_id=7902593b-4e35-410d-83b4-f8cf8abcee89)Delete snapshot failed, due to snapshot busy.", 
            "date_created": "2015-06-11 13:52:29.157261+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looks like the rbd driver in cinder just need a retry loop like was added to nova here:\n\nhttps://review.openstack.org/#/c/169446/", 
            "date_created": "2015-06-11 14:01:08.354010+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The glusterfs failure is actually not the issue here, that was a check queue change where everything failed, so this is just the ceph issue.", 
            "date_created": "2015-06-11 14:07:09.432095+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "FWIW, i got a similar (yet different) failure for my patch , the same testcase, but failure in tempest cleanup/teardown\n\nSee http://logs.openstack.org/13/172813/8/check/check-tempest-dsvm-full-ceph/9731e7e/console.html\n\n<snip>\n2015-06-17 09:10:18.541 | {1} tearDownClass (tempest.thirdparty.boto.test_ec2_volumes.EC2VolumesTest) [0.000000s] ... FAILED\n\n2015-06-17 09:13:50.494 | ==============================\n2015-06-17 09:13:50.494 | Failed 1 tests - output below:\n2015-06-17 09:13:50.494 | ==============================\n2015-06-17 09:13:50.494 | \n2015-06-17 09:13:50.495 | tearDownClass (tempest.thirdparty.boto.test_ec2_volumes.EC2VolumesTest)\n2015-06-17 09:13:50.495 | -----------------------------------------------------------------------\n2015-06-17 09:13:50.495 | \n2015-06-17 09:13:50.495 | Captured traceback:\n2015-06-17 09:13:50.495 | ~~~~~~~~~~~~~~~~~~~\n2015-06-17 09:13:50.495 |     Traceback (most recent call last):\n2015-06-17 09:13:50.495 |       File \"tempest/test.py\", line 310, in tearDownClass\n2015-06-17 09:13:50.495 |         six.reraise(etype, value, trace)\n2015-06-17 09:13:50.495 |       File \"tempest/test.py\", line 293, in tearDownClass\n2015-06-17 09:13:50.495 |         teardown()\n2015-06-17 09:13:50.495 |       File \"tempest/thirdparty/boto/test.py\", line 283, in resource_cleanup\n2015-06-17 09:13:50.496 |         raise exceptions.TearDownException(num=fail_count)\n2015-06-17 09:13:50.496 |     tempest.exceptions.TearDownException: 2 cleanUp operation failed\n\n</snip>", 
            "date_created": "2015-06-17 09:38:30.808472+00:00", 
            "author": "https://api.launchpad.net/1.0/~dpkshetty"
        }, 
        {
            "content": "It looks like the root cause (for RBD) is in the test's cleanup resources step, it's not scheduling waits for volume status prior to issuing the next command.  It schedules client.delete_volume , which just checks for 202 response code:\n\n    https://git.openstack.org/cgit/openstack/tempest/tree/tempest/services/volume/json/volumes_client.py#n94\n    https://git.openstack.org/cgit/openstack/tempest/tree/tempest/thirdparty/boto/test_ec2_volumes.py#n67\n\nI suppose these operations are not dependent in the default backend, but for RBD there is enough delay to cause problems.  The snapshot from which the second volume was created is deleted before the new volume completes - which fails.  And then nova attempts to delete the original volume, which fails because a snapshot exists.\n\nIt seems to me that properly scheduling the waits in this test is the right way to go, rather than introducing retries in the driver. ", 
            "date_created": "2015-06-17 18:20:00.397840+00:00", 
            "author": "https://api.launchpad.net/1.0/~jbernard"
        }, 
        {
            "content": "FWIW, same issue as I saw in #4 i encountered today again.\n\nSee http://logs.openstack.org/04/209904/1/check/gate-tempest-dsvm-full-ceph/e2c9c49/console.html#_2015-08-06_13_33_02_575", 
            "date_created": "2015-08-07 05:20:26.276626+00:00", 
            "author": "https://api.launchpad.net/1.0/~dpkshetty"
        }, 
        {
            "content": "This is a top 5 race bug atm, it needs to be addressed", 
            "date_created": "2015-12-03 14:50:27.724684+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/254303", 
            "date_created": "2015-12-07 16:57:40.773940+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Jon Bernard (<email address hidden>) on branch: master\nReview: https://review.openstack.org/254303", 
            "date_created": "2015-12-07 17:51:10.186781+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Discussed this in cinder IRC today:\n\nhttp://eavesdrop.openstack.org/irclogs/%23openstack-cinder/%23openstack-cinder.2015-12-07.log.html#t2015-12-07T18:36:19\n\nIn tracing down http://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/console.html and the ebs test failure, the test basically does this:\n\n1. create a volume from an image\n2. boot a server from that volume\n3. create snapshot from the server (creates image and volume snapshots)\n4. delete the server\n5. create 2nd server from the image snapshot\n6. cleanup\n\nThe failure (race) is when deleting the volume snapshot, there is a backing volume that is dependent on the snapshot, and that's not deleted yet.  That 2nd volume is attached to the 2nd server (nova does this b/c of the image metadata that has the bdm snapshot info in it).\n\nIt sounds like there is a timing issue in the cinder rbd driver when deleting the dependent volume (for the snapshot) such that it's not gone when we try to delete the volume snapshot (it's considered busy), so that fails.\n\nSo as noted in comment 2, it sounds like when we delete a volume snapshot, the cinder rbd driver needs to wait for the backing volume to be deleted (from the ceph backend) before it can delete the volume snapshot.", 
            "date_created": "2015-12-07 19:21:10.209875+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "If the backing volume delete from ceph takes longer than the default 196 seconds (that tempest waits for), then we might need to just increase the delete timeout in the ceph job (maybe to 300 seconds) until https://review.openstack.org/#/c/205282/ lands.", 
            "date_created": "2015-12-07 19:27:36.258292+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "We could also just look at setting rbd_flatten_volume_from_snapshot=True in cinder.conf for the ceph job, since if I understand that code correctly, it should disassociate the backing volume from the snapshot in the ceph backend which should speed this up.", 
            "date_created": "2015-12-07 19:47:16.153168+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Added nova since I think this is a bug in nova.\n\nIn the test_create_ebs_image_and_check_boot test scenario:\n\n1. create volume1 from an image\n2. boot server1 from volume1 (wait for the server to be ACTIVE)\n3. create snapshot from server1 (creates image and volume snapshots)\n4. delete server1\n5. create server2 from the image snapshot (don't wait for it to be ACTIVE)\n6. delete server2 (could still be building/attaching volumes in the background)\n7. cleanup\n\nBecause the image snapshot has the volume1 snapshot BDM information, when we create server2 it constructs a snapshot bdm from the image meta here:\n\nhttps://github.com/openstack/nova/blob/master/nova/compute/api.py#L729\n\nAnd when booting server2 we use that snapshot bdm to create volume2:\n\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L1727\n\nhttps://github.com/openstack/nova/blob/master/nova/virt/block_device.py#L506\n\nhttps://github.com/openstack/nova/blob/master/nova/virt/block_device.py#L391\n\nThat's where we see this logged:\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_06_776\n\n2015-12-07 05:51:06.776 INFO nova.virt.block_device [req-0467c440-1f5b-4a74-9ba8-d266d6b0b182 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] [instance: 91817c91-6305-4e44-9f53-0eca5a27aa8d] Booting with volume None at /dev/vda\n\nSince the snapshot bdm doesn't have a volume_id it logs None.\n\nThis is the volume that's created though:\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_07_467\n\n2015-12-07 05:51:07.467 DEBUG keystoneclient.session [req-0467c440-1f5b-4a74-9ba8-d266d6b0b182 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] RESP: [202] Content-Length: 797 X-Compute-Request-Id: req-05b0da1b-3c95-41cd-a3c2-919b1ca9662d Connection: keep-alive Date: Mon, 07 Dec 2015 05:51:07 GMT Content-Type: application/json X-Openstack-Request-Id: req-05b0da1b-3c95-41cd-a3c2-919b1ca9662d \nRESP BODY: {\"volume\": {\"status\": \"creating\", \"user_id\": \"f3dc192b88054a3d80283f9696183e4a\", \"attachments\": [], \"links\": [{\"href\": \"http://127.0.0.1:8776/v2/37d3bb6fbd8b459ebd1459afe434900a/volumes/19efe5d8-fa71-4569-b806-dfe2e0080b7f\", \"rel\": \"self\"}, {\"href\": \"http://127.0.0.1:8776/37d3bb6fbd8b459ebd1459afe434900a/volumes/19efe5d8-fa71-4569-b806-dfe2e0080b7f\", \"rel\": \"bookmark\"}], \"availability_zone\": \"nova\", \"bootable\": \"false\", \"encrypted\": false, \"created_at\": \"2015-12-07T05:51:07.178269\", \"description\": \"\", \"updated_at\": null, \"volume_type\": \"ceph\", \"name\": \"\", \"replication_status\": \"disabled\", \"consistencygroup_id\": null, \"source_volid\": null, \"snapshot_id\": \"366fab34-8494-47ee-925e-1505b7521744\", \"multiattach\": false, \"metadata\": {}, \"id\": \"19efe5d8-fa71-4569-b806-dfe2e0080b7f\", \"size\": 1}}\n _http_log_response /usr/local/lib/python2.7/dist-packages/keystoneclient/session.py:216\n\nSince the test immediately deletes the volume after it's created (but before it's fully active), we see things in the logs like this:\n\n2015-12-07 05:51:15.251 DEBUG nova.compute.claims [req-0467c440-1f5b-4a74-9ba8-d266d6b0b182 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] [instance: 91817c91-6305-4e44-9f53-0eca5a27aa8d] Aborting claim: [Claim: 64 MB memory, 0 GB disk] abort /opt/stack/new/nova/nova/compute/claims.py:120\n\n2015-12-07 05:51:15.280 DEBUG nova.compute.utils [req-0467c440-1f5b-4a74-9ba8-d266d6b0b182 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] [instance: 91817c91-6305-4e44-9f53-0eca5a27aa8d] Conflict updating instance 91817c91-6305-4e44-9f53-0eca5a27aa8d. Expected: {'task_state': [u'block_device_mapping']}. Actual: {'task_state': u'deleting'} notify_about_instance_usage /opt/stack/new/nova/nova/compute/utils.py:285\n\nAnd we fail to detach the volume because we're checking for bdm.volume_id rather than snapshot_id:\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_16_600\n\n2015-12-07 05:51:16.600 DEBUG nova.compute.manager [req-6d0c06c0-2479-4854-87de-c9e4fc5d5cb4 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] [instance: 91817c91-6305-4e44-9f53-0eca5a27aa8d] Ignoring VolumeNotFound: Volume None could not be found. _shutdown_instance /opt/stack/new/nova/nova/compute/manager.py:2317\n\nAnd because our bdm does not have volume_id set, we don't delete volume2 in cinder:\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_16_651\n\n2015-12-07 05:51:16.651 DEBUG nova.compute.manager [req-6d0c06c0-2479-4854-87de-c9e4fc5d5cb4 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] [instance: 91817c91-6305-4e44-9f53-0eca5a27aa8d] terminating bdm BlockDeviceMapping(boot_index=0,connection_info='null',created_at=2015-12-07T05:51:06Z,delete_on_termination=True,deleted=False,deleted_at=None,destination_type='volume',device_name='/dev/vda',device_type='disk',disk_bus='virtio',guest_format=None,id=101,image_id=None,instance=<?>,instance_uuid=91817c91-6305-4e44-9f53-0eca5a27aa8d,no_device=False,snapshot_id='366fab34-8494-47ee-925e-1505b7521744',source_type='snapshot',updated_at=2015-12-07T05:51:06Z,volume_id=None,volume_size=1) _cleanup_volumes /opt/stack/new/nova/nova/compute/manager.py:2341", 
            "date_created": "2015-12-07 22:05:23.465825+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This is where we get the VolumeNotFound because we're passing bdm.volume_id of None to volume_api.delete:\n\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L2316\n\nAnd this is where we log that we're terminating the BDM but we're not actually since volume_id isn't set on it:\n\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L2342", 
            "date_created": "2015-12-07 22:06:54.211959+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "volume2 (created from the volume snapshot) is attached to server2 here:\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_12_600\n\n2015-12-07 05:51:12.600 DEBUG keystoneclient.session [req-0467c440-1f5b-4a74-9ba8-d266d6b0b182 tempest-TestVolumeBootPatternV2-1635783358 tempest-TestVolumeBootPatternV2-232134174] REQ: curl -g -i -X POST http://127.0.0.1:8776/v2/37d3bb6fbd8b459ebd1459afe434900a/volumes/19efe5d8-fa71-4569-b806-dfe2e0080b7f/action -H \"User-Agent: python-cinderclient\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -H \"X-Auth-Token: {SHA1}3f5cbbbb6773d2637527121a8ae0faa71adbc434\" -d '{\"os-attach\": {\"instance_uuid\": \"91817c91-6305-4e44-9f53-0eca5a27aa8d\", \"mountpoint\": \"/dev/vda\", \"mode\": \"rw\"}}' _http_log_request /usr/local/lib/python2.7/dist-packages/keystoneclient/session.py:198\n\nWe set the volume_id on the bdm here:\n\nhttps://github.com/openstack/nova/blob/master/nova/virt/block_device.py#L396\n\nAnd that would be updated in the database once the attach method is done.\n\nWe start deleting the instance here (1 second later):\n\nhttp://logs.openstack.org/20/218120/3/check/gate-tempest-dsvm-full-ceph/2349f2d/logs/screen-n-cpu.txt.gz#_2015-12-07_05_51_13_998\n\nBut I'm still not sure why the snapshot bdm isn't updated with the volume_id in the database after we created it.  Maybe we should save that off immediately before trying to attach the volume in case we need to teardown.", 
            "date_created": "2015-12-07 22:27:52.316703+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/254428", 
            "date_created": "2015-12-07 22:49:54.170835+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/254428\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4f2a46987cf705d5dea84e97ef2006342cc5d9c4\nSubmitter: Jenkins\nBranch:    master\n\ncommit 4f2a46987cf705d5dea84e97ef2006342cc5d9c4\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Dec 7 14:49:18 2015 -0800\n\n    Make sure bdm.volume_id is set after auto-creating volumes\n    \n    The test_create_ebs_image_and_check_boot test in Tempest does the\n    following:\n    \n    1. create volume1 from an image\n    2. boot server1 from volume1 with delete_on_termination=True and wait\n       for the server to be ACTIVE\n    3. create snapshot from server1 (creates image and volume snapshots)\n    4. delete server1\n    5. create server2 from the image snapshot (don't wait for it to be\n       ACTIVE - this auto-creates volume2 from the volume snapshot in\n       cinder and attaches server2 to it)\n    6. delete server2 (could still be building/attaching volumes in the\n       background)\n    7. cleanup\n    \n    There is a race when booting server2, which creates and attaches\n    volume2, and deleting server2 before it's active.\n    \n    The volume attach completes and updates the bdm.volume_id in the DB before\n    we get to _shutdown_instance, but after the delete request is in the API.\n    The compute API gets potentially stale BDMs and passes those over RPC to\n    the compute. So we add a check in _shutdown_instance to see if we have\n    potentially stale volume BDMs and refresh that list if so.\n    \n    The instance.uuid locks in build_and_run_instance and terminate_instance\n    create the mutex on the compute host such that the bdm.volume_id should\n    be set in the database after the volume attach and before terminate_instance\n    gets the lock. The bdm.volume_id could still be None in _shutdown_instance\n    if the volume create fails, but we don't have anything to teardown in cinder\n    in that case anyway.\n    \n    In the case of the race bug, deleting the volume snapshot in cinder fails\n    because volume2 was never deleted by nova, so the test fails in\n    teardown. Note that there is still potential for a race here, this does\n    not eliminate it, but should narrow the race window.\n    \n    This also cleans up the logging in attach_block_devices since there\n    may not be a volume_id at that point (depending on bdm.source_type).\n    \n    Closes-Bug: #1464259\n    \n    Change-Id: Ib60d60a5af35be89ad8afbcf44fcffe0b0ce2876\n", 
            "date_created": "2015-12-12 00:06:47.845429+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/258118", 
            "date_created": "2015-12-15 19:19:23.110048+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/258695", 
            "date_created": "2015-12-16 22:40:59.844526+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/258118\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=e8693e995daae1ed395e6f0d895674d86b76b4c2\nSubmitter: Jenkins\nBranch:    master\n\ncommit e8693e995daae1ed395e6f0d895674d86b76b4c2\nAuthor: Matt Riedemann <email address hidden>\nDate:   Tue Dec 15 11:18:49 2015 -0800\n\n    Refresh stale volume BDMs in terminate_connection\n    \n    Change Ib60d60a5af35be89ad8afbcf44fcffe0b0ce2876 meant\n    to address a race in deleting an instance while booting from\n    volume such that the BDMs passed to terminate_instance from\n    the API were stale (no volume_id set yet in the database).\n    \n    Without the volume_id, _shutdown_instance can't detach the\n    volumes and _cleanup_volumes can't delete them.\n    \n    The change fixed the race in _shutdown_instance by refreshing\n    the BDM list if necessary, but it didn't fix the problem of\n    deleting the volumes in _cleanup_volumes, which is still\n    getting a stale list of BDMs from the compute API.\n    \n    This moves the refresh BDM logic to the top-level method\n    terminate_instance so that the refreshed BDMs are passed\n    to both _shutdown_instance and _cleanup_volumes.\n    \n    There is some cleanup that can be done with\n    _delete_instance but since we'd like to backport this change\n    and the previous one, the cleanup is left for a later patch.\n    \n    Change-Id: Ib0980a92ebdfa9c83496cd29bfea47fc42d538c5\n    Closes-Bug: #1464259\n", 
            "date_created": "2015-12-17 20:34:59.313064+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Just a heads up,\n\nwe hit this in Glance's stable/liberty.\n\n\nhttp://logs.openstack.org/14/258314/2/check/gate-tempest-dsvm-full-ceph/1222c6a/console.html#_2015-12-18_03_40_06_979\n\n\n2015-12-18 03:40:06.979 | Captured traceback-1:\n2015-12-18 03:40:06.979 | ~~~~~~~~~~~~~~~~~~~~~\n2015-12-18 03:40:06.979 |     Traceback (most recent call last):\n2015-12-18 03:40:06.979 |       File \"tempest/scenario/manager.py\", line 104, in delete_wrapper\n2015-12-18 03:40:06.979 |         delete_thing(*args, **kwargs)\n2015-12-18 03:40:06.979 |       File \"tempest/services/volume/base/base_volumes_client.py\", line 106, in delete_volume\n2015-12-18 03:40:06.979 |         resp, body = self.delete(\"volumes/%s\" % str(volume_id))\n2015-12-18 03:40:06.979 |       File \"/opt/stack/new/tempest/.tox/all/local/lib/python2.7/site-packages/tempest_lib/common/rest_client.py\", line 290, in delete\n2015-12-18 03:40:06.980 |         return self.request('DELETE', url, extra_headers, headers, body)\n2015-12-18 03:40:06.980 |       File \"/opt/stack/new/tempest/.tox/all/local/lib/python2.7/site-packages/tempest_lib/common/rest_client.py\", line 639, in request\n2015-12-18 03:40:06.980 |         resp, resp_body)\n2015-12-18 03:40:06.980 |       File \"/opt/stack/new/tempest/.tox/all/local/lib/python2.7/site-packages/tempest_lib/common/rest_client.py\", line 697, in _error_checker\n2015-12-18 03:40:06.980 |         raise exceptions.BadRequest(resp_body, resp=resp)\n2015-12-18 03:40:06.980 |     tempest_lib.exceptions.BadRequest: Bad request\n2015-12-18 03:40:06.980 |     Details: {u'message': u'Invalid volume: Volume still has 1 dependent snapshots.', u'code': 400}", 
            "date_created": "2015-12-18 12:11:33.881150+00:00", 
            "author": "https://api.launchpad.net/1.0/~flaper87"
        }, 
        {
            "content": "Flavio, there is a backport to stable/liberty proposed.", 
            "date_created": "2015-12-21 22:23:22.540112+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "So far since the nova change merged in master, logstash is looking better:\n\nhttp://status.openstack.org/elastic-recheck/index.html#1464259\n\nThere is still a race here, but it seems to have been mitigated a bit.", 
            "date_created": "2015-12-21 22:24:43.749875+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/258695\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=288c87f531d8ffbaeae3923335077b1968174a22\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit 288c87f531d8ffbaeae3923335077b1968174a22\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Dec 7 14:49:18 2015 -0800\n\n    Refresh stale volume BDMs in terminate_connection\n    \n    NOTE(mriedem): This is a two-part change that is squashed for stable,\n    where the second change basically moves the code from the first change\n    up a level to actually fix the bug in nova, else it was only partially\n    fixed.\n    \n    --\n    \n    Make sure bdm.volume_id is set after auto-creating volumes\n    \n    The test_create_ebs_image_and_check_boot test in Tempest does the\n    following:\n    \n    1. create volume1 from an image\n    2. boot server1 from volume1 with delete_on_termination=True and wait\n       for the server to be ACTIVE\n    3. create snapshot from server1 (creates image and volume snapshots)\n    4. delete server1\n    5. create server2 from the image snapshot (don't wait for it to be\n       ACTIVE - this auto-creates volume2 from the volume snapshot in\n       cinder and attaches server2 to it)\n    6. delete server2 (could still be building/attaching volumes in the\n       background)\n    7. cleanup\n    \n    There is a race when booting server2, which creates and attaches\n    volume2, and deleting server2 before it's active.\n    \n    The volume attach completes and updates the bdm.volume_id in the DB before\n    we get to _shutdown_instance, but after the delete request is in the API.\n    The compute API gets potentially stale BDMs and passes those over RPC to\n    the compute. So we add a check in _shutdown_instance to see if we have\n    potentially stale volume BDMs and refresh that list if so.\n    \n    The instance.uuid locks in build_and_run_instance and terminate_instance\n    create the mutex on the compute host such that the bdm.volume_id should\n    be set in the database after the volume attach and before terminate_instance\n    gets the lock. The bdm.volume_id could still be None in _shutdown_instance\n    if the volume create fails, but we don't have anything to teardown in cinder\n    in that case anyway.\n    \n    In the case of the race bug, deleting the volume snapshot in cinder fails\n    because volume2 was never deleted by nova, so the test fails in\n    teardown. Note that there is still potential for a race here, this does\n    not eliminate it, but should narrow the race window.\n    \n    This also cleans up the logging in attach_block_devices since there\n    may not be a volume_id at that point (depending on bdm.source_type).\n    \n    Conflicts:\n            nova/compute/manager.py\n    \n    NOTE(mriedem): The conflict is due to 48a07fe48 not being in liberty.\n    \n    Closes-Bug: #1464259\n    \n    Change-Id: Ib60d60a5af35be89ad8afbcf44fcffe0b0ce2876\n    (cherry picked from commit 4f2a46987cf705d5dea84e97ef2006342cc5d9c4)\n    \n    *** This is the 2nd commit message:\n    \n    Refresh stale volume BDMs in terminate_connection\n    \n    Change Ib60d60a5af35be89ad8afbcf44fcffe0b0ce2876 meant\n    to address a race in deleting an instance while booting from\n    volume such that the BDMs passed to terminate_instance from\n    the API were stale (no volume_id set yet in the database).\n    \n    Without the volume_id, _shutdown_instance can't detach the\n    volumes and _cleanup_volumes can't delete them.\n    \n    The change fixed the race in _shutdown_instance by refreshing\n    the BDM list if necessary, but it didn't fix the problem of\n    deleting the volumes in _cleanup_volumes, which is still\n    getting a stale list of BDMs from the compute API.\n    \n    This moves the refresh BDM logic to the top-level method\n    terminate_instance so that the refreshed BDMs are passed\n    to both _shutdown_instance and _cleanup_volumes.\n    \n    There is some cleanup that can be done with\n    _delete_instance but since we'd like to backport this change\n    and the previous one, the cleanup is left for a later patch.\n    \n    Conflicts:\n            nova/compute/manager.py\n            nova/tests/unit/compute/test_compute_mgr.py\n    \n    NOTE(mriedem): The manager.py conflict is due to the timer() usage\n    not being in liberty. The test_compute_mgr.py conflict is due to\n    test_reset_reloads_rpcapi not being in liberty.\n    \n    Change-Id: Ib0980a92ebdfa9c83496cd29bfea47fc42d538c5\n    Closes-Bug: #1464259\n    (cherry picked from commit e8693e995daae1ed395e6f0d895674d86b76b4c2)\n", 
            "date_created": "2016-01-09 12:06:58.661382+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 12.0.1 release.", 
            "date_created": "2016-01-19 15:34:03.080810+00:00", 
            "author": "https://api.launchpad.net/1.0/~doug-hellmann"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 12.0.1 release.", 
            "date_created": "2016-01-19 15:34:05.863738+00:00", 
            "author": "https://api.launchpad.net/1.0/~doug-hellmann"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.0.0.0b2 development milestone.", 
            "date_created": "2016-01-21 13:43:24.293171+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.0.0.0b2 development milestone.", 
            "date_created": "2016-01-21 13:43:35.094000+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Puppet OpenStack CI is now hitting the bug since we try to deploy Glance/Nova/Cinder with RBD backend:\n\nhttp://logs.openstack.org/24/281624/9/check/gate-puppet-openstack-integration-scenario001-tempest-dsvm-centos7/9e29774/console.html#_2016-02-22_21_51_54_713\n\nhttp://logs.openstack.org/24/281624/9/check/gate-puppet-openstack-integration-scenario001-tempest-dsvm-centos7/9e29774/logs/cinder/volume.txt.gz#_2016-02-22_21_46_55_989\n\nAnyone is working on it?", 
            "date_created": "2016-02-22 22:17:57.446941+00:00", 
            "author": "https://api.launchpad.net/1.0/~emilienm"
        }, 
        {
            "content": "I found out this bug:\n\nhttp://logs.openstack.org/24/281624/9/check/gate-puppet-openstack-integration-scenario001-tempest-dsvm-centos7/9e29774/logs/cinder/volume.txt.gz#_2016-02-22_21_46_55_983\n\nShould I report a new bug in Cinder? Or should we keep this bug?", 
            "date_created": "2016-02-22 22:21:07.025058+00:00", 
            "author": "https://api.launchpad.net/1.0/~emilienm"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/289252", 
            "date_created": "2016-03-07 10:37:58.460919+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "\n@Emilien Macchi (emilienm) : Please try this fix  https://review.openstack.org/289252 ", 
            "date_created": "2016-03-07 10:41:18.985201+00:00", 
            "author": "https://api.launchpad.net/1.0/~mjura"
        }, 
        {
            "content": "I've opened another bug report for this follow up issue https://bugs.launchpad.net/cinder/+bug/1554045", 
            "date_created": "2016-03-07 13:12:57.328729+00:00", 
            "author": "https://api.launchpad.net/1.0/~mjura"
        }, 
        {
            "content": "Hello.\n\nToday we hit this issue, Liberty release, CEPH for both GLANCE and CINDER\n\nThe use case was \"create a cinder volume from glance image'\n\ncinder create --image <id> --name \"MyVol\" 40\n\nThe cinder volume says \"available\" however, when you try to boot, you can see that the Volume is in fact, not finished being copied from glance into cinder.   (one issue).\n\nThen trying to delete this Volume, it says deleteing, then goes back to available - and in cinder-volume logs, i have the same error (ImageBusy).   However ,there is no indication of how to get around this - i have waited about 3 hours now - so there is no way that its still \"copying\" at this point.\n\nIs there a workaround at this point?   \n\nShould i report a bug for Cinder reporting that a Volume is available (when doing Glance to Cinder volume creation) when it fact its \"not\"?\n\nmany thanks!\nDaniel \n", 
            "date_created": "2016-03-23 23:17:04.290544+00:00", 
            "author": "https://api.launchpad.net/1.0/~daniel-smith-v"
        }
    ]
}
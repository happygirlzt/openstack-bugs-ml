{
    "status": "In Progress", 
    "last_updated": "2016-11-09 15:20:47.744925+00:00", 
    "description": "I recently found a bug in Mitaka, and it appears to be still present in master.\n\nI was testing a separate patch by doing resizes, and bugs in my code had resulted in a number of incomplete resizes involving compute-1.  I then did a resize from compute-0 to compute-0, and saw compute-1's resource usage go up when it ran the resource audit.\n\nThis got me curious, so I went digging and discovered a gap in the current resource audit logic.  The problem arises if:\n    \n    1) You have one or more stale migrations which didn't complete\n    properly that involve the current compute node.\n    \n    2) The instance from the uncompleted migration is currently doing a\n    resize/migration that does not involve the current compute node.\n    \nWhen this happens, _update_usage_from_migrations() will be passed in the stale migration, and since the instance is in fact in a resize state, the current compute node will erroneously account for the instance.  (Even though the instance isn't doing anything involving the current compute node.)\n    \nThe fix is to check that the instance migration ID matches the ID of the migration being analyzed.  This will work because in the case of the stale migration we will have hit the error case in _pair_instances_to_migrations(), and so the instance will be lazy-loaded from the DB, ensuring that its migration ID is up-to-date.", 
    "tags": [
        "compute"
    ], 
    "importance": "Medium", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1600304", 
    "owner": "https://api.launchpad.net/1.0/~cbf123", 
    "id": 1600304, 
    "index": 4575, 
    "created": "2016-07-08 17:05:23.779901+00:00", 
    "title": "_update_usage_from_migrations() can end up processing stale migrations", 
    "comments": [
        {
            "content": "I recently found a bug in Mitaka, and it appears to be still present in master.\n\nI was testing a separate patch by doing resizes, and bugs in my code had resulted in a number of incomplete resizes involving compute-1.  I then did a resize from compute-0 to compute-0, and saw compute-1's resource usage go up when it ran the resource audit.\n\nThis got me curious, so I went digging and discovered a gap in the current resource audit logic.  The problem arises if:\n    \n    1) You have one or more stale migrations which didn't complete\n    properly that involve the current compute node.\n    \n    2) The instance from the uncompleted migration is currently doing a\n    resize/migration that does not involve the current compute node.\n    \nWhen this happens, _update_usage_from_migrations() will be passed in the stale migration, and since the instance is in fact in a resize state, the current compute node will erroneously account for the instance.  (Even though the instance isn't doing anything involving the current compute node.)\n    \nThe fix is to check that the instance migration ID matches the ID of the migration being analyzed.  This will work because in the case of the stale migration we will have hit the error case in _pair_instances_to_migrations(), and so the instance will be lazy-loaded from the DB, ensuring that its migration ID is up-to-date.", 
            "date_created": "2016-07-08 17:05:23.779901+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Not sure why it didn't link, but https://review.openstack.org/#/c/339715/ is the proposed fix.", 
            "date_created": "2016-07-08 17:20:21.260284+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Change abandoned by Michael Still (<email address hidden>) on branch: master\nReview: https://review.openstack.org/339715\nReason: This patch has been sitting unchanged for more than 12 weeks. I am therefore going to abandon it to keep the nova review queue sane. Please feel free to restore the change if you're still working on it.", 
            "date_created": "2016-10-27 07:34:57.604112+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "How does one go about recreating this w/o buggy local patches? I have a hard time trusting a fix for bugs in local code out of tree.", 
            "date_created": "2016-11-08 23:49:41.350071+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I believe you could get into this case as follows:\n\n1) There is a migration in progress and the source or dest compute node dies.\n2) The migrating instance goes into an ERROR state, so you evacuate it to another compute node.\n3) You recover the compute node that died.\n4) You migrate the instance between two other compute nodes.\n5) While the migration is in progress, the resource audit runs on the recovered compute node.", 
            "date_created": "2016-11-09 15:12:58.318343+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "I'd think that rather than the resource tracker dealing with failed migrations, we'd clean those up when the failed compute node is recovered, something like what we have here:\n\nhttps://github.com/openstack/nova/blob/569e463f02b0a631d341b06849217657a22c4569/nova/compute/manager.py#L636", 
            "date_created": "2016-11-09 15:20:47.247432+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }
    ]
}
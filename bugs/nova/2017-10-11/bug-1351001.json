{
    "status": "Fix Released", 
    "last_updated": "2014-10-16 08:58:41.111527+00:00", 
    "description": "Description of problem:\n\nUsing nova force-delete $instance-id fails when an instance is in status \"BUILD\" and OS-EXT-STS:task_state \"deleting\".  However, \"nova delete\" does seem to work after several tries.\n\nVersion-Release number of selected component (if applicable):\n\n2013.2 (Havana)\n\nHow reproducible:\n\n\nSteps to Reproduce:\n1. find a seemingly hung instance\n2. fire off nova-delete\n3. watch it complain\n\nActual results:\n\n[root@host02 ~(keystone_admin)]$ nova force-delete 3a83b712-4667-44c1-a83d-ada164ff78d1\nERROR: Cannot 'forceDelete' while instance is in vm_state building (HTTP 409) (Request-ID: req-22737c83-32f4-4c6d-ae9c-09a542556907)\n\n\nExpected results:\n\ndo the needful.\n\n\nAdditional info:\n\nHere are some logs obtained from this behavior, this is on RHOS4 / RHEL6.5:\n\n--snip--\n\n[root@host02 ~(keystone_admin)]$ nova force-delete 3a83b712-4667-44c1-a83d-ada164ff78d1\nERROR: Cannot 'forceDelete' while instance is in vm_state building (HTTP 409) (Request-ID: req-22737c83-32f4-4c6d-ae9c-09a542556907)\n[root@host02 ~(keystone_admin)]$ nova list --all-tenants | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n| 3a83b712-4667-44c1-a83d-ada164ff78d1 | bcrochet-foreman                                   | BUILD   | deleting     | NOSTATE     | default=192.168.87.7; foreman_int=192.168.200.6; foreman_ext=192.168.201.6 \n\n[root@host02 ~(keystone_admin)]$ nova show 3a83b712-4667-44c1-a83d-ada164ff78d1\n+--------------------------------------+----------------------------------------------------------------------------+\n| Property                             | Value                                                                      |\n+--------------------------------------+----------------------------------------------------------------------------+\n| status                               | BUILD                                                                      |\n| updated                              | 2014-04-16T20:56:44Z                                                       |\n| OS-EXT-STS:task_state                | deleting                                                                   |\n| OS-EXT-SRV-ATTR:host                 | host08.oslab.priv                                                          |\n| foreman_ext network                  | 192.168.201.6                                                              |\n| key_name                             | foreman-ci                                                                 |\n| image                                | rhel-guest-image-6-6.5-20140116.1-1 (253354e7-8d65-4d95-b134-6b423d125579) |\n| hostId                               | 4b98ba395063916c15f5b96a791683fa5d116109987c6a6b0b8de2f1                   |\n| OS-EXT-STS:vm_state                  | building                                                                   |\n| OS-EXT-SRV-ATTR:instance_name        | instance-000099e9                                                          |\n| foreman_int network                  | 192.168.200.6                                                              |\n| OS-SRV-USG:launched_at               | None                                                                       |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | host08.oslab.priv                                                          |\n| flavor                               | m1.large (4)                                                               |\n| id                                   | 3a83b712-4667-44c1-a83d-ada164ff78d1                                       |\n| security_groups                      | [{u'name': u'default'}, {u'name': u'default'}, {u'name': u'default'}]      |\n| OS-SRV-USG:terminated_at             | None                                                                       |\n| user_id                              | 13090770bacc46ccb8fb7f5e13e5de98                                           |\n| name                                 | bcrochet-foreman                                                           |\n| created                              | 2014-04-16T20:27:51Z                                                       |\n| tenant_id                            | f8e6ba11caa94ea98d24ec819eb746fd                                           |\n| OS-DCF:diskConfig                    | MANUAL                                                                     |\n| metadata                             | {}                                                                         |\n| os-extended-volumes:volumes_attached | []                                                                         |\n| accessIPv4                           |                                                                            |\n| accessIPv6                           |                                                                            |\n| progress                             | 0                                                                          |\n| OS-EXT-STS:power_state               | 0                                                                          |\n| OS-EXT-AZ:availability_zone          | nova                                                                       |\n| default network                      | 192.168.87.7                                                               |\n| config_drive                         |                                                                            |\n+--------------------------------------+----------------------------------------------------------------------------+\n\n[root@host08 ~]# virsh list --all | grep instance-000099e9\n[root@host08 ~]# \n\n[root@host02 ~(keystone_admin)]$ nova delete 3a83b712-4667-44c1-a83d-ada164ff78d1\n[root@host02 ~(keystone_admin)]$ nova list --all-tenants | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n| 3a83b712-4667-44c1-a83d-ada164ff78d1 | bcrochet-foreman                                   | BUILD   | deleting     | NOSTATE     | default=192.168.87.7; foreman_int=192.168.200.6; foreman_ext=192.168.201.6\n\n** insert several more \"nova delete\" calls **\n\n( back on compute host08 after API called for delete..)\n\n2014-04-22 12:46:21.253 8427 INFO nova.virt.libvirt.driver [-] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Instance destroyed successfully.\n2014-04-22 12:46:21.668 8427 INFO nova.compute.manager [-] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] During sync_power_state the instance has a pending task. Skip.\n2014-04-22 12:46:21.701 8427 INFO nova.virt.libvirt.driver [req-dddc2bcf-539a-48d9-a870-c5cfcccb6916 54b9f83693c84bf2b72286e9609eee36 210a99a1e68f43218f4cab705c908d45] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Deleting instance files /var/lib/nova/instances/e9f49326-7703-4035-b3fe-57b4e11bb07b\n2014-04-22 12:46:21.702 8427 INFO nova.virt.libvirt.driver [req-dddc2bcf-539a-48d9-a870-c5cfcccb6916 54b9f83693c84bf2b72286e9609eee36 210a99a1e68f43218f4cab705c908d45] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Deletion of /var/lib/nova/instances/e9f49326-7703-4035-b3fe-57b4e11bb07b complete\n\n-- snip --\n\nOf relative importance, this particular instance did not show up in virsh but I could see it in the qemu-kvm process table:\n\n( tracking instance down to host08 )\n[root@host02 ~(keystone_admin)]$ cat /var/log/nova/scheduler.log | grep -i 3a83b712-4667-44c1-a83d-ada164ff78d1\n2014-04-16 20:27:52.243 28987 INFO nova.scheduler.filter_scheduler [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] Attempting to build 1 instance(s) uuids: [u'3a83b712-4667-44c1-a83d-ada164ff78d1']\n2014-04-16 20:27:52.462 28987 INFO nova.scheduler.filter_scheduler [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] Choosing host WeighedHost [host: host08.oslab.priv, weight: 15249.0] for instance 3a83b712-4667-44c1-a83d-ada164ff78d1\n\n( see successful build on host08 nova compute) \n\n[root@host08 ~]# cat /var/log/nova/compute.log | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n2014-04-16 20:27:52.708 8427 AUDIT nova.compute.manager [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Starting instance...\n2014-04-16 20:27:53.836 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Attempting claim: memory 8192 MB, disk 80 GB, VCPUs 4\n2014-04-16 20:27:53.837 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total Memory: 64401 MB, used: 49152.00 MB\n2014-04-16 20:27:53.837 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Memory limit: 96601.50 MB, free: 47449.50 MB\n2014-04-16 20:27:53.838 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total Disk: 842 GB, used: 471.00 GB\n2014-04-16 20:27:53.838 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Disk limit not specified, defaulting to unlimited\n2014-04-16 20:27:53.839 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total CPU: 16 VCPUs, used: 25.00 VCPUs\n2014-04-16 20:27:53.840 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] CPU limit not specified, defaulting to unlimited\n2014-04-16 20:27:53.840 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Claim successful\n2014-04-16 20:27:54.812 8427 INFO nova.virt.libvirt.driver [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Creating image\n2014-04-16 20:27:58.683 8427 INFO nova.virt.libvirt.driver [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Injecting key into image 253354e7-8d65-4d95-b134-6b423d125579\n\n( shows up in process table )\n\n[root@host08 ~(keystone_admin)]$ ps -ef | grep instance | grep 3a83b712-4667-44c1-a83d\nnova     22510  8427  0 Apr16 ?        00:04:30 /usr/libexec/qemu-kvm -global virtio-blk-pci.scsi=off -nodefconfig -nodefaults -nographic -machine accel=kvm:tcg -cpu host,+kvmclock -m 500 -no-reboot -kernel /var/tmp/.guestfs-162/kernel.8427 -initrd /var/tmp/.guestfs-162/initrd.8427 -device virtio-scsi-pci,id=scsi -drive file=/var/lib/nova/instances/3a83b712-4667-44c1-a83d-ada164ff78d1/disk,cache=none,format=qcow2,id=hd0,if=none -device scsi-hd,drive=hd0 -drive file=/var/tmp/.guestfs-162/root.8427,snapshot=on,id=appliance,if=none,cache=unsafe -device scsi-hd,drive=appliance -device virtio-serial -serial stdio -device sga -chardev socket,path=/tmp/libguestfss0eEyk/guestfsd.sock,id=channel0 -device virtserialport,chardev=channel0,name=org.libguestfs.channel.0 -append panic=1 console=ttyS0 udevtimeout=600 no_timer_check acpi=off printk.time=1 cgroup_disable=memory root=/dev/sdb selinux=0 TERM=linux\n\n( but virsh does not know about it )\n\n[root@host08 ~]# virsh list --all | grep instance-000099e9 \n[root@host08 ~]# \n[root@host08 ~]# for x in `virsh list --all | grep -v Name | awk '{print $2}'`; do virsh dumpxml $x | grep 3a83b712-4667-44c1-a83d-ada164ff78d1; done\n[root@host08 ~]#", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1351001", 
    "owner": "https://api.launchpad.net/1.0/~cyeoh-0", 
    "id": 1351001, 
    "index": 3974, 
    "created": "2014-07-31 19:16:03.462835+00:00", 
    "title": "nova force-delete does not delete 'BUILD' state instances", 
    "comments": [
        {
            "content": "Description of problem:\n\nUsing nova force-delete $instance-id fails when an instance is in status \"BUILD\" and OS-EXT-STS:task_state \"deleting\".  However, \"nova delete\" does seem to work after several tries.\n\nVersion-Release number of selected component (if applicable):\n\n2013.2 (Havana)\n\nHow reproducible:\n\n\nSteps to Reproduce:\n1. find a seemingly hung instance\n2. fire off nova-delete\n3. watch it complain\n\nActual results:\n\n[root@host02 ~(keystone_admin)]$ nova force-delete 3a83b712-4667-44c1-a83d-ada164ff78d1\nERROR: Cannot 'forceDelete' while instance is in vm_state building (HTTP 409) (Request-ID: req-22737c83-32f4-4c6d-ae9c-09a542556907)\n\n\nExpected results:\n\ndo the needful.\n\n\nAdditional info:\n\nHere are some logs obtained from this behavior, this is on RHOS4 / RHEL6.5:\n\n--snip--\n\n[root@host02 ~(keystone_admin)]$ nova force-delete 3a83b712-4667-44c1-a83d-ada164ff78d1\nERROR: Cannot 'forceDelete' while instance is in vm_state building (HTTP 409) (Request-ID: req-22737c83-32f4-4c6d-ae9c-09a542556907)\n[root@host02 ~(keystone_admin)]$ nova list --all-tenants | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n| 3a83b712-4667-44c1-a83d-ada164ff78d1 | bcrochet-foreman                                   | BUILD   | deleting     | NOSTATE     | default=192.168.87.7; foreman_int=192.168.200.6; foreman_ext=192.168.201.6 \n\n[root@host02 ~(keystone_admin)]$ nova show 3a83b712-4667-44c1-a83d-ada164ff78d1\n+--------------------------------------+----------------------------------------------------------------------------+\n| Property                             | Value                                                                      |\n+--------------------------------------+----------------------------------------------------------------------------+\n| status                               | BUILD                                                                      |\n| updated                              | 2014-04-16T20:56:44Z                                                       |\n| OS-EXT-STS:task_state                | deleting                                                                   |\n| OS-EXT-SRV-ATTR:host                 | host08.oslab.priv                                                          |\n| foreman_ext network                  | 192.168.201.6                                                              |\n| key_name                             | foreman-ci                                                                 |\n| image                                | rhel-guest-image-6-6.5-20140116.1-1 (253354e7-8d65-4d95-b134-6b423d125579) |\n| hostId                               | 4b98ba395063916c15f5b96a791683fa5d116109987c6a6b0b8de2f1                   |\n| OS-EXT-STS:vm_state                  | building                                                                   |\n| OS-EXT-SRV-ATTR:instance_name        | instance-000099e9                                                          |\n| foreman_int network                  | 192.168.200.6                                                              |\n| OS-SRV-USG:launched_at               | None                                                                       |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | host08.oslab.priv                                                          |\n| flavor                               | m1.large (4)                                                               |\n| id                                   | 3a83b712-4667-44c1-a83d-ada164ff78d1                                       |\n| security_groups                      | [{u'name': u'default'}, {u'name': u'default'}, {u'name': u'default'}]      |\n| OS-SRV-USG:terminated_at             | None                                                                       |\n| user_id                              | 13090770bacc46ccb8fb7f5e13e5de98                                           |\n| name                                 | bcrochet-foreman                                                           |\n| created                              | 2014-04-16T20:27:51Z                                                       |\n| tenant_id                            | f8e6ba11caa94ea98d24ec819eb746fd                                           |\n| OS-DCF:diskConfig                    | MANUAL                                                                     |\n| metadata                             | {}                                                                         |\n| os-extended-volumes:volumes_attached | []                                                                         |\n| accessIPv4                           |                                                                            |\n| accessIPv6                           |                                                                            |\n| progress                             | 0                                                                          |\n| OS-EXT-STS:power_state               | 0                                                                          |\n| OS-EXT-AZ:availability_zone          | nova                                                                       |\n| default network                      | 192.168.87.7                                                               |\n| config_drive                         |                                                                            |\n+--------------------------------------+----------------------------------------------------------------------------+\n\n[root@host08 ~]# virsh list --all | grep instance-000099e9\n[root@host08 ~]# \n\n[root@host02 ~(keystone_admin)]$ nova delete 3a83b712-4667-44c1-a83d-ada164ff78d1\n[root@host02 ~(keystone_admin)]$ nova list --all-tenants | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n| 3a83b712-4667-44c1-a83d-ada164ff78d1 | bcrochet-foreman                                   | BUILD   | deleting     | NOSTATE     | default=192.168.87.7; foreman_int=192.168.200.6; foreman_ext=192.168.201.6\n\n** insert several more \"nova delete\" calls **\n\n( back on compute host08 after API called for delete..)\n\n2014-04-22 12:46:21.253 8427 INFO nova.virt.libvirt.driver [-] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Instance destroyed successfully.\n2014-04-22 12:46:21.668 8427 INFO nova.compute.manager [-] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] During sync_power_state the instance has a pending task. Skip.\n2014-04-22 12:46:21.701 8427 INFO nova.virt.libvirt.driver [req-dddc2bcf-539a-48d9-a870-c5cfcccb6916 54b9f83693c84bf2b72286e9609eee36 210a99a1e68f43218f4cab705c908d45] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Deleting instance files /var/lib/nova/instances/e9f49326-7703-4035-b3fe-57b4e11bb07b\n2014-04-22 12:46:21.702 8427 INFO nova.virt.libvirt.driver [req-dddc2bcf-539a-48d9-a870-c5cfcccb6916 54b9f83693c84bf2b72286e9609eee36 210a99a1e68f43218f4cab705c908d45] [instance: e9f49326-7703-4035-b3fe-57b4e11bb07b] Deletion of /var/lib/nova/instances/e9f49326-7703-4035-b3fe-57b4e11bb07b complete\n\n-- snip --\n\nOf relative importance, this particular instance did not show up in virsh but I could see it in the qemu-kvm process table:\n\n( tracking instance down to host08 )\n[root@host02 ~(keystone_admin)]$ cat /var/log/nova/scheduler.log | grep -i 3a83b712-4667-44c1-a83d-ada164ff78d1\n2014-04-16 20:27:52.243 28987 INFO nova.scheduler.filter_scheduler [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] Attempting to build 1 instance(s) uuids: [u'3a83b712-4667-44c1-a83d-ada164ff78d1']\n2014-04-16 20:27:52.462 28987 INFO nova.scheduler.filter_scheduler [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] Choosing host WeighedHost [host: host08.oslab.priv, weight: 15249.0] for instance 3a83b712-4667-44c1-a83d-ada164ff78d1\n\n( see successful build on host08 nova compute) \n\n[root@host08 ~]# cat /var/log/nova/compute.log | grep 3a83b712-4667-44c1-a83d-ada164ff78d1\n2014-04-16 20:27:52.708 8427 AUDIT nova.compute.manager [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Starting instance...\n2014-04-16 20:27:53.836 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Attempting claim: memory 8192 MB, disk 80 GB, VCPUs 4\n2014-04-16 20:27:53.837 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total Memory: 64401 MB, used: 49152.00 MB\n2014-04-16 20:27:53.837 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Memory limit: 96601.50 MB, free: 47449.50 MB\n2014-04-16 20:27:53.838 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total Disk: 842 GB, used: 471.00 GB\n2014-04-16 20:27:53.838 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Disk limit not specified, defaulting to unlimited\n2014-04-16 20:27:53.839 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Total CPU: 16 VCPUs, used: 25.00 VCPUs\n2014-04-16 20:27:53.840 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] CPU limit not specified, defaulting to unlimited\n2014-04-16 20:27:53.840 8427 AUDIT nova.compute.claims [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Claim successful\n2014-04-16 20:27:54.812 8427 INFO nova.virt.libvirt.driver [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Creating image\n2014-04-16 20:27:58.683 8427 INFO nova.virt.libvirt.driver [req-5e3af99e-658e-45cc-86e0-e64f462ff5ab 13090770bacc46ccb8fb7f5e13e5de98 f8e6ba11caa94ea98d24ec819eb746fd] [instance: 3a83b712-4667-44c1-a83d-ada164ff78d1] Injecting key into image 253354e7-8d65-4d95-b134-6b423d125579\n\n( shows up in process table )\n\n[root@host08 ~(keystone_admin)]$ ps -ef | grep instance | grep 3a83b712-4667-44c1-a83d\nnova     22510  8427  0 Apr16 ?        00:04:30 /usr/libexec/qemu-kvm -global virtio-blk-pci.scsi=off -nodefconfig -nodefaults -nographic -machine accel=kvm:tcg -cpu host,+kvmclock -m 500 -no-reboot -kernel /var/tmp/.guestfs-162/kernel.8427 -initrd /var/tmp/.guestfs-162/initrd.8427 -device virtio-scsi-pci,id=scsi -drive file=/var/lib/nova/instances/3a83b712-4667-44c1-a83d-ada164ff78d1/disk,cache=none,format=qcow2,id=hd0,if=none -device scsi-hd,drive=hd0 -drive file=/var/tmp/.guestfs-162/root.8427,snapshot=on,id=appliance,if=none,cache=unsafe -device scsi-hd,drive=appliance -device virtio-serial -serial stdio -device sga -chardev socket,path=/tmp/libguestfss0eEyk/guestfsd.sock,id=channel0 -device virtserialport,chardev=channel0,name=org.libguestfs.channel.0 -append panic=1 console=ttyS0 udevtimeout=600 no_timer_check acpi=off printk.time=1 cgroup_disable=memory root=/dev/sdb selinux=0 TERM=linux\n\n( but virsh does not know about it )\n\n[root@host08 ~]# virsh list --all | grep instance-000099e9 \n[root@host08 ~]# \n[root@host08 ~]# for x in `virsh list --all | grep -v Name | awk '{print $2}'`; do virsh dumpxml $x | grep 3a83b712-4667-44c1-a83d-ada164ff78d1; done\n[root@host08 ~]#", 
            "date_created": "2014-07-31 19:16:03.462835+00:00", 
            "author": "https://api.launchpad.net/1.0/~sgordon"
        }, 
        {
            "content": "So perhaps this is a documentation/nomenclature issue\n\nnova delete is for deleting a server\nnova force-delete is for deleting an server which has previously been deleted but not yet reclaimed", 
            "date_created": "2014-08-01 02:13:17.595023+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyeoh-0"
        }, 
        {
            "content": "Looking at the CLI Reference Guide [1][2] the documentation is currently as follows (note that it is automatically generated from the strings in the code so needs to be fixed in the source):\n\n\"\"\"\nnova delete command\n\nusage: nova delete <server> [<server> ...]\n\nImmediately shut down and delete specified server(s).\n\nPositional arguments\n\n<server>\n\n    Name or ID of server(s). \n\"\"\"\n\n\"\"\"\nnova force-delete command\n\nusage: nova force-delete <server>\n\nForce delete a server.\n\nPositional arguments\n\n<server>\n\n    Name or ID of server. \n\"\"\"\n\nI don't think this highlights the nuance you note very clearly.\n\n[1] http://docs.openstack.org/cli-reference/content/novaclient_commands.html#novaclient_subcommand_delete\n[2] http://docs.openstack.org/cli-reference/content/novaclient_commands.html#novaclient_subcommand_force-delete", 
            "date_created": "2014-08-01 02:32:07.358747+00:00", 
            "author": "https://api.launchpad.net/1.0/~sgordon"
        }, 
        {
            "content": "Yea the documentation public documentation is definitely inconsistent with what it actually does and I think the original intent of force delete from the API point of view eg:\n\nin the extension itself (which is called actually called deferred delete):\n\n    def _force_delete(self, req, id, body):\n        \"\"\"Force delete of instance before deferred cleanup.\"\"\"\n\nand in the compute code:\n\n    def force_delete(self, context, instance):\n        \"\"\"Force delete a previously deleted (but not reclaimed) instance.\"\"\"\n        self._delete_instance(context, instance)\n\nSo I think there was when the Nova code was written (as opposed to the python-novaclient and the associated documentation) that force delete was there where someone wanted to \"hurry up\" the deferred delete process of Nova, rather than actually be used to delete a running instance\n\nHowever having thought about it a bit, I think it does make sense that force delete could delete a running an instance and not just one that is deferred delete state. It is sort of an API change so we'll see what others think - it might need to be deferred to the next version of the API.\n", 
            "date_created": "2014-08-01 03:27:24.339624+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyeoh-0"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/111157", 
            "date_created": "2014-08-01 04:55:49.083285+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/111157\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=eb19c346b5fd8c2cb10f50a32f689e88a6a79d92\nSubmitter: Jenkins\nBranch:    master\n\ncommit eb19c346b5fd8c2cb10f50a32f689e88a6a79d92\nAuthor: Chris Yeoh <email address hidden>\nDate:   Fri Aug 1 14:14:55 2014 +0930\n\n    Allow forceDelete to delete running instances\n    \n    Currently attempting to force delete a running instance\n    fails while a normal delete can succeed. This changes the\n    behaviour of force_delete so any instance can be deleted\n    and not just ones which have already been soft deleted.\n    \n    Also catch InstanceIsLocked exceptions at the API layer\n    and now also don't need to catch InstanceInvalidState\n    exceptions since they are no longer raised\n    \n    DocImpact: forceDelete will now delete any instance and not\n      just ones which have already been soft deleted\n    \n    Change-Id: I06a476701ccb47d8f7fd6e43c394ccb3fac2b3aa\n    Closes-Bug: 1351001\n", 
            "date_created": "2014-09-16 12:17:16.016629+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
{
    "status": "Incomplete", 
    "last_updated": "2017-08-28 15:57:06.271606+00:00", 
    "description": "I'm running the current Icehouse code in devstack.  I was looking at the code and noticed something suspicious.\n\nIt looks like if we try to migrate a shared-storage instance and fail and end up rolling back we could end up with messed-up networking on the destination host.\n\nWhen setting up a live migration we unconditionally run ComputeManager.pre_live_migration() on the destination host to do various things including setting up networks on the host.\n\nIf something goes wrong with the live migration in ComputeManager._rollback_live_migration() we will only call self.compute_rpcapi.rollback_live_migration_at_destination() if we're doing block migration or volume-backed migration that isn't shared storage.\n\nHowever, looking at ComputeManager.rollback_live_migration_at_destination(), I also see it cleaning up networking as well as block device.  If we never call that cleanup code, then the networking stuff that was done in pre_live_migration() won't get rolled back.", 
    "tags": [
        "compute", 
        "live-migration", 
        "openstack-version.icehouse"
    ], 
    "importance": "Medium", 
    "heat": 24, 
    "link": "https://bugs.launchpad.net/nova/+bug/1284719", 
    "owner": "None", 
    "id": 1284719, 
    "index": 3806, 
    "created": "2014-02-25 16:41:55.861037+00:00", 
    "title": "buggy live migration rollback when using shared storage", 
    "comments": [
        {
            "content": "I'm running the current Icehouse code in devstack.  I was looking at the code and noticed something suspicious.\n\nIt looks like if we try to migrate a shared-storage instance and fail and end up rolling back we could end up with messed-up networking on the destination host.\n\nWhen setting up a live migration we unconditionally run ComputeManager.pre_live_migration() on the destination host to do various things including setting up networks on the host.\n\nIf something goes wrong with the live migration in ComputeManager._rollback_live_migration() we will only call self.compute_rpcapi.rollback_live_migration_at_destination() if we're doing block migration or volume-backed migration that isn't shared storage.\n\nHowever, looking at ComputeManager.rollback_live_migration_at_destination(), I also see it cleaning up networking as well as block device.  If we never call that cleanup code, then the networking stuff that was done in pre_live_migration() won't get rolled back.", 
            "date_created": "2014-02-25 16:41:55.861037+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Related: https://bugs.launchpad.net/nova/+bug/1423845 (has merged fix)", 
            "date_created": "2015-03-31 04:02:51.118706+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/172117", 
            "date_created": "2015-04-09 15:27:06.775725+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by John Garbutt (<email address hidden>) on branch: master\nReview: https://review.openstack.org/172117\nReason: This patch seems to have stalled, lets abandon it.\nPlease restore the patch if that is no longer true.\n\nAny questions, please catch me via email or on IRC johnthetubaguy", 
            "date_created": "2015-09-04 16:25:56.559646+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "@Pushkar, are you working on this issue? I'm going to introduce refactoring for clean-up phase of live-migration, that also will fix this bug.", 
            "date_created": "2016-02-09 07:03:33.423166+00:00", 
            "author": "https://api.launchpad.net/1.0/~tdurakov"
        }, 
        {
            "content": "Removing assignee due to lack of progress.", 
            "date_created": "2016-08-16 05:23:23.868490+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }, 
        {
            "content": "chris can you confirm this is still an issue in Master?", 
            "date_created": "2017-06-14 15:31:09.769855+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Automatically discovered version icehouse in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 16:00:33.782405+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2017-08-27 04:18:12.097300+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "I've confirmed that as of today the master branch still seems susceptible to this bug.  In pre_live_migration() we call self.network_api.setup_networks_on_host(context, instance, self.host)\n\nIn rollback_live_migration_at_destination() we call self.network_api.setup_networks_on_host(context, instance, self.host, teardown=True), but we only call self.compute_rpcapi.rollback_live_migration_at_destination() if the \"do_cleanup\" flag is set in _rollback_live_migration().\n\nThat flag is returned by _live_migration_cleanup_flags(), and it's only set if is_shared_instance_path is false.  So if you have a shared filesystem accessible from both compute nodes (NFS, for example) then we'll hit this bug.", 
            "date_created": "2017-08-28 15:57:05.733355+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }
    ]
}
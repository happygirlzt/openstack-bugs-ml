{
    "status": "Invalid", 
    "last_updated": "2016-08-23 15:06:39.449431+00:00", 
    "description": "1. version\n==========\n\nkilo 2015.1.0\n\n3. Reproduce steps:\n===================\n\n3.1 environment described\n--------------------------\n\nI have one compute node  with 32 CPUs. Its NUMA topology is as follows\uff1a\n\n    2 sockets * 8 cores * 2 thread = 32\n\nI have configured cpu_allocation_ratio=2 in 'nova.conf'.\n\n3.2 create a flavor with 32 CPUs\n--------------------------------\n\nCreate a flavor and set NUMA properties as follows:\n\n    $ nova flavor-key 32cpu set hw:numa_nodes=2 \\\n      hw:numa_cpus.0=\"0-15\" hw:numa_mem.0=256 \\\n      hw:numa_cpus.1=\"16-31\" hw:numa_mem.1=256 \\\n      hw:cpu_policy=dedicated\n\n    $ nova flavor-show 32cpu\n    +----------------------------+-------+\n    | Property                   | Value |\n    +----------------------------+-------+\n    | OS-FLV-DISABLED:disabled   | False |\n    | OS-FLV-EXT-DATA:ephemeral  | 0     |\n    | disk                       | 1     |\n    | extra_specs                | {}    |\n    | id                         | 7     |\n    | name                       | 32cpu |\n    | os-flavor-access:is_public | True  |\n    | ram                        | 512   |\n    | rxtx_factor                | 1.0   |\n    | swap                       |       |\n    | vcpus                      | 32    |\n    +----------------------------+-------+\n\nCreate a VM using this flavor.\n\n3.3 create another flavor with 20 cpus\n--------------------------------------\n\nCreate another flavor and set NUMA properties as follows:\n\n    $ nova flavor-key 20cpu set hw:numa_nodes=2 \\\n      hw:numa_cpus.0=\"0-9\" hw:numa_mem.0=256 \\\n      hw:numa_cpus.1=\"10-19\" hw:numa_mem.1=256 \\\n      hw:cpu_policy=share\n\nCreate another VM using this flavor.\n\n4 Expected Results\n==================\n\nExpected result: create failed\nActual result \uff1a create success\n\nIf 'cpu_policy=dedicated' is configured, this should mean the VM has exclusive rights to the specified CPU(s). Even with cpu_allocation, a VM created with the 'shared' CPU policy is sure to use the compute node's CPU resources. As such, the above should not be possible and the VM should fail when exist to schedule.", 
    "tags": [
        "kilo-backport-potential", 
        "liberty-backport-potential", 
        "numa"
    ], 
    "importance": "Undecided", 
    "heat": 14, 
    "link": "https://bugs.launchpad.net/nova/+bug/1515536", 
    "owner": "None", 
    "id": 1515536, 
    "index": 5844, 
    "created": "2015-11-12 09:54:38.460026+00:00", 
    "title": "create vm with share cpu_policy  breaks exist vm with dedicated cpu_policy when use cpu_allocation", 
    "comments": [
        {
            "content": "1. version\nkilo 2015.1.0\n\n\n3. Reproduce steps:\n\n3.1  environment described\n\nI have one compute node  with 32 cpus\uff0c\n\nIt\u2019s numa_topology is as follows\uff1a\n2 sockets * 8 cores * 2 thread = 32 \n\nset cpu_allocation_ratio=2 in nova.conf\n\n3.2 create a flavor  with 32 cpus\uff1a\n\n[root@nail-SBCJ-5-3-13 nova(keystone_admin)]# nova flavor-show 30cpu\n+----------------------------+-------+\n| Property                   | Value |\n+----------------------------+-------+\n| OS-FLV-DISABLED:disabled   | False |\n| OS-FLV-EXT-DATA:ephemeral  | 0     |\n| disk                       | 1     |\n| extra_specs                | {}    |\n| id                         | 7     |\n| name                       | 32cpu |\n| os-flavor-access:is_public | True  |\n| ram                        | 512   |\n| rxtx_factor                | 1.0   |\n| swap                       |       |\n| vcpus                      | 32    |\n+----------------------------+-------+\n\n3.2 Set the numa properties as follows\uff0cthen\uff0ccreate vm use the flavor\nnova flavor-key 32cpu  set hw:numa_nodes=2 hw:numa_cpus.0=\"0-15\" hw:numa_mem.0=256 hw:numa_cpus.1=\"16-31\" hw:numa_mem.1=256 hw:cpu_policy=dedicated\n\n3.3 create another flavor with 20 cpus \uff0cSet the numa properties as follows\uff0cthen\uff0ccreate vm use the flavor\nnova flavor-key 20cpu  set hw:numa_nodes=2 hw:numa_cpus.0=\"0-9\" hw:numa_mem.0=256 hw:numa_cpus.1=\"10-19\" hw:numa_mem.1=256 hw:cpu_policy=share\nExpected result: create failed\nActual result \uff1a create success\n \n4 \n4.1\nif cpu_policy=dedicated  mean the vm has exclusive rights to the specified CPU,\nThen the above phenomenon is not reasonable. \nbecause Even with cpu_allocation, the vm create with share cpu policy is sure to use the compute node's CPU.\nso\uff0c the vm should create failed when exist vm with dedicated take up all cpu on compute node\n\n4.2\nif cpu_policy=dedicated  mean the vm Can only use he specified CPU\u3002\nthen\uff0c The following phenomenon is not reasonable\uff1a\nIn the same environment\ncreate flavor with 20 cpus \uff0cSet the numa properties as follows\uff0cthen\uff0ccreate 2 vm use the flavor\nnova flavor-key 20cpu hw:numa_nodes=2 hw:numa_cpus.0=\"0-9\" hw:numa_mem.0=256 hw:numa_cpus.1=\"10-19\" hw:numa_mem.1=256 hw:cpu_policy=dedicated\nResult \uff1a one create success\uff0c one create failed\n\nIn a word, no matter how to explain \uff0cthe 4.1 and 4.2 must have one is not reasonable", 
            "date_created": "2015-11-12 09:54:38.460026+00:00", 
            "author": "https://api.launchpad.net/1.0/~ni-jinquan"
        }, 
        {
            "content": "I will like to take a look on this, assign it on myself.", 
            "date_created": "2016-06-30 02:39:21.560154+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "I think you need to make sure how many numa nodes you host has:\n\nI tried in my environment, even I have a cpu like this , but I have only 1 numa node.\n\nThread(s) per core:    2\nCore(s) per socket:    18\nSocket(s):             2\n\nubuntu@jfz1r04h06:~/devstack$ lscpu | grep NUMA\nNUMA node(s):          1\nNUMA node0 CPU(s):     0-71\n\n\nSo scheduler will fail with this flavor (which you required 2 numa nodes)\n\nnova flavor-key 32cpu set hw:numa_nodes=2 hw:numa_cpus.0=\"0-15\" hw:numa_mem.0=256 hw:numa_cpus.1=\"16-31\" hw:numa_mem.1=256 hw:cpu_policy=dedicated\n", 
            "date_created": "2016-06-30 07:02:30.502248+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "BTW, can you tell your libvirt output of virsh capabilities ? I think there maybe a bug from libvirt. check my reported bug here https://bugs.launchpad.net/nova/+bug/1593526", 
            "date_created": "2016-06-30 07:15:58.449166+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "Hi, Eli Qiao, yeah, There are two numa nodes on my host:\n\n[root@sbcj-chenling-02-slot11 ~]# lscpu |grep NUMA\nNUMA node(s)\uff1a         2\nNUMA node0 CPU(s)\uff1a    0-3,8-11\nNUMA node1 CPU(s)\uff1a    4-7,12-15\n\n\nThe output of virsh capabilities put in the attach file: virsh-output.txt.\nI will  have a look  the bug : https://bugs.launchpad.net/nova/+bug/1593526.\nThank you!", 
            "date_created": "2016-06-30 07:54:45.943590+00:00", 
            "author": "https://api.launchpad.net/1.0/~ni-jinquan"
        }, 
        {
            "content": "Hi, again, Eli Qiao, i had see the bug you patse, it seems like that is not the same problem.\nI leave some comment in the bug link, and i hope that will be useful.\nThank you!", 
            "date_created": "2016-06-30 08:10:11.733230+00:00", 
            "author": "https://api.launchpad.net/1.0/~ni-jinquan"
        }, 
        {
            "content": "@Jinquan\nthanks for your reply, hmm... libvirt works well, but I see you report this bug Since Liberty(it's EOL).\nHave you seen it from latest nova?", 
            "date_created": "2016-06-30 08:15:41.179880+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "Removing assignee due to lack of activity.", 
            "date_created": "2016-08-16 05:15:09.565865+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }, 
        {
            "content": "This is not a bug. By definition, instances with the 'shared' have no awareness of the requirements of those with 'dedicated'. You should use host aggregates to separate instances using 'dedicated' policies from the other. Refer to the documentation for more info (the caution)\n\n    http://docs.openstack.org/admin-guide/compute-cpu-topologies.html#customizing-instance-cpu-pinning-policies", 
            "date_created": "2016-08-23 15:06:25.770786+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }
    ]
}
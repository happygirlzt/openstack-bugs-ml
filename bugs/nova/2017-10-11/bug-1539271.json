{
    "status": "Invalid", 
    "last_updated": "2016-10-19 18:00:22.349473+00:00", 
    "description": "The following message in nova gate test logs shows that libvirt live migration can stall on some sort of deadlock:\n\n2016-01-28 16:53:20.878 INFO nova.virt.libvirt.driver [req-692a1f4f-16aa-4d93-a694-1f7eef4df9f6 tempest-LiveBlockMigrationTestJSON-1471114638 tempest-LiveBlockMigrationTestJSON-1937054400] [instance: 7b1bc0e2-a6a9-4d85-a3f9-4568d52d1f1b] Migration running for 30 secs, memory 100% remaining; (bytes processed=0, remaining=0, total=0)\n\nAdditionally, the libvirt logger thread seems to be deadlocked before this happens.", 
    "tags": [
        "doc-needed", 
        "libvirt"
    ], 
    "importance": "High", 
    "heat": 28, 
    "link": "https://bugs.launchpad.net/nova/+bug/1539271", 
    "owner": "None", 
    "id": 1539271, 
    "index": 1873, 
    "created": "2016-01-28 21:33:42.458365+00:00", 
    "title": "Libvirt live block migration migration stalls", 
    "comments": [
        {
            "content": "The following message in nova gate test logs shows that libvirt live migration can stall on some sort of deadlock:\n\n2016-01-28 16:53:20.878 INFO nova.virt.libvirt.driver [req-692a1f4f-16aa-4d93-a694-1f7eef4df9f6 tempest-LiveBlockMigrationTestJSON-1471114638 tempest-LiveBlockMigrationTestJSON-1937054400] [instance: 7b1bc0e2-a6a9-4d85-a3f9-4568d52d1f1b] Migration running for 30 secs, memory 100% remaining; (bytes processed=0, remaining=0, total=0)\n\nAdditionally, the libvirt logger thread seems to be deadlocked before this happens.", 
            "date_created": "2016-01-28 21:33:42.458365+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "Is this volume-backed only or also block storage live migrate? Because I'm seeing a random abort for live-migrate here:\n\nhttp://logs.openstack.org/20/283820/2/check/gate-tempest-dsvm-multinode-full/5427c27/logs/subnode-2/screen-n-cpu.txt.gz#_2016-02-25_15_38_03_441", 
            "date_created": "2016-03-01 17:09:15.728848+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Nevermind, the volume-backed live migration abort issue is tracked in bug 1524898.", 
            "date_created": "2016-03-01 17:10:33.403105+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looking at one of the failures, it's failing on live block migration:\n\nhttp://logs.openstack.org/84/286084/3/check/gate-tempest-dsvm-multinode-full/a5d70a2/logs/subnode-2/screen-n-cpu.txt.gz#_2016-03-01_14_04_26_797\n\n2016-03-01 14:04:26.797 6007 DEBUG nova.compute.manager [req-628be0b1-1e5f-4416-ae04-9b1e8c9e2280 tempest-LiveBlockMigrationTestJSON-479077322 tempest-LiveBlockMigrationTestJSON-2028817034] live_migration data is LibvirtLiveMigrateData(bdms=[],block_migration=True,disk_available_mb=52224,disk_over_commit=False,filename='tmpKVEUlV',graphics_listen_addr_spice=127.0.0.1,graphics_listen_addr_vnc=127.0.0.1,image_type='default',instance_relative_path='f85413c1-6ebb-4c3a-8f6f-aadcbff5970d',is_shared_block_storage=False,is_shared_instance_path=False,is_volume_backed=False,migration=Migration(2),serial_listen_addr='127.0.0.1',target_connect_addr=None) _do_live_migration /opt/stack/new/nova/nova/compute/manager.py:5216\n\n..\n\n2016-03-01 14:04:57.971 6007 INFO nova.virt.libvirt.driver [req-628be0b1-1e5f-4416-ae04-9b1e8c9e2280 tempest-LiveBlockMigrationTestJSON-479077322 tempest-LiveBlockMigrationTestJSON-2028817034] [instance: f85413c1-6ebb-4c3a-8f6f-aadcbff5970d] Migration running for 30 secs, memory 100% remaining; (bytes processed=0, remaining=0, total=0)", 
            "date_created": "2016-03-01 17:14:22.078936+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Some updates:\n\n1. I can not reproduce it from my local environment with latest nova code and tempest cases when doing live block migration.\n2. searched other gate-tempest-dsvm-multinode-full, only find  \"Migration running for 0 secs\" and the migration will be finished within 5 seconds.\n\nWill debug more if the stuck is from libvirt thread.", 
            "date_created": "2016-03-08 01:27:35.856198+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "I can not find any helpful information from libvirtd log,  libvirtd is doing his job, we need to check if qemu has such issue that block migration no response at all.\n\n7506 2016-03-01 14:04:27.810+0000: 5409: debug : doPeer2PeerMigrate3:3819 : Prepare3 0x7f28f4000a90                           \n  7507 2016-03-01 14:04:27.810+0000: 5409: debug : virStreamNew:15667 : conn=0x7f28f4000a90, flags=0 \n  7508 2016-03-01 14:04:27.810+0000: 5409: debug : qemuDomainObjEnterRemote:1406 : Entering remote (vm=0x7f2904021a20 name=insta\n       nce-00000005)\n  7509 2016-03-01 14:04:28.044+0000: 5406: debug : virDomainGetJobStats:17332 : dom=0x7f28ec01a000, (VM: name=instance-00000005,\n        uuid=f85413c1-6ebb-4c3a-8f6f-aadcbff5970d), type=0x7f28ec0114e0, params=0x7f291774eb58, nparams=0x7f291774eb54, flags=0\n  7510 2016-03-01 14:04:28.045+0000: 5406: debug : virDomainFree:2270 : dom=0x7f28ec01a000, (VM: name=instance-00000005, uuid=f8\n       5413c1-6ebb-4c3a-8f6f-aadcbff5970d)\n  7511 2016-03-01 14:04:28.547+0000: 5408: debug : virDomainGetJobStats:17332 : dom=0x7f2900002b50, (VM: name=instance-00000005,\n        uuid=f85413c1-6ebb-4c3a-8f6f-aadcbff5970d), type=0x7f2900003050, params=0x7f291674cb58, nparams=0x7f291674cb54, flags=0\n  7512 2016-03-01 14:04:28.547+0000: 5408: debug : virDomainFree:2270 : dom=0x7f2900002b50, (VM: name=instance-00000005, uuid=f8\n       5413c1-6ebb-4c3a-8f6f-aadcbff5970d)\n  7513 2016-03-01 14:04:29.056+0000: 5407: debug : virDomainGetJobStats:17332 : dom=0x7f28fc009ac0, (VM: name=instance-00000005,\n        uuid=f85413c1-6ebb-4c3a-8f6f-aadcbff5970d), type=0x7f28fc002be0, params=0x7f2916f4db58, nparams=0x7f2916f4db54, flags=0\n  7514 2016-03-01 14:04:29.056+0000: 5407: debug : virDomainFree:2270 : dom=0x7f28fc009ac0, (VM: name=instance-00000005, uuid=f8\n       5413c1-6ebb-4c3a-8f6f-aadcbff5970d)\n  7515 2016-03-01 14:04:29.558+0000: 5410: debug : virDomainGetJobStats:17332 : dom=0x7f28f8000b00, (VM: name=instance-00000005,\n        uuid=f85413c1-6ebb-4c3a-8f6f-aadcbff5970d), type=0x7f28f8001350, params=0x7f291574ab58, nparams=0x7f291574ab54, flags=0\n  7516 2016-03-01 14:04:29.558+0000: 5410: debug : virDomainFree:2270 : dom=0x7f28f8000b00, (VM: name=instance-00000005, uuid=f8\n       5413c1-6ebb-4c3a-8f6f-aadcbff5970d)\n", 
            "date_created": "2016-03-08 06:14:05.837455+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "oh, yes the rate of failure is high on gate, http://status.openstack.org/elastic-recheck/\n\nCan we try different version libvirt and qemu (libvirt 1.1.2 and qemu 2.0.0) ?\n\nWe may need to involve libvirt and qemu team.", 
            "date_created": "2016-03-08 06:25:37.408187+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "We are seeing this problem occasionally on certain specific instances. For most instances live migration works fine but we have one or two where it fails with the entries in the nova log about it running for 30 seconds and nothing transferred.\n\nVersions we are running are:\n\nlibvirt    1.2.12-0ubuntu14.2~cloud0\nqemu     1:2.2+dfsg-5expubuntu9.6~cloud0\n\n", 
            "date_created": "2016-03-08 09:59:02.524341+00:00", 
            "author": "https://api.launchpad.net/1.0/~martin-begley"
        }, 
        {
            "content": "We (well - Paul Carlton) seems to be able to reproduce this one reliably. Eli, let him know if you need any help. We are trying to build a version with up to date libvirt and qemu to see if it makes any difference.", 
            "date_created": "2016-03-09 15:58:18.377979+00:00", 
            "author": "https://api.launchpad.net/1.0/~pmurray"
        }, 
        {
            "content": "Thanks pmurray,\n@Paul Carlton , can you tell how you reproduce this issue? I have same version (libvirt/qemu) with the gate has, I tried mutiple time to run live-block-migration tempest case, but I can not see this issue in my Env.\n\nEli.", 
            "date_created": "2016-03-11 01:39:52.595569+00:00", 
            "author": "https://api.launchpad.net/1.0/~taget-9"
        }, 
        {
            "content": "I'll recreate it and post details", 
            "date_created": "2016-03-14 13:29:59.234452+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "Also, if you're recreating locally, please point out your environment (number of CPUs, RAM, etc). The VMs in the openstack CI are 8 VCPU and 8 GB RAM.", 
            "date_created": "2016-03-15 14:38:38.627529+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looking at the libvirtd.txt.gz matching the job Matt references in comment #3.\n\n  http://logs.openstack.org/84/286084/3/check/gate-tempest-dsvm-multinode-full/a5d70a2/logs/subnode-2/libvirt/libvirtd.txt.gz\n\nWe see the QEMU progress information is actually a dead^H^H^H^Hred-herring.\n\nLooking at the libvirtd logs we see libvirt preparing the migration:\n\n2016-03-01 14:04:27.804+0000: 5409: debug : qemuDomainObjSetJobPhase:964 : Setting 'migration out' phase to 'begin3'\n2016-03-01 14:04:27.805+0000: 5409: debug : qemuProcessAutoDestroyActive:4775 : vm=instance-00000005\n2016-03-01 14:04:27.805+0000: 5409: debug : virCloseCallbacksGet:197 : vm=instance-00000005, uuid=f85413c1-6ebb-4c3a-8f6f-aadcbff5970d, conn=(nil)\n2016-03-01 14:04:27.805+0000: 5409: debug : virCloseCallbacksGet:207 : cb=(nil)\n2016-03-01 14:04:27.805+0000: 5409: warning : qemuMigrationBeginPhase:2058 : NBD in tunnelled migration is currently not supported\n2016-03-01 14:04:27.805+0000: 5409: debug : qemuMigrationEatCookie:1041 : cookielen=0 cookie='<null>'\n2016-03-01 14:04:27.807+0000: 5409: debug : qemuMigrationBakeCookie:1018 : cookielen=281 cookie=<qemu-migration>\n  <name>instance-00000005</name>\n  <uuid>f85413c1-6ebb-4c3a-8f6f-aadcbff5970d</uuid>\n  <hostname>devstack-trusty-2-node-ovh-gra1-8426544-169319</hostname>\n  <hostuuid>bc8bf596-e420-a944-bbe9-1a00c7fe0060</hostuuid>\n  <feature name='lockstate'/>\n</qemu-migration>\n\n2016-03-01 14:04:27.808+0000: 5409: debug : qemuDomainDefFormatBuf:1503 : Removing default USB controller from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.808+0000: 5409: debug : qemuDomainDefFormatBuf:1524 : Removing default pci-root from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.809+0000: 5409: debug : qemuDomainDefFormatBuf:1503 : Removing default USB controller from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.809+0000: 5409: debug : qemuDomainDefFormatBuf:1524 : Removing default pci-root from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.810+0000: 5409: debug : qemuDomainDefFormatBuf:1503 : Removing default USB controller from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.810+0000: 5409: debug : qemuDomainDefFormatBuf:1524 : Removing default pci-root from domain 'instance-00000005' for migration compatibility\n2016-03-01 14:04:27.810+0000: 5409: debug : doPeer2PeerMigrate3:3819 : Prepare3 0x7f28f4000a90\n2016-03-01 14:04:27.810+0000: 5409: debug : virStreamNew:15667 : conn=0x7f28f4000a90, flags=0\n2016-03-01 14:04:27.810+0000: 5409: debug : qemuDomainObjEnterRemote:1406 : Entering remote (vm=0x7f2904021a20 name=instance-00000005)\n\n\nAfer this point there are never any more messages for thread 5409 - it is stuck waiting at the qemuDomainObjEnterRemote remote command.\n\nIt looks like the destination libvirtd is not responding to the domainMigratePrepareTunnel3 API call the source libvirtd runs. This means source libvirtd is not starting hte migration, and thus it forever reports 0% progress / 100% remaining.\n\nSo this isn't actually a case of QEMU getting stuck, but rather libvirtd getting stuck and QEMU not ever starting migration.\n\n\nSo the real question here is why the destination libvirtd does not respond...\n\n\nSo looking at the libvirtd logs on the target host \n\nhttp://logs.openstack.org/84/286084/3/check/gate-tempest-dsvm-multinode-full/a5d70a2/logs/libvirt/libvirtd.txt.gz\n\nWe see the connection opened\n\n2016-03-01 14:04:27.791+0000: 12981: debug : virConnectOpen:1316 : name=qemu:///system\n\nand the migration prepare step started:\n\n2016-03-01 14:04:27.810+0000: 12981: debug : virDomainMigratePrepareTunnel3Params:6834 : conn=0x7f03e000a740, stream=0x7f03e000a2c0, params=0x7f03e000a090, nparams=1, cookiein=0x7f03e000bbd0, cooki\n\nWhen we build QEMU command line args though everything goes to pot just after libvirtd creates the TAP device:\n\n2016-03-01 14:04:28.134+0000: 12981: debug : qemuBuildCommandLine:7760 : conn=0x7f03e000a740 driver=0x7f03e4165b70 def=0x7f03e0014c20 mon=0x7f03e0001430 json=1 qemuCaps=0x7f03e0000a10 migrateFrom=stdio migrateFD=24 snapshot=(nil) vmop=5\n2016-03-01 14:04:28.134+0000: 12981: warning : x86Decode:1515 : Preferred CPU model gate64 not allowed by hypervisor; closest supported model will be used\n2016-03-01 14:04:28.135+0000: 12981: info : virNetDevProbeVnetHdr:122 : Enabling IFF_VNET_HDR\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:347 : dispatching to max 0 clients, called from event watch 6\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:360 : event not handled.\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:347 : dispatching to max 0 clients, called from event watch 6\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:360 : event not handled.\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:347 : dispatching to max 0 clients, called from event watch 6\n2016-03-01 14:04:28.135+0000: 12977: debug : virNetlinkEventCallback:360 : event not handled.\n2016-03-01 14:06:57.769+0000: 12977: debug : virNetlinkEventCallback:347 : dispatching to max 0 clients, called from event watch 6\n2016-03-01 14:06:57.770+0000: 12977: debug : virNetlinkEventCallback:360 : event not handled.\n\n\nThe netliink messages repeat forever more and no thread in that libvirtd ever responds again.\n\nIf I had to guess I'd say this smells like a deadlock in the nwfilter code since that's what I think should be running at this point", 
            "date_created": "2016-03-15 15:18:14.811293+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I've recreated this on my multi node devstack running in vmware workstation on my laptop\nThe devstack vms have 8gb memory and 4 vcpus each, the host has 32gb of memory. \n\nlibvirt log on sender show an attempt to migrate without nbd then retry with it...\n\n2016-03-17 09:02:47.861+0000: 98655: debug : qemuDomainObjEndJob:1206 : Stopping job: migration operation (async=migration out vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:47.861+0000: 98655: debug : virDomainFree:2270 : dom=0x7f5d880011e0, (VM: name=instance-00000001, uuid=f5df6c53-c836-4a3c-957d-06a3b0377b0b)\n2016-03-17 09:02:48.528+0000: 98651: debug : qemuDomainObjExitRemote:1415 : Exited remote (vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.528+0000: 98651: debug : doPeer2PeerMigrate3:3884 : Perform3 0x7f5d90000af0 uri=tcp:devstack-compute1:49152\n2016-03-17 09:02:48.528+0000: 98651: debug : qemuDomainObjSetJobPhase:964 : Setting 'migration out' phase to 'perform3'\n2016-03-17 09:02:48.528+0000: 98651: debug : doNativeMigrate:3456 : driver=0x7f5d941bd8e0, vm=0x7f5d941a2230, uri=tcp:devstack-compute1:49152, cookiein=<qemu-migration>\n  <name>instance-00000001</name>\n  <uuid>f5df6c53-c836-4a3c-957d-06a3b0377b0b</uuid>\n  <hostname>devstack-compute1</hostname>\n  <hostuuid>564d37d6-fbcf-3b5e-3a68-6f375aa68bfc</hostuuid>\n  <nbd port='49155'/>\n</qemu-migration>\n, cookieinlen=244, cookieout=0x7f5da761b8c0, cookieoutlen=0x7f5da761b8a4, flags=93, resource=1000, graphicsuri=<null>\n2016-03-17 09:02:48.528+0000: 98651: debug : qemuMigrationRun:3223 : driver=0x7f5d941bd8e0, vm=0x7f5d941a2230, cookiein=<qemu-migration>\n  <name>instance-00000001</name>\n  <uuid>f5df6c53-c836-4a3c-957d-06a3b0377b0b</uuid>\n  <hostname>devstack-compute1</hostname>\n  <hostuuid>564d37d6-fbcf-3b5e-3a68-6f375aa68bfc</hostuuid>\n  <nbd port='49155'/>\n</qemu-migration>\n, cookieinlen=244, cookieout=0x7f5da761b8c0, cookieoutlen=0x7f5da761b8a4, flags=93, resource=1000, spec=0x7f5da761b730 (dest=1, fwd=0), dconn=0x7f5d941aa520, graphicsuri=<null>\n2016-03-17 09:02:48.528+0000: 98651: debug : qemuMigrationEatCookie:1041 : cookielen=244 cookie='<qemu-migration>\n  <name>instance-00000001</name>\n  <uuid>f5df6c53-c836-4a3c-957d-06a3b0377b0b</uuid>\n  <hostname>devstack-compute1</hostname>\n  <hostuuid>564d37d6-fbcf-3b5e-3a68-6f375aa68bfc</hostuuid>\n  <nbd port='49155'/>\n</qemu-migration>\n'\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuMigrationCookieXMLParseStr:964 : xml=<qemu-migration>\n  <name>instance-00000001</name>\n  <uuid>f5df6c53-c836-4a3c-957d-06a3b0377b0b</uuid>\n  <hostname>devstack-compute1</hostname>\n  <hostuuid>564d37d6-fbcf-3b5e-3a68-6f375aa68bfc</hostuuid>\n  <nbd port='49155'/>\n</qemu-migration>\n\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuDomainObjBeginJobInternal:1050 : Starting job: async nested (async=migration out vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuDomainObjBeginJobInternal:1092 : Started job: async nested (async=migration out vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7f5d8c000a40 vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuMonitorDriveMirror:3145 : mon=0x7f5d8c000a40, device=drive-virtio-disk0, file=nbd:devstack-compute1:49155:exportname=drive-virtio-disk0, format=<null>, bandwidth=1000, flags=3\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"drive-mirror\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"target\":\"nbd:devstack-compute1:49155:exportname=drive-virtio-disk0\",\"speed\":1048576000,\"sync\":\"top\",\"mode\":\"existing\"},\"id\":\"libvirt-610\"}' for write with FD -1\n2016-03-17 09:02:48.529+0000: 98651: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f5d8c000a40 msg={\"execute\":\"drive-mirror\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"target\":\"nbd:devstack-compute1:49155:exportname=drive-virtio-disk0\",\"speed\":1048576000,\"sync\":\"top\",\"mode\":\"existing\"},\"id\":\"libvirt-610\"}^M\n fd=-1\n\nThen I see \n\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuDomainObjBeginJobInternal:1050 : Starting job: async nested (async=migration out vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuDomainObjBeginJobInternal:1092 : Started job: async nested (async=migration out vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7f5d8c000a40 vm=0x7f5d941a2230 name=instance-00000001)\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuMonitorBlockJob:3314 : mon=0x7f5d8c000a40, device=drive-virtio-disk0, base=<null>, bandwidth=0M, info=0x7f5da761b5b0, mode=1, modern=1\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"query-block-jobs\",\"id\":\"libvirt-611\"}' for write with FD -1\n2016-03-17 09:02:48.537+0000: 98651: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f5d8c000a40 msg={\"execute\":\"query-block-jobs\",\"id\":\"libvirt-611\"}\n2016-03-17 09:02:48.544+0000: 98651: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=0 rxObject=0x56433fbb5380\n2016-03-17 09:02:48.544+0000: 98651: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7f5d8c000a40 vm=0x7f5d941a2230 name=instance-00000001)\n2\n\nrepeated until it gets cancelled\n\nOn the target I get\n\n2016-03-17 09:02:48.527+0000: 44399: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"nbd-server-add\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"writable\":true},\"id\":\"libvirt-18\"}' for write with FD -1\n2016-03-17 09:02:48.527+0000: 44399: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f73c400ae20 msg={\"execute\":\"nbd-server-add\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"writable\":true},\"id\":\"libvirt-18\"}^M\n fd=-1\n2016-03-17 09:02:48.527+0000: 44397: debug : qemuMonitorIOWrite:504 : QEMU_MONITOR_IO_WRITE: mon=0x7f73c400ae20 buf={\"execute\":\"nbd-server-add\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"writable\":true},\"id\":\"libvirt-18\"}^M\n len=108 ret=108 errno=11\n2016-03-17 09:02:48.528+0000: 44397: debug : qemuMonitorIOProcess:396 : QEMU_MONITOR_IO_PROCESS: mon=0x7f73c400ae20 buf={\"return\": {}, \"id\": \"libvirt-18\"}^M\n len=36\n2016-03-17 09:02:48.528+0000: 44397: debug : qemuMonitorJSONIOProcessLine:157 : Line [{\"return\": {}, \"id\": \"libvirt-18\"}]\n2016-03-17 09:02:48.528+0000: 44397: debug : qemuMonitorJSONIOProcessLine:177 : QEMU_MONITOR_RECV_REPLY: mon=0x7f73c400ae20 reply={\"return\": {}, \"id\": \"libvirt-18\"}\n2016-03-17 09:02:48.528+0000: 44397: debug : qemuMonitorJSONIOProcess:226 : Total used 36 bytes out of 36 available in buffer\n2016-03-17 09:02:48.528+0000: 44399: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=0 rxObject=0x564118c9c0a0\n2016-03-17 09:02:48.528+0000: 44399: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7f73c400ae20 vm=0x7f73c4000f90 name=instance-00000001)\n2016-03-17 09:02:48.529+0000: 44399: debug : qemuMigrationBakeCookie:1018 : cookielen=244 cookie=<qemu-migration>\n  <name>instance-00000001</name>\n  <uuid>f5df6c53-c836-4a3c-957d-06a3b0377b0b</uuid>\n  <hostname>devstack-compute1</hostname>\n  <hostuuid>564d37d6-fbcf-3b5e-3a68-6f375aa68bfc</hostuuid>\n  <nbd port='49155'/>\n</qemu-migration>\n\n2016-03-17 09:02:48.529+0000: 44399: debug : qemuDomainCleanupAdd:2270 : vm=instance-00000001, cb=0x7f73cd021ea0\n2016-03-17 09:02:48.529+0000: 44399: debug : virObjectEventNew:643 : obj=0x7f73c40141d0\n2016-03-17 09:02:48.529+0000: 44399: debug : qemuDomainObjReleaseAsyncJob:1006 : Releasing ownership of 'migration in' async job\n\n\n", 
            "date_created": "2016-03-17 12:59:04.761631+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "Also fails on my debian (hlinux) system using libvirt 1.2.16 and qemu 2.5, also with 8gb memory and 4 vpcus see attachements", 
            "date_created": "2016-03-17 13:27:51.443164+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "", 
            "date_created": "2016-03-17 13:28:33.839428+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "hlinux environment...\n\nroot@paul-1:/var/log/libvirt# sudo dpkg -l | grep libvirt \nii  libvirt-bin                      1.2.16-1+hlinux1                     amd64        programs for the libvirt library\nii  libvirt-clients                  1.2.16-1+hlinux1                     amd64        programs for the libvirt library\nii  libvirt-daemon                   1.2.16-1+hlinux1                     amd64        programs for the libvirt library\nii  libvirt-daemon-system            1.2.16-1+hlinux1                     amd64        Libvirt daemon configuration files\nii  libvirt-dev                      1.2.16-1+hlinux1                     amd64        development files for the libvirt library\nii  libvirt0                         1.2.16-1+hlinux1                     amd64        library for interfacing with different virtualization systems\nroot@paul-1:/var/log/libvirt# sudo dpkg -l | grep qemu \nii  ipxe-qemu                        1.0.0+git-20160216.d73982f-1+hlinux1 all          PXE boot firmware - ROM images for qemu\nii  qemu-kvm                         2.5+dfsg-5+hlinux2                   amd64        QEMU Full virtualization on x86 hardware\nii  qemu-system-common               2.5+dfsg-5+hlinux2                   amd64        QEMU full system emulation binaries (common files)\nii  qemu-system-x86                  2.5+dfsg-5+hlinux2                   amd64        QEMU full system emulation binaries (x86)\nii  qemu-utils                       2.5+dfsg-5+hlinux2                   amd64        QEMU utilities\n", 
            "date_created": "2016-03-17 15:17:57.548548+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "local.conf for hlinux env\n\n[[local|localrc]]\n#RECLONE=yes\nGIT_BASE=https://git.openstack.org\nNETWORK_GATEWAY=10.1.0.1\nUSE_SCREEN=True\n\nDEST=/home/$LOGNAME/openstack\nADMIN_PASSWORD=Olivia17\nMYSQL_PASSWORD=Olivia17\nRABBIT_PASSWORD=Olivia17\nSERVICE_PASSWORD=Olivia17\nSERVICE_TOKEN=tokentoken\nSWIFT_HASH=12go358snjw24501\n\nLOGFILE=$DEST/logs/stack.sh.log\nVERBOSE=True\nSCREEN_LOGDIR=$DEST/logs/screen\n\nHOST_IP=10.10.10.4\nSERVICE_HOST=10.10.10.4\nGLANCE_HOSTPORT=$SERVICE_HOST:9292\nMYSQL_HOST=$SERVICE_HOST\nRABBIT_HOST=$SERVICE_HOST\nQ_HOST=$SERVICE_HOST\n\nFORCE_CONFIG_DRIVE=False\nDATA_DIR=$DEST/stuff\nACTIVE_TIMEOUT=90\nBOOT_TIMEOUT=90\nASSOCIATE_TIMEOUT=60\nTERMINATE_TIMEOUT=60\nDATABASE_PASSWORD=secretdatabase\nROOTSLEEP=0\nENABLED_SERVICES=c-api,c-bak,c-sch,c-vol,ceilometer-acentral,ceilometer-acompute,ceilometer-alarm-evaluator,ceilometer-alarm-notifier,ceilometer-anotification,ceilometer-api,ceilometer-collector,cinder,dstat,g-api,g-reg,h-api,h-api-cfn,h-api-cw,h-eng,heat,horizon,key,mysql,n-api,n-cond,n-crt,n-obj,n-sch,n-cauth,n-novnc,q-agt,q-dhcp,q-l3,q-meta,q-metering,q-svc,quantum,rabbit,tempest\nSKIP_EXERCISES=boot_from_volume,bundle,euca\n\n# Screen console logs will capture service logs.\nSYSLOG=False\nVERBOSE=True\nVIRT_DRIVER=libvirt\nSWIFT_REPLICAS=1\nLOG_COLOR=False\nREQUIREMENTS_MODE=strict\nCINDER_PERIODIC_INTERVAL=10\nexport OS_NO_CACHE=True\nCEILOMETER_BACKEND=mysql\nLIBS_FROM_GIT=\n# set this until all testing platforms have libvirt >= 1.2.11\n# see bug #1501558\nEBTABLES_RACE_FIX=True\n\nZAQAR_BACKEND=mongodb\nCINDER_SECURE_DELETE=False\nAPI_RATE_LIMIT=False\nVOLUME_BACKING_FILE_SIZE=24G\n#FORCE_CONFIG_DRIVE=False\n\nTEMPEST_HTTP_IMAGE=http://git.openstack.org/static/openstack.png\nTEMPEST_RUN_VALIDATION=True\n\n## Neutron options\nFIXED_RANGE=10.1.0.0/20\nFLOATING_RANGE=192.168.200.0/24\n#Q_FLOATING_ALLOCATION_POOL=start=192.168.100.200,end=192.168.100.254\nFIXED_NETWORK_SIZE=4096\n\n#Q_L3_ENABLED=True\n#Q_USE_PROVIDERNET_FOR_PUBLIC=True\n#Q_USE_SECGROUP=True\nQ_USE_DEBUG_COMMAND=True\n\n#FLAT_INTERFACE=eth10\n#PUBLIC_BRIDGE=br-ex\n#PUBLIC_INTERFACE=eth10\nPUBLIC_NETWORK_GATEWAY=192.168.200.1\n#OVS_PHYSICAL_BRIDGE=br-ex\n\n#ENABLE_TENANT_VLANS=True\n#TENANT_VLAN_RANGE=1000:1999\n#PHYSICAL_NETWORK=public\n#PUBLIC_PHYSICAL_NETWORK=public\n#OVS_BRIDGE_MAPPINGS=public:br-ex\n\n#PROVIDER_SUBNET_NAME=\"provider_net\"\n#PROVIDER_NETWORK_TYPE=\"vlan\"\n#SEGMENTATION_ID=2010\n\nEBTABLES_RACE_FIX=True\nLIBVIRT_TYPE=kvm\n\n\n[[post-config|$NOVA_CONF]]\n[DEFAULT]\nnovncproxy_host = 0.0.0.0\nnovncproxy_port = 6080\n\n[vnc]\nenabled=True\n\n\n[libvirt]\ninject_partition = 1\nblock_migration_flag = VIR_MIGRATE_LIVE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_NON_SHARED_INC,VIR_MIGRATE_UNDEFINE_SOURCE\nlive_migration_bandwidth = 1000\nlive_migration_flag = VIR_MIGRATE_LIVE,VIR_MIGRATE_PEER2PEER\n\nsnapshots_directory = $DATA_DIR/nova/tmp\nsnapshot_image_format = qcow2\nlive_migration_uri = qemu+tcp://%s/system\n\n[compute]\ninstances_path = $DATA_DIR/nova\n\n[serial_console]\n# Host on which to listen for incoming requests (string value)\nserialproxy_host=0.0.0.0\nserialproxy_port=6083\nenabled=False\nport_range=10000:20000\nbase_url=ws://$SERVICE_HOST:6083/\nlisten=$SERVICE_HOST\nproxyclient_address=$SERVICE_HOST\n\n[[post-config|$NEUTRON_CONF]]\n[DEFAULT]\nnetwork_device_mtu = 1400\nadvertise_mtu = True\n\n# compute nodes are same, less control services", 
            "date_created": "2016-03-17 15:18:34.897644+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "The gate uses 8 VCPUs, do you see any difference when using 8 VCPUs?", 
            "date_created": "2016-03-17 15:20:31.470283+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The libvirtd logs from Paul's local reproducer don't show the same issue that we see in the gate. Specifically in the gate, it is pretty clear that libvirtd itself is hanging, but Paul's logs don't show any sign of this.\n\nSo I'd be surprised if Paul's local issue is the same as the gate issue. More lilkely we've have two separate issues that just appear superficially the same from Nova's POV.", 
            "date_created": "2016-03-17 15:37:59.685854+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Daniel: Can you tell what Paul's logs are showing? It looks like that's trying to use drive-mirror but I don't see it getting as far as an actual migrate?", 
            "date_created": "2016-03-17 16:12:24.786658+00:00", 
            "author": "https://api.launchpad.net/1.0/~dgilbert-h"
        }, 
        {
            "content": "FWIW, adding some related notes from observing the logs above:\n\nFrom the Gate logs: Block migration didn't even begin -- on the source node, 'drive-mirror' didn't even start;  and on the target node, NBD server was not started either.\n\nHowever, from Paul's logs: We can at least see that 'drive-mirror' began on the source node.  And on the target node, NBD server was started, and the disk ('drive-virtio-disk0') that's being was migrated was added, _however_ the NBD server was not stopped yet (by issuing the command 'nbd-server-stop').  \n\nThus, confirming Dan's theory above that the Gate failures and Paul's failures are different.\n\n- - -\n\nIn a successful live block migration (with 'drive-mirror'+ NBD approach): From my previous testing (a few months ago, as you can see from time-stamps; tested with: libvirt-daemon-driver-qemu-1.2.13.1-2 qemu-system-x86-2.3.1-7), on the source node, you should see messages that indicate drive mirroring was complete:\n\n...\n2015-09-29 18:47:45.021+0000: 8285: debug : qemuMigrationDriveMirror:1800 : Drive mirroring of 'drive-virtio-disk0' completed\n...\n\nIIUC, this message on the source means the out going migration has been completed successfully:\n\n...\n2015-09-26 12:52:07.908+0000: 758: debug : qemuDomainObjSetJobPhase:1231 : Setting 'migration out' phase to 'perform3_done'\n...\n\n- - -", 
            "date_created": "2016-03-17 17:42:17.605288+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "Paul: If you reproduce that one can you post the qemu logs from both source and target (/var/log/libvirt/qemu/instance-whatever.log);  I doubt they'll show much but they're worth a look.", 
            "date_created": "2016-03-18 10:52:05.999186+00:00", 
            "author": "https://api.launchpad.net/1.0/~dgilbert-h"
        }, 
        {
            "content": "Hi,\ni have the same issue,\nattached the libvirtd logs in debug mode for source and destination.\nthis is the command i ran:\nnova live-migration --block-migrate 10294c91-26b1-4d78-b54f-6005a58cdb70 j3cbb22\nd9cbb22 is the source\nand j3cbb22 the destination", 
            "date_created": "2016-04-19 11:40:03.253261+00:00", 
            "author": "https://api.launchpad.net/1.0/~alan-civita2"
        }, 
        {
            "content": "the second attachment", 
            "date_created": "2016-04-19 11:40:32.177295+00:00", 
            "author": "https://api.launchpad.net/1.0/~alan-civita2"
        }, 
        {
            "content": "A quick workaround, that worked for us, was to disable the live-migrate tunnelling, by removing the  VIR_MIGRATE_TUNNELLED flag and adding  VIR_MIGRATE_NON_SHARED_INC.\nHope this helps\nAlan", 
            "date_created": "2016-05-13 14:15:04.971642+00:00", 
            "author": "https://api.launchpad.net/1.0/~alan-civita2"
        }, 
        {
            "content": "Will try that thanks", 
            "date_created": "2016-05-17 15:00:50.677847+00:00", 
            "author": "https://api.launchpad.net/1.0/~paul-carlton2"
        }, 
        {
            "content": "Solving inconsistency: changing bug status from \"In progress\" to \"Confirmed\" as it has not assigned to anyone.", 
            "date_created": "2016-06-02 18:52:16.432358+00:00", 
            "author": "https://api.launchpad.net/1.0/~pushkar-umaranikar"
        }, 
        {
            "content": "We aren't seeing this on xenial nodes in the multinode live migration job anymore, so newer libvirt/qemu might have taken care of this one for us.", 
            "date_created": "2016-06-30 14:31:48.784458+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "We are still facing this on http://logs.openstack.org/50/349450/5/check/gate-tempest-dsvm-multinode-live-migration/60eb3b3/console.html#_2016-08-25_19_14_13_416233", 
            "date_created": "2016-08-25 21:12:18.656892+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "\nnova-cpu.log:\n\nhttp://logs.openstack.org/50/349450/5/check/gate-tempest-dsvm-multinode-live-migration/60eb3b3/logs/subnode-2/screen-n-cpu.txt.gz#_2016-08-25_19_13_38_903\n\n2016-08-25 19:13:38.904 25185 DEBUG nova.virt.libvirt.driver [req-04d0773e-0085-4605-ad4e-bd98329b6c4d tempest-LiveAutoBlockMigrationV225TestJSON-2087766141 tempest-LiveAutoBlockMigrationV225TestJSON-2087766141] [instance: 0240bf1c-d2cf-407f-9ecf-7acfd891b4c0] Migration operation thread notification thread_finished /opt/stack/new/nova/nova/virt/libvirt/driver.py:6273\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 457, in fire_timers\n    timer()\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py\", line 58, in __call__\n    cb(*args, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 168, in _do_send\n    waiter.switch(result)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n    result = function(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/utils.py\", line 1066, in context_wrapper\n    return func(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5874, in _live_migration_operation\n    instance=instance)\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 220, in __exit__\n    self.force_reraise()\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5870, in _live_migration_operation\n    bandwidth=CONF.libvirt.live_migration_bandwidth)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/guest.py\", line 568, in migrate\n    destination, params=params, flags=flags)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 186, in doit\n    result = proxy_call(self._autowrap, f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\n    rv = execute(f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\n    six.reraise(c, e, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\n    rv = meth(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1833, in migrateToURI3\n    if ret == -1: raise libvirtError ('virDomainMigrateToURI3() failed', dom=self)\nlibvirtError: internal error: unable to execute QEMU command 'migrate-incoming': Failed to bind socket: Address already in use\n", 
            "date_created": "2016-08-25 21:28:22.297339+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "humm, my report should be separated from this bug report because it is different from the original report. That is https://bugs.launchpad.net/nova/+bug/1617073", 
            "date_created": "2016-08-25 22:08:39.857151+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "humm, my report should be separated from this bug report because it is different from the original report. That is https://bugs.launchpad.net/nova/+bug/1617073", 
            "date_created": "2016-08-25 22:17:08.061610+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "I think we can close this out as far as the gate / CI system is concerned. We don't see these failures in the jobs that use xenial nodes anymore, which have newer libvirt/qemu, so we suspect newer libvirt/qemu was the solution.", 
            "date_created": "2016-10-19 17:58:40.033161+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "If anything, we could add a release note to nova that documents this as a known issue on ubuntu trusty nodes but hasn't been seen since moving to xenial, and then list the libvirt/qemu versions in that release note between trusty and xenial test nodes.", 
            "date_created": "2016-10-19 17:59:48.050911+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }
    ]
}
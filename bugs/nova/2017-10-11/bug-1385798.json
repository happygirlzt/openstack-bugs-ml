{
    "status": "Expired", 
    "last_updated": "2017-06-16 17:26:23.502289+00:00", 
    "description": "When disconnecting a volume from an instance the ISCSI multipath connection is not always cleaned up correctly. When running the temepest tests we see test failures related to this as the connection is not closed, but then it is requesting to disconnect through the cinder driver which ends up breaking the iscsi connection. The end result being that there are still entries in /dev/disk/by-path for the old ISCSI connections, but they are in an error state and cannot be used.\n\nIn the syslog we get errors like:\n\nOct 25 17:23:21 localhost kernel: [ 2974.200680]  connection44:0: detected conn error (1020)\nOct 25 17:23:21 localhost kernel: [ 2974.200819]  connection43:0: detected conn error (1020)\nOct 25 17:23:21 localhost iscsid: Kernel reported iSCSI connection 44:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\nOct 25 17:23:21 localhost iscsid: Kernel reported iSCSI connection 43:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\n\nAfter running the tests if I run \"multipath -l\" there are numerous entries (which shouldn't exist anymore), and if I run \"iscsiadm -m node\" it will show the connections to the backend, even though they are supposed to have been disconnected (and have been on the backend via the cinder driver).\n\nThe disconnect code in cinder/brick seems to not suffere from these issues, from the looks of the source code it works a little bit differently when disconnecting multipath volumes and will always clean up the scsi connection first. We might need to do something more like that in nova/virt/libvirt/volume.py too.\n\nI'm seeing this on the latest master and Juno branches, haven't yet tested on icehouse but looks like it will probably repro there too.", 
    "tags": [
        "kilo-backport-potential"
    ], 
    "importance": "Undecided", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1385798", 
    "owner": "None", 
    "id": 1385798, 
    "index": 5243, 
    "created": "2014-10-26 03:26:38.742124+00:00", 
    "title": "Multipath ISCSI connections left open after disconnecting volume with libvirt", 
    "comments": [
        {
            "content": "When disconnecting a volume from an instance the ISCSI multipath connection is not always cleaned up correctly. When running the temepest tests we see test failures related to this as the connection is not closed, but then it is requesting to disconnect through the cinder driver which ends up breaking the iscsi connection. The end result being that there are still entries in /dev/disk/by-path for the old ISCSI connections, but they are in an error state and cannot be used.\n\nIn the syslog we get errors like:\n\nOct 25 17:23:21 localhost kernel: [ 2974.200680]  connection44:0: detected conn error (1020)\nOct 25 17:23:21 localhost kernel: [ 2974.200819]  connection43:0: detected conn error (1020)\nOct 25 17:23:21 localhost iscsid: Kernel reported iSCSI connection 44:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\nOct 25 17:23:21 localhost iscsid: Kernel reported iSCSI connection 43:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\n\nAfter running the tests if I run \"multipath -l\" there are numerous entries (which shouldn't exist anymore), and if I run \"iscsiadm -m node\" it will show the connections to the backend, even though they are supposed to have been disconnected (and have been on the backend via the cinder driver).\n\nThe disconnect code in cinder/brick seems to not suffere from these issues, from the looks of the source code it works a little bit differently when disconnecting multipath volumes and will always clean up the scsi connection first. We might need to do something more like that in nova/virt/libvirt/volume.py too.\n\nI'm seeing this on the latest master and Juno branches, haven't yet tested on icehouse but looks like it will probably repro there too.", 
            "date_created": "2014-10-26 03:26:38.742124+00:00", 
            "author": "https://api.launchpad.net/1.0/~patrick-east"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/130964", 
            "date_created": "2014-10-26 05:58:53.365865+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Patrick East (<email address hidden>) on branch: master\nReview: https://review.openstack.org/130964\nReason: Abandoning for now and waiting for the brick changes", 
            "date_created": "2014-11-20 19:34:43.088131+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Removing \"In Progress\" status and assignee as change is abandoned.", 
            "date_created": "2015-02-12 16:46:54.561761+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "There is a monster backport patch proposed for this on stable/kilo: https://review.openstack.org/#/c/229152\n\nIt sounds like this is already fixed in os-brick which nova uses since liberty, and the attempt is to get the changes from os-brick all backported to nova's libvirt iscsi volume driver in kilo using this bug as the coordinator.", 
            "date_created": "2015-11-02 20:55:39.836441+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: stable/kilo\nReview: https://review.openstack.org/229152\nReason: We're going to EOL kilo in the next week or so so I'm going to abandon this, I don't think it's worth landing at this point. Anyone that needs it is probably already running with it out of tree, or needs to upgrade to liberty.", 
            "date_created": "2016-05-04 02:18:05.639836+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:54:04.287800+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ]
}
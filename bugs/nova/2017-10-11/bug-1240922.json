{
    "status": "Fix Released", 
    "last_updated": "2014-04-17 09:15:09.112397+00:00", 
    "description": "Hi guys,\n\nI have a suspend vm with an attached volume, if I detached volume while instance is in suspend state it can't be resumed properly.\nIt happens with both Windows and Linux vm's.\n\nLibVirt error:\n2494: error : qemuMonitorIORead:502 : Unable to read from monitor: Connection reset by peer\n\nPackets versioning in Ubuntu 12.04:\nii  libvirt-bin                       1.0.2-0ubuntu11.13.04.2~cloud0              programs for the libvirt library\nii  libvirt-dev                       1.0.2-0ubuntu11.13.04.2~cloud0              development files for the libvirt library\nii  libvirt0                          1.0.2-0ubuntu11.13.04.2~cloud0              library for interfacing with different virtualization systems\nii  python-libvirt                    1.0.2-0ubuntu11.13.04.2~cloud0              libvirt Python bindings\nii  kvm                               1:84+dfsg-0ubuntu16+1.0+noroms+0ubuntu14.10 dummy transitional package from kvm to qemu-kvm\nii  qemu-common                       1.0+noroms-0ubuntu14.10                     qemu common functionality (bios, documentation, etc)\nii  qemu-kvm                          1.0+noroms-0ubuntu14.10                     Full virtualization on i386 and amd64 hardware\nii  nova-common                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - common files\nii  nova-compute                      1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node\nii  nova-compute-kvm                  1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node (KVM)\nii  python-nova                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute Python libraries\nii  python-novaclient                 1:2.13.0-0ubuntu1~cloud0                    client library for OpenStack Compute API", 
    "tags": [
        "icehouse-rc-potential", 
        "libvirt", 
        "volumes"
    ], 
    "importance": "Medium", 
    "heat": 24, 
    "link": "https://bugs.launchpad.net/nova/+bug/1240922", 
    "owner": "https://api.launchpad.net/1.0/~ndipanov", 
    "id": 1240922, 
    "index": 3657, 
    "created": "2013-10-17 10:53:21.980958+00:00", 
    "title": "VM don't resume after detaching volume", 
    "comments": [
        {
            "content": "Hi guys,\n\nI have a suspend vm with an attached volume, if I detached volume while instance is in suspend state it can't be resumed properly.\nIt happens with both Windows and Linux vm's.\n\nLibVirt error:\n2494: error : qemuMonitorIORead:502 : Unable to read from monitor: Connection reset by peer\n\nPackets versioning in Ubuntu 12.04:\nii  libvirt-bin                       1.0.2-0ubuntu11.13.04.2~cloud0              programs for the libvirt library\nii  libvirt-dev                       1.0.2-0ubuntu11.13.04.2~cloud0              development files for the libvirt library\nii  libvirt0                          1.0.2-0ubuntu11.13.04.2~cloud0              library for interfacing with different virtualization systems\nii  python-libvirt                    1.0.2-0ubuntu11.13.04.2~cloud0              libvirt Python bindings\nii  kvm                               1:84+dfsg-0ubuntu16+1.0+noroms+0ubuntu14.10 dummy transitional package from kvm to qemu-kvm\nii  qemu-common                       1.0+noroms-0ubuntu14.10                     qemu common functionality (bios, documentation, etc)\nii  qemu-kvm                          1.0+noroms-0ubuntu14.10                     Full virtualization on i386 and amd64 hardware\nii  nova-common                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - common files\nii  nova-compute                      1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node\nii  nova-compute-kvm                  1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node (KVM)\nii  python-nova                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute Python libraries\nii  python-novaclient                 1:2.13.0-0ubuntu1~cloud0                    client library for OpenStack Compute API", 
            "date_created": "2013-10-17 10:53:21.980958+00:00", 
            "author": "https://api.launchpad.net/1.0/~kelfen"
        }, 
        {
            "content": "I got some logs in libvirtd as following:\n\n2013-10-18 03:18:06.721+0000: 22179: error : qemuSetupDiskPathAllow:82 : Unable to allow access for disk path /dev/disk/by-path/ip-9.111.242.65:3260-iscsi-iqn.2010-10.org.openstack:volume-5c973e8e-9854-47d9-bb9f-df0415359217-lun-1: No such file or directory\n2013-10-18 03:18:07.168+0000: 22179: error : virSecurityDACRestoreSecurityFileLabel:143 : cannot resolve symlink /dev/disk/by-path/ip-9.111.242.65:3260-iscsi-iqn.2010-10.org.openstack:volume-5c973e8e-9854-47d9-bb9f-df0415359217-lun-1: No such file or directory\n2013-10-18 03:18:07.606+0000: 22179: error : qemuRemoveCgroup:562 : internal error Unable to find cgroup for instance-0000000d\n2013-10-18 03:18:07.606+0000: 22179: warning : qemuProcessStop:3561 : Failed to remove cgroup for instance-0000000d\n\n", 
            "date_created": "2013-10-18 03:18:39.590532+00:00", 
            "author": "https://api.launchpad.net/1.0/~jay-lau-513"
        }, 
        {
            "content": "Its more like to be a kvm bug: http://support.abiquo.com/entries/23646362-VM-s-on-KVM-won-t-start", 
            "date_created": "2013-10-18 03:23:43.575762+00:00", 
            "author": "https://api.launchpad.net/1.0/~jay-lau-513"
        }, 
        {
            "content": "Bellantuono, do you have any strong requirement to detach volume for a suspended VM? \n\nI want to disable detach volume for a suspend/paused VM, make sense? Thanks.", 
            "date_created": "2013-10-18 08:23:24.947034+00:00", 
            "author": "https://api.launchpad.net/1.0/~jay-lau-513"
        }, 
        {
            "content": "No, I don't have any requirement to detach volume for a suspend VM.\nDisable detach volume when the vm is in suspend/pause mode seems to me an excellent idea!\n\nThanks a lot for the support!\n", 
            "date_created": "2013-10-19 23:35:58.309658+00:00", 
            "author": "https://api.launchpad.net/1.0/~kelfen"
        }, 
        {
            "content": "The solution to this problem may not be all that simple.\n\nWhen you do a live migration between nodes the instance is suspended, copied, then resumed on the target host. When the instance has a volume attached, it appears that the volume is detached after the instance is suspended. I haven't dug into the code to confirm this, but that's the way it looks to me from the logs.", 
            "date_created": "2014-02-05 12:39:48.305886+00:00", 
            "author": "https://api.launchpad.net/1.0/~jesse-pretorius"
        }, 
        {
            "content": "here's the stack trace that shows exactly what the issue is:\n\n2014-03-25 18:41:10.791 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Failed to open file '/dev/disk/by-path/ip-192.168.123.25:3260-iscsi-iqn.2010-10.org.openstack:volume-cecf407a-1c3b\n-4e70-a84f-d34c93fa3f2a-lun-1': No such file or directory\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py\", line 133, in _dispatch_and_reply\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py\", line 176, in _dispatch\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py\", line 122, in _do_dispatch\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/exception.py\", line 88, in wrapped\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     payload)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/exception.py\", line 71, in wrapped\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 278, in decorated_function\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     pass\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 264, in decorated_function\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 329, in decorated_function\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     function(self, context, *args, **kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 306, in decorated_function\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     e, sys.exc_info())\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 293, in decorated_function\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 3729, in resume_instance\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     block_device_info)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 2131, in resume\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     vifs_already_plugged=True)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 3602, in _create_domain_and_network\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     power_on=power_on)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 3512, in _create_domain\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     domain.XMLDesc(0))\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 3507, in _create_domain\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     domain.createWithFlags(launch_flags)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 179, in doit\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 139, in proxy_call\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     rv = execute(f,*args,**kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 77, in tworker\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     rv = meth(*args,**kwargs)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 738, in createWithFlags\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n2014-03-25 18:41:10.791 TRACE oslo.messaging.rpc.dispatcher libvirtError: Failed to open file '/dev/disk/by-path/ip-192.168.123.25:3260-iscsi-iqn.2010-10.org.openstack:volume-cecf407a-1c3b-4e70-a84f-d34c93fa3f2a-lun-1': No such file or directory\n\n\nThis is suspicious behaviour since our domains are persistent and we use the VIR_DOMAIN_AFFECT_CONFIG flag when detaching volumes.\n\nI don't think the solution is to disable deatach for suspended instances. I think we should dig deeper and see why resume does not pick up that the volume has been detached.", 
            "date_created": "2014-03-25 18:12:32.074939+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Looks like I spoke too soon - it seems that the xml we define the domain with in _create_domain_and_network is the correct one, however when we call createWithFlags on such a domain - it seems to throw the above exception.\n\nIt might be libvirt caching something it shouldn't be. I'll get danpb to take a look at this one.", 
            "date_created": "2014-03-25 18:30:08.943419+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "NB, you must not, as a general rule, make any changes to the guest config associated with a managed save image. Doing so creates guest visible ABI changes which will cause the guest to crash & burn when restoring.  The only changes it is safe to make are those which leave the guest visible ABI unchanged", 
            "date_created": "2014-03-26 10:37:31.574905+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Patch proposed https://review.openstack.org/#/c/83505/", 
            "date_created": "2014-03-27 17:08:19.309427+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/83505\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d0948a1fb0a4c425310f0cf0aea5b28680dc4817\nSubmitter: Jenkins\nBranch:    master\n\ncommit d0948a1fb0a4c425310f0cf0aea5b28680dc4817\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Thu Mar 27 18:01:22 2014 +0100\n\n    Disable volume attach/detach for suspended instances\n    \n    As described in the bug - some hypervisors (libvirt) do not support\n    this. It is best to disable it in the API to provide a consistent user\n    experience.\n    \n    Also adds a test to prevent an accidental regression.\n    \n    Change-Id: I5b404cca22cffecbaf524e2511810e5341242052\n    Closes-bug: #1240922\n", 
            "date_created": "2014-03-27 23:20:18.974931+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
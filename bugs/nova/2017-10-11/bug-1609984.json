{
    "status": "Fix Released", 
    "last_updated": "2017-06-16 17:27:41.107165+00:00", 
    "description": "nova/compute/api.py::_local_cleanup_bdm_volumes passes a fake connector to Cinder to ask it to terminate a connection to a volume.  Many Cinder volume drivers need a valid connector that has a real 'host' value in order to terminate the connection on the array.\n\nThe connector being passed in is:\n'connector': {u'ip': u'127.0.0.1', u'initiator': u'iqn.fake'}\n\n\n2016-08-04 13:56:41.672 DEBUG cinder.volume.drivers.hpe.hpe_3par_iscsi [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] ==> terminate_connection: call {'volume': <cinder.db.sqla\nlchemy.models.Volume object at 0x7f1f2f130d10>, 'connector': {u'ip': u'127.0.0.1', u'initiator': u'iqn.fake'}, 'self': <cinder.volume.drivers.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver object at 0x7f1ee0858950>, 'kwargs': {'force': False}} fr\nom (pid=45144) trace_logging_wrapper /opt/stack/cinder/cinder/utils.py:843\n2016-08-04 13:56:41.705 DEBUG cinder.volume.drivers.hpe.hpe_3par_common [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Connecting to 3PAR from (pid=45144) client_login /opt/st\nack/cinder/cinder/volume/drivers/hpe/hpe_3par_common.py:350\n2016-08-04 13:56:42.164 DEBUG cinder.volume.drivers.hpe.hpe_3par_common [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Disconnect from 3PAR REST and SSH 1278aedb-8579-4776-8d8\n5-c46ec93a0551 from (pid=45144) client_logout /opt/stack/cinder/cinder/volume/drivers/hpe/hpe_3par_common.py:374\n2016-08-04 13:56:42.187 DEBUG cinder.volume.drivers.hpe.hpe_3par_iscsi [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] <== terminate_connection: exception (513ms) KeyError('hos\nt',) from (pid=45144) trace_logging_wrapper /opt/stack/cinder/cinder/utils.py:853\n2016-08-04 13:56:42.188 ERROR cinder.volume.manager [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Terminate volume connection failed: 'host'\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager Traceback (most recent call last):\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/volume/manager.py\", line 1457, in terminate_connection\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     force=force)\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/utils.py\", line 847, in trace_logging_wrapper\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     result = f(*args, **kwargs)\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/volume/drivers/hpe/hpe_3par_iscsi.py\", line 478, in terminate_connection\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     hostname = common._safe_hostname(connector['host'])\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager KeyError: 'host'\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager \n2016-08-04 13:56:42.193 ERROR oslo_messaging.rpc.server [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Exception during message handling\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server Traceback (most recent call last):\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 133, in _process_incoming\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 150, in dispatch\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 121, in _do_dispatch\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     result = func(ctxt, **new_args)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/opt/stack/cinder/cinder/volume/manager.py\", line 1462, in terminate_connection\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     raise exception.VolumeBackendAPIException(data=err_msg)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Terminate volume connection failed: 'host'\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server", 
    "tags": [
        "shelve", 
        "volumes"
    ], 
    "importance": "High", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1609984", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1609984, 
    "index": 1963, 
    "created": "2016-08-04 21:08:09.190028+00:00", 
    "title": "volume-detach fails for shelved instance", 
    "comments": [
        {
            "content": "nova/compute/api.py::_local_cleanup_bdm_volumes passes a fake connector to Cinder to ask it to terminate a connection to a volume.  Many Cinder volume drivers need a valid connector that has a real 'host' value in order to terminate the connection on the array.\n\nThe connector being passed in is:\n'connector': {u'ip': u'127.0.0.1', u'initiator': u'iqn.fake'}\n\n\n2016-08-04 13:56:41.672 DEBUG cinder.volume.drivers.hpe.hpe_3par_iscsi [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] ==> terminate_connection: call {'volume': <cinder.db.sqla\nlchemy.models.Volume object at 0x7f1f2f130d10>, 'connector': {u'ip': u'127.0.0.1', u'initiator': u'iqn.fake'}, 'self': <cinder.volume.drivers.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver object at 0x7f1ee0858950>, 'kwargs': {'force': False}} fr\nom (pid=45144) trace_logging_wrapper /opt/stack/cinder/cinder/utils.py:843\n2016-08-04 13:56:41.705 DEBUG cinder.volume.drivers.hpe.hpe_3par_common [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Connecting to 3PAR from (pid=45144) client_login /opt/st\nack/cinder/cinder/volume/drivers/hpe/hpe_3par_common.py:350\n2016-08-04 13:56:42.164 DEBUG cinder.volume.drivers.hpe.hpe_3par_common [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Disconnect from 3PAR REST and SSH 1278aedb-8579-4776-8d8\n5-c46ec93a0551 from (pid=45144) client_logout /opt/stack/cinder/cinder/volume/drivers/hpe/hpe_3par_common.py:374\n2016-08-04 13:56:42.187 DEBUG cinder.volume.drivers.hpe.hpe_3par_iscsi [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] <== terminate_connection: exception (513ms) KeyError('hos\nt',) from (pid=45144) trace_logging_wrapper /opt/stack/cinder/cinder/utils.py:853\n2016-08-04 13:56:42.188 ERROR cinder.volume.manager [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Terminate volume connection failed: 'host'\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager Traceback (most recent call last):\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/volume/manager.py\", line 1457, in terminate_connection\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     force=force)\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/utils.py\", line 847, in trace_logging_wrapper\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     result = f(*args, **kwargs)\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager   File \"/opt/stack/cinder/cinder/volume/drivers/hpe/hpe_3par_iscsi.py\", line 478, in terminate_connection\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager     hostname = common._safe_hostname(connector['host'])\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager KeyError: 'host'\n2016-08-04 13:56:42.188 TRACE cinder.volume.manager \n2016-08-04 13:56:42.193 ERROR oslo_messaging.rpc.server [req-6a382dfe-d1a5-47e7-99bc-e2a383124cd8 aa5ab308cd5b47eb9b2798ec9e2abb32 4b136f9898994fec81393c3b8210980b] Exception during message handling\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server Traceback (most recent call last):\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 133, in _process_incoming\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 150, in dispatch\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 121, in _do_dispatch\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     result = func(ctxt, **new_args)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server   File \"/opt/stack/cinder/cinder/volume/manager.py\", line 1462, in terminate_connection\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server     raise exception.VolumeBackendAPIException(data=err_msg)\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Terminate volume connection failed: 'host'\n2016-08-04 13:56:42.193 TRACE oslo_messaging.rpc.server", 
            "date_created": "2016-08-04 21:08:09.190028+00:00", 
            "author": "https://api.launchpad.net/1.0/~walter-boring"
        }, 
        {
            "content": "Does this fix it? https://review.openstack.org/#/c/257853/", 
            "date_created": "2016-08-04 21:34:49.185939+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Updated https://review.openstack.org/#/c/257853/ to try and see if that fixes this.", 
            "date_created": "2016-08-04 22:13:45.012641+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/257853\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=33510d4be990417d2a3a428106f6f745db5af6ed\nSubmitter: Jenkins\nBranch:    master\n\ncommit 33510d4be990417d2a3a428106f6f745db5af6ed\nAuthor: Andrea Rosa <email address hidden>\nDate:   Tue Dec 15 11:46:10 2015 +0000\n\n    Use stashed volume connector in _local_cleanup_bdm_volumes\n    \n    When we perform a local delete in the compute API during the volumes\n    cleanup, Nova calls the volume_api.terminate_connection passing a fake\n    volume connector. That call is useless and it has no real effect on the\n    volume server side.\n    \n    With a686185fc02ec421fd27270a343c19f668b95da6 in mitaka we started\n    stashing the connector in the bdm.connection_info so this change\n    tries to use that if it's available, which it won't be for attachments\n    made before that change -or- for volumes attached to an instance in\n    shelved_offloaded state that were never unshelved (because the actual\n    volume attach for those instances happens on the compute manager after\n    you unshelve the instance).\n    \n    If we can't find a stashed connector, or we find one whose host does\n    not match the instance.host, we punt and just don't call\n    terminate_connection at all since calling it with a fake connector\n    can actually make some cinder volume backends fail and then the\n    volume is orphaned.\n    \n    Closes-Bug: #1609984\n    \n    Co-Authored-By: Matt Riedemann <email address hidden>\n    \n    Change-Id: I9f9ead51238e27fa45084c8e3d3edee76a8b0218\n", 
            "date_created": "2016-08-06 00:21:45.326527+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/352050\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=2df5ef79aaa0da8e70d02a006d50deaffe0a149e\nSubmitter: Jenkins\nBranch:    stable/mitaka\n\ncommit 2df5ef79aaa0da8e70d02a006d50deaffe0a149e\nAuthor: Andrea Rosa <email address hidden>\nDate:   Tue Dec 15 11:46:10 2015 +0000\n\n    Use stashed volume connector in _local_cleanup_bdm_volumes\n    \n    When we perform a local delete in the compute API during the volumes\n    cleanup, Nova calls the volume_api.terminate_connection passing a fake\n    volume connector. That call is useless and it has no real effect on the\n    volume server side.\n    \n    With a686185fc02ec421fd27270a343c19f668b95da6 in mitaka we started\n    stashing the connector in the bdm.connection_info so this change\n    tries to use that if it's available, which it won't be for attachments\n    made before that change -or- for volumes attached to an instance in\n    shelved_offloaded state that were never unshelved (because the actual\n    volume attach for those instances happens on the compute manager after\n    you unshelve the instance).\n    \n    If we can't find a stashed connector, or we find one whose host does\n    not match the instance.host, we punt and just don't call\n    terminate_connection at all since calling it with a fake connector\n    can actually make some cinder volume backends fail and then the\n    volume is orphaned.\n    \n    Closes-Bug: #1609984\n    \n    Co-Authored-By: Matt Riedemann <email address hidden>\n    \n    Change-Id: I9f9ead51238e27fa45084c8e3d3edee76a8b0218\n    (cherry picked from commit 33510d4be990417d2a3a428106f6f745db5af6ed)\n", 
            "date_created": "2016-08-24 04:12:54.069302+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 14.0.0.0b3 development milestone.", 
            "date_created": "2016-09-02 01:15:00.802059+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.1.2 release.", 
            "date_created": "2016-10-10 13:20:24.786614+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.1.2 release.", 
            "date_created": "2016-11-10 02:06:30.929241+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
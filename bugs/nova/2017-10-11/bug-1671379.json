{
    "status": "In Progress", 
    "last_updated": "2017-06-28 12:04:59.950859+00:00", 
    "description": "Description\n===========\nNormaly, VM which migrates to destination node can send several RARP packets during KVM's live-migration in a simple ovs + vlan environment after a bug is fixed.\nThe ovs + vlan bug url\uff1a \nhttps://bugs.launchpad.net/neutron/+bug/1414559\n\nIn neutron ML2 hierarchical port binding environment,\nI find that the physical port associated to a vlan physical provider's ovs bridge on destination node cannot dump any rarp packets when VM migrates to destination node.\n\nSteps to reproduce\n==================\n1. create a vxlan type network:   netA\n2. create a subnet for netA:      subA\n3. create a vm in compute1 node:  vmA\n4. tcpdump the physical port associated to a ovs bridge in compute2 node:\n\u00a0 tcpdump -i ens33 -w ens33.pcap\n5. live migrate the vm to the other compute node: compute2 node\n6. open ens33.pcap in wireshark\n\nExpected result\n===============\nfind several rarp packets\n\nActual result\n=============\nfind not any rarp packets\n\nEnvironment\n===========\nOpenStack\uff1aKilo  2015.1.2\nOS: CentOS 7.1.1503\nLibvirt\uff1a1.2.17\n\nLogs & Configs\n==============\nhierarchical port binding configuration:\ncontroller node:\n#neutron\n/etc/neutron/plugins/ml2/ml2_conf.ini\n[ml2]\ntype_drivers = vxlan,vlan\ntenant_network_types = vxlan,vlan\nmechanism_drivers=ml2_h3c,openvswitch\n#ml2_h3c, a mechanism driver owned by New H3C Group which is a provider of New IT\n#solutions , allocates dynamic vlan segment for the existing mechanism driver\n#\"openvswitch\"\n\n[ml2_type_vlan]\nnetwork_vlan_ranges = compute1_physicnet1:100:1000, compute2_physicnet1:100:1000\n[ml2_type_vxlan]\nvni_ranges=1:500\n\ncompute1 node:\n#neutron\n/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini\n[ovs]\nbridge_mappings=compute1_physicnet1:br-ens33\n\ncompute2 node:\n#neutron\n/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini\n[ovs]\nbridge_mappings=compute2_physicnet1:br-ens33\n\nAnalysis\n==============\nAfter reading the live-migration relevant code of nova, neutron-server and neutron-openvswitch-agent, I think that it may be a bug.\n\nThe brief relevant process\uff1a\n\n1. source compute node(nova-compute)  compute1 node\n\u00a0\u00a0self.driver(libvirt).live_migration\n\u00a0\u00a0\u00a0dom.migrateToURI2 ---------------Excecute migration to dest node\n\u00a0\u00a0\u00a0\u00a0self._live_migration_monitor------------------ Monitor migration finished\n\u00a0\u00a0\u00a0\u00a0\u00a0self._post_live_migration ---------------- Migration finished\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.compute_rpcapi.post_live_migration_at_destination  --- Notify\n\u00a0\u00a0\u00a0                                                        destination node\n\n2.1. destination compute node (neutron-openvswitch-agent)   compute2 node\n\u00a0\u00a0\u00a0rpc_loop   ------ monitor vm's tapxxxx port plug\n\u00a0\u00a0\u00a0\u00a0self.process_network_ports\n\u00a0\u00a0\u00a0\u00a0\u00a0self.treat_devices_added_or_updated\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.plugin_rpc.get_devices_details_list  -------The port details shows that\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0the port still is bound to\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"compute1_physicnet1\", not the physical network\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0provider \"compute2_physicnet1\" existing in\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  destination compute node.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.treat_vif_port\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.port_bound\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.provision_local_vlan ---  There is not matched physical bridge at\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0the time. As a result, the tap port can not been set any\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0vlan tag.Eventually, br-ens33, the physical bridge,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0drops rarp packets from the starting vm.\n\n2.2 destination compute node (nova-compute)   compute2 node\n\u00a0\u00a0\u00a0post_live_migration_at_destination   nova/compute/manager.py\n\u00a0\u00a0\u00a0\u00a0self.network_api.migrate_instance_finish\n\u00a0\u00a0\u00a0\u00a0\u00a0self._update_port_binding_for_instance ------------Notify neutron migrate port\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 binding:host_id\n\n3. controller node(neutron-server)\n\u00a0\u00a0\u00a0ml2_h3c: fill self._new_bound_segment and self._next_segments_to_bind with\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 compute2_physicnet1 for openvswitch driver\n\u00a0\u00a0\u00a0openvswitch: bind port with compute2_physicnet1's allocated segment from level 0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 driver ml2_h3c\n\nIn the current process of kilo, ml2 driver finishes port bind at the last step 3.\nit's too late to make neutron-openvswitch-agent get suitable port details from\nneutron-server to set correct vlan tag for vm port and adds relevant flow for ovs bridges\nthat nova  notifies neutron-server the event that port changes binding_hostid in ml2\nhierarchical port binding.\n\nIt seems that liberty, mitaka exists the same problem.", 
    "tags": [
        "live-migration", 
        "openstack-version.kilo"
    ], 
    "importance": "Undecided", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1671379", 
    "owner": "None", 
    "id": 1671379, 
    "index": 6718, 
    "created": "2017-03-09 08:21:53.267906+00:00", 
    "title": "The first VM of one network in one compute node cannot send RARP packets during KVM's live-migration in a neutron ML2 hierachical port binding environment whose second mechanism driver was configured as the existing OVS driver  'openvswitch'", 
    "comments": [
        {
            "content": "Description\n===========\nNormaly, VM which migrates to destination node can send several RARP packets during KVM's live-migration in my openstack environment.\nIn neutron ML2 hierarchical port binding environment,\nI find that the physical port associated to a vlan physical provider's ovs bridge on destination node cannot dump any rarp packets when VM migrates to destination node.\n\n\n\nSteps to reproduce\n==================\n1. create a vxlan type network:   netA    \n2. create a subnet for netA:      subA\n3. create a vm in compute1 node:  vmA\n4. tcpdump the physical port associated to a ovs bridge in compute2 node:  tcpdump -i ens33 -w ens33.pcap \n5. live migrate the vm to the other compute node: compute2 node \n6. open ens33.pcap in wireshark\n\n\n\n\nExpected result\n===============\nfind several rarp packets \n\n\n\n\n\nActual result\n=============\nfind not any rarp packets\n\n\nEnvironment\n===========\nOpenStack\uff1aKilo  2015.1.2\nOS: CentOS 7.1.1503\nLibvirt\uff1a1.2.17\n\n\n\n\nLogs & Configs\n==============\nhierarchical port binding configuration:\ncontroller node: \n#neutron   \n/etc/neutron/plugins/ml2/ml2_conf.ini\n[ml2]\ntype_drivers = vxlan,vlan\ntenant_network_types = vxlan,vlan\nmechanism_drivers=ml2_h3c,openvswitch \n#ml2_h3c, a mechanism driver owned by New H3C Group which is a provider of New IT solutions , allocates dynamic\n#vlan segment for the existing mechanism driver \"openvswitch\"  \n\n[ml2_type_vlan]\nnetwork_vlan_ranges = compute1_physicnet1:100:1000, compute2_physicnet1:100:1000\n[ml2_type_vxlan]\nvni_ranges=1:500\n\n\n\ncompute1 node:\n#neutron   \n/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini\n[ovs]\nbridge_mappings=compute1_physicnet1:br-ens33\n\n\ncompute2 node:\n#neutron   \n/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini\n[ovs]\nbridge_mappings=compute2_physicnet1:br-ens33\n\n\n\n\n\n\nAnalysis\n==============\nAfter reading the live-migration relevant code of nova, neutron-server and neutron-openvswitch-agent, I think that it may be a bug.\n\nThe brief relevant process\uff1a\n\n1. source compute node(nova-compute)  compute1 node\n   self.driver(libvirt).live_migration\n         dom.migrateToURI2 ---------------Excecute migration to dest node\n         self._live_migration_monitor------------------ Monitor migration finished\n             self._post_live_migration ---------------- Migration finished\n                 self.compute_rpcapi.post_live_migration_at_destination  --------- Notify destination node\n\n\n2.1. destination compute node (neutron-openvswitch-agent)   compute2 node\n   rpc_loop   ------ monitor vm's tapxxxx port plug     \n      self.process_network_ports\n         self.treat_devices_added_or_updated\n              self.plugin_rpc.get_devices_details_list  -------The port details shows that the port still is bound to \n                                                               \"compute1_physicnet1\", not the physical network\n                                                                provider \"compute2_physicnet1\" existing in\n                                                               destination compute node.\n              self.treat_vif_port\n                  self.port_bound\n                      self.provision_local_vlan -----------  There is not matched physical bridge at the time. As a                \n                                                             result, the tap port can not been set any vlan tag. \n                                                             Eventually, br-ens33, the physical bridge, drops rarp \n                                                             packets from the starting vm. \n    \n   \n\n2.2 destination compute node (nova-compute)   compute2 node\n    post_live_migration_at_destination   nova/compute/manager.py\n        self.network_api.migrate_instance_finish\n            self._update_port_binding_for_instance ------------Notify neutron migrate port binding:host_id\n\n\n\n\n3. controller node(neutron-server)   \n   ml2_h3c: fill self._new_bound_segment and self._next_segments_to_bind with compute2_physicnet1\n            for openvswitch driver \n   openvswitch: bind port with compute2_physicnet1's allocated segment from level 0 driver ml2_h3c\n\n\nIn the current process of kilo, ml2 driver finishes port bind at the last step 3. \nit's too late to make neutron-openvswitch-agent get suitable port details from neutron-server \nto set correct vlan tag for vm port and adds relevant flow for ovs bridges that nova  notifies neutron-server the \nevent that port changes binding_hostid in ml2 hierarchical port binding.\n\nIt seems that liberty, mitaka exists the same problem.", 
            "date_created": "2017-03-09 08:21:53.267906+00:00", 
            "author": "https://api.launchpad.net/1.0/~gooduone"
        }, 
        {
            "content": "This sounds like something that wouldn't be resolved until we have the multiple port bindings spec in place to get the switch mech driver to wire up the additional vlan.", 
            "date_created": "2017-03-09 19:15:57.201339+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevinbenton"
        }, 
        {
            "content": "Can you see if the solution in https://bugs.launchpad.net/neutron/+bug/1414559 would have helped?", 
            "date_created": "2017-03-09 19:17:49.922524+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevinbenton"
        }, 
        {
            "content": "https://bugs.launchpad.net/neutron/+bug/1414559\n\nI read the bug 's description and fixed codes last week.\n\nThe fixed codes only can resolve this problem that one ml2 driver named \"openvswitch\" cannot send rarp packets during live-migration rather than the multiple ml2 driver port bindings problem I committed\n\n", 
            "date_created": "2017-03-13 06:26:40.296833+00:00", 
            "author": "https://api.launchpad.net/1.0/~gooduone"
        }, 
        {
            "content": "It might be related to this https://bugs.launchpad.net/nova/+bug/1605016. Have patch for it where port binding in done based on libvirt events instead if happening in 3 rd step as mentioned above. https://review.openstack.org/#/c/434870/. Will try to replicate this issue on top of my patch and see what happens.", 
            "date_created": "2017-03-27 15:20:38.946488+00:00", 
            "author": "https://api.launchpad.net/1.0/~siva-radhakrishnan"
        }, 
        {
            "content": "Hello, Zhipeng Shen, have you tested the patch mentioned in comment #4?", 
            "date_created": "2017-03-28 08:48:32.469508+00:00", 
            "author": "https://api.launchpad.net/1.0/~tdurakov"
        }, 
        {
            "content": "Hello, Timofey Durakov. I'm sorry to reply so late.\n\nI'm busy doing somethings recently, and I should be able to test it next month.\n", 
            "date_created": "2017-04-06 07:39:14.200387+00:00", 
            "author": "https://api.launchpad.net/1.0/~gooduone"
        }, 
        {
            "content": "Hello, Timofey Durakov, Sivasathurappan Radhakrishnan\nI don't test the patch mentioned in comment #4 yet.\nThe libvirt version of the bug (https://bugs.launchpad.net/nova/+bug/1671379)  environment is 1.2.17\uff0c it is less than the lowest version of the post-copy bug ( https://bugs.launchpad.net/neutron/+bug/1414559).\n\n\nI'm sure that the bug I committed  is concerned with  ml2-hierarchical-port-binding.\n\nml2-hierarchical-port-binding pages:\nSpecification:\nhttp://specs.openstack.org/openstack/neutron-specs/specs/kilo/ml2-hierarchical-port-binding.html\n\nLaunchpad blueprint:\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-hierarchical-port-binding\n", 
            "date_created": "2017-04-30 10:34:55.598480+00:00", 
            "author": "https://api.launchpad.net/1.0/~gooduone"
        }, 
        {
            "content": "Automatically discovered version kilo in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:57:56.364521+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2014-09-16 13:05:16.289472+00:00", 
    "description": "Description of problem:\nWhile testing around Bug 1020979, I've installed RHEL 6.5 on a volume (size, 50 GB), took a snapshot of the volume and tried to launch an instance with it.\n\nThe topology of RHOS is:\n- Cloud controller + compute node.\n- Stand alone compute node\n- Stand alone Cinder with GlusterFS back end.\n- Stand alone Glance\n\nThe compute logs:\n\n2014-03-03 17:54:49.322 9544 INFO nova.virt.libvirt.firewall [req-c986c6d9-3c36-4bfe-943e-a80d62d15ae1 None None] [instance: 2ec5753e-70bc-428f-8a2c-15cd56a56400] Ensuring static filters\n2014-03-03 17:54:50.554 9544 ERROR nova.openstack.common.threadgroup [-] End of file while reading data: Input/output error\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py\", line 117, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     x.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py\", line 49, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.thread.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 168, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/event.py\", line 116, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py\", line 187, in switch\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 194, in main\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/service.py\", line 65, in run_service\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     service.start()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 164, in start\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 802, in pre_start_hook\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 4886, in update_available_resource\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     nodenames = set(self.driver.get_available_nodes())\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/driver.py\", line 963, in get_available_nodes\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     stats = self.get_host_stats(refresh=refresh)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4432, in get_host_stats\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.host_state.get_host_stats(refresh=refresh)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 386, in host_state\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self._host_state = HostState(self)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4832, in __init__\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.update_status()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4886, in update_status\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.driver.get_pci_passthrough_devices()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 3670, in get_pci_passthrough_devices\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     pci_dev = self._get_pcidev_info(name)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 3624, in _get_pcidev_info\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     xmlstr = virtdev.XMLDesc(0)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib64/python2.6/site-packages/libvirt.py\", line 3843, in XMLDesc\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     if ret is None: raise libvirtError ('virNodeDeviceGetXMLDesc() failed')\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup libvirtError: End of file while reading data: Input/output error\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup \n\nVersion-Release number of selected component (if applicable):\nopenstack-nova-common-2013.2.2-2.el6ost.noarch\nopenstack-nova-compute-2013.2.2-2.el6ost.noarch\npython-novaclient-2.15.0-2.el6ost.noarch\n\nHow reproducible:\n100%\n\nSteps to Reproduce:\n1. Configure the compute node to run with libgfapi\n2. Install an OS on a volume.\n3. Take a snapshot of the volume.\n4. Launch an instance from the volume's snapshot.\n\nActual results:\nThe instance failed to launch.\n\nExpected results:\nThe instance Active.\n\nAdditional info:", 
    "tags": [
        "gluster", 
        "volumes"
    ], 
    "importance": "Medium", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1287622", 
    "owner": "None", 
    "id": 1287622, 
    "index": 3815, 
    "created": "2014-03-04 09:35:22.678340+00:00", 
    "title": "fail to launch an instance from a volume snapshot with Gluster libgfapi", 
    "comments": [
        {
            "content": "Description of problem:\nWhile testing around Bug 1020979, I've installed RHEL 6.5 on a volume (size, 50 GB), took a snapshot of the volume and tried to launch an instance with it.\n\nThe topology of RHOS is:\n- Cloud controller + compute node.\n- Stand alone compute node\n- Stand alone Cinder with GlusterFS back end.\n- Stand alone Glance\n\nThe compute logs:\n\n2014-03-03 17:54:49.322 9544 INFO nova.virt.libvirt.firewall [req-c986c6d9-3c36-4bfe-943e-a80d62d15ae1 None None] [instance: 2ec5753e-70bc-428f-8a2c-15cd56a56400] Ensuring static filters\n2014-03-03 17:54:50.554 9544 ERROR nova.openstack.common.threadgroup [-] End of file while reading data: Input/output error\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py\", line 117, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     x.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py\", line 49, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.thread.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 168, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/event.py\", line 116, in wait\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py\", line 187, in switch\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 194, in main\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/openstack/common/service.py\", line 65, in run_service\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     service.start()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 164, in start\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 802, in pre_start_hook\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 4886, in update_available_resource\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     nodenames = set(self.driver.get_available_nodes())\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/driver.py\", line 963, in get_available_nodes\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     stats = self.get_host_stats(refresh=refresh)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4432, in get_host_stats\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     return self.host_state.get_host_stats(refresh=refresh)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 386, in host_state\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self._host_state = HostState(self)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4832, in __init__\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.update_status()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 4886, in update_status\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     self.driver.get_pci_passthrough_devices()\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 3670, in get_pci_passthrough_devices\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     pci_dev = self._get_pcidev_info(name)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py\", line 3624, in _get_pcidev_info\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     xmlstr = virtdev.XMLDesc(0)\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup   File \"/usr/lib64/python2.6/site-packages/libvirt.py\", line 3843, in XMLDesc\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup     if ret is None: raise libvirtError ('virNodeDeviceGetXMLDesc() failed')\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup libvirtError: End of file while reading data: Input/output error\n2014-03-03 17:54:50.554 9544 TRACE nova.openstack.common.threadgroup \n\nVersion-Release number of selected component (if applicable):\nopenstack-nova-common-2013.2.2-2.el6ost.noarch\nopenstack-nova-compute-2013.2.2-2.el6ost.noarch\npython-novaclient-2.15.0-2.el6ost.noarch\n\nHow reproducible:\n100%\n\nSteps to Reproduce:\n1. Configure the compute node to run with libgfapi\n2. Install an OS on a volume.\n3. Take a snapshot of the volume.\n4. Launch an instance from the volume's snapshot.\n\nActual results:\nThe instance failed to launch.\n\nExpected results:\nThe instance Active.\n\nAdditional info:", 
            "date_created": "2014-03-04 09:35:22.678340+00:00", 
            "author": "https://api.launchpad.net/1.0/~yrabl"
        }, 
        {
            "content": "What version of gluster and qemu do you have?  I believe you need qemu >=1.3 and glusterfs >= 3.4.", 
            "date_created": "2014-04-12 00:11:03.299712+00:00", 
            "author": "https://api.launchpad.net/1.0/~thang-pham"
        }, 
        {
            "content": "Also, does the new volume appear under your gluster mount point?", 
            "date_created": "2014-04-12 00:44:36.022932+00:00", 
            "author": "https://api.launchpad.net/1.0/~thang-pham"
        }, 
        {
            "content": "I had some trouble at first, but after re-compiling qemu to have glusterfs enabled, I was able to launch an instance from gluster volume on a compute node configured with libgfapi.  This site helped me to get it working https://www.gluster.org/category/qemu/.\n\nAccording to it:\n\nWhile building QEMU from source, in addition to the normal configuration options, ensure that \u2013enable-uuid and \u2013enable-glusterfs options are is specified explicitly with ./configure script.  (Update Feb 2013: A fix in QEMU-1.3 time frame makes the use of \u2013enable-uuid unnecessary for GlusterFS support in QEMU)\n\nUpdate Aug 2013: Starting with QEMU-1.6, pkg-config is used to configure the GlusterFS backend in QEMU. If you are using GlusterFS compiled and installed from sources, then the GlusterFS package config file (glusterfs-api.pc) might not be present at the standard path and you will have to explicitly add the path by executing this command before running the QEMU configure script:", 
            "date_created": "2014-04-12 02:00:27.636188+00:00", 
            "author": "https://api.launchpad.net/1.0/~thang-pham"
        }, 
        {
            "content": "can you retest this with the above information?", 
            "date_created": "2014-06-18 18:43:37.213179+00:00", 
            "author": "https://api.launchpad.net/1.0/~tjones-i"
        }, 
        {
            "content": "Long time incomplete bug", 
            "date_created": "2014-09-16 13:05:15.531122+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
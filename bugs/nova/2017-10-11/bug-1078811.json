{
    "status": "Invalid", 
    "last_updated": "2013-03-26 14:59:07.781173+00:00", 
    "description": "I'm seeing some odd behavior with nova-compute taking exponentially longer to fully create/spawn instances as the number of instances on any given node increases.\n\nThis doesn't seem to be load/CPU utilization related as I'm  still fairly idle on that front (my two nova-compute nodes have 40 cores each....4x 10 core sockets).\n\nOccasionally the instance spawn seems to get caught in some audit task where nova-compute will run 'qemu-img info' against each of the instance disks. This process typically inputs a very large delay between the instance spawn AMQP message to nova-compute (see timestamp 2012-11-13 05:59:54 in the attached logfile), and nova-compute generating the XML for the instance to send to libvirt/KVM (see timestamp 2012-11-13 06:19:26 in the attached logfile). While in this example it took about 20 minutes, this can cause the instances to take as much as an hour to spawn, depending on how many instances are hosted on that nova-compute node.\n\nIn other occasions (even with a similarly high instance count, and sometimes right after an instance that took significantly longer to create), there is no delay between the instance creation task start (see timestamp 2012-11-13 06:30:09 in the attached logfile) and the XML generation for libvirt (see timestamp 2012-11-13 06:30:21 in the attached logfile). Likewise, these instances create predictably and consistently (usually about 2 or 3 minutes after sending the XML to libvirt/KVM on a busy host).\n\nThe inconsistent and sometimes excessively long instance creation time is definitely causing me some headaches with automation and general expectations to set around performance in our OpenStack environment.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 24, 
    "link": "https://bugs.launchpad.net/nova/+bug/1078811", 
    "owner": "None", 
    "id": 1078811, 
    "index": 3205, 
    "created": "2012-11-14 17:21:57.695748+00:00", 
    "title": "nova-compute instance creation time increases as instance count increases", 
    "comments": [
        {
            "content": "I'm seeing some odd behavior with nova-compute taking exponentially longer to fully create/spawn instances as the number of instances on any given node increases.\n\nThis doesn't seem to be load/CPU utilization related as I'm  still fairly idle on that front (my two nova-compute nodes have 40 cores each....4x 10 core sockets).\n\nOccasionally the instance spawn seems to get caught in some audit task where nova-compute will run 'qemu-img info' against each of the instance disks. This process typically inputs a very large delay between the instance spawn AMQP message to nova-compute (see timestamp 2012-11-13 05:59:54 in the attached logfile), and nova-compute generating the XML for the instance to send to libvirt/KVM (see timestamp 2012-11-13 06:19:26 in the attached logfile). This can cause the instances to take as much as an hour to spawn, depending on how many instances are hosted on that nova-compute node.\n\nIn other occasions (even with a similarly high instance count, and sometimes right after an instance that took significantly longer to create), there is no delay between the instance creation task start (see timestamp 2012-11-13 06:30:09 in the attached logfile) and the XML generation for libvirt (see timestamp 2012-11-13 06:30:21 in the attached logfile). Likewise, these instances create predictably and consistently (usually about 3 or 4 minutes).\n\nThe inconsistent and sometimes excessively long instance creation time is definitely causing me some headaches with automation and general expectations to set around performance in our OpenStack environment.", 
            "date_created": "2012-11-14 17:21:57.695748+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "", 
            "date_created": "2012-11-14 17:21:57.695748+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "Adding a truncated logfile that still includes the aforementioned timestamps. If you don't care to look through other duplications of this same behavior, you should still be able to find everything mentioned here.", 
            "date_created": "2012-11-14 17:28:16.361517+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "Do u have the md5/image caching/hashing on? Can u give us a top output of your loaded 'node'", 
            "date_created": "2012-11-19 23:41:03.396910+00:00", 
            "author": "https://api.launchpad.net/1.0/~harlowja"
        }, 
        {
            "content": "The qemu-img info commands are coming from the compute node's periodic resource tracker.  I think it should be quick to execute, or at least, it is where I've tried it.  I suspect it's a red herring.  The image hashing that Josh mentioned will definitely cause this kind of long pause if you have any large base image files.\n\nTo check if it's the resource tracker, I think you can just disable it: comment out the call to update_available_resource around nova/compute/manager.py:2700.\n\nIf it's the image cache checksum code, you'll see nova compute itself burning 100% CPU on one of your nodes.  gdb or perf top will show it in some crypto hash routine.", 
            "date_created": "2012-11-20 00:23:28.012817+00:00", 
            "author": "https://api.launchpad.net/1.0/~timjr"
        }, 
        {
            "content": "Disabling image checksums explicitly (checksum_base_images=false) seems to have improved the performance, but I don't have any solid comparison data to know for sure. I might spend a bit later to investigate this further, but this might need a doc change if this is the case. The documentation indicates that this feature is disabled by default:\nhttp://docs.openstack.org/trunk/openstack-compute/admin/content/hypervisor-configuration-basics.html\n\nI'm still seeing the occasional delays. I'm going to spend some time today disabling the periodic resource tracker features and see if that takes me any further.", 
            "date_created": "2012-11-20 18:29:43.376856+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "Up until recently, you'd have to actually remove the .sha1 property from the images in the image cache on the compute nodes to get it to stop checksumming.  It used to be in a separate file called <image hash>.sha1, in the instances/_base directory, but then it became one of the keys in a file names <image hash>.info, which is in json format.", 
            "date_created": "2012-11-21 01:58:12.286142+00:00", 
            "author": "https://api.launchpad.net/1.0/~timjr"
        }, 
        {
            "content": "Matt, is this still happening for you?", 
            "date_created": "2012-12-05 03:40:36.014410+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Apologies for the lack of updates.\n\nDisabling image hashing did not resolve the issue. I think the next logical step would be to disable periodic resource audits and see if that improve things. I now *only* see the delay when an audit occurs.\n\nFor now, I've moved to a more scale-out design (5 smaller compute hosts instead of 2 huge ones) which has worked wonders.\n\nAnything else I can provide or should I run a comparison with enabled/disabled periodic resource audits?", 
            "date_created": "2012-12-05 03:54:26.114365+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "Both of the aforementioned functions were contributing to my performance issues.\n\nI saw an improvement in performance by disabling image hashing (explicitly set checksum_base_images=false in nova.conf). This should likely be split as a documentation update for the following document which states that this is already disabled by default, or changed to actually make it the default:\nhttp://docs.openstack.org/trunk/openstack-compute/admin/content/hypervisor-configuration-basics.html\n\nThe audit was definitely still contributing though, and after I had disabled image hashing, my instance creation delays occurred only around an audit task. I saw a delay as large as 8 minutes when there were ~120 instances on a single nova-compute host. At this point load on the nova-compute host was still around 1-3 with over 85% idle CPU.\n\nWith the audit task disabled (comment the call to update_available_resource in nova/compute/manager.py:2734), performance was normal with no more instance creation delays.\n\nThis causes a number of headaches for clouds attempting large VM/host consolidation ratios, which is very possible with larger hardware configurations. It would be awesome if nova-compute didn't just hang out and wait for the audit task to finish (maybe this could be pushed to another process or thread(s)?). 120 instances is nowhere near the capacity of our nova-compute hosts, but the audit delay makes performance pretty unpredictable when deploying new workloads.\n\nGlad to provide log information/top screenshots from these two tests if it would be any additional help.", 
            "date_created": "2012-12-11 19:59:55.880494+00:00", 
            "author": "https://api.launchpad.net/1.0/~mmalesky"
        }, 
        {
            "content": "could this bug be caused by instance_claim and update_available_resource in resource_tracker.py tripping over each other to claim COMPUTE_RESOURCE_SEMAPHORE?", 
            "date_created": "2012-12-14 21:32:14.165273+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/18532", 
            "date_created": "2012-12-21 07:56:56.337252+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Unassigning since the review was abandoned, reassign to you if you still work on it.", 
            "date_created": "2013-01-09 11:33:30.193920+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Since we use eventlet monkey patch lots of modules, includes `thread` `os` .etc .  Just fork another process seems not graceful and may cause some confused bugs.\n\nDo u think  create a new service named `nova-period` is good?", 
            "date_created": "2013-02-21 04:10:19.521718+00:00", 
            "author": "https://api.launchpad.net/1.0/~gtt116"
        }, 
        {
            "content": "There has been some thought put into how to move the image cache verification to its own process so it doesn't block nova-compute, but its not going to make it into grizzly. This is mainly because it requires a pretty large reworking of the code, which we don't want to do this close to a release (it might destabilizing things).\n\n@openstack-manuals: I've added you guys so you can check the comment about image cache verification being stated to be off in the docs that is reported in comment #9 here.\n\nI'm going to thing about smallish improvements (stuff I can get into grizzly) to the audit process today and will get back to you.", 
            "date_created": "2013-03-11 11:37:00.337344+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "I took another look at this just now. The resource tracker needs to run these \"qemu-img info\" commands to know how badly the disk on the machine is over committed (the image files aren't full populated, so the size the file things it is can be significantly bigger than what the filesystems thinks it is). We could add some caching here, but I think that's too risky for grizzly.\n\nAlso, executing the external command counts as a greenthread yield, which means that this shouldn't be blocking other instances from starting. I can't see any locks that would get in the way from a visual inspection.\n\n@matt -- can you confirm that things are behaving reasonably when you have the image cache verification turned off? I'm not sure there's much else we can do here for you this release.", 
            "date_created": "2013-03-11 14:15:40.303000+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Resolving this one, but please reopen if you disagree. I have filed https://bugs.launchpad.net/nova/+bug/1154162 to track adding caching to our calls to \"qemu-img info\".", 
            "date_created": "2013-03-12 15:44:40.864936+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "To me, the documentation is correct in that it shows checksum_base_images=false which is what is in the nova.conf.sample file in the nova repository. Marking won't fix for docs.", 
            "date_created": "2013-03-26 14:58:58.340910+00:00", 
            "author": "https://api.launchpad.net/1.0/~annegentle"
        }
    ]
}
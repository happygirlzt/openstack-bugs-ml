{
    "status": "Fix Released", 
    "last_updated": "2014-10-16 08:53:32.942409+00:00", 
    "description": "http://logs.openstack.org/62/114062/3/gate/gate-nova-python27/2536ea4/console.html\n\n FAIL: nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_terminate_sigterm\n\n014-08-15 13:46:09.155 | INFO [nova.tests.integrated.api.client] Doing GET on /v2/openstack//flavors/detail\n2014-08-15 13:46:09.155 | INFO [nova.tests.integrated.test_multiprocess_api] sent launcher_process pid: 10564 signal: 15\n2014-08-15 13:46:09.155 | INFO [nova.tests.integrated.test_multiprocess_api] waiting on process 10566 to exit\n2014-08-15 13:46:09.155 | INFO [nova.wsgi] Stopping WSGI server.\n2014-08-15 13:46:09.155 | }}}\n2014-08-15 13:46:09.156 | \n2014-08-15 13:46:09.156 | Traceback (most recent call last):\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 206, in test_terminate_sigterm\n2014-08-15 13:46:09.156 |     self._terminate_with_signal(signal.SIGTERM)\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 194, in _terminate_with_signal\n2014-08-15 13:46:09.156 |     self.wait_on_process_until_end(pid)\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 146, in wait_on_process_until_end\n2014-08-15 13:46:09.157 |     time.sleep(0.1)\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/greenthread.py\", line 31, in sleep\n2014-08-15 13:46:09.157 |     hub.switch()\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 287, in switch\n2014-08-15 13:46:09.157 |     return self.greenlet.switch()\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 339, in run\n2014-08-15 13:46:09.158 |     self.wait(sleep_time)\n2014-08-15 13:46:09.158 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/poll.py\", line 82, in wait\n2014-08-15 13:46:09.158 |     sleep(seconds)\n2014-08-15 13:46:09.158 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/fixtures/_fixtures/timeout.py\", line 52, in signal_handler\n2014-08-15 13:46:09.158 |     raise TimeoutException()\n2014-08-15 13:46:09.158 | TimeoutException", 
    "tags": [
        "in-stable-icehouse", 
        "testing"
    ], 
    "importance": "Critical", 
    "heat": 34, 
    "link": "https://bugs.launchpad.net/nova/+bug/1357578", 
    "owner": "https://api.launchpad.net/1.0/~sdague", 
    "id": 1357578, 
    "index": 203, 
    "created": "2014-08-15 22:25:14.846554+00:00", 
    "title": "Unit test:  nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_terminate_sigterm timing out in gate", 
    "comments": [
        {
            "content": "http://logs.openstack.org/62/114062/3/gate/gate-nova-python27/2536ea4/console.html\n\n FAIL: nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_terminate_sigterm\n\n014-08-15 13:46:09.155 | INFO [nova.tests.integrated.api.client] Doing GET on /v2/openstack//flavors/detail\n2014-08-15 13:46:09.155 | INFO [nova.tests.integrated.test_multiprocess_api] sent launcher_process pid: 10564 signal: 15\n2014-08-15 13:46:09.155 | INFO [nova.tests.integrated.test_multiprocess_api] waiting on process 10566 to exit\n2014-08-15 13:46:09.155 | INFO [nova.wsgi] Stopping WSGI server.\n2014-08-15 13:46:09.155 | }}}\n2014-08-15 13:46:09.156 | \n2014-08-15 13:46:09.156 | Traceback (most recent call last):\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 206, in test_terminate_sigterm\n2014-08-15 13:46:09.156 |     self._terminate_with_signal(signal.SIGTERM)\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 194, in _terminate_with_signal\n2014-08-15 13:46:09.156 |     self.wait_on_process_until_end(pid)\n2014-08-15 13:46:09.156 |   File \"nova/tests/integrated/test_multiprocess_api.py\", line 146, in wait_on_process_until_end\n2014-08-15 13:46:09.157 |     time.sleep(0.1)\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/greenthread.py\", line 31, in sleep\n2014-08-15 13:46:09.157 |     hub.switch()\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 287, in switch\n2014-08-15 13:46:09.157 |     return self.greenlet.switch()\n2014-08-15 13:46:09.157 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 339, in run\n2014-08-15 13:46:09.158 |     self.wait(sleep_time)\n2014-08-15 13:46:09.158 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/hubs/poll.py\", line 82, in wait\n2014-08-15 13:46:09.158 |     sleep(seconds)\n2014-08-15 13:46:09.158 |   File \"/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/fixtures/_fixtures/timeout.py\", line 52, in signal_handler\n2014-08-15 13:46:09.158 |     raise TimeoutException()\n2014-08-15 13:46:09.158 | TimeoutException", 
            "date_created": "2014-08-15 22:25:14.846554+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Elastic recheck patch: https://review.openstack.org/#/c/114680/", 
            "date_created": "2014-08-30 00:18:02.374519+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Bug 1364494 appears to be related.", 
            "date_created": "2014-09-06 00:04:02.100111+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Maybe some clues here, i.e. there might be an eventlet.Timeout that's leaking:\n\nhttp://lists.openstack.org/pipermail/openstack-dev/2014-September/045272.html", 
            "date_created": "2014-09-07 15:13:46.258834+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Matt, I think the eventlet.Timeout thing is a red herring. fixtures.Timeout uses the signal module from the standard Python library to interrupt the process after timeout_secs. It does not use eventlet.Timeout.  That said, there are two places I found that are incorrectly using eventlet.timeout.Timeout() without a context manager or calling close():\n\njaypipes@cranky:~/repos/openstack/nova/nova$ ack-grep \"Timeout\\(\"\ntests/virt/libvirt/test_driver.py\n8925:                raise eventlet.timeout.Timeout()\n\ntests/virt/hyperv/test_vmops.py\n196:        mock_with_timeout.side_effect = etimeout.Timeout()\n", 
            "date_created": "2014-09-08 21:23:23.232439+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/120901", 
            "date_created": "2014-09-11 20:16:20.953552+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Jay Pipes (<email address hidden>) on branch: master\nReview: https://review.openstack.org/120901\nReason: going to split this into log-only and other stuff.", 
            "date_created": "2014-09-11 20:29:08.810047+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/120909", 
            "date_created": "2014-09-11 21:01:02.435617+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "marked as critical because seeing this in the gate, 7 hits in 24 hours", 
            "date_created": "2014-09-11 21:45:30.023328+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/120909\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=608630b0b27dd13847b1e149541c2e1799500421\nSubmitter: Jenkins\nBranch:    master\n\ncommit 608630b0b27dd13847b1e149541c2e1799500421\nAuthor: Jay Pipes <email address hidden>\nDate:   Thu Sep 11 16:59:28 2014 -0400\n\n    Adds LOG statements in multiprocess API test\n    \n    Renames _terminate_workers() to _wait_for_workers_to_end() to accurately\n    match what is done in that method. Adds a bunch of LOG.info() and\n    LOG.warn() calls in the various methods in the test to get better\n    debugging information when bug # 1357578 rears its ugly head.\n    \n    Change-Id: I6698b7c71ec651c812138c8dd93b1e1b33ee1178\n    Related-bug: #1357578\n", 
            "date_created": "2014-09-13 02:31:12.088541+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Jay,\n\nwould adding a loop with the following help?\nos.waitpid(-1, os.WNOHANG)\n\nhttps://gist.github.com/matthewstory/4547282\nhttps://github.com/kanaka/noVNC/issues/47\n\nthanks,\ndims", 
            "date_created": "2014-09-16 02:29:13.365549+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "I'm afraid I don't quite understand the train of thought implied by the discussion about what the actual *issue* is. On the face of it, it simply looks like the test took too long and the stack trace is \"interesting\" because we just happened to be in the eventlet trampoline code.  This doesn't seem too surprising, so is it suspected that there is a worker hung somewhere or some other obvious thing that shouldn't be happening?\n\n\n", 
            "date_created": "2014-09-17 16:53:20.957131+00:00", 
            "author": "https://api.launchpad.net/1.0/~beagles"
        }, 
        {
            "content": "Okay, after reviewing the commit messages in the patch(es) and the\ncomments in the code, it seems pretty obvious. It does appear that the\ngist of the bug is simply \"this times out but we don't know why at the\nmoment\". Unless the timeout is set wicked short, considering the test a\nvalid timeout scenario seems pretty improbable. Good test to find\nsomething so weird.\n\nSo are these a few of the choices?\n\n- Workers don't shutdown properly when they get the signal. \n\n- Somehow the signal isn't getting to a worker in the first place. At\n  the moment, this seems possible only if something were to happen to\n  the ProcessLauncher so that it would \"forget\" about the child/worker\n  PID, i.e. leaking a child process. One scenario would be some kind of\n  oddness where a worker process caused os.waitpid() to return some kind\n  of status for itself in _wait_child(). The logging Jay Pipes added\n  will hilight any weirdness like that.\n\n- By some very strange twist, a previous test case didn't cleanup\n  properly and there is a timeout hanging around to cause trouble? Is\n  that even possible?\n", 
            "date_created": "2014-09-17 21:09:43.186950+00:00", 
            "author": "https://api.launchpad.net/1.0/~beagles"
        }, 
        {
            "content": "I managed to get a reproduce by creating a slow vm: ubuntu 14.04 in vbox, 1 G ram, 2 vcpu, set to 50% of cpu performance.\n\ntox -epy27 -- --until-fail multiprocess\n\nOn the 3rd time through I got the following:\n\nrunning=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \\\nOS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \\\nOS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \\\n${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmp7YX5uh\nrunning=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \\\nOS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \\\nOS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \\\n${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpU5Qsw_\n{0} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_killed_worker_recover [5.688803s] ... ok\n\nCaptured stderr:\n~~~~~~~~~~~~~~~~\n    /home/sdague/nova/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/default_comparator.py:35: SAWarning: The IN-predicate on \"instances.uuid\" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance.\n      return o[0](self, self.expr, op, *(other + o[1:]), **kwargs)\n    \n{0} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_killed_worker_recover [2.634592s] ... ok\n{0} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_restart_sighup [1.565492s] ... ok\n{0} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_terminate_sigterm [2.400319s] ... ok\n{1} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_restart_sighup [160.043131s] ... FAILED\n{1} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_terminate_sigkill [2.317150s] ... ok\n{1} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_terminate_sigterm [2.274788s] ... ok\n{1} nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_terminate_sigkill [2.089225s] ... ok\n\nand.... hang\n\nSo, testr is correctly killing the restart test when it times out. It is also correctly moving on to additional tests. However it is then in a hung state and can't finish once the tests are done.\n\nWhy the test timed out, I don't know. However the fact that testr is going crazy is an issue all by itself.", 
            "date_created": "2014-09-18 17:12:28.146438+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "I've deleted the testr task: testr is (correctly) waiting for the test workers to finish (http://paste.openstack.org/show/112990/) - we can open a feature request to have a testr supervisor timeout facility, but its orthogonal to this nova issue.\n\nSean has grabbed stuff with gdb:\n20727 - a worker we think - http://paste.openstack.org/show/112998/ - this appears to be waiting for a new request\n20725 - parent of the worker, stuck in the poll hub - http://paste.openstack.org/show/113014/\n\nLooking at the test code there are lots of issues that will make this test fragile:\n - it assumes 0 delay between /a/ worker being spawned and all of them being spawned \n\n - I was going to say that we don't set the multiprocessing workers 'daemon' flag - https://docs.python.org/2/library/multiprocessing.html and this will cause a parent process that exits to hang [same as with threads] unless the process object is killed out of band. OTOH if we want to guarantee all in-progress requests finish cleanly, this is appropriate - but we have no timeout protection on each request - but I observe that actually we don't even use multiprocessing - we've grown our own thing :(.\n\n - it uses tearDown which is not guaranteed to run if setUp has issues.\n\nThe hanging process is from test_restart_sighup.\n\nThis test doesn't seem to test what SIGHUP does - SIGHUP doesn't restart any children, and the test doesn't strace the processes to see if SIGHUP is actually propogated to children, so the call to _wait should be a 100% no-op, *except* that we want to know the processes aren't being killed - thats why we cross check the pids. But the use of _wait means we'll exit on the first check - not allowing any time for the service process to do the wrong thing. That is, it can falsely pass.\n\n\nThe service code or handling signals is racy: it sets sigcaught to the received signal but multiple signals may be delivered before any are processed. I don't have reason to believe that that is whats happening here.\n\nThis is a worry:\n            except fixtures.TimeoutException:\n\nwe catch it, but we don't re-raise it. So we could 'pass' after being timed out, if it happens in tearDown. That seems wrong to me.", 
            "date_created": "2014-09-18 22:01:18.104580+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Looking further, _start_child in service.py is leaky - interrupt that at the wrong time and we won't capture the child pid, which will lead to us not killing it, and that will result in a stuck process tree.", 
            "date_created": "2014-09-18 22:03:55.309738+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Oh, for clarity; testr is hung here because the pipe it opened to the test process is still open. That means that this leaked worker process has the same stdout as the subunit.run process. Pushing up a simple fix which will stop this timing out in the same way (but obviously won't fix the underlying bug).", 
            "date_created": "2014-09-18 22:05:06.032604+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/122543", 
            "date_created": "2014-09-18 22:09:47.126500+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/122545", 
            "date_created": "2014-09-18 22:18:25.361575+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/122545\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4acf31fb3ac97fe84efb5833da58fa90c145d947\nSubmitter: Jenkins\nBranch:    master\n\ncommit 4acf31fb3ac97fe84efb5833da58fa90c145d947\nAuthor: Robert Collins <email address hidden>\nDate:   Fri Sep 19 10:14:48 2014 +1200\n\n    tests: kill worker pids as well on timeouts\n    \n    Workers that don't exit can also trigger a timeout, and we want our\n    test environment to be super robust - kill them aggressively too.\n    \n    Also raise the timeout error so that we don't mask this when it\n    happens.\n    \n    Change-Id: Iee606720f98e9a5e8a11f162eb885c0a00a2345e\n    Partial-Bug: #1357578\n", 
            "date_created": "2014-09-19 05:58:15.036119+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/122543\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=7d4a17f2ad72bf1ffa979f57a59bb2bd2727afe3\nSubmitter: Jenkins\nBranch:    master\n\ncommit 7d4a17f2ad72bf1ffa979f57a59bb2bd2727afe3\nAuthor: Robert Collins <email address hidden>\nDate:   Fri Sep 19 10:07:52 2014 +1200\n\n    Close standard fds in test child process\n    \n    The std fd's are shared with parent processes, which makes testr think\n    that we still have test workers, even though this isn't a test\n    process, its a live server.\n    \n    Change-Id: I516a6a9bd9a98abffeb182189038763ea94618f6\n    Partial-Bug: #1357578\n", 
            "date_created": "2014-09-20 01:18:59.230675+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I don't see any outstanding reviews for this bug. Is this correct or have I missed something?", 
            "date_created": "2014-09-22 01:54:33.917708+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "There are no outstanding reviews, no, but there are a still a dozen or more failures in the gate since the last patch from lifeless was merged. So, unfortunately, I don't believe that those two patches have resolved this issue entirely.", 
            "date_created": "2014-09-22 18:03:00.099176+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "My patches were intended to help the diagnosis of the failure, not\nprevent it: the issue is real - we're leaking processes that we fail\nto shut down; my patches should have helped us get more logging and\ndiagnostics out when it happens.\n", 
            "date_created": "2014-09-22 19:38:55+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/123426", 
            "date_created": "2014-09-23 12:23:48.564302+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/123427", 
            "date_created": "2014-09-23 12:23:57.162618+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "So... my current theory is the following:\n\n    def _get_workers(self):\n        # NOTE(hartsocks): use of ps checks the process table for child pid\n        # entries these processes may be ended but not reaped so ps may\n        # show processes that are still being cleaned out of the table.\n        f = os.popen('ps ax -o pid,ppid,command')\n        # Skip ps header\n        f.readline()\n\n        processes = [tuple(int(p) for p in l.strip().split()[:2])\n                     for l in f.readlines()]\n        workers = [p for p, pp in processes if pp == self.pid]\n        LOG.info('in _get_workers(), workers: %r for PPID %d',\n                 workers, self.pid)\n        return workers\n\nCollects the subprocesses via ps. \n\nHowever, when we loop on them, we assume that they were all sent the correct SIGTERM previously. And we os.kill(pid, 0), which doesn't actually send a kill signal.\n\nSo, if some earlier test failed on us, and it spawned a worker that we didn't expect, we could very easily stall polling the status of that process. And we'll never even try the rest of them based on the way the for loop works.\n\nProposed fix is to send a SIGTERM on every loop iteration. ", 
            "date_created": "2014-09-23 12:34:19.963283+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "This article from Doug Hellmann might be useful:\n\nhttp://pymotw.com/2/signal/", 
            "date_created": "2014-09-23 14:14:53.779353+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/123467", 
            "date_created": "2014-09-23 15:08:27.151482+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/123521", 
            "date_created": "2014-09-23 17:27:12.723089+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Here's one more theory - Lifeless noticed correctly that \"_start_child in service.py is leaky\" but that really should not matter because child processes should die if the parent  dies an unexpected death (from openstack/common/service.py):\n\nThis bit happens in the forked worker:\n\n def _child_process(self, service):\n        self._child_process_handle_signal()\n\n        # Reopen the eventlet hub to make sure we don't share an epoll\n        # fd with parent and/or siblings, which would be bad\n        eventlet.hubs.use_hub()\n\n        # Close write to ensure only parent has it open\n        os.close(self.writepipe)\n        # Create greenthread to watch for parent to close pipe\n        eventlet.spawn_n(self._pipe_watcher)\n\nand the _pipe_watcher method:\n\n   def _pipe_watcher(self):\n        # This will block until the write end is closed when the parent\n        # dies unexpectedly\n        self.readpipe.read()\n\n        LOG.info(_LI('Parent process has died unexpectedly, exiting'))\n\n        sys.exit(1)\n\ninteresting bit is that it will only raise a SystemExit (not sure why it doesn't do the os._exit there) in a fork of a testr process that might actually be handling it in some way that can cause unexpected behavior?", 
            "date_created": "2014-09-23 21:03:02.610736+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/123467\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=ff259324abf66f436b5098a6eaf8d781ac57140d\nSubmitter: Jenkins\nBranch:    master\n\ncommit ff259324abf66f436b5098a6eaf8d781ac57140d\nAuthor: Sean Dague <email address hidden>\nDate:   Tue Sep 23 11:02:55 2014 -0400\n\n    Add further debug logging for multiprocess test\n    \n    In order to track down where we are stalling add some additional\n    informational logging into the cleanup path.\n    \n    Change-Id: Ia12b1049b3a63851bde69cb5f7c3fe31b0d12c41\n    Related-Bug: #1357578\n", 
            "date_created": "2014-09-24 03:18:18.047772+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Further to comment #30 - actually after looking at it more closely it seems that we do check for SystemExit in _spawn which should account for the _pipe_watching green thread killing a worker, so it's probably not that.", 
            "date_created": "2014-09-24 07:41:58.360524+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "So one more thing I noticed while looking at this mess. The 'launcher' worker we spawn is forked off of a prcess that ran the whole setUp for the integrated_helpers._IntegratedTestBase that starts a bunch of services in the same process (meaning a bunch of greentrheads). I am not sure how they play with fork(), but usually best practice is to re-initiate the eventlet hub after fork.\n\nI can imagine that we may be timing out because of a deadlock especially in the SIGINT case since the 'launcher' process needs to get to calling os.kill(SIGINT) on it's children to kill them.\n\nHow about we attempt to re-initialize eventlet in the 'launcher' process?", 
            "date_created": "2014-09-24 09:08:36.691160+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/123667", 
            "date_created": "2014-09-24 09:25:59.664229+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/123742", 
            "date_created": "2014-09-24 14:21:32.117689+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/123427", 
            "date_created": "2014-09-24 14:37:53.220880+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/123426", 
            "date_created": "2014-09-24 14:38:00.951699+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/123742\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=f3c99ba1efb1c039ec47dfdf29781f2e05f32b81\nSubmitter: Jenkins\nBranch:    master\n\ncommit f3c99ba1efb1c039ec47dfdf29781f2e05f32b81\nAuthor: Sean Dague <email address hidden>\nDate:   Wed Sep 24 10:18:48 2014 -0400\n\n    remove test_multiprocess_api\n    \n    This test suite should be removed for the following reasons.\n    \n    1) it's not a unit test\n    \n    2) it's failing randomly, a lot, and depends on timing of the host for\n    process start and signal handling which are provably not predicable on\n    slower machines\n    \n    3) it's triggering testr bugs in the ways that it fails which means we\n    don't actually get any subunit in the fail scenario, which makes it\n    actually impossible to move forward on fixes.\n    \n    Change-Id: Ieb7c02115c12cd9f6c9fa67691a643aed7632784\n    Closes-Bug: #1357578\n", 
            "date_created": "2014-09-24 16:07:15.626685+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/icehouse\nReview: https://review.openstack.org/123976", 
            "date_created": "2014-09-25 08:37:28.386731+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Jay Pipes (<email address hidden>) on branch: master\nReview: https://review.openstack.org/123521\nReason: \nThis test has been removed now.", 
            "date_created": "2014-09-26 03:34:53.574702+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Nikola Dipanov (<email address hidden>) on branch: master\nReview: https://review.openstack.org/123667\nReason: We decided to rip the code out instead of fixing it! :)", 
            "date_created": "2014-09-26 08:08:00.716291+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/123976\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4e6371b82a68c265c010fb2a2edb7235319d36b7\nSubmitter: Jenkins\nBranch:    stable/icehouse\n\ncommit 4e6371b82a68c265c010fb2a2edb7235319d36b7\nAuthor: Sean Dague <email address hidden>\nDate:   Wed Sep 24 10:18:48 2014 -0400\n\n    remove test_multiprocess_api\n    \n    This test suite should be removed for the following reasons.\n    \n    1) it's not a unit test\n    \n    2) it's failing randomly, a lot, and depends on timing of the host for\n    process start and signal handling which are provably not predicable on\n    slower machines\n    \n    3) it's triggering testr bugs in the ways that it fails which means we\n    don't actually get any subunit in the fail scenario, which makes it\n    actually impossible to move forward on fixes.\n    \n    Conflicts:\n    \tnova/tests/integrated/test_multiprocess_api.py\n    \n    Change-Id: Ieb7c02115c12cd9f6c9fa67691a643aed7632784\n    Closes-Bug: #1357578\n    (cherry picked from commit f3c99ba1efb1c039ec47dfdf29781f2e05f32b81)\n", 
            "date_created": "2014-09-26 10:41:31.940112+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
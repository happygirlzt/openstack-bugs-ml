{
    "status": "Fix Released", 
    "last_updated": "2013-10-17 11:44:36.237726+00:00", 
    "description": "When virt driver supports multiple nodes and one node is removed from driver support the compute_nodes in DB are not synched with the driver list. This will cause scheduler to pick bad host resulting in this error:\n\n| fault | {u'message': u'NovaException', u'code': 500, u'details': u'helium51 is not a valid node managed by this compute host. |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 223, in decorated_function |\n| | return function(self, context, *args, **kwargs) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 1149, in run_instance |\n| | do_run_instance() |\n| | File \"/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py\", line 242, in inner |\n| | retval = f(*args, **kwargs) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 1148, in do_run_instance |\n| | admin_password, is_first_time, node, instance) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 802, in _run_instance |\n| | self._set_instance_error_state(context, instance[\\'uuid\\']) |\n| | File \"/usr/lib64/python2.6/contextlib.py\", line 23, in __exit__ |\n| | self.gen.next() |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 756, in _run_instance |\n| | rt = self._get_resource_tracker(node) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 353, in _get_resource_tracker |\n| | raise exception.NovaException(msg) |\n| | ', u'created': u'2013-03-06T16:47:52Z'} \n\nTwo things I see in the code:\n\nfirst the list of known hosts is not reflecting the DB list but a list from driver.get_available_nodes:\n\nknown_nodes = set(self._resource_tracker_dict.keys())\n\nWhich then will never yield orphan compute_nodes in this statement:\n\nfor nodename in known_nodes - nodenames\n\nSecondly, even if we fix to get known_nodes from the DB through conductor\n\nThis code will always raise and exception:\n\nfor nodename in known_nodes - nodenames:\n    rt = self._get_resource_tracker(nodename)\n    rt.update_available_resource(context, delete=True)\n\nbecause _get_resource_tracker will always check the nodename is in driver.get_available_nodes\n\nTo replicate this you could just change your hypervisor_hostname which will create a new record in nova.compute_nodes table leaving the old record around. This will simulate a compute node that is not supported anymore in a multi-node scenario.\n\nSuggestion:\n\nRemove logic to delete orphan compute_nodes from compute.manager and move to compute.resource_tracker under the _sync_compute_node method which already loops through all compute_nodes records for compute service", 
    "tags": [
        "baremetal"
    ], 
    "importance": "Low", 
    "heat": 14, 
    "link": "https://bugs.launchpad.net/nova/+bug/1161193", 
    "owner": "https://api.launchpad.net/1.0/~dperaza", 
    "id": 1161193, 
    "index": 5720, 
    "created": "2013-03-28 03:48:58.065619+00:00", 
    "title": "Compute manager fails to cleanup compute_nodes not reported by driver", 
    "comments": [
        {
            "content": "When virt driver supports multiple nodes and one node is removed from driver support the compute_nodes in DB are not synched with the driver list. This will cause scheduler to pick bad host resulting in this error:\n\n| fault | {u'message': u'NovaException', u'code': 500, u'details': u'helium51 is not a valid node managed by this compute host. |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 223, in decorated_function |\n| | return function(self, context, *args, **kwargs) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 1149, in run_instance |\n| | do_run_instance() |\n| | File \"/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py\", line 242, in inner |\n| | retval = f(*args, **kwargs) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 1148, in do_run_instance |\n| | admin_password, is_first_time, node, instance) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 802, in _run_instance |\n| | self._set_instance_error_state(context, instance[\\'uuid\\']) |\n| | File \"/usr/lib64/python2.6/contextlib.py\", line 23, in __exit__ |\n| | self.gen.next() |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 756, in _run_instance |\n| | rt = self._get_resource_tracker(node) |\n| | File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 353, in _get_resource_tracker |\n| | raise exception.NovaException(msg) |\n| | ', u'created': u'2013-03-06T16:47:52Z'} \n\nTwo things I see in the code:\n\nfirst the list of known hosts is not reflecting the DB list but a list from driver.get_available_nodes:\n\nknown_nodes = set(self._resource_tracker_dict.keys())\n\nWhich then will never yield orphan compute_nodes in this statement:\n\nfor nodename in known_nodes - nodenames\n\nSecondly, even if we fix to get known_nodes from the DB through conductor\n\nThis code will always raise and exception:\n\nfor nodename in known_nodes - nodenames:\n    rt = self._get_resource_tracker(nodename)\n    rt.update_available_resource(context, delete=True)\n\nbecause _get_resource_tracker will always check the nodename is in driver.get_available_nodes\n\nTo replicate this you could just change your hypervisor_hostname which will create a new record in nova.compute_nodes table leaving the old record around. This will simulate a compute node that is not supported anymore in a multi-node scenario.\n\nSuggestion:\n\nRemove logic to delete orphan compute_nodes from compute.manager and move to compute.resource_tracker under the _sync_compute_node method which already loops through all compute_nodes records for compute service", 
            "date_created": "2013-03-28 03:48:58.065619+00:00", 
            "author": "https://api.launchpad.net/1.0/~dperaza"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/25592", 
            "date_created": "2013-03-28 05:57:44.695926+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/25592\nCommitted: http://github.com/openstack/nova/commit/45ce810ab42b202565088dce55db15374204f638\nSubmitter: Jenkins\nBranch:    master\n\ncommit 45ce810ab42b202565088dce55db15374204f638\nAuthor: David Peraza <email address hidden>\nDate:   Wed Mar 27 12:44:22 2013 +0000\n\n    Cleans up orphan compute_nodes not cleaned up by compute manager\n    \n    Fixes bug 1161193\n    \n    Orphan compute_node records can cause the scheduler to pick\n    compute nodes that are not handled by driver anymore. This\n    can happen if you rename your hypervisor_hostname or in a\n    multi-node support where driver does not support a node\n    anymore for whatever reason (hardare failure for example).\n    \n    Also, removing resource tracker logic to delete nodes since\n    resource trackers built in compute manager will never accept\n    nodes not reported by driver\n    \n    Change-Id: I742d2e81ec0592d952ee5736aa8dce1e5598ef80\n", 
            "date_created": "2013-04-12 16:27:26.025422+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/grizzly\nReview: https://review.openstack.org/26906", 
            "date_created": "2013-04-13 03:09:29.352210+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/26906\nCommitted: http://github.com/openstack/nova/commit/7e527ca1a3d6da39e89867619216252a83eec1ba\nSubmitter: Jenkins\nBranch:    stable/grizzly\n\ncommit 7e527ca1a3d6da39e89867619216252a83eec1ba\nAuthor: David Peraza <email address hidden>\nDate:   Wed Mar 27 12:44:22 2013 +0000\n\n    Cleans up orphan compute_nodes not cleaned up by compute manager\n    \n    Fixes bug 1161193\n    \n    Orphan compute_node records can cause the scheduler to pick\n    compute nodes that are not handled by driver anymore. This\n    can happen if you rename your hypervisor_hostname or in a\n    multi-node support where driver does not support a node\n    anymore for whatever reason (hardare failure for example).\n    \n    Also, removing resource tracker logic to delete nodes since\n    resource trackers built in compute manager will never accept\n    nodes not reported by driver\n    \n    Change-Id: I742d2e81ec0592d952ee5736aa8dce1e5598ef80\n", 
            "date_created": "2013-04-27 14:30:37.890451+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2013-10-20 04:32:49.882939+00:00", 
    "description": "When running tempest tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern fails with the server going into an ERROR state.\n\nFrom the console log:\n\nTraceback (most recent call last):\n  File \"tempest/scenario/test_volume_boot_pattern.py\", line 154, in test_volume_boot_pattern\nkeypair)\n  File \"tempest/scenario/test_volume_boot_pattern.py\", line 53, in _boot_instance_from_volume\ncreate_kwargs=create_kwargs)\n  File \"tempest/scenario/manager.py\", line 390, in create_server\nself.status_timeout(client.servers, server.id, 'ACTIVE')\n  File \"tempest/scenario/manager.py\", line 290, in status_timeout\nself._status_timeout(things, thing_id, expected_status=expected_status)\n  File \"tempest/scenario/manager.py\", line 338, in _status_timeout\nself.config.compute.build_interval):\n  File \"tempest/test.py\", line 237, in call_until_true\nif func():\n  File \"tempest/scenario/manager.py\", line 329, in check_status\nraise exceptions.BuildErrorException(message)\n  BuildErrorException: Server %(server_id)s failed to build and is in ERROR status\nDetails: <Server: scenario-server-89179012> failed to get to expected status. In ERROR state.\n\nThe exception:\nhttp://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339/logs/screen-n-cpu.txt.gz#_2013-09-24_04_44_31_806\n\nLogs are located here:\nhttp://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339\n\n-----------------\n\nOriginally the failure was (before some changes to timeouts in tempest):\n\nt178.1: tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern[compute,image,volume]_StringException: Empty attachments:\n\u00a0\u00a0stderr\n\u00a0\u00a0stdout\n\npythonlogging:'': {{{\n2013-09-16 15:59:44,214 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:44,417 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:45,348 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:45,495 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:47,644 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:48,762 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:49,879 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:50,980 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:00:52,581 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:52,897 Authentication (publickey) successful!\n2013-09-16 16:00:53,105 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:53,428 Authentication (publickey) successful!\n2013-09-16 16:00:53,431 Secsh channel 1 opened.\n2013-09-16 16:00:53,607 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:53,875 Authentication (publickey) successful!\n2013-09-16 16:00:53,880 Secsh channel 1 opened.\n2013-09-16 16:01:58,999 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:01:59,288 Authentication (publickey) successful!\n2013-09-16 16:01:59,457 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:01:59,784 Authentication (publickey) successful!\n2013-09-16 16:01:59,801 Secsh channel 1 opened.\n2013-09-16 16:02:00,005 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:00,080 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,127 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,192 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,414 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:02,494 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:03,615 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:04,724 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:05,825 Starting new HTTP connection (1): 127.0.0.1\n}}}\n\nTraceback (most recent call last):\n\u00a0\u00a0File \"tempest/scenario/test_volume_boot_pattern.py\", line 157, in test_volume_boot_pattern\n\u00a0\u00a0\u00a0\u00a0ssh_client = self._ssh_to_server(instance_from_snapshot, keypair)\n\u00a0\u00a0File \"tempest/scenario/test_volume_boot_pattern.py\", line 101, in _ssh_to_server\n\u00a0\u00a0\u00a0\u00a0private_key=keypair.private_key)\n\u00a0\u00a0File \"tempest/scenario/manager.py\", line 453, in get_remote_client\n\u00a0\u00a0\u00a0\u00a0return RemoteClient(ip, username, pkey=private_key)\n\u00a0\u00a0File \"tempest/common/utils/linux/remote_client.py\", line 47, in __init__\n\u00a0\u00a0\u00a0\u00a0if not self.ssh_client.test_connection_auth():\n\u00a0\u00a0File \"tempest/common/ssh.py\", line 148, in test_connection_auth\n\u00a0\u00a0\u00a0\u00a0connection = self._get_ssh_connection()\n\u00a0\u00a0File \"tempest/common/ssh.py\", line 70, in _get_ssh_connection\n\u00a0\u00a0\u00a0\u00a0time.sleep(bsleep)\n\u00a0\u00a0File \"/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py\", line 52, in signal_handler\n\u00a0\u00a0\u00a0\u00a0raise TimeoutException()\nTimeoutException\n\nFull logs here:\n\nhttp://logs.openstack.org/80/43280/9/gate/gate-tempest-devstack-vm-full/cba22ae/testr_results.html.gz\n\nLooks a bit like the instance failed to boot.", 
    "tags": [
        "havana-rc-potential", 
        "testing"
    ], 
    "importance": "Undecided", 
    "heat": 48, 
    "link": "https://bugs.launchpad.net/nova/+bug/1226337", 
    "owner": "None", 
    "id": 1226337, 
    "index": 4093, 
    "created": "2013-09-17 00:18:35.533473+00:00", 
    "title": "tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern 'qemu-nbd: Failed to bdrv_open'", 
    "comments": [
        {
            "content": "t178.1: tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern[compute,image,volume]_StringException: Empty attachments:\n  stderr\n  stdout\n\npythonlogging:'': {{{\n2013-09-16 15:59:44,214 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:44,417 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:45,348 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:45,495 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:47,644 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:48,762 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:49,879 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 15:59:50,980 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:00:52,581 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:52,897 Authentication (publickey) successful!\n2013-09-16 16:00:53,105 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:53,428 Authentication (publickey) successful!\n2013-09-16 16:00:53,431 Secsh channel 1 opened.\n2013-09-16 16:00:53,607 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:00:53,875 Authentication (publickey) successful!\n2013-09-16 16:00:53,880 Secsh channel 1 opened.\n2013-09-16 16:01:58,999 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:01:59,288 Authentication (publickey) successful!\n2013-09-16 16:01:59,457 Connected (version 2.0, client dropbear_2012.55)\n2013-09-16 16:01:59,784 Authentication (publickey) successful!\n2013-09-16 16:01:59,801 Secsh channel 1 opened.\n2013-09-16 16:02:00,005 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:00,080 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,127 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,192 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:01,414 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:02,494 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:03,615 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:04,724 Starting new HTTP connection (1): 127.0.0.1\n2013-09-16 16:02:05,825 Starting new HTTP connection (1): 127.0.0.1\n}}}\n\nTraceback (most recent call last):\n  File \"tempest/scenario/test_volume_boot_pattern.py\", line 157, in test_volume_boot_pattern\n    ssh_client = self._ssh_to_server(instance_from_snapshot, keypair)\n  File \"tempest/scenario/test_volume_boot_pattern.py\", line 101, in _ssh_to_server\n    private_key=keypair.private_key)\n  File \"tempest/scenario/manager.py\", line 453, in get_remote_client\n    return RemoteClient(ip, username, pkey=private_key)\n  File \"tempest/common/utils/linux/remote_client.py\", line 47, in __init__\n    if not self.ssh_client.test_connection_auth():\n  File \"tempest/common/ssh.py\", line 148, in test_connection_auth\n    connection = self._get_ssh_connection()\n  File \"tempest/common/ssh.py\", line 70, in _get_ssh_connection\n    time.sleep(bsleep)\n  File \"/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py\", line 52, in signal_handler\n    raise TimeoutException()\nTimeoutException\n\nFull logs here:\n\nhttp://logs.openstack.org/80/43280/9/gate/gate-tempest-devstack-vm-full/cba22ae/testr_results.html.gz\n\nLooks a bit like the instance failed to boot.", 
            "date_created": "2013-09-17 00:18:35.533473+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyeoh-0"
        }, 
        {
            "content": "Logstash query: @message:\"qemu-nbd: Failed to bdrv_open\" AND @fields.build_status:\"FAILURE\" AND @fields.filename:\"logs/screen-n-cpu.txt\"", 
            "date_created": "2013-09-23 18:03:45.852631+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "Looking at the logs for a test run there is a repeating INFO message which becomes a warning (after it retries enough times) when nova compute tries to create the server:\n\nhttp://logs.openstack.org/83/47283/1/gate/gate-tempest-devstack-vm-full/cdc49f9/logs/screen-n-cpu.txt.gz#_2013-09-23_14_16_22_131\n\n2013-09-23 14:16:20.126 INFO nova.virt.disk.mount.nbd [req-2e800637-20ac-486c-b3dd-cff14742e2a0 tempest.scenario.manager348477648-user tempest.scenario.manager1015858706-tenant] NBD mount error: qemu-nbd error: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf qemu-nbd -c /dev/nbd7 /opt/stack/data/nova/instances/af550eec-7246-4500-898d-b22eb45109a8/disk\nExit code: 1\nStdout: ''\nStderr: \"qemu-nbd: Failed to bdrv_open '/opt/stack/data/nova/instances/af550eec-7246-4500-898d-b22eb45109a8/disk': No such file or directory\\n\"", 
            "date_created": "2013-09-23 18:06:15.968867+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "I should scroll down more in the future, here is the actual stacktrace that is causing the failure (it's may be related to the above messages)\n\nhttp://logs.openstack.org/83/47283/1/gate/gate-tempest-devstack-vm-full/cdc49f9/logs/screen-n-cpu.txt.gz#_2013-09-23_14_18_41_867\n\n\n2013-09-23 14:18:41.867 ERROR nova.compute.manager [req-25a33c89-f4ba-4250-96e0-a122bb16b3c2 tempest.scenario.manager348477648-user tempest.scenario.manager1015858706-tenant] [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7] Instance failed to spawn\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7] Traceback (most recent call last):\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 1408, in _spawn\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     block_device_info)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2068, in spawn\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     write_to_disk=True)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 3026, in to_xml\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     disk_info, rescue, block_device_info)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2912, in get_guest_config\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     inst_type):\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2738, in get_guest_storage_config\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     info)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1030, in volume_driver_method\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     return method(connection_info, *args, **kwargs)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/openstack/common/lockutils.py\", line 246, in inner\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     return f(*args, **kwargs)\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]   File \"/opt/stack/new/nova/nova/virt/libvirt/volume.py\", line 277, in connect_volume\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7]     % (host_device))\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7] NovaException: iSCSI device not found at /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2010-10.org.openstack:volume-b31ec3e4-74f0-4aa8-aa25-2fcdf70e8d1b-lun-1\n2013-09-23 14:18:41.867 23218 TRACE nova.compute.manager [instance: 196feb6a-fdd7-4ff6-9fb9-863b92c834a7] ", 
            "date_created": "2013-09-23 19:22:20.742762+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "better logstash query:\n@message:\"NovaException: iSCSI device not found at\" AND @fields.build_status:\"FAILURE\" AND @fields.filename:\"logs/screen-n-cpu.txt\"", 
            "date_created": "2013-09-23 19:23:59.882299+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "Another example of this bug is:\n\nThe exception:\nhttp://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339/logs/screen-n-cpu.txt.gz#_2013-09-24_04_44_31_806\n\n2013-09-24 04:16:36.948 | Traceback (most recent call last):\n2013-09-24 04:16:36.948 | File \"tempest/scenario/test_volume_boot_pattern.py\", line 154, in test_volume_boot_pattern\n2013-09-24 04:16:36.948 | keypair)\n2013-09-24 04:16:36.948 | File \"tempest/scenario/test_volume_boot_pattern.py\", line 53, in _boot_instance_from_volume\n2013-09-24 04:16:36.948 | create_kwargs=create_kwargs)\n2013-09-24 04:16:36.948 | File \"tempest/scenario/manager.py\", line 390, in create_server\n2013-09-24 04:16:36.949 | self.status_timeout(client.servers, server.id, 'ACTIVE')\n2013-09-24 04:16:36.949 | File \"tempest/scenario/manager.py\", line 290, in status_timeout\n2013-09-24 04:16:36.949 | self._status_timeout(things, thing_id, expected_status=expected_status)\n2013-09-24 04:16:36.949 | File \"tempest/scenario/manager.py\", line 338, in _status_timeout\n2013-09-24 04:16:36.949 | self.config.compute.build_interval):\n2013-09-24 04:16:36.949 | File \"tempest/test.py\", line 237, in call_until_true\n2013-09-24 04:16:36.950 | if func():\n2013-09-24 04:16:36.950 | File \"tempest/scenario/manager.py\", line 329, in check_status\n2013-09-24 04:16:36.950 | raise exceptions.BuildErrorException(message)\n2013-09-24 04:16:36.950 | BuildErrorException: Server %(server_id)s failed to build and is in ERROR status\n2013-09-24 04:16:36.950 | Details: <Server: scenario-server-89179012> failed to get to expected status. In ERROR state.", 
            "date_created": "2013-09-24 14:13:16.651754+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "Not a tempest bug, I would like to see it as critical bug in another project (nova or cinder or libvirt  o tgtd ..) before skipping.", 
            "date_created": "2013-09-26 09:26:17.117184+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "This does not seem like a nova bug. As per the error that was extracted in comment #3 (thanks Matthew) - this error occurs when nova cannot find the iscsi device after CONF.num_iscsi_scan_tries (by default 3), sleeping try_no ** 2 secs in between - so 14 secs by default. A simple solution might be to increase this limit for tempest runs.\n\n", 
            "date_created": "2013-09-26 15:58:40.684077+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Adding cinder. ", 
            "date_created": "2013-09-26 18:41:01.776470+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Nikoa, Joe et all;\n\nMissing something here in terms of Cinder being the cause of the server not coming up?  Any insight?", 
            "date_created": "2013-09-27 15:25:22.057397+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Looking at this and here's my take:\n\n* We create the volume and target in Cinder\n* Nova grabs the volume info and starts building the instance\n* Nova issues the discovery, and actually succesfully logs in to the target\n* Nova attempts to actually make the attach and map to /dev/disk-by-path\n\nThe last step is what actually never seems to complete in the allowed period of time with the given retries.\n\nI don't have a good theory here as I haven't been able to reproduce the error, but the only thing that I can suggest at this point is that we either increase the retry factor from 2 to 3 to give us almost 30 seconds before the last retry, or the same thing could be accomplished by changing the default for CONF.num_iscsi_retries", 
            "date_created": "2013-09-27 20:05:16.618514+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "\n\nI am out of the office until 10/07/2013.\n\nI will take vacation from 28th Sep to 7th Oct . If have any urgent, please\ncall 13910806810\n\n\nNote: This is an automated response to your message  \"[Bug 1226337] Re:\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern\tflake\nfailure\" sent on 09/28/2013 4:05:16.\n\nThis is the only notification you will receive while this person is away.", 
            "date_created": "2013-09-27 20:27:35+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhikunliu"
        }, 
        {
            "content": "Additional logging it appears the target is no good, small sample it appears lun 1 is missing.  Looking into how we might address this.", 
            "date_created": "2013-09-29 15:26:56.677066+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "I received this as well. Logs: http://logs.openstack.org/63/48863/1/check/check-tempest-devstack-vm-full/318679b/\n\nHowever, I wanted to point out that I also received a failure in \"process-returncode\".\n\nft1.1: process-returncode_StringException: Binary content:\n  traceback (test/plain;charset=utf8)\n\nShouldn't that be \"text/plain\"? \n\nSee also the end of http://logs.openstack.org/63/48863/1/check/check-tempest-devstack-vm-full/318679b/subunit_log.txt.gz\n\ntags: -worker-0\ntime: 2013-09-29 20:22:01.851443Z\ntags: worker-0\ntest: process-returncode\ntime: 2013-09-29 20:22:01.851443Z\nfailure: process-returncode [ multipart\nContent-Type: test/plain;charset=utf8\ntraceback\nC\nreturncode 10\n]\n", 
            "date_created": "2013-09-29 20:57:01.258936+00:00", 
            "author": "https://api.launchpad.net/1.0/~iosctr"
        }, 
        {
            "content": "intriguing bug report - https://bugs.launchpad.net/ubuntu/+source/tgt/+bug/1048008", 
            "date_created": "2013-09-30 03:47:45.552795+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "In the syslog the initiator  related messages are shorter than usual :\n\nThe message on failure:\nSep 24 04:44:17 devstack-precise-hpcloud-az3-378359 kernel: [ 4997.993316] scsi7 : iSCSI Initiator over TCP/IP\nSep 24 04:44:17 devstack-precise-hpcloud-az3-378359 kernel: [ 4998.499703] scsi 7:0:0:0: RAID              IET      Controller       0001 PQ: 0 ANSI: 5\nSep 24 04:44:17 devstack-precise-hpcloud-az3-378359 kernel: [ 4998.500015] scsi 7:0:0:0: Attached scsi generic sg4 type 12\nSep 24 04:44:17 devstack-precise-hpcloud-az3-378359 iscsid: connection6:0 is operational now\n\n\nThe usual message looks like:\nSep 24 04:43:13 devstack-precise-hpcloud-az3-378359 kernel: [ 4934.758730] scsi6 : iSCSI Initiator over TCP/IP\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.266718] scsi 6:0:0:0: RAID              IET      Controller       0001 PQ: 0 ANSI: 5\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.267148] scsi 6:0:0:0: Attached scsi generic sg2 type 12\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.268207] scsi 6:0:0:1: Direct-Access     IET      VIRTUAL-DISK     0001 PQ: 0 ANSI: 5\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.269186] sd 6:0:0:1: Attached scsi generic sg3 type 0\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.272887] sd 6:0:0:1: [sdb] 2097152 512-byte logical blocks: (1.07 GB/1.00 GiB)\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.273303] sd 6:0:0:1: [sdb] Write Protect is off\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.273309] sd 6:0:0:1: [sdb] Mode Sense: 49 00 00 08\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.273617] sd 6:0:0:1: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.275856]  sdb: unknown partition table\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 kernel: [ 4935.277354] sd 6:0:0:1: [sdb] Attached SCSI disk\nSep 24 04:43:14 devstack-precise-hpcloud-az3-378359 iscsid: connection5:0 is operational now\n\n\nWhen the after nova/libvirt gives up (usual prologue):\nSep 24 04:44:32 devstack-precise-hpcloud-az3-378359 kernel: [ 5013.039383] sd 4:0:0:1: [sda] Synchronizing SCSI cache\nSep 24 04:44:32 devstack-precise-hpcloud-az3-378359 kernel: [ 5013.291661]  connection3:0: detected conn error (1020)\nSep 24 04:44:32 devstack-precise-hpcloud-az3-378359 tgtd: conn_close(101) connection closed, 0x22f8f28 1\nSep 24 04:44:32 devstack-precise-hpcloud-az3-378359 tgtd: conn_close(107) sesson 0x22fa100 1\nSep 24 04:44:33 devstack-precise-hpcloud-az3-378359 kernel: [ 5014.128859]  connection6:0: detected conn error (1020)\nSep 24 04:44:33 devstack-precise-hpcloud-az3-378359 tgtd: conn_close(101) connection closed, 0x230c418 1\nSep 24 04:44:33 devstack-precise-hpcloud-az3-378359 tgtd: conn_close(107) sesson 0x230c6e0 1\n\n\nBut the prior [sda] related message is missing!!.\n\n", 
            "date_created": "2013-09-30 08:55:33.220790+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Looking at a latest run which has additional debug information\n\nLooking for volume-a109df91-d5b4-47b6-9c3e-17e514a08fe4\n\nI see the disk -  http://logs.openstack.org/76/48876/4/check/check-tempest-devstack-vm-postgres-full/2ce7533/logs/screen-c-vol.txt.gz#_2013-09-30_13_08_30_878\nand then it's gone - http://logs.openstack.org/76/48876/4/check/check-tempest-devstack-vm-postgres-full/2ce7533/logs/screen-c-vol.txt.gz#_2013-09-30_13_08_32_538", 
            "date_created": "2013-09-30 14:14:37.122724+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/48970", 
            "date_created": "2013-09-30 14:49:12.373588+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/48970\nCommitted: http://github.com/openstack/cinder/commit/d9514d796185220715b09c8c17e71eeb1337ba2f\nSubmitter: Jenkins\nBranch:    master\n\ncommit d9514d796185220715b09c8c17e71eeb1337ba2f\nAuthor: John Griffith <email address hidden>\nDate:   Mon Sep 30 08:45:52 2013 -0600\n\n    Dont retry if target creation succeeds\n    \n    The target creation retry loop was not breaking when\n    the target was succesfully created.  This is \"sometimes\" ok,\n    however in other cases this will actually cause a failure in\n    the second create and the backing Lun will be deleted and not\n    created again succesfully due most likely to a busy status on the\n    target.\n    \n    Add a break in the try block.\n    \n    Change-Id: I875f6302868651b5b42d4796cd2714fba35e525e\n    Closes-Bug: #1226337\n", 
            "date_created": "2013-10-01 03:53:56.341917+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Believe this can be marked invalid for Nova.  ", 
            "date_created": "2013-10-01 13:08:39.149963+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Looks like this is still happening: http://logs.openstack.org/07/49107/1/check/check-tempest-devstack-vm-postgres-full/6e09abc/logs/screen-n-cpu.txt.gz#_2013-10-01_10_58_48_112\n\nBut with a much much lower frequency.  logstash reports 4 hits since the fix (https://review.openstack.org/48970) merged.\n\nI think this is worth re-opening but not as critical", 
            "date_created": "2013-10-01 16:35:58.369857+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/49229", 
            "date_created": "2013-10-01 18:06:18.270388+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reopened and left as critical although the frequency is in fact significantly lower.  I believe the follow up patch in 49229 will hit the rest of it for us once we get it to land.\n\nI don't want to downgrade from critical yet until it lands and I'm feeling more confident with this particular issue.", 
            "date_created": "2013-10-02 02:11:17.608734+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/49229\nCommitted: http://github.com/openstack/cinder/commit/1aef7b6304af6c19b8ceb81cbd9a3c83bb4880de\nSubmitter: Jenkins\nBranch:    master\n\ncommit 1aef7b6304af6c19b8ceb81cbd9a3c83bb4880de\nAuthor: John Griffith <email address hidden>\nDate:   Tue Oct 1 12:05:14 2013 -0600\n\n    Check for backing lun on iscsi target create\n    \n    Check to verify the backing lun was actually created and not just\n    the controller lun.  If it was NOT created, attempt to issue\n    tgtadm --op new to see if we can recover.\n    \n    If this fails, then we want to actually fail in Cinder rather than\n    pretending that everything went well, so we'll log the error and raise.\n    \n    Change-Id: I3cabab0d214c051267638a627664df2b673236e3\n    Closes-Bug: #1226337\n", 
            "date_created": "2013-10-03 00:34:00.018180+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "The elastic recheck can suggest this bug number even if the test case not failed.", 
            "date_created": "2013-10-19 05:49:15.503329+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "https://review.openstack.org/#/c/52740/  test_stamp_pattern test case also triggers this bug, and it is not fixed.", 
            "date_created": "2013-10-20 04:32:47.542503+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }
    ]
}
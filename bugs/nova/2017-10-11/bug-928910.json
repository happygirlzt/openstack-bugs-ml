{
    "status": "Fix Released", 
    "last_updated": "2012-04-05 10:08:13.195292+00:00", 
    "description": "Hi All,\n\nWe hit a performance issue with KVM on our Diablo based build, which as far as I can see still applies to Essex.\n\nWhilst I have a fix for our baseline, I\u2019m unlikely to produce an Essex based fix in the time that the community would want.   As it took quite me quite a while to work out what was happening I\u2019m hoping that by positing the details of the problem and how we\u2019ve addressed it someone else can quickly produce an Essex equivalent.   Happy to answer any questions or help out in any other way.\n\nThe symptom is that we saw a degradation in VM start-up time on KVM that is directly proportional to the number of VMs already running on the host, and reaches a critical tipping point when that number reaches around 30.\n\nWhat we found is that there are two places in the compute manager code that make calls to libvirt which take around 2 seconds, and these calls are made for all VMs on the host.    These loops do not get pre-empted.\n\n1)\tDuring creation of a VM the compute manager checks if a VM with the same name already exists (in Daiblo this was in _run_instance, in Essex its been refactored as  _check_instance_not_already_created) \n\n2)\tThe _sync_power_states periodic task which updates the power_state value in the DB.\n\nNote that neither of these gets pre-empted, so the compute manager will block all other processing for ~2 seconds x number_of_VMs.     Once the number of VMs reached 30 then the compute manager can block for over 60 seconds, which in turn means that the service update tread will not run in time and the compute service will become un-available (in itself this isn\u2019t a bad thing as it stops the scheduler from sending any more requests to that host).\n\nThe Periodic task has the biggest impact as it runs with an interval of 60 seconds \u2013 so at minute intervals the compute_manager is blocked for over a minute.   Also requests for new VMs not take over a minute to get through the _check_instance_not_already_created \u2013 so by the time they do reach a pre-emption point (such as the call to the network_manager) the periodic task will be ready to run again and block for a further minute.  This explains the dramatic degredation we saw in start up time once the number of running instances got past 30.\n \nIts possible that a more efficient way of getting the power state from libvirt can be found (it seems that like me libvirt is build for comfort rather that speed) \u2013 but we haven\u2019t found one yet, so instead we took the following approach:\n\n-\tRefactor _check_instance_not_already_created into a direct virt driver method (rather that iterating through the list) so that different implementations can provide a direct check for a specific instance.   For example it may be possible to just look directly at the file system rather than rely on libvirt.   We\u2019ve also added a flag to disable this check, which is what we\u2019re doing for now on the basis that the low risk of a failure later in creation is better that the cost of this check as it stands.\n\n-\tModify list_instances_details in the libvirt driver so that it has a sleep(0) within the loop.   This prevents this from blocking all other eventlets\n\n-\tCreate a separate eventlet to run the _sync_power_state \u2013 this is because even with the sleep(0) in place it would otherwise still block other periodic tasks, some of which (such as checks on resource allocation and billing data generation) we want to run every minute even if the _sync_power_state is going to take longer than that to run.    This new eventlet now runs less freqeuently \u2013 say every 10 minutes.   Note that just changing the interval for _sync_power_state isn\u2019t enough, it need to be in a separate eventlet to avoid blocking other periodic tasks.    Maybe a more general fix would be to create separate eventlets for every periodic task \u2013 rather than just have them run at different intervals in the same eventlet\n\n         In seems  that in Essex the EC2 API no longer returns power_state as it did in Diablo, so the user experience isn\u2019t affected by the increased refresh interval.  However there are some cases in the code that do rely on the value in the DB. In all but one case these can be changed to just refreshes it first from libvirt:\n\ncompute/manager.py:init_host() : to see which VMs need to be restarted.   Can\u2019t change this as it\u2019s a direct comparison of DB state with virt state, so the risk that some VMs fail to be started increases with a longer interval between sync_power-state\n\n compute/manager.py:_shutdown_instance() : - To raise an exception if the instance is already shutdown.  \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:set_admin_password():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:inject_file():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:update_agent():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:get_diagnostics():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n\nHope this helps and is clear \u2013 as I said happy to answer any questions (<email address hidden>)", 
    "tags": [], 
    "importance": "Critical", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/928910", 
    "owner": "https://api.launchpad.net/1.0/~jaypipes", 
    "id": 928910, 
    "index": 48, 
    "created": "2012-02-08 15:23:31.780084+00:00", 
    "title": "Major performance issue with libvirt related to number of VMs on a host", 
    "comments": [
        {
            "content": "Hi All,\n\nWe hit a performance issue with KVM on our Diablo based build, which as far as I can see still applies to Essex.\n\nWhilst I have a fix for our baseline, I\u2019m unlikely to produce an Essex based fix in the time that the community would want.   As it took quite me quite a while to work out what was happening I\u2019m hoping that by positing the details of the problem and how we\u2019ve addressed it someone else can quickly produce an Essex equivalent.   Happy to answer any questions or help out in any other way.\n\nThe symptom is that we saw a degradation in VM start-up time on KVM that is directly proportional to the number of VMs already running on the host, and reaches a critical tipping point when that number reaches around 30.\n\nWhat we found is that there are two places in the compute manager code that make calls to libvirt which take around 2 seconds, and these calls are made for all VMs on the host.    These loops do not get pre-empted.\n\n1)\tDuring creation of a VM the compute manager checks if a VM with the same name already exists (in Daiblo this was in _run_instance, in Essex its been refactored as  _check_instance_not_already_created) \n\n2)\tThe _sync_power_states periodic task which updates the power_state value in the DB.\n\nNote that neither of these gets pre-empted, so the compute manager will block all other processing for ~2 seconds x number_of_VMs.     Once the number of VMs reached 30 then the compute manager can block for over 60 seconds, which in turn means that the service update tread will not run in time and the compute service will become un-available (in itself this isn\u2019t a bad thing as it stops the scheduler from sending any more requests to that host).\n\nThe Periodic task has the biggest impact as it runs with an interval of 60 seconds \u2013 so at minute intervals the compute_manager is blocked for over a minute.   Also requests for new VMs not take over a minute to get through the _check_instance_not_already_created \u2013 so by the time they do reach a pre-emption point (such as the call to the network_manager) the periodic task will be ready to run again and block for a further minute.  This explains the dramatic degredation we saw in start up time once the number of running instances got past 30.\n \nIts possible that a more efficient way of getting the power state from libvirt can be found (it seems that like me libvirt is build for comfort rather that speed) \u2013 but we haven\u2019t found one yet, so instead we took the following approach:\n\n-\tRefactor _check_instance_not_already_created into a direct virt driver method (rather that iterating through the list) so that different implementations can provide a direct check for a specific instance.   For example it may be possible to just look directly at the file system rather than rely on libvirt.   We\u2019ve also added a flag to disable this check, which is what we\u2019re doing for now on the basis that the low risk of a failure later in creation is better that the cost of this check as it stands.\n\n-\tModify list_instances_details in the libvirt driver so that it has a sleep(0) within the loop.   This prevents this from blocking all other eventlets\n\n-\tCreate a separate eventlet to run the _sync_power_state \u2013 this is because even with the sleep(0) in place it would otherwise still block other periodic tasks, some of which (such as checks on resource allocation and billing data generation) we want to run every minute even if the _sync_power_state is going to take longer than that to run.    This new eventlet now runs less freqeuently \u2013 say every 10 minutes.   Note that just changing the interval for _sync_power_state isn\u2019t enough, it need to be in a separate eventlet to avoid blocking other periodic tasks.    Maybe a more general fix would be to create separate eventlets for every periodic task \u2013 rather than just have them run at different intervals in the same eventlet\n\n         In seems  that in Essex the EC2 API no longer returns power_state as it did in Diablo, so the user experience isn\u2019t affected by the increased refresh interval.  However there are some cases in the code that do rely on the value in the DB. In all but one case these can be changed to just refreshes it first from libvirt:\n\ncompute/manager.py:init_host() : to see which VMs need to be restarted.   Can\u2019t change this as it\u2019s a direct comparison of DB state with virt state, so the risk that some VMs fail to be started increases with a longer interval between sync_power-state\n\n compute/manager.py:_shutdown_instance() : - To raise an exception if the instance is already shutdown.  \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:set_admin_password():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:inject_file():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:update_agent():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n compute/manager.py:get_diagnostics():  To check is the state is running \u2013 can add an explicit call to self_get_power_state()\n\n\nHope this helps and is clear \u2013 as I said happy to answer any questions (<email address hidden>)", 
            "date_created": "2012-02-08 15:23:31.780084+00:00", 
            "author": "https://api.launchpad.net/1.0/~philip-day"
        }, 
        {
            "content": "The existing method in _check_instance_not_already_created() of looking for a supplied instance name by looping over the return from driver.list_instances() is horribly inefficient.\n\nRun the attached script to show how inefficient it is...\n\nOn a host with just 10 tiny instances running, doing a simple libvirt.connection.lookupByName(XXX) is an order of magnitude faster than looping through the results of listDomainsID() and looking up the name of the instance using lookupByID():\n\njpipes@librebox:~/repos/junk$ python test_check_instance_name.py \nNum running domains: 10\n\nname in [conn.lookupByID(i).name()\nfor i in conn.listDomainsID()]\nFound: False\n took 0.00712 seconds\n\ntry: conn.lookupByName(x)\n    found = True\nexcept libvirt.libvirtError:\n    found = False\n\nlibvir: QEMU error : Domain not found: no domain with matching name 'instance-1234567'\nFound: False\n took 0.00083 seconds\n", 
            "date_created": "2012-02-08 20:01:02.454653+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "", 
            "date_created": "2012-02-08 20:01:19.730883+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Based on the difference in efficiency shown by the attached patch, I think an easy win would be to simply change the implementation of _check_instance_not_already_created() to instead check to see if the driver supports a lookup by Name, and if so, use that.", 
            "date_created": "2012-02-08 20:02:50.686350+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/3919", 
            "date_created": "2012-02-08 20:52:45.420873+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Jay's change to _check_instance_not_already_created() is a good one (thanks Jay), but wanted to be sure that everyone understood that its just one part of the problem.   \r\n\r\nThe time spent in _sync_power_state() is at least as big if not a bigger issue  - and the refactor to make this a seperate thread with an explict yield between each instance is also needed.", 
            "date_created": "2012-02-09 12:17:40.648049+00:00", 
            "author": "https://api.launchpad.net/1.0/~philip-day"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/3919\nCommitted: http://github.com/openstack/nova/commit/27c11c4bb4e5c54282caf49cba666f45cfc590c2\nSubmitter: Jenkins\nBranch:    master\n\ncommit 27c11c4bb4e5c54282caf49cba666f45cfc590c2\nAuthor: Jay Pipes <email address hidden>\nDate:   Wed Feb 8 15:51:59 2012 -0500\n\n    Remedies LP Bug #928910 - Use libvirt lookupByName() to check existence\n    \n    Make determining if an instance exists on a host\n    more efficient by adding an instance_exists() method to the\n    base virt driver that can be overridden by drivers that\n    have a more efficient mechanism of looking up an instance\n    by its ID / name. Modifies the _check_instance_already_created\n    method of the compute manager to use this new instance_exists() method.\n    \n    Someone from Citrix should look into how to make the instance_exists()\n    method in the Xen and VMWare virt drivers more efficient than the\n    base \"loop over all domains and see if the instance ID exists\" method\n    now in the base driver class.\n    \n    Change-Id: Ibf219788f9c104698057367da89300a060945778\n", 
            "date_created": "2012-02-11 01:04:05.132189+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Setting back to In Progress while I address part 2 of the bug.", 
            "date_created": "2012-02-11 01:12:00.371908+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/4061", 
            "date_created": "2012-02-12 18:41:04.259977+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/4061\nCommitted: http://github.com/openstack/nova/commit/1c8ad4553b4b8d404f941c5297e3f6e42c9f7e6a\nSubmitter: Jenkins\nBranch:    master\n\ncommit 1c8ad4553b4b8d404f941c5297e3f6e42c9f7e6a\nAuthor: Jay Pipes <email address hidden>\nDate:   Sun Feb 12 13:34:14 2012 -0500\n\n    Completes fix for LP #928910 - libvirt performance\n    \n    This patch adds the remainder of the recommended fixes\n    from the original bug report:\n    \n    * Modifies methods in the compute manager that relied on\n      the DB power state to be in sync with the virt driver to\n      instead just query the power state of the instance from the\n      virt driver. This enables us to set the periodic tick to 10\n      for the problematic compute.manager.Manager._sync_power_states()\n      method.\n    * Modifies the _sync_power_states method in the following ways:\n     ** Replace the call to driver.list_instances_detail() to a new,\n        driver-overrideable get_num_instances() call\n     ** For each instance known by the database, call driver.get_info()\n        separately inside the loop instead of calling the expensive\n        list_instances_detail() method that can take a very long time\n        to complete on hosts with lots of instances\n     ** Call greenthread.sleep(0) before each call to update the\n        database power state, enabling other periodic tasks to do work\n    \n    Once again, I left an inefficient default implementation of the\n    new driver.get_num_instances() method in the base driver class. I\n    need help from folks who understand the Xen/VMWare drivers to do\n    an override for get_num_instances() in those drivers that calls\n    the underlying XenAPI or VMWare API.\n    \n    Change-Id: I88002689cdda32124423da320f8c542e286be51b\n", 
            "date_created": "2012-02-17 04:10:26.491871+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
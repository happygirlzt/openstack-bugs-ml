{
    "status": "Invalid", 
    "last_updated": "2017-02-02 14:53:50.566222+00:00", 
    "description": "I am getting a \"Failed to connect to server(code:1006)\" error when accessing the console of an instance. Refreshing the page fixes this. \n\nI am deploying cs:trusty/nova-cloud-controller-63 along with cs:trusty/openstack-dashboard-16 for kilo. I have also tried deploying the latest revision of both charms but hitting the same issue. \n\nHere is my config for both charms:\n\nopenstack-dashboard:\n    vip: \"192.168.100.57\"\nhacluster-dashboard:\n    cluster_count: 3\n    corosync_transport: unicast\n    monitor_host: \"192.168.100.1\"\n\nnova-cloud-controller:\n    network-manager: \"Neutron\"\n    console-access-protocol: \"novnc\"\n    openstack-origin: \"cloud:trusty-kilo\"\n    quantum-security-groups: \"yes\"\n    region: \"serverstack\"\n    vip: \"192.168.100.58\"\nhacluster-nova-cc:\n    cluster_count: 3\n    monitor_host: \"192.168.100.1\"\n    corosync_transport: unicast\n\nLet me know if this is a config issue.", 
    "tags": [
        "console", 
        "liberty-backport-potential", 
        "mitaka-backport-potential", 
        "openstack"
    ], 
    "importance": "Undecided", 
    "heat": 36, 
    "link": "https://bugs.launchpad.net/nova/+bug/1550422", 
    "owner": "None", 
    "id": 1550422, 
    "index": 6064, 
    "created": "2016-02-26 17:10:54.861388+00:00", 
    "title": "Failed to connect to server(code:1006) -  Console access issue", 
    "comments": [
        {
            "content": "I am getting a \"Failed to connect to server(code:1006)\" error when accessing the console of an instance. Refreshing the page fixes this. \n\nI am deploying cs:trusty/nova-cloud-controller-63 along with cs:trusty/openstack-dashboard-16 for kilo. I have also tried deploying the latest revision of both charms but hitting the same issue. \n\nHere is my config for both charms:\n\nopenstack-dashboard:\n    vip: \"192.168.100.57\"\nhacluster-dashboard:\n    cluster_count: 3\n    corosync_transport: unicast\n    monitor_host: \"192.168.100.1\"\n\nnova-cloud-controller:\n    network-manager: \"Neutron\"\n    console-access-protocol: \"novnc\"\n    openstack-origin: \"cloud:trusty-kilo\"\n    quantum-security-groups: \"yes\"\n    region: \"serverstack\"\n    vip: \"192.168.100.58\"\nhacluster-nova-cc:\n    cluster_count: 3\n    monitor_host: \"192.168.100.1\"\n    corosync_transport: unicast\n\nLet me know if this is a config issue.", 
            "date_created": "2016-02-26 17:10:54.861388+00:00", 
            "author": "https://api.launchpad.net/1.0/~bbaqar"
        }, 
        {
            "content": "Add the following information.\n\n* nova version (kilo?)\n* log message (log files)\n- nova-novncproxy\n- nova-consoleauth\n- nova-api\n", 
            "date_created": "2016-06-06 23:43:27.074104+00:00", 
            "author": "https://api.launchpad.net/1.0/~natsume-takashi"
        }, 
        {
            "content": "Hi Takashi,\nProviding you the required information on Bilal's behalf.\n\n* Nova version is Mitaka  (We're hitting the same issue on Kilo as well, haven't tested this out on Liberty)\n* log files attached\n\nVersions:\n* nova-novncproxy\n      13.0.0\n* nova-consoleauth\n      13.0.0\n* nova-api-os-compute\n      13.0.0", 
            "date_created": "2016-06-17 23:27:28.251043+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "", 
            "date_created": "2016-06-17 23:28:25.912148+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "", 
            "date_created": "2016-06-17 23:28:55.719451+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "", 
            "date_created": "2016-06-17 23:29:27.835133+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "This really looks like this is a configuration issue, the juju charms folks should weigh in first. ", 
            "date_created": "2016-07-01 12:20:18.554754+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Hi Sean,\nThanks for the response. The configuration shared initially were of Kilo, below is the configuration we're using for Mitaka. We are not facing this issue on a single controller environment, its only hitting in HA environment. \n\nHere is the nova.conf of one compute\nhttp://paste.ubuntu.com/18295909/\nDo you think, this might be due to config issue in nova configuration file?\n \n\nopenstack-dashboard:\n     vip: \"<DASHBOARD_VIP>\"\n     openstack-origin: \"cloud:trusty-mitaka\"\nhacluster-dashboard:\n     cluster_count: 3\n     corosync_transport: unicast\n     monitor_host: \"<MONITOR_HOST_IP>\"\nnova-compute:\n     enable-live-migration: \"True\"\n     enable-resize: \"True\"\n     openstack-origin: \"cloud:trusty-mitaka\"\n     migration-auth-type: \"ssh\"\n     manage-neutron-plugin-legacy-mode: False\nnova-cloud-controller:\n     network-manager: \"Neutron\"\n     console-access-protocol: \"novnc\"\n     openstack-origin: \"cloud:trusty-mitaka\"\n     region: \"serverstack\"\n     vip: \"<NOVA_CC_VIP>\"\nhacluster-nova-cc:\n     cluster_count: 3\n     monitor_host: \"<MONITOR_HOST_IP>\"\n     corosync_transport: unicast", 
            "date_created": "2016-07-02 10:54:47.256115+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "I agree with Sean - this is probably a charm problem; for console access in HA configurations, there are a few options for caching of access tokens for the nova-consoleauth process - you can either you the memcache charm with multiple nova-consoleauth processes running OR the charms can configure things so that only a single process runs across the clustered units.", 
            "date_created": "2016-07-02 17:00:01.019971+00:00", 
            "author": "https://api.launchpad.net/1.0/~james-page"
        }, 
        {
            "content": "The \"single-nova-consoleauth\" config option is \"True\" in my case, which allows only a single consoleauth service to be running. I will try out the memcache charm and will update if it solves the issue.", 
            "date_created": "2016-07-03 09:56:34.466272+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "Thanks James, using memcached charm solved this issue. Thanks for helping me out.", 
            "date_created": "2016-08-27 16:05:59.518699+00:00", 
            "author": "https://api.launchpad.net/1.0/~junaidali"
        }, 
        {
            "content": "Marking all bug tasks as invalid, as switching to using memcache resolved this issue.", 
            "date_created": "2017-02-02 14:53:39.772812+00:00", 
            "author": "https://api.launchpad.net/1.0/~james-page"
        }
    ]
}
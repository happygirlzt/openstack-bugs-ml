{
    "status": "Opinion", 
    "last_updated": "2012-07-12 14:20:15.882294+00:00", 
    "description": "This problem ocurred in a stable-diablo, ubuntu 11.10, kvm 2-compute-node cluster with multi_host that has been running for two months with multiple users. A user reported that she could not ssh into 2 of her vms. I saw that those vms were running on the same compute node and on that compute node, nova-compute was running but had stopped making any new log entries. Also 'virsh list' hung. She also tried to reboot and then delete the vms.  The nova-compute log had errors like:\n\nError: trying to destroy already destroyed instance: 200\n\nThe nova-network log had the following. The really scary thing is that this same kind of error, for instance 200, appeared at about the same time in the nova-network log of the *other* compute node. Somehow the other compute node was trying to do network operations for a vm that was owned by a different compute node. Unless I misunderstand how multi_host works this should not be possible. The log files are large so I will attach a file with the time-window snippets. I have the full log files if any one wants them. The ids of the two vms were 155 and 200.\n\nRestarting libvirt did not help and I had to reboot the compute node e to restore sanity to the system. At that point nova was confused in that it had\nmarked the 2 vms as gone but they were still running even after the reboot and I did not have auto-restart set for vms. I had to kill them with virsh. As an aside, in this two month run the\nissue of libvirt hanging happened at least one other time for no reason. In that case restarting libvirt fixed the problems.\n\n2012-02-27 17:35:27,309 DEBUG nova.network.manager [43e0bcc2-2048-4875-b289-0270d1973c0a tester testproject] floating IP deallocation for instance |200| from (pid=1201) deallocate_for_instance /usr/lib/python2.7/dist-packages/nova/network/manager.py:251\n2012-02-27 17:35:27,314 DEBUG nova.network.manager [43e0bcc2-2048-4875-b289-0270d1973c0a tester testproject] network deallocation for instance |200| from (pid=1201) deallocate_for_instance /usr/lib/python2.7/dist-packages/nova/network/manager.py:465\n2012-02-27 17:35:27,445 ERROR nova.rpc [4a59acea-7ec9-4eff-b346-7afcae98cfbc None None] Exception during message handling\n(nova.rpc): TRACE: Traceback (most recent call last):\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py\", line 620, in _process_data\n(nova.rpc): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 275, in deallocate_for_instance\n(nova.rpc): TRACE:     super(FloatingIP, self).deallocate_for_instance(context, **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 468, in deallocate_for_instance\n(nova.rpc): TRACE:     self.deallocate_fixed_ip(context, fixed_ip['address'], **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 641, in deallocate_fixed_ip\n(nova.rpc): TRACE:     instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 404, in _do_trigger_security_group_members_refresh_for_instance\n(nova.rpc): TRACE:     instance_ref = self.db.instance_get(admin_context, instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/api.py\", line 500, in instance_get\n(nova.rpc): TRACE:     return IMPL.instance_get(context, instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 119, in wrapper\n(nova.rpc): TRACE:     return f(*args, **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 1171, in instance_get\n(nova.rpc): TRACE:     raise exception.InstanceNotFound(instance_id=instance_id)\n(nova.rpc): TRACE: InstanceNotFound: Instance 200 could not be found.\n(nova.rpc): TRACE:", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/943312", 
    "owner": "None", 
    "id": 943312, 
    "index": 2630, 
    "created": "2012-02-29 14:50:58.099573+00:00", 
    "title": "Strange apparent atomicity failure in nova", 
    "comments": [
        {
            "content": "This problem ocurred in a stable-diablo, ubuntu 11.10, kvm 2-compute-node cluster with multi_host that has been running for two months with multiple users. A user reported that she could not ssh into 2 of her vms. I saw that those vms were running on the same compute node and on that compute node, nova-compute was running but had stopped making any new log entries. Also 'virsh list' hung. She also tried to reboot and then delete the vms.  The nova-compute log had errors like:\n\nError: trying to destroy already destroyed instance: 200\n\nThe nova-network log had the following. The really scary thing is that this same kind of error, for instance 200, appeared at about the same time in the nova-network log of the *other* compute node. Somehow the other compute node was trying to do network operations for a vm that was owned by a different compute node. Unless I misunderstand how multi_host works this should not be possible. The log files are large so I will attach a file with the time-window snippets. I have the full log files if any one wants them. The ids of the two vms were 155 and 200.\n\nRestarting libvirt did not help and I had to reboot the compute node e to restore sanity to the system. At that point nova was confused in that it had\nmarked the 2 vms as gone but they were still running even after the reboot and I did not have auto-restart set for vms. I had to kill them with virsh. As an aside, in this two month run the\nissue of libvirt hanging happened at least one other time for no reason. In that case restarting libvirt fixed the problems.\n\n2012-02-27 17:35:27,309 DEBUG nova.network.manager [43e0bcc2-2048-4875-b289-0270d1973c0a tester testproject] floating IP deallocation for instance |200| from (pid=1201) deallocate_for_instance /usr/lib/python2.7/dist-packages/nova/network/manager.py:251\n2012-02-27 17:35:27,314 DEBUG nova.network.manager [43e0bcc2-2048-4875-b289-0270d1973c0a tester testproject] network deallocation for instance |200| from (pid=1201) deallocate_for_instance /usr/lib/python2.7/dist-packages/nova/network/manager.py:465\n2012-02-27 17:35:27,445 ERROR nova.rpc [4a59acea-7ec9-4eff-b346-7afcae98cfbc None None] Exception during message handling\n(nova.rpc): TRACE: Traceback (most recent call last):\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/rpc/impl_kombu.py\", line 620, in _process_data\n(nova.rpc): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 275, in deallocate_for_instance\n(nova.rpc): TRACE:     super(FloatingIP, self).deallocate_for_instance(context, **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 468, in deallocate_for_instance\n(nova.rpc): TRACE:     self.deallocate_fixed_ip(context, fixed_ip['address'], **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 641, in deallocate_fixed_ip\n(nova.rpc): TRACE:     instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/network/manager.py\", line 404, in _do_trigger_security_group_members_refresh_for_instance\n(nova.rpc): TRACE:     instance_ref = self.db.instance_get(admin_context, instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/api.py\", line 500, in instance_get\n(nova.rpc): TRACE:     return IMPL.instance_get(context, instance_id)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 119, in wrapper\n(nova.rpc): TRACE:     return f(*args, **kwargs)\n(nova.rpc): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 1171, in instance_get\n(nova.rpc): TRACE:     raise exception.InstanceNotFound(instance_id=instance_id)\n(nova.rpc): TRACE: InstanceNotFound: Instance 200 could not be found.\n(nova.rpc): TRACE:", 
            "date_created": "2012-02-29 14:50:58.099573+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-02-29 14:50:58.099573+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Thanks for the very detailed report David\n\nAs detailed as it is, I'm going to have to mark it as Incomplete because (a) we've no idea how to reproduce this and (b) an awful lot has changed since Diablo which might have fixed this\n\nIf you can come up with a reliable reproducer on Diablo or see the issue again with Essex or Folsom, please do re-open", 
            "date_created": "2012-06-07 11:33:04.627743+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "I obviously can't reproduce this. The system was long-running and I don't even know exactly what the user did. It's probably some race somewhere which can't occur anymore if we are lucky.", 
            "date_created": "2012-06-07 12:55:49.542872+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Can't be reproduced. Please set back to \"New\" if you see it again", 
            "date_created": "2012-07-12 14:20:13.090329+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }
    ]
}
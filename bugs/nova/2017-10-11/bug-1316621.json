{
    "status": "Fix Released", 
    "last_updated": "2017-06-01 15:40:30.294020+00:00", 
    "description": "Sometimes request to associate floating IP may fail, when using nova network with libvirt like:\n\n> http://192.168.1.12:8774/v2/258a4b20c77240bf9b386411430683fa/servers/a9e734e4-5310-4191-a7f0-78fca4b367e7/action\n> \n> BadRequest: Bad request\n> Details: {'message': 'Error. Unable to associate floating ip', 'code': '400'}\n\nReal issue is that ebtables rootwrap call fails:\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 192.168.32.10 ! --ip-dst 192.168.32.0/22 -j redirect --redirect-target ACCEPT\nExit code: 255\nStdout: ''\nStderr: \"Unable to update the kernel. Two possible causes:\\n1. Multiple ebtables programs were executing simultaneously. The ebtables\\n   userspace tool doesn't by default support multiple ebtables programs running\\n   concurrently. The ebtables option --concurrent or a tool like flock can be\\n   used to support concurrent scripts that update the ebtables kernel tables.\\n2. The kernel doesn't support a certain ebtables extension, consider\\n   recompiling your kernel or insmod the extension.\\n.\\n\"\n\nIt happens like once in whole tempest run, and also not always, so kernel support and other reasons should not apply here.\nProbably already mentioned in https://<email address hidden>/msg23422.html.\n\nAs that call in nova is synchronized, locked, it could be that nova can actually race with libvirt itself calling ebtables?", 
    "tags": [
        "in-stable-newton", 
        "in-stable-ocata", 
        "libvirt", 
        "testing"
    ], 
    "importance": "Medium", 
    "heat": 30, 
    "link": "https://bugs.launchpad.net/nova/+bug/1316621", 
    "owner": "https://api.launchpad.net/1.0/~cfb-n", 
    "id": 1316621, 
    "index": 3892, 
    "created": "2014-05-06 14:20:27.235495+00:00", 
    "title": "ebtables calls can race with libvirt", 
    "comments": [
        {
            "content": "Sometimes request to associate floating IP may fail, when using nova network with libvirt like:\n\n> http://192.168.1.12:8774/v2/258a4b20c77240bf9b386411430683fa/servers/a9e734e4-5310-4191-a7f0-78fca4b367e7/action\n> \n> BadRequest: Bad request\n> Details: {'message': 'Error. Unable to associate floating ip', 'code': '400'}\n\nReal issue is that ebtables rootwrap call fails:\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 192.168.32.10 ! --ip-dst 192.168.32.0/22 -j redirect --redirect-target ACCEPT\nExit code: 255\nStdout: ''\nStderr: \"Unable to update the kernel. Two possible causes:\\n1. Multiple ebtables programs were executing simultaneously. The ebtables\\n   userspace tool doesn't by default support multiple ebtables programs running\\n   concurrently. The ebtables option --concurrent or a tool like flock can be\\n   used to support concurrent scripts that update the ebtables kernel tables.\\n2. The kernel doesn't support a certain ebtables extension, consider\\n   recompiling your kernel or insmod the extension.\\n.\\n\"\n\nIt happens like once in whole tempest run, and also not always, so kernel support and other reasons should not apply here.\nProbably already mentioned in https://<email address hidden>/msg23422.html.\n\nAs that call in nova is synchronized, locked, it could be that nova can actually race with libvirt itself calling ebtables?", 
            "date_created": "2014-05-06 14:20:27.235495+00:00", 
            "author": "https://api.launchpad.net/1.0/~psedlak"
        }, 
        {
            "content": "Happened with Havana on RHEL6 and Icehouse on RHEL 7.\nAs it's flaky I don't have detailed info mostly common logs or versions - though as it's with both Havana and Icehouse on different versions of kernel etc, it seems as not related anyway.\n\nAttaching part of nova-network.log showing that locks were obtained and command failed.\n", 
            "date_created": "2014-05-06 14:27:28.617199+00:00", 
            "author": "https://api.launchpad.net/1.0/~psedlak"
        }, 
        {
            "content": "Well that is annoying. If it is that rare, perhaps doing a few retrys is good enough. I'm not sure if there is an easy way to do a shared lock with kvm.", 
            "date_created": "2014-06-18 16:17:57.268574+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "We ignore the exit code on the delete we do before an insert of a rule, which leaves me thinking a retry would be hard to implement here. I guess we could change the delete to check the list of ebtables rules to make sure the entry exists, but I am unsure how expensive that would be.", 
            "date_created": "2014-06-18 23:33:59.209373+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "This bug can be reproduced by rally. For example, you can run boot-run-command-delete task. \nthe import point here is that you should test you cloud with high pressure, then the bug will be reproduced.\nI use rally test my could.\nrally configuration is \n{\n    \"VMTasks.boot_runcommand_delete\": [\n        {\n            \"args\": {\n                \"flavor\": {\n                    \"name\": \"m1.small\"\n                },\n                \"image\": {\n                    \"name\": \"ubuntu-12-04-raw-rally-test\"\n                },\n                \"script\": \"/home/rally/rally/doc/samples/ec_script/ubuntu_ls_test.sh\",\n                \"interpreter\": \"bash\",\n                \"username\": \"root\",\n                \"floating_network\": \"LTQ\",\n                \"use_floatingip\": true,\n                \"availability_zone\": \"dell420\",\n            },\n            \"runner\": {\n                \"type\": \"constant\",\n                \"times\": 1000,\n                \"concurrency\": 40,\n                \"timeout\": 6000\n            },\n            \"context\": {\n                \"users\": {\n                    \"tenants\": 1,\n                    \"users_per_tenant\": 1\n                },\n                \"quotas\": {\n                     \"nova\": {\n                         \"instances\": -1,\n                         \"cores\": -1,\n                         \"ram\": -1,\n                         \"fixed_ips\": -1,\n                         \"floating_ips\": -1,\n                     }\n                 },\n            }\n        }\n    ]\n}\n\n\nthe rally post error is \n2014-08-15 13:55:53.602 29457 INFO rally.benchmark.runners.base [-] Task bd820c37-2eaf-49d6-99b8-7952d453197d | ITER: 747 END: Error <class 'novaclient.exceptions.BadRequest'>: Error. Unable to associate floating ip (HTTP 400) (Request-ID: req-fa6fa661-e41d-4235-9da7-74ba882dd3c8)\n\nthe nova-network.log in the same time is \n2014-08-15 13:55:52.990 23291 DEBUG nova.network.linux_net [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] IPTablesManager.apply completed with success _apply /usr/lib/python2.7/dist-packages/nova/network/linux_net.py:451\n2014-08-15 13:55:52.991 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Released file lock \"iptables\" at /var/lock/nova/nova-iptables lock /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:208\n2014-08-15 13:55:52.991 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Got semaphore \"ebtables\" lock /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:166\n2014-08-15 13:55:52.992 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Attempting to grab file lock \"ebtables\" lock /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:176\n2014-08-15 13:55:52.993 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Got file lock \"ebtables\" at /var/lock/nova/nova-ebtables lock /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:204\n2014-08-15 13:55:52.993 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Got semaphore / lock \"ensure_ebtables_rules\" inner /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:245\n2014-08-15 13:55:52.994 23291 DEBUG nova.openstack.common.processutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -D PREROUTING --logical-in br100 -p ipv4 --ip-src 10.5.101.41 ! --ip-dst 10.5.101.0/24 -j redirect --redirect-target ACCEPT execute /usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py:147\n2014-08-15 13:55:53.081 23291 DEBUG nova.openstack.common.processutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Result was 255 execute /usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py:172\n2014-08-15 13:55:53.083 23291 DEBUG nova.openstack.common.processutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 10.5.101.41 ! --ip-dst 10.5.101.0/24 -j redirect --redirect-target ACCEPT execute /usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py:147\n2014-08-15 13:55:53.170 23291 DEBUG nova.openstack.common.processutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Result was 255 execute /usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py:172\n2014-08-15 13:55:53.171 23291 DEBUG nova.openstack.common.lockutils [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Released file lock \"ebtables\" at /var/lock/nova/nova-ebtables lock /usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py:208\n2014-08-15 13:55:53.270 23291 ERROR nova.openstack.common.rpc.amqp [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Exception during message handling\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 461, in _process_data\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     **args)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py\", line 172, in dispatch\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 390, in _associate_floating_ip\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     do_associate()\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py\", line 246, in inner\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 383, in do_associate\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     interface=interface)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 367, in do_associate\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     interface, fixed['network'])\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/l3.py\", line 115, in add_floating_ip\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     l3_interface_id, network)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 762, in ensure_floating_forward\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     ensure_ebtables_rules(*floating_ebtables_rules(fixed_ip, network))\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py\", line 246, in inner\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 1590, in ensure_ebtables_rules\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     _execute(*cmd, run_as_root=True)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 1190, in _execute\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     return utils.execute(*cmd, **kwargs)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 177, in execute\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     return processutils.execute(*cmd, **kwargs)\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py\", line 178, in execute\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp     cmd=' '.join(cmd))\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp Command: sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 10.5.101.41 ! --ip-dst 10.5.101.0/24 -j redirect --redirect-target ACCEPT\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp Exit code: 255\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp Stdout: ''\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp Stderr: \"The kernel doesn't support a certain ebtables extension, consider recompiling your kernel or insmod the extension.\\n\"\n2014-08-15 13:55:53.270 23291 TRACE nova.openstack.common.rpc.amqp \n2014-08-15 13:55:53.273 23291 ERROR nova.openstack.common.rpc.common [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] Returning exception Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 10.5.101.41 ! --ip-dst 10.5.101.0/24 -j redirect --redirect-target ACCEPT\nExit code: 255\nStdout: ''\nStderr: \"The kernel doesn't support a certain ebtables extension, consider recompiling your kernel or insmod the extension.\\n\" to caller\n2014-08-15 13:55:53.273 23291 ERROR nova.openstack.common.rpc.common [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 461, in _process_data\\n    **args)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py\", line 172, in dispatch\\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 390, in _associate_floating_ip\\n    do_associate()\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py\", line 246, in inner\\n    return f(*args, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 383, in do_associate\\n    interface=interface)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/floating_ips.py\", line 367, in do_associate\\n    interface, fixed[\\'network\\'])\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/l3.py\", line 115, in add_floating_ip\\n    l3_interface_id, network)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 762, in ensure_floating_forward\\n    ensure_ebtables_rules(*floating_ebtables_rules(fixed_ip, network))\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py\", line 246, in inner\\n    return f(*args, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 1590, in ensure_ebtables_rules\\n    _execute(*cmd, run_as_root=True)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/network/linux_net.py\", line 1190, in _execute\\n    return utils.execute(*cmd, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 177, in execute\\n    return processutils.execute(*cmd, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py\", line 178, in execute\\n    cmd=\\' \\'.join(cmd))\\n', 'ProcessExecutionError: Unexpected error while running command.\\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf ebtables -t nat -I PREROUTING --logical-in br100 -p ipv4 --ip-src 10.5.101.41 ! --ip-dst 10.5.101.0/24 -j redirect --redirect-target ACCEPT\\nExit code: 255\\nStdout: \\'\\'\\nStderr: \"The kernel doesn\\'t support a certain ebtables extension, consider recompiling your kernel or insmod the extension.\\\\n\"\\n']\n2014-08-15 13:55:53.274 23291 DEBUG nova.openstack.common.rpc.amqp [req-fa6fa661-e41d-4235-9da7-74ba882dd3c8 737b99c364a64253920c67313655e171 a8c948f6e70648608603b5079537c525] UNIQUE_ID is e652955be8314743ad38bd3f6621a29c. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:341\n\n\nHope this help some one to fixed the bug also.", 
            "date_created": "2014-08-15 06:28:35.565511+00:00", 
            "author": "https://api.launchpad.net/1.0/~jazeltq-k"
        }, 
        {
            "content": "The ebtables problem is also talked about here.\nhttp://www.spinics.net/linux/fedora/libvirt-users/msg06645.html", 
            "date_created": "2014-08-15 07:08:45.841149+00:00", 
            "author": "https://api.launchpad.net/1.0/~jazeltq-k"
        }, 
        {
            "content": "Marking as high, because this has been seen more recently in bringing up multi-node gate tests.", 
            "date_created": "2014-11-05 16:10:40.945964+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "This patch to upstream libvirt adds use of --concurrent to ebtables and --wait to iptables/ip6tables.\n\nhttps://www.redhat.com/archives/libvir-list/2014-November/msg00330.html\n\nFor this to help the race condition we'd need to modify Nova to use the same args too.", 
            "date_created": "2014-11-11 15:30:30.892622+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "@berrange\n\nThats excellent. I was going to propose a change to do just that now that I'm back from vacation. Since thats already done I can work on the other required pieces to make that work in nova.\n\nSince this is currently hurting the gate the current plan is the following.\n\n1) Submit a quick fix that adds a simple retry to nova for ebtables. This should get the gate working smoothly again.\n\n2) Add support timing out long running commans to oslo.concurrency.processutils. ebtables --concurrent will block, forever until it gets the lock. We need a way to reliable time this out after some period of time to prevent nova blocking on this forever.\n\n3) Once we can timeout an operation in processutils we can patch nova to use --concurrent.\n\nI should have patch #1 up in the next day.", 
            "date_created": "2014-11-19 20:17:26.678027+00:00", 
            "author": "https://api.launchpad.net/1.0/~cfb-n"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/136217", 
            "date_created": "2014-11-21 02:41:45.475768+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/136217\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=fb9b2058051b771732f4425c97651128c8060441\nSubmitter: Jenkins\nBranch:    master\n\ncommit fb9b2058051b771732f4425c97651128c8060441\nAuthor: Chet Burgess <email address hidden>\nDate:   Thu Nov 20 18:29:15 2014 -0800\n\n    Retry ebtables on race\n    \n    Calls to ebtables can race with libvirt and cause nova, or libvirt\n    to fail to apply ebtables rules.\n    \n    The goal of this patch is to provide a simple fix to improve the\n    stability of the gate.\n    \n    We now call ebtables in a simple loop that retries on failure.\n    Long term we want to update nova to make use of the --concurrent\n    flag in newer versions of ebtables. The --concurrent flag\n    implements a lock to prevent multiple invocations of ebtables from\n    racing. This will require a newer libvirt and the ability to\n    timeout long running execs (--concurrent can block forever if it\n    never gets the lock).\n    \n    A future patch is forthcoming to add support for --concurrent.\n    \n    DocImpact\n    Add ebtables_exec_attempts option (default=3).\n    \n    Change-Id: I3e04782ac4678581462f9bee4bb10d5f3b223457\n    Partial-Bug: #1316621\n", 
            "date_created": "2014-11-26 05:11:52.492636+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Will someone with the proper permissions please change the status back to medium? We have a work around for the gate now. I'm still tracking the long term fix for K but the immediate symptoms have been addressed.", 
            "date_created": "2014-12-01 19:42:06.506613+00:00", 
            "author": "https://api.launchpad.net/1.0/~cfb-n"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/140514", 
            "date_created": "2014-12-09 22:55:08.496702+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/140514\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4f418727f7de689a2387d3a7a2cc90ae9503c91e\nSubmitter: Jenkins\nBranch:    master\n\ncommit 4f418727f7de689a2387d3a7a2cc90ae9503c91e\nAuthor: Chet Burgess <email address hidden>\nDate:   Tue Dec 9 14:51:40 2014 -0800\n\n    Add backoff to ebtables retry\n    \n    We need a backoff between ebtables retries. In some tempest tests we\n    have seen the retries complete in 100ms and still fail.\n    \n    We now sleep for ebtables_retry_interval * loop count seconds. With\n    a default of 1.0 this means by default we sleep for 1.0s, 2.0s, and\n    3.0s before we finally giving up.\n    \n    Change-Id: I0b9b664a592364bedd11124a1ec921d8ea011704\n    Partial-Bug: #1316621\n", 
            "date_created": "2014-12-11 02:47:43.572438+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Looks like this has merged, switching status to \"Fix Committed\"", 
            "date_created": "2015-03-04 13:09:36.255726+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "The patch from danpb merged into upstream libvirt:\n\nhttp://libvirt.org/git/?p=libvirt.git;a=commit;h=dc33e6e4a5a5d429198b2c63ff6b63729353e2cf\n\nIt's in version 1.2.11 which is way too new for what we're testing with in the gate.", 
            "date_created": "2015-04-02 22:49:29.777919+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/431773\nCommitted: https://git.openstack.org/cgit/openstack/neutron/commit/?id=486e2f4eb5a02c98958582e366a4d6081ea897e0\nSubmitter: Jenkins\nBranch:    master\n\ncommit 486e2f4eb5a02c98958582e366a4d6081ea897e0\nAuthor: Kevin Benton <email address hidden>\nDate:   Thu Feb 9 15:10:20 2017 -0800\n\n    Pass --concurrent flag to ebtables calls\n    \n    This flag will force ebtables to acquire a lock so we don't\n    have to worry about ebtables errors occuring if something else\n    on the system is trying to use ebtables as well.\n    \n    Closes-Bug: #1316621\n    Change-Id: I695c01e015fdc201df8f23d9b48f9d3678240266\n", 
            "date_created": "2017-02-21 19:47:50.793157+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/neutron 11.0.0.0b1 development milestone.", 
            "date_created": "2017-04-14 09:17:56.347266+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/ocata\nReview: https://review.openstack.org/460916", 
            "date_created": "2017-04-28 10:05:50.604245+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/newton\nReview: https://review.openstack.org/460917", 
            "date_created": "2017-04-28 10:06:51.614819+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/460916\nCommitted: https://git.openstack.org/cgit/openstack/neutron/commit/?id=470833d36d5313d549c9905d9a36af8cfbcc3330\nSubmitter: Jenkins\nBranch:    stable/ocata\n\ncommit 470833d36d5313d549c9905d9a36af8cfbcc3330\nAuthor: Kevin Benton <email address hidden>\nDate:   Thu Feb 9 15:10:20 2017 -0800\n\n    Pass --concurrent flag to ebtables calls\n    \n    This flag will force ebtables to acquire a lock so we don't\n    have to worry about ebtables errors occuring if something else\n    on the system is trying to use ebtables as well.\n    \n    Closes-Bug: #1316621\n    Change-Id: I695c01e015fdc201df8f23d9b48f9d3678240266\n    (cherry picked from commit 486e2f4eb5a02c98958582e366a4d6081ea897e0)\n", 
            "date_created": "2017-04-28 22:27:44.791599+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/460917\nCommitted: https://git.openstack.org/cgit/openstack/neutron/commit/?id=f6ae49b020859b7878a992d2bb158b2c912a5765\nSubmitter: Jenkins\nBranch:    stable/newton\n\ncommit f6ae49b020859b7878a992d2bb158b2c912a5765\nAuthor: Kevin Benton <email address hidden>\nDate:   Thu Feb 9 15:10:20 2017 -0800\n\n    Pass --concurrent flag to ebtables calls\n    \n    This flag will force ebtables to acquire a lock so we don't\n    have to worry about ebtables errors occuring if something else\n    on the system is trying to use ebtables as well.\n    \n    Closes-Bug: #1316621\n    Change-Id: I695c01e015fdc201df8f23d9b48f9d3678240266\n    (cherry picked from commit 486e2f4eb5a02c98958582e366a4d6081ea897e0)\n    (cherry picked from commit 470833d36d5313d549c9905d9a36af8cfbcc3330)\n", 
            "date_created": "2017-04-28 23:50:21.491840+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/neutron 9.4.0 release.", 
            "date_created": "2017-06-01 15:38:40.929903+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/neutron 10.0.2 release.", 
            "date_created": "2017-06-01 15:40:26.626788+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
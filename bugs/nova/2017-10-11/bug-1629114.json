{
    "status": "Expired", 
    "last_updated": "2017-08-27 04:18:43.257149+00:00", 
    "description": "Description:\n\nCeph RBD live-migration fails to modify rbd_user/libvirt secret UUID to the receiving hosts information, causing live migration to fail.\n\nSteps to reproduce:\n\nCompute node A:\n\n/etc/nova/nova.conf:\nrbd_user=compute_node_A\nrbd_secret_uuid = secretA\n\nSecret file:\n/etc/libvirt/secrets/secretA.xml\n\nCompute node B:\n\n/etc/nova/nova.conf:\nrbd_user=compute_node_B\nrbd_secret_uuid = secretB\n\nSecret file:\n/etc/libvirt/secret/secretB.xml\n\n\nExpected result:\n\nLive migration completes\n\nCurrent result:\n\nLive migration fails because it sets the secret/key/id to the information from cmopute_node_A instead of compute_node_B.\n\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29 18:50:40.613 175448 ERROR nova.virt.libvirt.driver [req-77ce1a5a-6588-420d-8c77-7b106e4ca3f0 4c8a770be6c54c23bbf20e8a63803d63 2d98cd4d4fdf43f5b9db5e39846922d8 - - -] [instance: b4407d16-8946-45a0-8e58-3a1bf8b0edfc] Live Migration failure: internal error: process exited while connecting to monitor: 2016-09-29T18:50:40.220091Z qemu-system-x86_64: -drive file=rbd:nova/b4407d16-8946-45a0-8e58-3a1bf8b0edfc_disk:id=nova-compute-c07:keysomecephkey:auth_supported=cephx\\;none:mon_host=[fd2d\\:dec4\\:cf59\\:3c12\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c13\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c14\\:0\\:1\\:\\:]\\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none: error connecting\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.246712Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.274406Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: Traceback (most recent call last):\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 457, in fire_timers\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     timer()\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/timer.py\", line 58, in __call__\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     cb(*args, **kw)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/event.py\", line 168, in _do_send\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     waiter.switch(result)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     result = function(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 1145, in context_wrapper\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     return func(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 6104, in _live_migration_operation\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     instance=instance)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 220, in __exit__\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     self.force_reraise()\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     six.reraise(self.type_, self.value, self.tb)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 6064, in _live_migration_operation\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     migration_flags)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 186, in doit\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     result = proxy_call(self._autowrap, f, *args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     rv = execute(f, *args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     six.reraise(c, e, tb)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     rv = meth(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 1833, in migrateToURI3\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     if ret == -1: raise libvirtError ('virDomainMigrateToURI3() failed', dom=self)\nSep 29 18:50:40 compute_node_A nova-compute[175448]: libvirtError: internal error: process exited while connecting to monitor: 2016-09-29T18:50:40.220091Z qemu-system-x86_64: -drive file=rbd:nova/b4407d16-8946-45a0-8e58-3a1bf8b0edfc_disk:id=nova-compute-c07:key=somecephkey:auth_supported=cephx\\;none:mon_host=[fd2d\\:dec4\\:cf59\\:3c12\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c13\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c14\\:0\\:1\\:\\:]\\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none: error connecting\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.246712Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.274406Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29 18:50:40.703 175448 ERROR nova.virt.libvirt.driver [req-77ce1a5a-6588-420d-8c77-7b106e4ca3f0 4c8a770be6c54c23bbf20e8a63803d63 2d98cd4d4fdf43f5b9db5e39846922d8 - - -] [instance: b4407d16-8946-45a0-8e58-3a1bf8b0edfc] Migration operation has aborted\n\nEnvironment\n===========\n1. OpenStack Mitaka from Ubuntu\n\n2. KVM\n\n2. Ceph\n\n3. Neutron with Calico\n\n----\n\nMore information:\n\nThe issue occurs when the new libvirt XML config is generated for the disk configuration:\n\nhttps://github.com/openstack/nova/blob/643aed652d0e51e36dbe7cb106285b51e3b5941b/nova/virt/libvirt/volume/net.py#L67\n\n        if (conf.source_protocol == 'rbd' and\n                CONF.libvirt.rbd_secret_uuid):\n            conf.auth_secret_uuid = CONF.libvirt.rbd_secret_uuid\n            auth_enabled = True  # Force authentication locally\n            if CONF.libvirt.rbd_user:\n                conf.auth_username = CONF.libvirt.rbd_user\n\nInstead of getting the configuration information from the remote host (node B when live migrating from node A -> node B) it is pulling the information from the local /etc/nova/nova.conf file (using the CONF object) instead of getting that information from the remote host that the VM is about to be migrated to.\n\nnode A's nova.conf file does not match node B's nova.conf file when it comes to the \"rbd_user\"/\"rbd_secret\".\n\nThis causes failures to migrate the VM over because Ceph won't let compute_node_B authenticate because there are no credentials.", 
    "tags": [
        "openstack-version.mitaka", 
        "sts"
    ], 
    "importance": "Undecided", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1629114", 
    "owner": "None", 
    "id": 1629114, 
    "index": 6459, 
    "created": "2016-09-29 21:28:36.877001+00:00", 
    "title": "Ceph RBD live-migration failure due to wrong rbd_user/rbd_secret", 
    "comments": [
        {
            "content": "Description:\n\nCeph RBD live-migration fails to modify rbd_user/libvirt secret UUID to the receiving hosts information, causing live migration to fail.\n\nSteps to reproduce:\n\nCompute node A:\n\n/etc/nova/nova.conf:\nrbd_user=compute_node_A\nrbd_secret_uuid = secretA\n\nSecret file:\n/etc/libvirt/secrets/secretA.xml\n\nCompute node B:\n\n/etc/nova/nova.conf:\nrbd_user=compute_node_B\nrbd_secret_uuid = secretB\n\nSecret file:\n/etc/libvirt/secret/secretB.xml\n\n\nExpected result:\n\nLive migration completes\n\nCurrent result:\n\nLive migration fails because it sets the secret/key/id to the information from cmopute_node_A instead of compute_node_B.\n\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29 18:50:40.613 175448 ERROR nova.virt.libvirt.driver [req-77ce1a5a-6588-420d-8c77-7b106e4ca3f0 4c8a770be6c54c23bbf20e8a63803d63 2d98cd4d4fdf43f5b9db5e39846922d8 - - -] [instance: b4407d16-8946-45a0-8e58-3a1bf8b0edfc] Live Migration failure: internal error: process exited while connecting to monitor: 2016-09-29T18:50:40.220091Z qemu-system-x86_64: -drive file=rbd:nova/b4407d16-8946-45a0-8e58-3a1bf8b0edfc_disk:id=nova-compute-c07:keysomecephkey:auth_supported=cephx\\;none:mon_host=[fd2d\\:dec4\\:cf59\\:3c12\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c13\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c14\\:0\\:1\\:\\:]\\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none: error connecting\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.246712Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.274406Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: Traceback (most recent call last):\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 457, in fire_timers\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     timer()\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/hubs/timer.py\", line 58, in __call__\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     cb(*args, **kw)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/event.py\", line 168, in _do_send\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     waiter.switch(result)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     result = function(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 1145, in context_wrapper\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     return func(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 6104, in _live_migration_operation\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     instance=instance)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 220, in __exit__\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     self.force_reraise()\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     six.reraise(self.type_, self.value, self.tb)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 6064, in _live_migration_operation\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     migration_flags)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 186, in doit\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     result = proxy_call(self._autowrap, f, *args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     rv = execute(f, *args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     six.reraise(c, e, tb)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     rv = meth(*args, **kwargs)\nSep 29 18:50:40 compute_node_A nova-compute[175448]:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 1833, in migrateToURI3\nSep 29 18:50:40 compute_node_A nova-compute[175448]:     if ret == -1: raise libvirtError ('virDomainMigrateToURI3() failed', dom=self)\nSep 29 18:50:40 compute_node_A nova-compute[175448]: libvirtError: internal error: process exited while connecting to monitor: 2016-09-29T18:50:40.220091Z qemu-system-x86_64: -drive file=rbd:nova/b4407d16-8946-45a0-8e58-3a1bf8b0edfc_disk:id=nova-compute-c07:key=somecephkey:auth_supported=cephx\\;none:mon_host=[fd2d\\:dec4\\:cf59\\:3c12\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c13\\:0\\:1\\:\\:]\\:6789\\;[fd2d\\:dec4\\:cf59\\:3c14\\:0\\:1\\:\\:]\\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none: error connecting\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.246712Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29T18:50:40.274406Z qemu-system-x86_64: network script /etc/qemu-ifdown failed with status 256\nSep 29 18:50:40 compute_node_A nova-compute[175448]: 2016-09-29 18:50:40.703 175448 ERROR nova.virt.libvirt.driver [req-77ce1a5a-6588-420d-8c77-7b106e4ca3f0 4c8a770be6c54c23bbf20e8a63803d63 2d98cd4d4fdf43f5b9db5e39846922d8 - - -] [instance: b4407d16-8946-45a0-8e58-3a1bf8b0edfc] Migration operation has aborted\n\nEnvironment\n===========\n1. OpenStack Mitaka from Ubuntu\n\n2. KVM\n\n2. Ceph\n\n3. Neutron with Calico\n\n----\n\nMore information:\n\nThe issue occurs when the new libvirt XML config is generated for the disk configuration:\n\nhttps://github.com/openstack/nova/blob/643aed652d0e51e36dbe7cb106285b51e3b5941b/nova/virt/libvirt/volume/net.py#L67\n\n        if (conf.source_protocol == 'rbd' and\n                CONF.libvirt.rbd_secret_uuid):\n            conf.auth_secret_uuid = CONF.libvirt.rbd_secret_uuid\n            auth_enabled = True  # Force authentication locally\n            if CONF.libvirt.rbd_user:\n                conf.auth_username = CONF.libvirt.rbd_user\n\nInstead of getting the configuration information from the remote host (node B when live migrating from node A -> node B) it is pulling the information from the local /etc/nova/nova.conf file (using the CONF object) instead of getting that information from the remote host that the VM is about to be migrated to.\n\nnode A's nova.conf file does not match node B's nova.conf file when it comes to the \"rbd_user\"/\"rbd_secret\".\n\nThis causes failures to migrate the VM over because Ceph won't let compute_node_B authenticate because there are no credentials.", 
            "date_created": "2016-09-29 21:28:36.877001+00:00", 
            "author": "https://api.launchpad.net/1.0/~bregeer-ctl"
        }, 
        {
            "content": "Do note that iSCSI already pulls the secret from the remote using _get_secret_uuid(), so the only thing left is somehow fetching the information about the rbd_user from the remote nova.conf.", 
            "date_created": "2016-09-29 22:35:07.923224+00:00", 
            "author": "https://api.launchpad.net/1.0/~bregeer-ctl"
        }, 
        {
            "content": "Hi Billy, my understanding hitherto has been that the fact you can't live-migrate between compute hosts with different Ceph credentials is actually by-design. If you have compute hosts with different Ceph credentials it could be for a number of reasons like host aggregates, availability zones or regions, some of which rely on the assumption that this it is not allowed or possible so share credentials between these separations.\n", 
            "date_created": "2016-10-11 10:16:39.505164+00:00", 
            "author": "https://api.launchpad.net/1.0/~hopem"
        }, 
        {
            "content": "@Bert - ftr, iSCSI does not pull the secret from the remote. the _get_secret_uuid() only pulls the information from the local hypervisor and uses the local hypervisor's credentials.", 
            "date_created": "2016-10-17 19:23:54.418890+00:00", 
            "author": "https://api.launchpad.net/1.0/~billy-olsen"
        }, 
        {
            "content": "@billy: What's the point of calling create for the secret on the local host to be migrated from then if the secret doesn't exist:\n\nhttps://github.com/openstack/nova/blob/643aed652d0e51e36dbe7cb106285b51e3b5941b/nova/virt/libvirt/volume/net.py#L37\n\nit pulls the password from the netdisk properties... that means that if the remote host doesn't have the secret, then live migration with iSCSI would also fail, yet the secret will be created locally (where it should already exist because the VM is being moved FROM there).\n\nis self.connnection._host not a connection to the remote libvirt?", 
            "date_created": "2016-10-17 19:40:55.493983+00:00", 
            "author": "https://api.launchpad.net/1.0/~bregeer-ctl"
        }, 
        {
            "content": "Billy Olsen, I noticed you've assigned this bug to yourself. If you are currently working on a patch, can you please change the status of this bug to In Progress? If you are not working, please remove yourself as the assignee.", 
            "date_created": "2017-02-01 20:16:31.939119+00:00", 
            "author": "https://api.launchpad.net/1.0/~sujitha-neti"
        }, 
        {
            "content": "Is this a volume-backed instance that you're migrating, or just ephemeral (no cinder involved)?\n\nBecause in Ocata we changed where we get the rbd credentials from for a volume-backed instance:\n\nhttps://review.openstack.org/#/c/389399/\n\nI see you're on Mitaka, but would that patch help you?", 
            "date_created": "2017-03-13 23:50:46.424969+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Some of the instances do have attached volumes; some do not.  Thanks for the patch - will assess applicability with Billy Olsen.", 
            "date_created": "2017-04-05 17:17:26.465974+00:00", 
            "author": "https://api.launchpad.net/1.0/~dparrish"
        }, 
        {
            "content": "Automatically discovered version mitaka in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 16:04:38.769239+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2017-08-27 04:18:38.690434+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ]
}
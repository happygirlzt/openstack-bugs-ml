{
    "status": "Fix Released", 
    "last_updated": "2015-04-30 09:25:43.309577+00:00", 
    "description": "I have found that nova-conductor when run as multi process (default), shares the handle to zookeeper process that causes a lock probably inside zookeeper.c. Probably some internal zookeeper structures like sockets are shared and this is not allowed by zookeeper.\n\nCheckout the consequences.\n\nThere is similar complementary bug but there are other effect - multiple unnecessary registration and over-use of resources.\n\nhttps://bugs.launchpad.net/nova/+bug/1382153\n\nHow to reproduce:\n-----------------\n\ndevstack + ubuntu 14.04 + zookeeper 3.4.5\n\nnova.conf:\n\n[DEFAULT]\nservicegroup_driver = zk\n\n[conductor]\nworkers = 2\n\nthen run nova-conductor.\n\nWe can observer in logs (with debug=True):\n\nDEBUG evzookeeper.membership [req-xxx None None] Membership._join on /servicegroups/conductor/somehost\n\nbut there is no following expected:\n\nDEBUG evzookeeper.membership [req-xxx None None ] created zknode /servicegroups/conductor/somehost\n\nWe can check that zookeeper conductor node wasn't created:\n\n/usr/share/zookeeper/bin/zkCli.sh ls /servicegroups\n\nI investigated that the problem lies only in zookeeper c library implementation and is not caused by python zookeeper bindings evzookeeper.\n\nHere is a little snippet that show that program is blocked when zookeeper handle is used by child process (requires only zookeeper server and python).\n\nhttp://paste.openstack.org/show/129636/ (attached)\n\nWe can check the logs in zookeeper-server and observer that the request for creation from client isn't send to zookeeper-server at all.\n\nI was trying to go deeply inside internals of zookeeper.c but I couldn't find a clue why it isn't working.\n\nFrom the point of evzookeeper (zk.driver), the callback isn't called so green thread just waiting infinitely for response.\n\nConsequences\n------------\n\nNova-conductor works fine (because communication with zookeeper is in backgrounded green thread) but:\n\na) the namespace in zookeeper /servicegroups/conductor isn't created (if namespace wasn't created before)\nb) the ephemeral node for conductors in namespace isn't created (if namespace somehow exists)\n\nThe effects from the perspective of OpenStack cluster are:\n\n* effect of a) causes internal exceptions in nova-api service and therefore 'novaclient service-list' and horizon/\"System Information\"/\"Compute services\" doesn't work because of\n\u00a0\u00a0exceptions 'NoNodeException: no node' followed by 'ServiceGroupUnavailable: The service from servicegroup driver ZooKeeperDriver is temporarily unavailable.'\n\u00a0\u00a0So it isn't to possible to list any working services only because the namespace for conductors wasn't prepared (in reality all services working, zookeeper is working)\n\n\u00a0\u00a0Additionally it causes internal horizon 500 TemplateSyntaxError in horizon when trying to list all hypervisors at /admin/hypervisors/.\n\n* effect of b) causes that service-list or \"System Information\" gives a false negative: it shows service is down when in reality service is working\n\nAFAIK only nova-conductor is affected by this for now, because it is the only one of nova services that passes `workers` argument to openstack.common.service.launch(server, workers) and it is based on that are service.Service (not WSGIService based).\nIf workers>1 `launch` function starts the service by ProcessLanucher. ProcessLauncher is responsible for forking. The problem is that service object is already created with initialized zk driver object (in parent process).\nZk driver object is already initialized with connection (handle) that will be shared by child processes. Then in Service.start (in fork) there is a try to join servicegroup that doesn't work.\n\nI checked how sharing common resource (socket) affects other drivers. It's not a problem for memcache or db driver, because connection to memcache/db is created in lazy manner (connection/socket isn't created until required by child process).\n\nPossible solutions:\n1. simple but not clean: initialize zookeeper driver in lazy manner (like db/memcache), so each process will create own handle to zookeeper, ignoring the problem that each process tries to create the same node in zookeeper\n2. refactor base nova.service.Service that only parent process is responsible for joining the servicegroups - requires a lot of work and maybe even a blueprint\n3. based on first solution but with a difference that parent process registers the parent node (host) and each subproccess registers subnode (pid) for example: /servicesgroups/conductor/HOST/PID - then get_all shouldn't check if HOST node exist but if is empty\n\nThe problem with zookeeper and forking isn't new for openstack:\n\nhttp://qnalist.com/questions/27169/how-to-deal-with-fork-properly-when-using-the-zkc-mt-lib\n\nbut the right solution wasn't found.", 
    "tags": [
        "conductor", 
        "servicegroups", 
        "zookeeper"
    ], 
    "importance": "Undecided", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1389782", 
    "owner": "https://api.launchpad.net/1.0/~michal-dulko-f", 
    "id": 1389782, 
    "index": 5258, 
    "created": "2014-11-05 16:54:21.886771+00:00", 
    "title": "Servicegroups: Multi process nova-conductor is unable to join servicegroups when zk driver is used", 
    "comments": [
        {
            "content": "I have found that nova-conductor run as multi process (workers > 2), shares the handle to zookeeper process that causes a lock probably inside zookeeper.c. Probably some internal zookeeper structures like sockets are shared and this is not allowed by zookeeper.\n\nThere is similar complementary bug but there are other effect - multiple unnecessary registration and over-use of resources.\n\nhttps://bugs.launchpad.net/nova/+bug/1382153\n\nHow to reproduce:\n-----------------\n\ndevstack + ubuntu 14.04 + zookeeper 3.4.5\n\nnova.conf:\n\n[DEFAULT]\nservicegroup_driver = zk\n\n[conductor]\nworkers = 2\n\nthen run nova-conductor.\n\nWe can observer in logs (with debug=True):\n\nDEBUG evzookeeper.membership [req-xxx None None] Membership._join on /servicegroups/conductor/somehost\n\nbut there is no following expected:\n    \nDEBUG evzookeeper.membership [req-xxx None None ] created zknode /servicegroups/conductor/somehost\n\nWe can check that zookeeper conductor node wasn't created:\n\n/usr/share/zookeeper/bin/zkCli.sh ls /servicegroups\n\nI investigated that the problem lies only in zookeeper c library implementation and is not caused by python zookeeper bindings evzookeeper. \n\nHere is a little snippet that show that program is blocked when zookeeper handle is used by child process (requires only zookeeper server and python).\n\nhttp://paste.openstack.org/show/129636/\n\nWe can check the logs in zookeeper-server and observer that the request for creation from client isn't send to zookeeper-server at all.\n\nI was trying to go deeply inside internals of zookeeper.c but I couldn't find a clue why it isn't working.\n\nFrom the point of evzookeeper (zk.driver), the callback isn't called so green thread just waiting infinitely for response.\n\nConsequences\n------------\n\nNova-conductor works fine because (communication with zookeeper is in backgrounded green thread) but:\n\na) the namespace in zookeeper /servicegroups/conductor isn't created (if namespace wasn't created before)\nb) the ephemeral node for conductors in namespace isn't created (if namespace somehow exists)\n\nThe effect from the perspective OpenStack cluster is:\n\n* effect of a) causes internal exceptions in nova-api service and therefore 'novaclient service-list' and horizon/\"System Information\"/\"Compute services\" doesn't work because of  \n  exceptions 'NoNodeException: no node' followed by 'ServiceGroupUnavailable: The service from servicegroup driver ZooKeeperDriver is temporarily unavailable.'\n  So it isn't to possible to list any working services only because the namespace for conductors wasn't prepared (in reality all services working, zookeeper is working)\n\n  Additionally it causes internal horizon 500 TemplateSyntaxError in horizon when trying to list all hypervisors at /admin/hypervisors/.\n\n* effect of b) causes that service-list or \"System Information\" gives a false negative: it shows service is down when in reality service is working\n\nAFAIK only nova-conductor is affected by this for now, because it is the only one of nova services that passes `workers` argument to openstack.common.service.launch(server, workers) and it is based on that are service.Service (not WSGIService based).\nIf workers>1 launch starts the service by ProcessLanucher. ProcessLauncher is responsible for forking. The problem is that service object is already created with initialized zk driver object (in parent process).\nZk driver object is already initialized with connection (handle) that will be shared by child processes. Then in Service.start (in fork) there is a try to join servicegroup that doesn't work.\n\nI checked how sharing common resource (socket) affects other drivers. It's not a problem for memcache or db driver, because connection to memcache/db is created in lazy manner (connection/socket is created until required by child process).\n\nPossible solutions:\n1. simple but not clean: initialize zookeeper driver in lazy manner (like db/memcache), so each process will create own handle to zookeeper, ignoring the problem that each process tries to create the same node in zookeeper\n2. refactor base nova.service.Service that only parent process is responsible for joining the servicegroups - requires a lot of work a maybe even a blueprint\n3. based on first solution but with a difference that parent process register the parent node and each subproccess registers subnode (identified by pid) for example: /servicesgroups/conductor/HOST/PID - then get_all shouldn't check if HOST node exist but if is empty\n\nThe problem with zookeeper and forking isn't new for openstack: \n\nhttp://qnalist.com/questions/27169/how-to-deal-with-fork-properly-when-using-the-zkc-mt-lib\n\nbut the right solution wasn't found.", 
            "date_created": "2014-11-05 16:54:21.886771+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-palucki"
        }, 
        {
            "content": "", 
            "date_created": "2014-11-05 16:54:21.886771+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-palucki"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/133479", 
            "date_created": "2014-11-10 13:48:19.006966+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/133500", 
            "date_created": "2014-11-10 15:24:11.425464+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/133500\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=e61bf70146c47a99394a143c598ebd73409eca47\nSubmitter: Jenkins\nBranch:    master\n\ncommit e61bf70146c47a99394a143c598ebd73409eca47\nAuthor: Pawel Palucki <email address hidden>\nDate:   Fri Nov 7 14:41:49 2014 +0100\n\n    Fix conductor processes race trying to join servicegroup (zk driver)\n    \n    When conductor is run in multi process manner and zk (zookeeper) driver\n    is used as servicegroup driver, there is a problem because each process\n    tries to manage own Membership object to the same zookeeper path.\n    \n    This ends with raising exceptions:\n    \n    RuntimeError: Duplicated membership name /servicegroups/conductor/MEMBER_ID\n    \n    Zookeeper driver uses Membership (evzookeeper) class with path related\n    to service type and AFAIK it isn't correct that many process will be\n    responsible for the same ephemeral node. From my research it is not\n    supported but evzookeeper (Membership class) - so we can ignore the\n    exception or give each process his own node.\n    \n    If we ignore exception (silent it) and when first registered process dies\n    and the ephemeral node disappears, another process will create it. It will\n    work but hides the information about overall structure of services and\n    also causes that each process endlessly will be trying to create a node\n    (sending invalid create node requests to zookeeper). IMO is not a clean solution.\n    \n    So there is another solution that each process has its own node. This\n    fix does that.\n    \n    The best unique identifier for process is pid, so the chosen solution,\n    reorganizes the structure of zookeeper tree by adding one more level\n    with process ids.\n    \n    The zookeeper tree before looks like this:\n    \n    /servicesgroups/SERVICE/MEMBER\n    \n    and after path will look like this:\n    \n    /servicegroups/SERVICE/MEMBER/PID\n    eg.\n    /servicegroups/conductor/foo/12345\n    \n    This solution also assumes, that servicegroup driver will not check existence of\n    member node, but existence of subnodes (pids) - which corresponds to existence\n    of processes of given service.\n    \n    In general we will have more granular information about whole system -\n    for exmaple we can check number of processes of given service on each node.\n    \n    To answer the question: is service on given node works, we have to check\n    number of ephemeral \"pids\" nodes in get_all() method.\n    \n    Closes-bug: #1390511\n    \n    Related-bug: #1389782\n    Related-bug: #1382153\n    \n    Change-Id: I478845b6921dcfb9e9af5a45283a8569051b4f4f\n", 
            "date_created": "2015-01-29 21:49:30.449535+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/133479\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=afe86b6f29033a472cab1b52dd0724bb3c6dfb82\nSubmitter: Jenkins\nBranch:    master\n\ncommit afe86b6f29033a472cab1b52dd0724bb3c6dfb82\nAuthor: Michal Dulko <email address hidden>\nDate:   Wed Feb 4 12:44:12 2015 +0100\n\n    Fix conductor servicegroup joining when zk driver is used\n    \n    When conductor is run as multiprocess (default for multi core system) and\n    zk (zookeeper) is used as servicegroup_driver then conductor is unable to join\n    servicegroup because of shared zookeeper handle (and probably socket)\n    between parent and children processes.\n    \n    It's found the problem lies in zookeeper c library implementation.\n    Proof can be seen in related bug #1389782.\n    \n    This fix follows the idea used by memcache and db driver that\n    servicegroup_api._driver object is used in lazy manner.\n    This means that like connection to memcache and session to database,\n    zookeeper handle (zk session in driver) isn't created until required by\n    worker (child process).\n    \n    Additional note: before fix, during Service object creation the\n    prefix in zookeeper was created. That was the probably reason the session was\n    established so early. In my opinion the eagerness of this is not necessary\n    and namespace can be created by child process as well.\n    \n    Closes-Bug: #1389782\n    \n    Related-bug: #1390511\n    Related-bug: #1382153\n    \n    Change-Id: I9b386ef1f9268d19d04879ec89e5684170f3862a\n", 
            "date_created": "2015-02-06 18:37:49.177842+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
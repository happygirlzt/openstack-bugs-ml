{
    "status": "Invalid", 
    "last_updated": "2015-09-23 19:48:06.344907+00:00", 
    "description": "created cinder volume (fibre channel connectivity) then I attach it to a nova VM.\n\nHere is the multipath -l output  (all the path are in active state which is correct):\n-------------------------------------------------------------------------------------\nmpath380 (200173800fe0226d5) dm-0 IBM,2810XIV\nsize=964M features='1 queue_if_no_path' hwhandler='0' wp=rw\n`-+- policy='round-robin 0' prio=-1 status=active\n  |- 2:0:16:100 sde  8:64    active undef running\n  |- 2:0:13:100 sdb  8:16    active undef running\n  |- 2:0:15:100 sdd  8:48    active undef running\n  |- 3:0:4:100  sdi  8:128   active undef running\n  |- 3:0:1:100  sdf  8:80    active undef running\n  |- 3:0:2:100  sdg  8:96    active undef running\n  `- 3:0:3:100  sdh  8:112   active undef running\n\nBut when I detach the volume from the nova VM, the multipath -l output  (all the paths are in failed status) :\n-------------------------------------------------------------------------------------\nmpath380 (200173800fe0226d5) dm-0 IBM,2810XIV\nsize=964M features='0' hwhandler='0' wp=rw\n`-+- policy='round-robin 0' prio=-1 status=enabled\n  |- 2:0:16:100 sde  8:64    failed undef running\n  |- 2:0:15:100 sdd  8:48    failed undef running\n  |- 3:0:4:100  sdi  8:128   failed undef running\n  |- 3:0:1:100  sdf  8:80    failed undef running\n  |- 3:0:2:100  sdg  8:96    failed undef running\n  `- 3:0:3:100  sdh  8:112   failed undef running\n\nNote : Those failed paths in the multipath, caused a lot of bad paths messages in the /var/log/syslog.\n\n\nBasic information on the environment :\n---------------------------------------------------------------------\n- nova verions is Grizzly version 1:2013.1.1-0ubuntu2~cloud0, on ubuntu.\n- installed on the same host the cinder and the compute.\n- multipath-tools package version is 0.4.9-3ubuntu5\n\nHow do I recover thoses failed path :\n--------------------------------------------------------------\n1. per bad device path in multipath -l, I executed :\n     # echo 1 > /sys/block/${device_path}/device/delete\n\n2. per bad multipath device I executed :\n    dmsetup message ${multipath_device} 0 \"fail_if_no_path\"\n\n3. the refreshed the multipath by executed :\n    # multipath -F\n\nthen the multipath device and its device path (which was in failed state) gone!", 
    "tags": [
        "compute", 
        "multipath", 
        "volumes"
    ], 
    "importance": "High", 
    "heat": 76, 
    "link": "https://bugs.launchpad.net/nova/+bug/1208799", 
    "owner": "None", 
    "id": 1208799, 
    "index": 1174, 
    "created": "2013-08-06 10:47:03.605360+00:00", 
    "title": "after detach vol from VM the multipath device is in failed state", 
    "comments": [
        {
            "content": "created cinder volume (fibre channel connectivity) then I attach it to a nova VM.\n\nHere is the multipath -l output  (all the path are in active state which is correct):\n-------------------------------------------------------------------------------------\nmpath380 (200173800fe0226d5) dm-0 IBM,2810XIV\nsize=964M features='1 queue_if_no_path' hwhandler='0' wp=rw\n`-+- policy='round-robin 0' prio=-1 status=active\n  |- 2:0:16:100 sde  8:64    active undef running\n  |- 2:0:13:100 sdb  8:16    active undef running\n  |- 2:0:15:100 sdd  8:48    active undef running\n  |- 3:0:4:100  sdi  8:128   active undef running\n  |- 3:0:1:100  sdf  8:80    active undef running\n  |- 3:0:2:100  sdg  8:96    active undef running\n  `- 3:0:3:100  sdh  8:112   active undef running\n\nBut when I detach the volume from the nova VM, the multipath -l output  (all the paths are in failed status) :\n-------------------------------------------------------------------------------------\nmpath380 (200173800fe0226d5) dm-0 IBM,2810XIV\nsize=964M features='0' hwhandler='0' wp=rw\n`-+- policy='round-robin 0' prio=-1 status=enabled\n  |- 2:0:16:100 sde  8:64    failed undef running\n  |- 2:0:15:100 sdd  8:48    failed undef running\n  |- 3:0:4:100  sdi  8:128   failed undef running\n  |- 3:0:1:100  sdf  8:80    failed undef running\n  |- 3:0:2:100  sdg  8:96    failed undef running\n  `- 3:0:3:100  sdh  8:112   failed undef running\n\nNote : Those failed paths in the multipath, caused a lot of bad paths messages in the /var/log/syslog.\n\n\nBasic information on the environment :\n---------------------------------------------------------------------\n- nova verions is Grizzly version 1:2013.1.1-0ubuntu2~cloud0, on ubuntu.\n- installed on the same host the cinder and the compute.\n- multipath-tools package version is 0.4.9-3ubuntu5\n\nHow do I recover thoses failed path :\n--------------------------------------------------------------\n1. per bad device path in multipath -l, I executed :\n     # echo 1 > /sys/block/${device_path}/device/delete\n\n2. per bad multipath device I executed :\n    dmsetup message ${multipath_device} 0 \"fail_if_no_path\"\n\n3. the refreshed the multipath by executed :\n    # multipath -F\n\nthen the multipath device and its device path (which was in failed state) gone!", 
            "date_created": "2013-08-06 10:47:03.605360+00:00", 
            "author": "https://api.launchpad.net/1.0/~bshay"
        }, 
        {
            "content": "The same problem is reproduced on CentOS with havana release. Please, see the patch that solved the problem for me.", 
            "date_created": "2014-06-05 06:23:18.754488+00:00", 
            "author": "https://api.launchpad.net/1.0/~madkinder"
        }, 
        {
            "content": "I think the importance of this should be increased  -- after the 'dirty' detach all new volumes/VMs booted from volume fail to detect multipath device and result in a serious degradation of service.", 
            "date_created": "2014-06-05 06:28:50.594443+00:00", 
            "author": "https://api.launchpad.net/1.0/~ilja-t"
        }, 
        {
            "content": "Still reproducing this with Icehouse (RDO 2014.1.2-1.el6) and its a critical/blocker bug when using cinder with FC driver and multipath.", 
            "date_created": "2014-09-08 18:22:42.105347+00:00", 
            "author": "https://api.launchpad.net/1.0/~andres-active"
        }, 
        {
            "content": "Proper (online) cleanup procedure for failed path (multipath -F is intrusive!!!):\n\nMPATHDEV=\"/dev/dm-58\"\nmultipath -ll $MPATHDEV\nfor i in $( multipath -ll $MPATHDEV | awk '/ failed / { print $3 }' ); do echo \"Removing: $i\"; echo 1 > /sys/block/${i}/device/delete; done\nmultipath -ll $MPATHDEV\nmultipath -f $MPATHDEV\n\n\nMore details on Icehouse and volume detach/multipath cleanup problem:\n\n1) It does not happen always - when nova host is cleaned up from failed multipath devices it might work for a while again\n2) But once it happends it starts replicating itself (ie is reproducable) - until multipath cleanup is done manually (or host is rebooted)\n\nPerhaps its related to multipath/nova timing? ", 
            "date_created": "2014-11-13 06:57:29.997979+00:00", 
            "author": "https://api.launchpad.net/1.0/~andres-active"
        }, 
        {
            "content": "This issue is also linked to: https://bugs.launchpad.net/nova/+bug/1290681\n\nPatch presented previously by Ihor Kaharlichenko for Havana does not solve the problem in Icehouse anymore.", 
            "date_created": "2014-11-13 19:35:37.116118+00:00", 
            "author": "https://api.launchpad.net/1.0/~andres-active"
        }, 
        {
            "content": "Is this fixed by bug 1382440 in kilo?", 
            "date_created": "2015-04-24 15:48:36.306896+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "@Matt Riedemann \n\nthe patch you pointed seems only fix iscsi-based volume. I will check whether my fc-based volume works fine. ", 
            "date_created": "2015-04-28 14:24:42.559866+00:00", 
            "author": "https://api.launchpad.net/1.0/~luogangyi"
        }, 
        {
            "content": "I test it in my cluster, Icehouse, FC-SAN ,\n\nIt seems no problem.", 
            "date_created": "2015-04-29 07:41:00.579277+00:00", 
            "author": "https://api.launchpad.net/1.0/~luogangyi"
        }, 
        {
            "content": "I tested it in my test environment.\nIt seems this bug is already fixed in master for Liberty after migrating to os-brick library.\n", 
            "date_created": "2015-08-28 07:51:20.441150+00:00", 
            "author": "https://api.launchpad.net/1.0/~k-keiichi"
        }, 
        {
            "content": "This bug did exist back in Grizzly.  I don't think the FC based libvirt volume driver was doing an multipath -f <dev> after detach, which leads to seeing a dead multipath entry for multipath -l.    I believe this is already fixed.", 
            "date_created": "2015-09-23 16:12:30.929442+00:00", 
            "author": "https://api.launchpad.net/1.0/~walter-boring"
        }, 
        {
            "content": "For what it's worth, the current os-brick FC connector does do a multipath -f after removing the device from the host here:\n\nhttps://github.com/openstack/os-brick/blob/master/os_brick/initiator/connector.py#L1122", 
            "date_created": "2015-09-23 17:47:37.386391+00:00", 
            "author": "https://api.launchpad.net/1.0/~walter-boring"
        }, 
        {
            "content": "This bug originally started in our lab, and we haven't seen it happen for a long time. I think we can close it.", 
            "date_created": "2015-09-23 18:11:04.334974+00:00", 
            "author": "https://api.launchpad.net/1.0/~alonma"
        }, 
        {
            "content": "Per comment 11 it sounds like this should already be fixed in os-brick which nova uses in liberty.  So I'm marking the bug as invalid.  If this can be recreated in liberty then please re-open the bug with details.", 
            "date_created": "2015-09-23 19:48:05.613827+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2017-03-16 03:13:41.784856+00:00", 
    "description": "How to reproduce:\n1. Launch an instance.\n2. Create a volume with ceph backend.\n3. Attach the volume created in step 3.\n4. Kill nova-compute\n5. Delete the instance, this will go to local_delete\n6. Check volumes status using \"cinder list\", the volume is in \"available\" status\n7. Try to delete the volume, failed:\n2017-03-14 11:40:41.050 DEBUG oslo_messaging._drivers.amqpdriver mreceived message with unique_id: 061b4f9b52aa425d97811c066133b170 from (pid=480) __call__ /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:215 \n15:05 2017-03-14 11:40:41.056 DEBUG cinder.coordination req-774b4680-d861-4e16-bad4-a032ff0b3579  None  Lock \"7c7d03d9-3244-4923-b72e-459677ee48aa-delete_volume\" acquired by \"delete_volume\" :: waited 0.000s from (pid=480) _synchronized /opt/stack/cinder/cinder/coordination.py:300 \n15:05 2017-03-14 11:40:41.155 DEBUG cinder.volume.drivers.rbd req-774b4680-d861-4e16-bad4-a032ff0b3579  00;36madmin None connecting to ceph (timeout=-1). from (pid=480) _connect_to_rados /opt/stack/cinder/cinder/volume/drivers/rbd.py:299 \n15:05 2017-03-14 11:40:42.376 DEBUG cinder.volume.drivers.rbd req-774b4680-d861-4e16-bad4-a032ff0b3579   None volume has no backup snaps from (pid=480) _delete_backup_snaps /opt/stack/cinder/cinder/volume/drivers/rbd.py:660 \n15:05 2017-03-14 11:40:42.377 DEBUG cinder.volume.drivers.rbd req-774b4680-d861-4e16-bad4-a032ff0b3579 admin None Volume volume-7c7d03d9-3244-4923-b72e-459677ee48aa is not a clone.   from (pid=480) _get_clone_info /opt/stack/cinder/cinder/volume/drivers/rbd.py:683 \n15:06 2017-03-14 11:40:42.382 DEBUG cinder.volume.drivers.rbd req-774b4680-d861-4e16-bad4-a032ff0b3579   None deleting rbd volume volume-7c7d03d9-3244-4923-b72e-459677ee48aa   from (pid=480) delete_volume /opt/stack/cinder/cinder/volume/drivers/rbd.py:781 \n15:06 2017-03-14 11:40:42.570 DEBUG cinder.utils req-774b4680-d861-4e16-bad4-a032ff0b3579 admin None Failed attempt 1 from (pid=480) _print_stop /opt/stack/cinder/cinder/utils.py:780 \n...\n15:07 2017-03-14 11:41:12.950 WARNING cinder.volume.drivers.rbd req-774b4680-d861-4e16-bad4-a032ff0b3579 ^admin NoneImageBusy error raised while deleting rbd volume. This may have been caused by a connection from a client that has crashed and, if so, may be resolved by retrying the delete after 30 seconds has elapsed.\n15:07 2017-03-14 11:41:12.955 ERROR cinder.volume.manager req-774b4680-d861-4e16-bad4-a032ff0b3579 admin None^Unable to delete busy volume.", 
    "tags": [
        "cells"
    ], 
    "importance": "Undecided", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1672624", 
    "owner": "None", 
    "id": 1672624, 
    "index": 6727, 
    "created": "2017-03-14 07:15:31.451766+00:00", 
    "title": "Ceph volumes attached to local deleted instance could not be correctly handled", 
    "comments": [
        {
            "content": "How to reproduce:\n1. Launch an instance.\n2. Create a volume with ceph backend.\n3. Attach the volume created in step 3.\n4. Kill nova-compute\n5. Delete the instance, this will go to local_delete\n6. Check volumes status using \"cinder list\", the volume is in \"available\" status\n7. Try to delete the volume, failed:", 
            "date_created": "2017-03-14 07:15:31.451766+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhengzhenyu"
        }, 
        {
            "content": "Add the cells tag, as the work has added lots of new code paths for local delete cases. Seeing errors around inconsistent handling of quotas, notifications and other things that hit this code path.", 
            "date_created": "2017-03-14 14:05:21.312008+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "I couldn't reproduce this issue, I found that the volumes were correctly detached for the local delete.\r\n\r\n$ git rev-parse HEAD\r\nedf51119fa59ff8a3337abb9107a06fa33d3c68f\r\n\r\n$ nova boot --flavor m1.nano --block-device source=image,id=59f7eeb3-700a-456f-8d3f-9dfc4cce797b,dest=volume,size=1,shutdown=remove,bootindex=0 --poll hi\r\n\r\n+--------------------------------------+-------------------------------------------------+\r\n| Property                             | Value                                           |\r\n+--------------------------------------+-------------------------------------------------+\r\n| OS-DCF:diskConfig                    | MANUAL                                          |\r\n| OS-EXT-AZ:availability_zone          |                                                 |\r\n| OS-EXT-SRV-ATTR:host                 | -                                               |\r\n| OS-EXT-SRV-ATTR:hostname             | hi                                              |\r\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                               |\r\n| OS-EXT-SRV-ATTR:instance_name        |                                                 |\r\n| OS-EXT-SRV-ATTR:kernel_id            |                                                 |\r\n| OS-EXT-SRV-ATTR:launch_index         | 0                                               |\r\n| OS-EXT-SRV-ATTR:ramdisk_id           |                                                 |\r\n| OS-EXT-SRV-ATTR:reservation_id       | r-nk9ygaf0                                      |\r\n| OS-EXT-SRV-ATTR:root_device_name     | -                                               |\r\n| OS-EXT-SRV-ATTR:user_data            | -                                               |\r\n| OS-EXT-STS:power_state               | 0                                               |\r\n| OS-EXT-STS:task_state                | scheduling                                      |\r\n| OS-EXT-STS:vm_state                  | building                                        |\r\n| OS-SRV-USG:launched_at               | -                                               |\r\n| OS-SRV-USG:terminated_at             | -                                               |\r\n| accessIPv4                           |                                                 |\r\n| accessIPv6                           |                                                 |\r\n| adminPass                            | aiGbhNhGC7nB                                    |\r\n| config_drive                         |                                                 |\r\n| created                              | 2017-03-15T04:27:47Z                            |\r\n| description                          | -                                               |\r\n| flavor                               | m1.nano (42)                                    |\r\n| hostId                               |                                                 |\r\n| host_status                          |                                                 |\r\n| id                                   | 957b1fa8-0b9c-483d-a049-499cd4545188            |\r\n| image                                | Attempt to boot from volume - no image supplied |\r\n| key_name                             | -                                               |\r\n| locked                               | False                                           |\r\n| metadata                             | {}                                              |\r\n| name                                 | hi                                              |\r\n| os-extended-volumes:volumes_attached | []                                              |\r\n| progress                             | 0                                               |\r\n| security_groups                      | default                                         |\r\n| status                               | BUILD                                           |\r\n| tags                                 | []                                              |\r\n| tenant_id                            | 1dc696f01a67429a88a20273b5e52e10                |\r\n| updated                              | 2017-03-15T04:27:46Z                            |\r\n| user_id                              | 488ae3ccfbba4655b201fa7c8fbb2686                |\r\n+--------------------------------------+-------------------------------------------------+\r\n\r\nServer building... 100% complete\r\nFinished\r\n\r\n\r\n$ nova service-list\r\n\r\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\r\n| Id | Binary           | Host          | Zone     | Status  | State | Updated_at                 | Disabled Reason |\r\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\r\n| 7  | nova-conductor   | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:29:35.000000 | -               |\r\n| 10 | nova-scheduler   | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:29:33.000000 | -               |\r\n| 11 | nova-consoleauth | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:29:40.000000 | -               |\r\n| 12 | nova-compute     | ubuntu-xenial | nova     | enabled | down  | 2017-03-15T04:28:32.000000 | -               |\r\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\r\n\r\n$ nova list\r\n\r\n+--------------------------------------+------+--------+------------+-------------+---------------------------------+\r\n| ID                                   | Name | Status | Task State | Power State | Networks                        |\r\n+--------------------------------------+------+--------+------------+-------------+---------------------------------+\r\n| 957b1fa8-0b9c-483d-a049-499cd4545188 | hi   | ACTIVE | -          | Running     | public=2001:db8::5, 172.24.4.10 |\r\n+--------------------------------------+------+--------+------------+-------------+---------------------------------+\r\n\r\n$ nova delete hi\r\nRequest to delete server hi has been accepted.\r\n\r\n$ nova list\r\n+----+------+--------+------------+-------------+----------+\r\n| ID | Name | Status | Task State | Power State | Networks |\r\n+----+------+--------+------------+-------------+----------+\r\n+----+------+--------+------------+-------------+----------+\r\n\r\n$ cinder list\r\n+----+--------+------+------+-------------+----------+-------------+\r\n| ID | Status | Name | Size | Volume Type | Bootable | Attached to |\r\n+----+--------+------+------+-------------+----------+-------------+\r\n+----+--------+------+------+-------------+----------+-------------+", 
            "date_created": "2017-03-15 04:35:37.065310+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Just tried another case with shutdown=preserve and found I couldn't delete the volume from cinder, as you described. So far, it looks like a problem in cinder? The volume showed as \"available\" and not attached to anything in cinder, then the \"cinder delete\" failed to complete.\n\n$ nova boot --flavor m1.nano --block-device source=image,id=59f7eeb3-700a-456f-8d3f-9dfc4cce797b,dest=volume,size=1,shutdown=preserve,bootindex=0 --poll hi\n\n+--------------------------------------+-------------------------------------------------+\n| Property                             | Value                                           |\n+--------------------------------------+-------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                          |\n| OS-EXT-AZ:availability_zone          |                                                 |\n| OS-EXT-SRV-ATTR:host                 | -                                               |\n| OS-EXT-SRV-ATTR:hostname             | hi                                              |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                               |\n| OS-EXT-SRV-ATTR:instance_name        |                                                 |\n| OS-EXT-SRV-ATTR:kernel_id            |                                                 |\n| OS-EXT-SRV-ATTR:launch_index         | 0                                               |\n| OS-EXT-SRV-ATTR:ramdisk_id           |                                                 |\n| OS-EXT-SRV-ATTR:reservation_id       | r-1j6d6cne                                      |\n| OS-EXT-SRV-ATTR:root_device_name     | -                                               |\n| OS-EXT-SRV-ATTR:user_data            | -                                               |\n| OS-EXT-STS:power_state               | 0                                               |\n| OS-EXT-STS:task_state                | scheduling                                      |\n| OS-EXT-STS:vm_state                  | building                                        |\n| OS-SRV-USG:launched_at               | -                                               |\n| OS-SRV-USG:terminated_at             | -                                               |\n| accessIPv4                           |                                                 |\n| accessIPv6                           |                                                 |\n| adminPass                            | M6onRYNPbEJ4                                    |\n| config_drive                         |                                                 |\n| created                              | 2017-03-15T04:37:29Z                            |\n| description                          | -                                               |\n| flavor                               | m1.nano (42)                                    |\n| hostId                               |                                                 |\n| host_status                          |                                                 |\n| id                                   | 48c9f377-52d4-46b9-bd6b-d9de2d0dc540            |\n| image                                | Attempt to boot from volume - no image supplied |\n| key_name                             | -                                               |\n| locked                               | False                                           |\n| metadata                             | {}                                              |\n| name                                 | hi                                              |\n| os-extended-volumes:volumes_attached | []                                              |\n| progress                             | 0                                               |\n| security_groups                      | default                                         |\n| status                               | BUILD                                           |\n| tags                                 | []                                              |\n| tenant_id                            | 1dc696f01a67429a88a20273b5e52e10                |\n| updated                              | 2017-03-15T04:37:29Z                            |\n| user_id                              | 488ae3ccfbba4655b201fa7c8fbb2686                |\n+--------------------------------------+-------------------------------------------------+\n\nServer building... 100% complete\nFinished\n\n$ nova list\n\n+--------------------------------------+------+--------+------------+-------------+--------------------------------+\n| ID                                   | Name | Status | Task State | Power State | Networks                       |\n+--------------------------------------+------+--------+------------+-------------+--------------------------------+\n| 48c9f377-52d4-46b9-bd6b-d9de2d0dc540 | hi   | ACTIVE | -          | Running     | public=2001:db8::d, 172.24.4.2 |\n+--------------------------------------+------+--------+------------+-------------+--------------------------------+\n\n$ cinder list\n+--------------------------------------+--------+------+------+-------------+----------+--------------------------------------+\n| ID                                   | Status | Name | Size | Volume Type | Bootable | Attached to                          |\n+--------------------------------------+--------+------+------+-------------+----------+--------------------------------------+\n| 08e57275-e4d6-4e1e-a781-c5f3d27d06b3 | in-use |      | 1    | ceph        | true     | 48c9f377-52d4-46b9-bd6b-d9de2d0dc540 |\n+--------------------------------------+--------+------+------+-------------+----------+--------------------------------------+\n\n$ nova service-list\n\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\n| Id | Binary           | Host          | Zone     | Status  | State | Updated_at                 | Disabled Reason |\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\n| 7  | nova-conductor   | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:40:55.000000 | -               |\n| 10 | nova-scheduler   | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:40:54.000000 | -               |\n| 11 | nova-consoleauth | ubuntu-xenial | internal | enabled | up    | 2017-03-15T04:40:50.000000 | -               |\n| 12 | nova-compute     | ubuntu-xenial | nova     | enabled | down  | 2017-03-15T04:38:08.000000 | -               |\n+----+------------------+---------------+----------+---------+-------+----------------------------+-----------------+\n\n$ nova delete hi\n\nRequest to delete server hi has been accepted.\n\n$ nova list\n\n+----+------+--------+------------+-------------+----------+\n| ID | Name | Status | Task State | Power State | Networks |\n+----+------+--------+------------+-------------+----------+\n+----+------+--------+------------+-------------+----------+\n\n$ cinder list\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n| ID                                   | Status    | Name | Size | Volume Type | Bootable | Attached to |\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n| 08e57275-e4d6-4e1e-a781-c5f3d27d06b3 | available |      | 1    | ceph        | true     |             |\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n\n$ cinder delete 08e57275-e4d6-4e1e-a781-c5f3d27d06b3\nRequest to delete volume 08e57275-e4d6-4e1e-a781-c5f3d27d06b3 has been accepted.\n\n$ cinder list\n+--------------------------------------+----------+------+------+-------------+----------+-------------+\n| ID                                   | Status   | Name | Size | Volume Type | Bootable | Attached to |\n+--------------------------------------+----------+------+------+-------------+----------+-------------+\n| 08e57275-e4d6-4e1e-a781-c5f3d27d06b3 | deleting |      | 1    | ceph        | true     |             |\n+--------------------------------------+----------+------+------+-------------+----------+-------------+\n\nwith a trace in c-vol.log:\n\n2017-03-15 04:41:31.481 DEBUG cinder.volume.drivers.rbd [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] deleting rbd volume volume-08e57275-e4d6-4e1e-a781-c5f3d27d06b3 from (pid=3285) delete_volume /opt/stack/cinder/cinder/volume/drivers/rbd.py:781\n2017-03-15 04:41:31.514 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Failed attempt 1 from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:780\n2017-03-15 04:41:31.515 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Have been at this for 0.032 seconds from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:782\n2017-03-15 04:41:31.516 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Sleeping for 10.0 seconds from (pid=3285) _backoff_sleep /opt/stack/cinder/cinder/utils.py:774\n2017-03-15 04:41:41.553 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Failed attempt 2 from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:780\n2017-03-15 04:41:41.554 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Have been at this for 10.072 seconds from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:782\n2017-03-15 04:41:41.555 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Sleeping for 20.0 seconds from (pid=3285) _backoff_sleep /opt/stack/cinder/cinder/utils.py:774\n2017-03-15 04:42:01.593 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Failed attempt 3 from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:780\n2017-03-15 04:42:01.594 DEBUG cinder.utils [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Have been at this for 30.112 seconds from (pid=3285) _print_stop /opt/stack/cinder/cinder/utils.py:782\n2017-03-15 04:42:01.595 WARNING cinder.volume.drivers.rbd [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] ImageBusy error raised while deleting rbd volume. This may have been caused by a connection from a client that has crashed and, if so, may be resolved by retrying the delete after 30 seconds has elapsed.\n2017-03-15 04:42:01.600 ERROR cinder.volume.manager [req-17ce097b-c92d-43a2-99c2-f9ac05e21ad6 admin None] Unable to delete busy volume.\n", 
            "date_created": "2017-03-15 04:46:58.090385+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "@melanie witt, Hi, thanks for the test, but I think you might not doing it correctly,\nLVM volume can be correctly handled, but CEPH volume can't, and by looking at the code\nit is likely that only LVM driver can be correctly handled. I will ask my colleague update\nour test log to the report.", 
            "date_created": "2017-03-15 04:49:22.844111+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhengzhenyu"
        }, 
        {
            "content": "@melanie witt, Ah, looks like you are doing with CEPH backend, and yes, what we tried is\ncreate a volume first and then attach it to an instance, it should be equal with the\nshutdown=preserve scenario?\n\nAnd it is an Nova-Cinder issue, because nova didn't disconnect the volume due to compute down, and\ncurrently cinder can only handle LVM volume in this kind of scenario. So it is probably a common\nissue for all other volume backends except LVM.", 
            "date_created": "2017-03-15 05:14:57.764956+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhengzhenyu"
        }, 
        {
            "content": "LVM works well because  LVM removes export and remove the iscsi target by itself: \n\nhttps://github.com/openstack/cinder/blob/master/cinder/volume/targets/iscsi.py#L220\n\nBut ceph does nothing.\n\nhttps://github.com/openstack/cinder/blob/master/cinder/volume/drivers/rbd.py#L1019\nhttps://github.com/openstack/cinder/blob/master/cinder/volume/drivers/rbd.py#L1043\n\nSo when nove-compute is down and nova deletes the vm with local delete logic, the vm in hypervisor is still alive and the volume is still attached to the vm. Only when the vm is destroyed, the volume can be deleted successfully.\n\nTake a look at the cinder third part driver, we can see that most of drivers leave these two function empty (except dell_emc, dothill, hpe and so on). So I guess most of the drivers has this problem as well.\n\nreproduce:\nhttp://paste.openstack.org/show/602801/\n", 
            "date_created": "2017-03-15 07:24:20.968818+00:00", 
            "author": "https://api.launchpad.net/1.0/~wangxiyuan"
        }, 
        {
            "content": "To be clear, the issue here is that the domain is still active on the compute node and accessing the rbd volume so any attempt to delete the rbd volume itself fails.\n\nI can't see how that's anything other than a failure by the operator to ensure that the original compute node has been fenced before attempting a local delete of an active instance.", 
            "date_created": "2017-03-15 12:43:57.634492+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Well, the operator hasn't done a local delete, they've just done a delete. Nova did a local delete because nova compute wasn't running. We also wouldn't want to fence in this case because, in general, we don't want to kill running instances unless we really have to. Nova compute being down isn't 'really have to' under most circumstances because it can just be restarted: an HA response should focus on trying to get it back up again. I don't think we can reasonably blame the operator here.\n\nAs you say, though, this does mean that an attempt to delete the volume fails because ceph won't allow us to delete a volume that still has an active connection.\n\nPresumably when nova compute eventually comes back up we will clean up the running instance which was deleted. At this point, presumably the ceph volume can also be deleted. IOW, if we wait a bit for normal maintenance to happen, this will resolve itself automatically. I think that's a cloudy 'working as expected', especially for a weird edge case like this.", 
            "date_created": "2017-03-15 15:47:40.058714+00:00", 
            "author": "https://api.launchpad.net/1.0/~mbooth-9"
        }, 
        {
            "content": "Confirmed that the default cleanup periodic task in compute will destroy local deleted domains when nova-compute is brought back up. The default action is \"reap\" which will delete orphaned domains and the periodic task runs every 30 minutes by default. It doesn't run immediately when nova-compute starts, so it takes 30 minutes after nova-compute comes back up for the local deleted instance to be reaped.\n\nAfterward, the volume can be deleted from cinder.\n\nThe config options for cleaning local deleted instances are:\n\n    cfg.StrOpt(\"running_deleted_instance_action\",\n        default=\"reap\",\n        choices=('noop', 'log', 'shutdown', 'reap'),\n        help=\"\"\"\nThe compute service periodically checks for instances that have been\ndeleted in the database but remain running on the compute node. The\nabove option enables action to be taken when such instances are\nidentified.\n\nPossible values:\n\n* reap: Powers down the instances and deletes them(default)\n* log: Logs warning message about deletion of the resource\n* shutdown: Powers down instances and marks them as non-\n  bootable which can be later used for debugging/analysis\n* noop: Takes no action\n\n    cfg.IntOpt(\"running_deleted_instance_poll_interval\",\n        default=1800,\n        help=\"\"\"\nTime interval in seconds to wait between runs for the clean up action.\nIf set to 0, above check will be disabled. If \"running_deleted_instance\n_action\" is set to \"log\" or \"reap\", a value greater than 0 must be set.\n\nPossible values:\n\n* Any positive integer in seconds enables the option.\n* 0: Disables the option.\n* 1800: Default value.\n\n    cfg.IntOpt(\"running_deleted_instance_timeout\",\n        default=0,\n        help=\"\"\"\nTime interval in seconds to wait for the instances that have\nbeen marked as deleted in database to be eligible for cleanup.\n\nPossible values:\n\n* Any positive integer in seconds(default is 0).\n\n\nn-cpu.log:\n\n2017-03-15 16:54:44.939 DEBUG oslo_service.periodic_task [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] Running periodic task ComputeManager._cleanup_running_deleted_instances from (pid=14524) run_periodic_tasks /usr/local/lib/python2.7/dist-packages/oslo_service/periodic_task.py:215\n2017-03-15 16:54:44.947 DEBUG oslo_messaging._drivers.amqpdriver [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] CALL msg_id: b7adb6d039274979a8c970f83bbef78f exchange 'nova' topic 'conductor' from (pid=14524) _send /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:442\n2017-03-15 16:54:44.991 DEBUG oslo_messaging._drivers.amqpdriver [-] received reply msg_id: b7adb6d039274979a8c970f83bbef78f from (pid=14524) __call__ /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:299\n2017-03-15 16:54:44.995 INFO nova.compute.manager [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] [instance: 67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89] Destroying instance with name label 'instance-00000002' which is marked as DELETED but still present on host.\n2017-03-15 16:54:44.996 DEBUG oslo_messaging._drivers.amqpdriver [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] CALL msg_id: 8094e4320a41471b9e8d928c1fd23ef8 exchange 'nova' topic 'conductor' from (pid=14524) _send /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:442\n2017-03-15 16:54:45.013 DEBUG oslo_messaging._drivers.amqpdriver [-] received reply msg_id: 8094e4320a41471b9e8d928c1fd23ef8 from (pid=14524) __call__ /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:299\n2017-03-15 16:54:45.015 DEBUG oslo_concurrency.lockutils [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] Lock \"67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89-events\" acquired by \"nova.compute.manager._clear_events\" :: waited 0.000s from (pid=14524) inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:270\n2017-03-15 16:54:45.016 DEBUG oslo_concurrency.lockutils [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] Lock \"67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89-events\" released by \"nova.compute.manager._clear_events\" :: held 0.001s from (pid=14524) inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:282\n2017-03-15 16:54:45.017 INFO nova.compute.manager [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] [instance: 67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89] Terminating instance\n2017-03-15 16:54:45.017 DEBUG nova.objects.instance [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] Lazy-loading 'info_cache' on Instance uuid 67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89 from (pid=14524) obj_load_attr /opt/stack/nova/nova/objects/instance.py:1034\n2017-03-15 16:54:45.019 DEBUG oslo_messaging._drivers.amqpdriver [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] CALL msg_id: 19a63225ce49479c8583fef3b219c5c8 exchange 'nova' topic 'conductor' from (pid=14524) _send /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:442\n2017-03-15 16:54:45.073 DEBUG oslo_messaging._drivers.amqpdriver [-] received reply msg_id: 19a63225ce49479c8583fef3b219c5c8 from (pid=14524) __call__ /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:299\n2017-03-15 16:54:45.077 DEBUG nova.compute.manager [req-24c2664b-fe22-44a8-8e72-0b809adf50f1 None None] [instance: 67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89] Start destroying the instance on the hypervisor. from (pid=14524) _shutdown_instance /opt/stack/nova/nova/compute/manager.py:2242\n2017-03-15 16:54:45.546 INFO nova.virt.libvirt.driver [-] [instance: 67244c6e-7e04-4bf1-8dc2-06d6dcfb9a89] Instance destroyed successfully.\n\n$ cinder list\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n| ID                                   | Status    | Name | Size | Volume Type | Bootable | Attached to |\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n| f19c27c5-596f-4d30-9a3f-bd7df7b43e12 | available |      | 1    | ceph        | true     |             |\n+--------------------------------------+-----------+------+------+-------------+----------+-------------+\n\n$ cinder delete f19c27c5-596f-4d30-9a3f-bd7df7b43e12\nRequest to delete volume f19c27c5-596f-4d30-9a3f-bd7df7b43e12 has been accepted.\n\n$ cinder list\n+----+--------+------+------+-------------+----------+-------------+\n| ID | Status | Name | Size | Volume Type | Bootable | Attached to |\n+----+--------+------+------+-------------+----------+-------------+\n+----+--------+------+------+-------------+----------+-------------+\n", 
            "date_created": "2017-03-15 17:30:31.569790+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "This behavior is expected in the scenario where nova-compute is brought down but the instance domain is still running and keeping the volume busy. Usually in practice, nova-compute should only be down in the case of a failed compute host or one that has been properly fenced by the operator. However, if nova-compute goes down temporarily and an instance is local deleted, the instance domain will be reaped by the nova-compute periodic task after nova-compute comes back up. After the domain is reaped, the volume can be successfully deleted from cinder.", 
            "date_created": "2017-03-15 17:36:43.834880+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "This may also affect \"evacuate\" operation.", 
            "date_created": "2017-03-16 03:13:40.771964+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhengzhenyu"
        }
    ]
}
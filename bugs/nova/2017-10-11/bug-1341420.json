{
    "status": "Invalid", 
    "last_updated": "2017-09-22 17:28:45.088136+00:00", 
    "description": "There is a race between the scheduler in select_destinations, which selects a set of hosts, and the nova compute manager, which claims resources on those hosts when building the instance. The race is particularly noticable with Ironic, where every request will consume a full host, but can turn up on libvirt etc too. Multiple schedulers will likely exacerbate this too unless they are in a version of python with randomised dictionary ordering, in which case they will make it better :).\n\nI've put https://review.openstack.org/106677 up to remove a comment which comes from before we introduced this race.\n\nOne mitigating aspect to the race in the filter scheduler _schedule method attempts to randomly select hosts to avoid returning the same host in repeated requests, but the default minimum set it selects from is size 1 - so when heat requests a single instance, the same candidate is chosen every time. Setting that number higher can avoid all concurrent requests hitting the same host, but it will still be a race, and still likely to fail fairly hard at near-capacity situations (e.g. deploying all machines in a cluster with Ironic and Heat).\n\nFolk wanting to reproduce this: take a decent size cloud - e.g. 5 or 10 hypervisor hosts (KVM is fine). Deploy up to 1 VM left of capacity on each hypervisor. Then deploy a bunch of VMs one at a time but very close together - e.g. use the python API to get cached keystone credentials, and boot 5 in a loop.\n\nIf using Ironic you will want https://review.openstack.org/106676 to let you see which host is being returned from the selection.\n\nPossible fixes:\n\u00a0- have the scheduler be a bit smarter about returning hosts - e.g. track destination selection counts since the last refresh and weight hosts by that count as well\n\u00a0- reinstate actioning claims into the scheduler, allowing the audit to correct any claimed-but-not-started resource counts asynchronously\n\u00a0- special case the retry behaviour if there are lots of resources available elsewhere in the cluster.\n\nStats wise, I just testing a 29 instance deployment with ironic and a heat stack, with 45 machines to deploy onto (so 45 hosts in the scheduler set) and 4 failed with this race - which means they recheduled and failed 3 times each - or 12 cases of scheduler racing *at minimum*.\n\nbackground chat\n\n15:43 < lifeless> mikal: around? I need to sanity check something\n15:44 < lifeless> ulp, nope, am sure of it. filing a bug.\n15:45 < mikal> lifeless: ok\n15:46 < lifeless> mikal: oh, you're here, I will run it past you :)\n15:46 < lifeless> mikal: if you have ~5m\n15:46 < mikal> Sure\n15:46 < lifeless> so, symptoms\n15:46 < lifeless> nova boot <...> --num-instances 45 -> works fairly reliably. Some minor timeout related things to fix but nothing dramatic.\n15:47 < lifeless> heat create-stack <...> with a stack with 45 instances in it -> about 50% of instances fail to come up\n15:47 < lifeless> this is with Ironic\n15:47 < mikal> Sure\n15:47 < lifeless> the failure on all the instances is the retry-three-times failure-of-death\n15:47 < lifeless> what I believe is happening is this\n15:48 < lifeless> the scheduler is allocating the same weighed list of hosts for requests that happen close enough together\n15:49 < lifeless> and I believe its able to do that because the target hosts (from select_destinations) need to actually hit the compute node manager and have\n15:49 < lifeless>             with rt.instance_claim(context, instance, limits):\n15:49 < lifeless> happen in _build_and_run_instance\n15:49 < lifeless> before the resource usage is assigned\n15:49 < mikal> Is heat making 45 separate requests to the nova API?\n15:49 < lifeless> eys\n15:49 < lifeless> yes\n15:49 < lifeless> thats the key difference\n15:50 < lifeless> same flavour, same image\n15:50 < openstackgerrit> Sam Morrison proposed a change to openstack/nova: Remove cell api overrides for lock and unlock  https://review.openstack.org/89487\n15:50 < mikal> And you have enough quota for these instances, right?\n15:50 < lifeless> yes\n15:51 < mikal> I'd have to dig deeper to have an answer, but it sure does seem worth filing a bug for\n15:51 < lifeless> my theory is that there is enough time between select_destinations in the conductor, and _build_and_run_instance in compute for another request to come in the front door and be scheduled to the same host\n15:51 < mikal> That seems possible to me\n15:52 < lifeless> I have no idea right now about how to fix it (other than to have the resources provisionally allocated by the scheduler before it sends a reply), but I am guessing that might be contentious\n15:52 < mikal> I can't instantly think of a fix though -- we've avoided queue like behaviour for scheduling\n15:52 < mikal> How big is the clsuter compared with 45 instances?\n15:52 < mikal> Is it approximately the same size as that?\n15:52 < lifeless> (by provisionally allocated, I mean 'claim them and let the audit in 60 seconds fix it up if they are not actually used')\n15:53 < lifeless> sorry, not sure what yoy mean by that last question\n15:53 < mikal> So, if you have 45 ironic instances to schedule, and 45 identical machines to do it, then the probability of picking the same machine more than once to schedule on is very high\n15:53 < mikal> Wehereas if you had 500 machines, it would be low\n15:53 < lifeless> oh yes, all the hardware is homogeneous\n15:54 < lifeless> we believe this is common in clouds :)\n15:54 < mikal> And the cluster is sized at approximately 45 machines?\n15:54 < lifeless> the cluster is 46 machines but one is down for maintenance\n15:54 < lifeless> so 45 machines available to schedule onto.\n15:54 < mikal> Its the size of the cluster compared to the size of the set of instances which I'm most interested in\n15:54 < lifeless> However - and this is the interesting thing\n15:54 < lifeless> I tried a heat stack of 20 machines.\n15:54 < lifeless> same symptoms\n15:54 < mikal> Yeah, that's like the worst possible case for this algorithm\n15:54 < lifeless> about 30% failed due to scheduler retries.\n15:54 < mikal> Hmmm\n15:54 < mikal> That is unexpected to me\n15:55 < lifeless> that is when I dived into the code.\n15:55 < lifeless> the patch I pushed above will make it possible to see if my theory is correct\n15:55 < mikal> you were going to file a bug, right?\n15:56 < lifeless> I have the form open to file one with tasks on ironic and nova\n15:56 < mikal> I vote you do that thing\n15:56 < lifeless> seconded\n15:56 < lifeless> I might copy this transcript in as well\n15:57 < mikal> Works for me", 
    "tags": [
        "in-stable-liberty", 
        "scheduler"
    ], 
    "importance": "Wishlist", 
    "heat": 48, 
    "link": "https://bugs.launchpad.net/nova/+bug/1341420", 
    "owner": "https://api.launchpad.net/1.0/~cyx1231st", 
    "id": 1341420, 
    "index": 1573, 
    "created": "2014-07-14 04:15:03.138011+00:00", 
    "title": "gap between scheduler selection and claim causes spurious failures when the instance is the last one to fit", 
    "comments": [
        {
            "content": "There is a race between the scheduler in select_destinations, which selects a set of hosts, and the nova compute manager, which claims resources on those hosts when building the instance. The race is particularly noticable with Ironic, where ever request will consume a full host, but can turn up on libvirt etc too. Multiple schedulers will likely exacerbate this too unless they are in a version of python with randomised dictionary ordering, in which case they will make it better :).\n\n\nI've put https://review.openstack.org/106677 up to remove a comment which comes from before we introduced this race.\n\nOne mitigating aspect to the race in the filter scheduler _schedule method attempts to randomly select hosts to avoid returning the same host in repeated requests, but the default minimum set it selects from is size 1 - so when heat requests a single instance, the same candidate is chosen every time. Setting that number higher can avoid all concurrent requests hitting the same host, but it will still be a race, and still likely to fail fairly hard at near-capacity situations (e.g. deploying all machines in a cluster with Ironic and Heat).\n\nFolk wanting to reproduce this: take a decent size cloud - e.g. 5 or 10 hypervisor hosts (KVM is fine). Deploy up to 1 VM left of capacity on each hypervisor. Then deploy a bunch of VMs one at a time but very close together - e.g. use the python API to get cached keystone credentials, and boot 5 in a loop.\n\nIf using Ironic you will want https://review.openstack.org/106676 to let you see which host is being returned from the selection.\n\nPossible fixes:\n - have the scheduler be a bit smarter about returning hosts - e.g. track destination selection counts since the last refresh and weight hosts by that count as well\n - reinstate actioning claims into the scheduler, allowing the audit to correct any claimed-but-not-started resource counts asynchronously\n - special case the retry behaviour if there are lots of resources available elsewhere in the cluster.\n\nStats wise, I just testing a 29 instance deployment with ironic and a heat stack, with 45 machines to deploy onto (so 45 hosts in the scheduler set) and 4 failed with this race - which means they recheduled and failed 3 times each - or 12 cases of scheduler racing *at minimum*.\n\nbackground chat\n\n15:43 < lifeless> mikal: around? I need to sanity check something\n15:44 < lifeless> ulp, nope, am sure of it. filing a bug.\n15:45 < mikal> lifeless: ok\n15:46 < lifeless> mikal: oh, you're here, I will run it past you :)\n15:46 < lifeless> mikal: if you have ~5m\n15:46 < mikal> Sure\n15:46 < lifeless> so, symptoms\n15:46 < lifeless> nova boot <...> --num-instances 45 -> works fairly reliably. Some minor timeout related things to fix but nothing dramatic.\n15:47 < lifeless> heat create-stack <...> with a stack with 45 instances in it -> about 50% of instances fail to come up\n15:47 < lifeless> this is with Ironic\n15:47 < mikal> Sure\n15:47 < lifeless> the failure on all the instances is the retry-three-times failure-of-death\n15:47 < lifeless> what I believe is happening is this\n15:48 < lifeless> the scheduler is allocating the same weighed list of hosts for requests that happen close enough together\n15:49 < lifeless> and I believe its able to do that because the target hosts (from select_destinations) need to actually hit the compute node manager and have \n15:49 < lifeless>             with rt.instance_claim(context, instance, limits):    \n15:49 < lifeless> happen in _build_and_run_instance\n15:49 < lifeless> before the resource usage is assigned\n15:49 < mikal> Is heat making 45 separate requests to the nova API?\n15:49 < lifeless> eys\n15:49 < lifeless> yes\n15:49 < lifeless> thats the key difference\n15:50 < lifeless> same flavour, same image\n15:50 < openstackgerrit> Sam Morrison proposed a change to openstack/nova: Remove cell api overrides for lock and unlock  https://review.openstack.org/89487\n15:50 < mikal> And you have enough quota for these instances, right?\n15:50 < lifeless> yes\n15:51 < mikal> I'd have to dig deeper to have an answer, but it sure does seem worth filing a bug for\n15:51 < lifeless> my theory is that there is enough time between select_destinations in the conductor, and _build_and_run_instance in compute for another request to come in the front door and be scheduled to the same host\n15:51 < mikal> That seems possible to me\n15:52 < lifeless> I have no idea right now about how to fix it (other than to have the resources provisionally allocated by the scheduler before it sends a reply), but I am guessing that might be contentious\n15:52 < mikal> I can't instantly think of a fix though -- we've avoided queue like behaviour for scheduling\n15:52 < mikal> How big is the clsuter compared with 45 instances?\n15:52 < mikal> Is it approximately the same size as that?\n15:52 < lifeless> (by provisionally allocated, I mean 'claim them and let the audit in 60 seconds fix it up if they are not actually used')\n15:53 < lifeless> sorry, not sure what yoy mean by that last question\n15:53 < mikal> So, if you have 45 ironic instances to schedule, and 45 identical machines to do it, then the probability of picking the same machine more than once to schedule on is very high\n15:53 < mikal> Wehereas if you had 500 machines, it would be low\n15:53 < lifeless> oh yes, all the hardware is homogeneous\n15:54 < lifeless> we believe this is common in clouds :)\n15:54 < mikal> And the cluster is sized at approximately 45 machines?\n15:54 < lifeless> the cluster is 46 machines but one is down for maintenance\n15:54 < lifeless> so 45 machines available to schedule onto.\n15:54 < mikal> Its the size of the cluster compared to the size of the set of instances which I'm most interested in\n15:54 < lifeless> However - and this is the interesting thing\n15:54 < lifeless> I tried a heat stack of 20 machines.\n15:54 < lifeless> same symptoms\n15:54 < mikal> Yeah, that's like the worst possible case for this algorithm\n15:54 < lifeless> about 30% failed due to scheduler retries.\n15:54 < mikal> Hmmm\n15:54 < mikal> That is unexpected to me\n15:55 < lifeless> that is when I dived into the code.\n15:55 < lifeless> the patch I pushed above will make it possible to see if my theory is correct\n15:55 < mikal> you were going to file a bug, right?\n15:56 < lifeless> I have the form open to file one with tasks on ironic and nova\n15:56 < mikal> I vote you do that thing\n15:56 < lifeless> seconded\n15:56 < lifeless> I might copy this transcript in as well\n15:57 < mikal> Works for me", 
            "date_created": "2014-07-14 04:15:03.138011+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Ok, I've now confirmed that the weighed list order is the same each time, which means the hypothesis isn't disproved.", 
            "date_created": "2014-07-14 08:21:47.733128+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Here is a snippet showing the scheduler thinking that cc0a5455-0978-4687-988e-8f3161cbc521 has no instances on it [host: (undercloud, cc0a5455-0978-4687-988e-8f3161cbc521) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], returning it in a list, and then an instance rescheduling because it actually has no resources:\n\nscheduler.log:\n2014-07-14 07:59:34.478 3915 DEBUG nova.scheduler.filter_scheduler [req-d701a25b-7bbc-44ed-9b31-57245e291ae1 None] Weighed [WeighedHost [host: (undercloud, aa4c5e32-46a5-46f2-b312-262a2df23980) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, c63bc9b2-5340-4d46-837a-a4f9a77a00d3) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 1b44fa97-a41f-4aa1-a578-08e82b6ed624) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, a5104c42-37e1-48ef-a5b6-0ba2d8808d8e) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 75e03645-caa6-4d16-9f9d-eb66bf0df769) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, cc0a5455-0978-4687-988e-8f3161cbc521) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 47bbaf38-9e0d-43a0-932b-eb0c9ec7af26) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 9686995e-3ff0-4a10-8c3c-bf3e09ffa770) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 578cf11e-3d79-45ae-883e-583a83f46e23) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, a3f05b92-e693-4338-843b-62c8ccaad397) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 69dc8c40-dd79-4ed6-83a9-374dcb18c39b) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, a76e82f6-b454-4b24-accb-b260fa361f27) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 0270cb74-4784-46ef-8521-38697daafb1d) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, d69b7468-9b3a-44b1-b7b0-4fcf7e3452d5) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, dc86f4ac-b416-410b-ab03-7912f1b9a1b8) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, a9d71ef3-3a7f-4932-a952-534c1128a1e6) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, b07295ee-1c09-484c-9447-10b9efee340c) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, c37db96f-e689-4789-929c-4d45067ba654) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 101a1fba-acd7-4c9e-9e3c-8436c35e4cef) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 7097eae3-c049-4e5a-ad90-dcb54d82d575) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 17fdb7b4-a7cf-4f67-a736-cb84ec3d49e9) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, aede7973-6209-4144-a243-308a03d07e0b) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 3a2ab650-6daf-4a79-9eb4-2ea3f8af8faf) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, dd6eaa66-5289-4689-9034-0d7f706add47) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 381fd1a4-fb1b-411c-a0c8-538781afb48e) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, c4b0f4cc-ca2c-4225-9e84-87c1f4b08ebd) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 38595174-e830-46ce-855e-f54d080b5971) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 7a3ccba1-6db6-4b57-a11f-524ac23bf672) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 34ece3c2-7a0d-42fe-acc3-9e429f587c60) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 9c4dfded-92ef-45e7-9db4-e81ae0d9a249) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 16c374af-2919-4d7d-a281-a69d461adc59) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 69f95874-ddf9-4ede-a6ff-6178d4dcd67c) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 7196471a-12af-4430-89cb-01321f95c942) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 719312a2-f7f7-470b-8cc4-da399af66237) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, ed8bd319-d238-4679-b2c3-0f93d7604bbd) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 8ba5f5ff-3c15-4cb5-b0af-e3ac0f593b95) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 30837020-4728-459e-a5bd-dd19827ac1b5) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 53a0adf4-802c-4e52-8af4-2205e5d8343e) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0], WeighedHost [host: (undercloud, 75fad715-3029-43b8-a47c-891b680fe94e) ram:98304 disk:1638400 io_ops:0 instances:0, weight: 1.0]] _schedule /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py:274\n2014-07-14 07:59:37.940 3915 DEBUG nova.filters [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] Starting with 46 host(s) get_filtered_objects /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/filters.py:70\n2014-07-14 07:59:37.941 3915 DEBUG nova.scheduler.filters.retry_filter [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] Host [u'undercloud', u'cc0a5455-0978-4687-988e-8f3161cbc521'] fails.  Previously tried hosts: [[u'undercloud', u'cc0a5455-0978-4687-988e-8f3161cbc521']] host_passes /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/scheduler/filters/retry_filter.py:42\n\nnova-compute.log:\n2014-07-14 07:59:37.845 20886 DEBUG nova.openstack.common.lockutils [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] Got semaphore / lock \"instance_claim\" inner /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:324\n2014-07-14 07:59:37.845 20886 DEBUG nova.compute.resource_tracker [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] Memory overhead for 98304 MB instance; 0 MB instance_claim /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/resource_tracker.py:117\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Attempting claim: memory 98304 MB, disk 1600 GB, VCPUs 24\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Total memory: 98304 MB, used: 98304.00 MB\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] memory limit: 98304.00 MB, free: 0.00 MB\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Total disk: 1600 GB, used: 1600.00 GB\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] disk limit not specified, defaulting to unlimited\n2014-07-14 07:59:37.847 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Total CPUs: 24 VCPUs, used: 24.00 VCPUs\n2014-07-14 07:59:37.848 20886 AUDIT nova.compute.claims [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] CPUs limit not specified, defaulting to unlimited\n2014-07-14 07:59:37.848 20886 DEBUG nova.openstack.common.lockutils [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] Semaphore / lock released \"instance_claim\" inner /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/lockutils.py:328\n2014-07-14 07:59:37.848 20886 DEBUG nova.compute.manager [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Insufficient compute resources: Free memory 0.00 MB < requested 98304 MB. _build_and_run_instance /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py:1981\n2014-07-14 07:59:37.849 20886 DEBUG nova.compute.utils [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Insufficient compute resources: Free memory 0.00 MB < requested 98304 MB. notify_about_instance_usage /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/utils.py:291\n2014-07-14 07:59:37.850 20886 DEBUG nova.compute.manager [req-0015ebce-2a43-45d6-9dda-e7c0d1b61df9 None] [instance: 564bbc63-8bf2-4fa0-8353-ba246bd2ba15] Build of instance 564bbc63-8bf2-4fa0-8353-ba246bd2ba15 was re-scheduled: Insufficient compute resources: Free memory 0.00 MB < requested 98304 MB. do_build_and_run_instance /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py:1896\n", 
            "date_created": "2014-07-14 08:26:57.275235+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "It's possible to request multiple instances at once. They all have to be the same flavor IIRC. If you do this, the scheduler will ensure the whole request can be met. However if you make multiple requests then as you've seen, it kind of assumes all prior requests failed unless the DB has been updated to say they succeeded.\n\nTwo obvious fixes occur to me. One, sync the scheduler with the compute hosts. Generally unacceptable but might be OK in an early TripleO context. Two, make the scheduler more optimistic. Either way is going to be tricky, work on converting to use Nova objects and splitting out Gantt has basically frozen all other work on the scheduler.", 
            "date_created": "2014-07-14 08:34:41.270566+00:00", 
            "author": "https://api.launchpad.net/1.0/~alexisl"
        }, 
        {
            "content": "Right, N at once works because the list of hosts is returned and zipped across the count of instances requested, but we can't make heat do that (yet, if ever) :(.\n\nNot sure what you mean by syncing the scheduler with the compute hosts - do you mean moving the claim code into the scheduler, or something else?\n\nIn terms of optimism, that seems like a variation on the \"\n - have the scheduler be a bit smarter about returning hosts - e.g. track destination selection counts since the last refresh and weight hosts by that count as well\" suggestion I made in the bug ?", 
            "date_created": "2014-07-14 08:42:02.548519+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "By syncing I mean synchronizing, IE make the scheduler wait for the instance to succeed or fail creation before scheduling another instance. There'd have to be a timeout too because messages can go missing (unsure how theoretical that is). Not pretty, could slow down stack standup significantly.\n\nYou're right, similar suggestions. Rather than dest. selection counts it should track resources consumed by outstanding requests. Worst case resources are double-counted for a very short period if the DB gets updated before the claim is released. I've asked my colleague Paul Murray to provide his input, he knows this issue intimately.", 
            "date_created": "2014-07-14 08:48:31.406322+00:00", 
            "author": "https://api.launchpad.net/1.0/~alexisl"
        }, 
        {
            "content": "Cool, thank you! having read more of the code I see that the db scheduler is reading the entire hosts lists on every schedule request, which seems bonkers, but thats *clearly* a different discussion. However, it does simplify the choices we have, since we don't need to consider skewed views of the data, we just need to ensure that the resources are claimed immediately. ", 
            "date_created": "2014-07-14 09:09:57.724655+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/106716", 
            "date_created": "2014-07-14 09:23:27.847908+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This is horrid, but let me deploy the 29 node scenario perfectly first-time. https://review.openstack.org/106716", 
            "date_created": "2014-07-14 09:23:57.232433+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "It seems quite spiky as to triggering - I just had one test pass (29 deploys on a pool of 45, all worked), then the next run failed with 14 ERRORs due to this :/.", 
            "date_created": "2014-07-14 10:43:26.742199+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "I believe its an inherent part of the Nova design that compute nodes have to be able to reject and send requests back to the Scheduler (back to conductor in the new way of doing things) if they get a request for resources that they no longer have.\n\nFor sure you don't want a lot of concurrency in the scheduler (try rpc_thread_pool_size=2 , but I think compute managers in an Ironic setup will need to be able to follow the same basic approach.\n", 
            "date_created": "2014-07-14 10:52:57.419201+00:00", 
            "author": "https://api.launchpad.net/1.0/~philip-day"
        }, 
        {
            "content": "Sure, but this is pathological, we're scheduling incredibly poorly if requests come in at a fast enough rate. Ironic doesn't change *anything* about the basic approach, and the problem isn't unique to Ironic - KVM with machine sized VMs will behave identically, for instance.\n\nIMO we need to consider this bug a result of design decisions, and we can revisit those after learning about the issues they have....\n\nThe issue here is that there is a data structure maintained by the compute process (perhaps via conductor for actual DB writes) but consulted by the scheduler process(es).\n\nIf we take a leaf from the BASE design guidelines, we might have this look something like this as a small change to fix things, using a read-after-write pattern (which is in some ways heinous...)\n\nadd a scheduler grants table (timestamp, host, memory_gb, cpu_count, disk_gb)\n\nscheduler receives a request:\n - gets a baseline in-memory view using the current read-all-rows approach\n - updates that by reading outstanding grants (or all and discard-in-memory)\n - schedules\n - writes a grant to the DB (using DB set timestamp)\n - reads from the DB to see if there were other grants made for the same host(s), and if they would have invalidated the grant.\n   - if and only if they invalidate it and the timestamp on the invalidating grant was less than ours, fall over to the next viable host and remove the grant (we lost), otherwise we won the conflict.\n - return results\n\ncompute process receives a instance to build (or migrate):\n - takes its local claim-lock\n - reads the grant from the scheduler table (select from ... where host=self.hostname and cpu_count=...  order by timestamp asc limit 1)\n - updates its host row in the DB with the now available resources\n - also sets the last-grant timestamp with the timestamp from that grant\n\nAnd we add a periodic job to the scheduler to delete rows from the grant table where the timestamp is less than the timestamp in the compute table (or there is no matching compute row).\n\nThis would have the following properties:\n - min two more DB writes per scheduled instance (one to write the grant, one to delete it later)\n - min three more DB reads per scheduled instance (compute host has to read the grant, and the scheduler has to read all the unhandled grants and subtract them before scheduling, and scheduler has to look for conflicting grants after scheduling before returning)\n - up to viable-hosts extra writes and reads to handle falling over to the next viable host in the event of a conflict. Assume there are 10 schedulers and someone is flooding the system with identical requests. In each round one will one but it will rejoin with the next request and presumably come up with the same next host that all the others have fallen over to. In this scenario scheduling will end up single threaded (with 9 threads of spinlock failures at any point in time), but it won't deadlock or halt.", 
            "date_created": "2014-07-14 12:10:47.375999+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "I personally dislike any change in the scheduler where it would wait for the end of booting an instance.\nThat would generate a locking mechanism in the scheduler while we currently have a chance to get one lock-free.\n\nScheduler is doing decisions based on its internal representation. If the representation is not up-to-date,  there are big chances that eventually the request will fail but in that case, that's the role of formerly the compute node (and now the conductor) to issue a retry. Yes, that's by design that the scheduler can generate race conditions, but that also allows us to have an external scheduler without being necessary to remove the bits of locking mechanism.\n\n\nLong story short, we have to find out how to improve the retry mechanism in the conductor, not in the scheduler.", 
            "date_created": "2014-07-18 12:33:41.652036+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "@sylvain There are plenty of other proposals than 'interlock with instance booting' - so I disagree with you that improving the retry mechanism is the only option.\n\nThis is basically a consensus-view-of-resources problem, so any of the broad set of consensus answers will work.", 
            "date_created": "2014-07-24 18:43:45.717375+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "@lifeless - Do any of those consensus answers scale well and handle failure gracefully? ;)\n\nI think the issue here is that there are two sets of problems when you reach opposite ends of scale in both number of machines and what can fit on a single machine. At the \"we have so many resources you can consider them infinite\" end of the world an approximate view is the optimal one because is scales and there is a low likelyhood of being wrong. In the case presented in this bug report, where the problem is the system is approaching resource starvation, the optimal solution would be to have a globally consistent view of resources.\n\nThe current design is aimed at the first of these two.\n\nImproving the retry mechanism does make sense, but I would make sure that we do not sacrafice scalability and graceful failure handling in the process.\n\nSo anything that can shorten the delay in getting up to date information to the scheduler is good. Anything that introduces any kind of synchronisation is bad - including additional lookups in the database.", 
            "date_created": "2014-07-25 05:28:56.921617+00:00", 
            "author": "https://api.launchpad.net/1.0/~pmurray"
        }, 
        {
            "content": "+1 with @PaulMurray, IMHO we have to work for making sure we can reduce the delay in between computes and scheduler, but still without locking mechanism, because the opposite would impact the scability.\n", 
            "date_created": "2014-07-25 07:07:08.455020+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/106716\nReason: This review is > 4 weeks without comment and currently blocked by a core reviewer with a -2. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and contacting the reviewer with the -2 on this review to ensure you address their concerns.", 
            "date_created": "2014-11-20 15:20:39.064113+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "@PaulMurray, @Sylvain\n\n\nThis particular bug occurs with *one* scheduler instance, and 2 resources under subscription, and its a clear example of bad data hygiene - eventual consistency doesn't imply the poor behaviour we use.\n\nTrivially, caching the schedule operations we've made locally and folding those in until the hypervisor would fix this bug with no additional synchronisation overhead.\n\nI appreciate that everyone is concerned about the scheduler, but before saying we can't use a technique, we need to be clear about what our success criteria are. One of the big scheduling problems we have is that we have no specific success criteria, and its broken with bugs like this, so we get design pushback not on the basis of actually success or failure, but fear that it will be worse.\n\nFor instance, if we define success as 'be able to place up to 10000 uses/second on up to 1M resources and no more than 10 seconds downtime in the event of scheduler failure', then we have a metric we can examine to assess scheduling implementations. E.g. A single fast scheduler with HA via fail-over on a 5s heartbeat with 15s warmup time could meet this. As could a distributed scheduler with sharding, or possibly a consensus scheduler with sync-on-over-subscribe [e.g. only one retry ever triggered].", 
            "date_created": "2015-03-30 23:41:18.292175+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "This is not an Ironic bug.  It's just triggered by using Ironic.  This is a a Nova scheduling bug (and a PITA to fix :(", 
            "date_created": "2015-06-02 23:27:02.653493+00:00", 
            "author": "https://api.launchpad.net/1.0/~mrda"
        }, 
        {
            "content": "There was a spec proposed and accepted for Liberty that aims at fixing or at least mostly mittigating the described issues\n\nhttp://specs.openstack.org/openstack/nova-specs/specs/liberty/approved/host-state-level-locking.html\n\nHowever it did not make it for Liberty - I plan to re-propose for M, and it would be great to get feedback from Ironic folks of course!", 
            "date_created": "2015-08-30 10:54:41.775155+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/226235", 
            "date_created": "2015-09-22 10:15:50.482898+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/226235\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=608af87541240793425760c16e528d9b3ec02fee\nSubmitter: Jenkins\nBranch:    master\n\ncommit 608af87541240793425760c16e528d9b3ec02fee\nAuthor: Lucas Alvares Gomes <email address hidden>\nDate:   Mon Sep 21 16:36:04 2015 +0100\n\n    Ironic: Workaround to mitigate bug #1341420\n    \n    The bug #1341420 can cause the driver to try to deploy onto a node which\n    already has an instance associated with it, the way Ironic reserve a\n    node to a specific instance is associating the UUID of that instance\n    with the node via the instance_uuid field. This patch just makes the\n    request to associate that field non-retriable when the api returns HTTP\n    409 (Conflict) (that's the HTTP error indicating that the node is\n    already associated with another instance), so it can fail fast in\n    case the node is already pick and the RetryFilter could then pick\n    another node for that instance.\n    \n    Partial-Bug: #1341420\n    Change-Id: I697c5129893b18562182bbb3ceb8cb160e84c312\n", 
            "date_created": "2015-12-15 21:02:58.270421+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Did anyone notice the problem that `select_destination()` doesn't reuse host state cache to make decisions at all?\n\nThat is to say:\nScheduler will discard its host state and refresh it from DB each time at the beginning of `select_destination()`, instead of reusing the recent updated host state cache.\n\n### explanation ###\n\nRobert Collins pointed out in the bug description that scheduler works well in situation[1] when boot 45 instances in one command. But if he chose to boot these 45 instances in 45 concurrent commends in the second situation[2], there will be up to 50% failure, which is unacceptable. \n\nThe real difference between [1] and [2] is that:\n- In the first situation[1], scheduler will reuse the host state cache in the `for` loop [3], so that the following 44 schedule decisions are made on the INCREMENTAL UPDATED host state cache. Thus the result turns out to be accurate.\n- However in [2], each 45 requests will refresh the host state in `get_all_host_states()` by logic [4] in the beginning. Thus these 45 concurrent schedule decisions are all made based on the SAME db state. No wonder there are 50% failure caused by conflictions. Worse, it could be 97.77% failure if CONF.scheduler_host_subset_size is 1 and in the most idea condition.\n\nAnother thing to point out: \nCurrently, there is no `sleep(0)` or any asynchronous request(except for the experimental trusted filter) during filtering and weighing. So schedule operations including 'refresh host state from db data', 'filtering', 'weighing', 'consume host states', 'return decision' can be treat as an atomic operation as a whole. This strengthens my opinion that in situation[2], scheduler will use the 'almost' same host state cache in 45 concurrent requests. Thus definitely those decisions will conflict.\n\n[1] 15:46 < lifeless> nova boot <...> --num-instances 45 -> works fairly reliably. Some minor timeout related things to fix but nothing dramatic.\n[2] 15:47 < lifeless> heat create-stack <...> with a stack with 45 instances in it -> about 50% of instances fail to come up\n[3] https://github.com/openstack/nova/blob/master/nova/scheduler/filter_scheduler.py#L117-L149\n[4] https://github.com/openstack/nova/blob/master/nova/scheduler/host_manager.py#L160-L222\n\n### solution ###\n\nThe reasonable solution is to refresh host state only if the fetched data is newer than the db data which host state is based on.\n\nCurrent timestamp in compute_node table is not accurate enough because this record is only up to seconds. It should be at least in milliseconds or even in updated counts to determine whether a host state is outdated. This might deserve a bp to implement.", 
            "date_created": "2015-12-23 08:14:28.661365+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyx1231st"
        }, 
        {
            "content": "Hmm... rephrased because I lost the code snippet[1].\n\nBut the same problem still exists because of the line [2], why host state should be updated when it is already updated to the db state with the same timestamp? This leads to the problem that the concurrent schedule request can not benefit from host state cache, so there are conflicts.\n\n[1] https://github.com/openstack/nova/blob/master/nova/scheduler/host_manager.py#L162-L164 \n[2] https://github.com/openstack/nova/blob/master/nova/scheduler/host_manager.py#L163", 
            "date_created": "2015-12-23 09:01:31.641775+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyx1231st"
        }, 
        {
            "content": "<please forget about my previous comments>\n\nI've reproduced this bug with *one* scheduler instance based on the latest nova code.\n\n----- reproduce -----\n\nMy openstack environment:\nThere are 3 compute nodes(node2, node3, node4), each of them has limited 1489MB free RAM and plenty of other resources to boot instances. The scheduler is configured \"ram_allocation_ration = 1.0\" to prevent memory overcommit. And there is only one flavor that requests 400MB RAM, so that at most 3 instances are allowed to boot onto each compute node. 9 instances at most in this 3-node environment.\n\nThen I requested to boot 9 instances named p1, p2, p3, p4, p5, p6, p7, p8, p9 using 9 separate Restful API requests. I sent those requests in 3 groups, each group will send 3 requests in parallel, and those groups are sent in every 0.5 second. It takes a second to finish all those requests. \n\nThose 9 requests are expected to be successful, but here is the full log [2] showing that it is actually not.\nI also made some efforts to make those logs look more friendly [3].\nAs the log said, the request to schedule p8 is failed with 2 retries. Although it finally succeeded to select node3, it is likely to fail if there are more nodes. Imagine there could be up to 1000 nodes with much busier and complex requests.\n\nI modified scheduler logging a little to better understand what is happening inside scheduler: Add log to indicate the beginning and the end of `select_destination()` with the instance name from request_spec; Add log to indicate the start and end time around `objects.ComputeNodeList.get_all()` in `get_all_host_states()`; Most importantly, I added logs to the code[1] to show whether a host state is successfully updated from compute node db object.\n\n----- log analysis -----\n\nThings go well during #L1-L94, until the request 8582 schedules the instance p8 to node4. Note that node4 already has 3 instances at that time. The request will certainly fail because node4 cannot claim another 400MB memory. The request 8582 still makes this wrong decision because the cached host state told that node4 can fit another instance with 689MB memory(#L92). It looks all host state's fault! \n\nSo back again to check how host state of node4 went wrong... and here it is. After scheduling two instances to node4, #L66 shows that the host state was updated by database that told scheduler node4 has 1089MB RAM. That is to say, the db contained outdated message but scheduler still accepted it. But according to logic[1], the cache is assumed outdated because the update time of db is newer, why still wrong? \n\nBecause there is a time GAP between scheduler makes a decision and the compute node persistents this decision. For example in the log #L66 [2], the first request 132f was persisted to db at 17:19:45, but the second request 8de7 still hadn't been persisted to db by the time 17:19:45.242. However, the node4 state cache was updated by the third request 8582 at 17:19:44.935. So that host state is refreshed by db whose updated time is newer, but it caused the lost of consumption from request 8de7 and bad things happened!\n\nThe same bad happened again at the first retry of request 8582(the 10th column of [3]) to choose the wrong node2, because node2 state cache was updated by outdated data from sluggish db at #L113 [2].\n\n----- possible solution -----\n\nSynchronize host state cache with database is very important in making right decisions and supporting multiple scheduler instances. However, there is always a time gap between the scheduler making a consumption and this consumption is persisted to database. The scheduler may lost some recent consumptions after being refreshed from database. What if the scheduler can *remember* those recent consumptions as resource claims and reapply them to the updated host state cache? It can help fill the gap.\n\n----- the extra -----\n\nCurrent update model of host state cache has some issues. Firstly, a host state cache will be updated even if it is already the same with the latest db data according to code [1]. For example in previous log [2], #L16, L17, L29 are unnecessary updates regardless that they do no harm to making the right decision. Secondly, assume a host state cache is created based on db data1 at time t1, and later, there comes db data2 updated at t2(t2 > t1), but host state has already been consumed at t3(t3 > t2). Althouth data2 is newer than data1, scheduler will still discard the new information and use the older one, because `HostState.updated > compute.updated_at` according to code [1].\n\nAnother thing is, I see no race conditions between greenthreads after doing many experiments of sending requests in parallel. For example in log[4] I've triggered 6 successful parallel requests. The results are always correct if the database does not make troubles. I think it is because that there is no `sleep(0)` or any asynchronous request(except for the experimental trusted filter) during filtering, weighing and consuming. So these operations can be treat as atomic as a whole.\n\n\n[1] https://github.com/openstack/nova/blob/master/nova/scheduler/host_manager.py#L162-L164\n[2] http://paste.openstack.org/show/482724/\n[3] http://paste.openstack.org/show/482725/\n[4] http://paste.openstack.org/show/482728/", 
            "date_created": "2015-12-26 17:57:25.887558+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyx1231st"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/261920", 
            "date_created": "2015-12-28 08:04:37.928881+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Given the discussion around this in the scheduler meeting, moving this to a whishlist item.\n\nMuch of the behaviour here is as designed, and the retry loop should minimise the affects seen by API users. The caching_scheduler also helps for cases where you only run one scheduler, reducing the races even further, in most cases. The randomisation weigher also helps with that, somewhat.\n\nThe issue tends to be very deployment specific, and certainly the current scheduler is optimised for when the cloud generally has some free space, and gets expanded before it becomes full. Which was considered the typical cloud use case, at the time the scheduler was written.", 
            "date_created": "2016-01-04 15:52:46.410441+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "We have a 15 node test cluster in our lab, which is managed by Nova and Ironic. We have run into this issue when attempting to boot all 15 nodes, and in general a number of nodes will fail to boot. This can be seen when using heat or nova directly.\n\nWe have developed a couple of changes that drastically improve the behaviour.\n\nThe first change, which I believe fixes a bug, is to ensure appropriate resource limits are added to the Exact*Filter filters, in the same way as is done for the other filters. For instance, the DiskFilter writes the calculated disk_gb resource limit to a HostState object in host_passes, if it passes through the filter (see https://github.com/openstack/nova/blob/master/nova/scheduler/filters/disk_filter.py#L59). Conversely, the ExactDiskFilter does not set the disk_db limit on the host state (see https://github.com/openstack/nova/blob/master/nova/scheduler/filters/exact_disk_filter.py#L23). I think that this limit should be updated in the exact filters also. This limit, if present, is later checked in the compute service during the claim. This allows the claim to verify that the requested resources are available, with the synchronisation provided by doing the check in compute. What this achieves, is to force invalid claims to fail in the compute service, rather than succeeding and causing strange problems with multiple instances trying to provision a single Ironic node.\n\nThe second change we have developed is to recognise that with when scheduling onto homogeneous Ironic nodes, we will typically have a large number of compute hosts pass through the scheduler filters, all with equal weight. In the default configuration, with scheduler_host_subset_size = 1, concurrent requests will all schedule onto the same node, the first in the list. As discussed earlier, increasing this number reduces the chances of a collision, but how big should it be? If we go too high, then we undermine the weighting system, allowing lower weight hosts a chance to be scheduled when they would not otherwise be. Our solution to this is to extend the host subset size to include all top weight hosts. Therefore, we can set the -scheduler_host_subset_size to a sensible number, but allow scheduling onto all  of the 'equally best' nodes. Although the race still exists, the chance of hitting it is reduced.\n\nAny thoughts on these approaches?", 
            "date_created": "2016-01-26 14:47:17.277160+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/273154", 
            "date_created": "2016-01-27 17:03:16.695181+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I ran the attached script on a devstack instance setup with 3 Ironic nodes. I used the fake Ironic driver because it should not affect Nova scheduling behaviour. First, I ran the script on unmodified master. Second, I modified the Exact* filters to set the resource limits on the HostState objects.\n\nUnmodified:\nTotal runs 100 scheds 300 rescheds 203 errors 544\n\nWith limits applied to Exact* filters:\nTotal runs 100 scheds 300 rescheds 223 errors 0\n\nInterestingly, although this change removed all errors, the number of reschedules was slightly increased. This is not hugely surprising, as the race condition will be caught sooner, increasing the likelihood that it could happen again.", 
            "date_created": "2016-01-27 17:11:58.852319+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "I just ran the script again, this time with the second change described above, in which the host subset is extended to include all top weight hosts. The fix for the Exact* filter was also included.\n\nWith all top weight hosts change:\nTotal runs 100 scheds 300 rescheds 67 errors 0 fails 0\n\nThis result shows a dramatic reduction in the number of reschedule events (22%) vs the unmodified code (68%).", 
            "date_created": "2016-01-28 15:42:40.217085+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/273154\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=3471cc8b74444d04348d8e6dbc9c980641d4b0bc\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3471cc8b74444d04348d8e6dbc9c980641d4b0bc\nAuthor: Mark Goddard <email address hidden>\nDate:   Wed Jan 27 03:49:07 2016 +0000\n\n    Apply scheduler limits to Exact* filters\n    \n    The DiskFilter, RamFilter and CoreFilter scheduler filters set the\n    resource limit on the HostState object during filtering. This is\n    later checked in compute during the claim, and used to enforce\n    the allocation ratios in a safe manner. The Exact* filters do not\n    set the resource limits, so scheduler race conditions can result\n    in multiple bare metal instances claiming a single host.\n    \n    This change sets the resource limits on the HostState object in\n    the ExactCoreFilter, ExactDiskFilter and ExactRamFilter, ensuring\n    that only a single baremetal instance can claim a compute host.\n    \n    Change-Id: I31d0331afc4698046a4568935a95f70f30e335dd\n    Partial-Bug: #1341420\n    Co-Authored-By: Will Miller <email address hidden>\n", 
            "date_created": "2016-02-02 17:08:29.658669+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/276191", 
            "date_created": "2016-02-04 12:20:30.382857+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/276191\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=19e7abad96c5c8d19a442a41f799dabe9d600db8\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit 19e7abad96c5c8d19a442a41f799dabe9d600db8\nAuthor: Mark Goddard <email address hidden>\nDate:   Wed Jan 27 03:49:07 2016 +0000\n\n    Apply scheduler limits to Exact* filters\n    \n    The DiskFilter, RamFilter and CoreFilter scheduler filters set the\n    resource limit on the HostState object during filtering. This is\n    later checked in compute during the claim, and used to enforce\n    the allocation ratios in a safe manner. The Exact* filters do not\n    set the resource limits, so scheduler race conditions can result\n    in multiple bare metal instances claiming a single host.\n    \n    This change sets the resource limits on the HostState object in\n    the ExactCoreFilter, ExactDiskFilter and ExactRamFilter, ensuring\n    that only a single baremetal instance can claim a compute host.\n    \n     Conflicts:\n    \tnova/tests/unit/scheduler/filters/test_exact_core_filter.py\n    \tnova/tests/unit/scheduler/filters/test_exact_disk_filter.py\n    \tnova/tests/unit/scheduler/filters/test_exact_ram_filter.py\n    \n    Change-Id: I31d0331afc4698046a4568935a95f70f30e335dd\n    Partial-Bug: #1341420\n    Co-Authored-By: Will Miller <email address hidden>\n    (cherry picked from commit 3471cc8b74444d04348d8e6dbc9c980641d4b0bc)\n", 
            "date_created": "2016-03-01 02:53:18.271170+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "That bug report is still accurate, but given the fact that we're working towards having new resource providers for the scheduler and discussing actively on the implementation details, I'd by far prefer to close that bug and leave a spec better describing the problem.\n", 
            "date_created": "2016-04-18 10:12:14.714118+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "Fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/307219", 
            "date_created": "2016-04-18 14:02:14.396030+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Lee Yarwood (<email address hidden>) on branch: stable/liberty\nReview: https://review.openstack.org/307219", 
            "date_created": "2016-06-07 20:46:56.889492+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/261920\nReason: This review is > 6 weeks without comment, and failed Jenkins the last time it was checked. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and leaving a 'recheck' comment to get fresh test results.", 
            "date_created": "2016-12-09 21:05:51.160341+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "The issues was reproduced at ironic-multinode job  http://logs.openstack.org/75/427675/11/check/gate-tempest-dsvm-ironic-ipa-wholedisk-agent_ipmitool-tinyipa-multinode-ubuntu-xenial-nv/a66f08f/logs\n\nseveral instances were scheduled to the same ironic node.\n\n\nhttp://logs.openstack.org/75/427675/11/check/gate-tempest-dsvm-ironic-ipa-wholedisk-agent_ipmitool-tinyipa-multinode-ubuntu-xenial-nv/a66f08f/logs/screen-n-sch.txt.gz#_2017-02-03_18_46_35_112\n\n2017-02-03 18:46:35.112 21443 DEBUG nova.scheduler.filter_scheduler [req-f8358730-fc5a-4f56-959c-af8742c8c3c2 tempest-ServersTestJSON-264305795 tempest-ServersTestJSON-264305795] Selected host: WeighedHost [host: (ubuntu-xenial-2-node-osic-cloud1-s3500-7106433, e85b2653-5563-4742-b23f-7ceb15b2032f) ram: 384MB disk: 10240MB io_ops: 0 instances: 0, weight: 2.0] _schedule /opt/stack/new/nova/nova/scheduler/filter_scheduler.py:127\n\n\nhttp://logs.openstack.org/75/427675/11/check/gate-tempest-dsvm-ironic-ipa-wholedisk-agent_ipmitool-tinyipa-multinode-ubuntu-xenial-nv/a66f08f/logs/screen-n-sch.txt.gz#_2017-02-03_18_46_35_204\n\n2017-02-03 18:46:35.204 21443 DEBUG nova.scheduler.filter_scheduler [req-3ff28238-f02b-4914-9ee1-dc07a15f6c9d tempest-ServerActionsTestJSON-1934209030 tempest-ServerActionsTestJSON-1934209030] Selected host: WeighedHost [host: (ubuntu-xenial-2-node-osic-cloud1-s3500-7106433, e85b2653-5563-4742-b23f-7ceb15b2032f) ram: 384MB disk: 10240MB io_ops: 0 instances: 0, weight: 2.0] _schedule /opt/stack/new/nova/nova/scheduler/filter_scheduler.py:127\n2017-02-03 18:46:35.204 21443 DEBUG oslo_concurrency.lockutils [req-3ff28238-f02b-4914-9ee1-dc07a15f6c9d tempest-ServerActionsTestJSON-1934209030 tempest-ServerActionsTestJSON-1934209030] Lock \"(u'ubuntu-xenial-2-node-osic-cloud1-s3500-7106433', u'e85b2653-5563-4742-b23f-7ceb15b2032f')\" acquired by \"nova.scheduler.host_manager._locked\" :: waited 0.000s inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:270\n", 
            "date_created": "2017-02-03 20:24:09.294378+00:00", 
            "author": "https://api.launchpad.net/1.0/~vsaienko"
        }, 
        {
            "content": "What you describe is fundamental to how nova works right now. We speculate in the scheduler, and if we race between two, we handle it with a reschedule. Nova specifically states that scheduling every last resource is out of scope. When trying to do that (which is often the use case for ironic) you're likely to hit this race as you run out of capacity:\n\nhttps://github.com/openstack/nova/blob/master/doc/source/project_scope.rst#iaas-not-batch-processing\n\nIn the next few cycles we plan to move the claim process to the placement engine, which will eliminate most of these race-to-claim type issues, and in that situation things will be better for this kind of arrangement.\n\nUntil that point, this is not a bug though, because it's specifically how nova is designed to work.", 
            "date_created": "2017-02-03 22:41:45.940071+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "This is a really old bug and I don't think it applies to tripleo anymore (if ever). Setting to invalid for tripleo.", 
            "date_created": "2017-09-22 17:28:37.962222+00:00", 
            "author": "https://api.launchpad.net/1.0/~alex-schultz"
        }
    ]
}
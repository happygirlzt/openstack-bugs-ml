{
    "status": "Fix Released", 
    "last_updated": "2015-03-19 16:21:12.194072+00:00", 
    "description": "\nPerforming attach/detach interface on a VM sometimes results in an interface that can't be detached from the VM.\nI could triage it to the corrupted instance cache info due to non-atomic update of that information.\nDetails on how to reproduce the bug are as follows. Since this is due to a race condition, the test can take quite a bit of time before it hits the bug.\n\nSteps to reproduce:\n\n1) Devstack with trunk with the following local.conf:\ndisable_service n-net\nenable_service q-svc\nenable_service q-agt\nenable_service q-dhcp\nenable_service q-l3\nenable_service q-meta\nenable_service q-metering\nRECLONE=yes\n# and other options as set in the trunk's local\n\n2) Create few networks:\n$> neutron net-create testnet1\n$> neutron net-create testnet2\n$> neutron net-create testnet3\n$> neutron subnet-create testnet1 192.168.1.0/24\n$> neutron subnet-create testnet2 192.168.2.0/24\n$> neutron subnet-create testnet3 192.168.3.0/24\n\n2) Create a testvm in testnet1:\n$> nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm\n\n3) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:\n--------\n#! /bin/bash\nc=10000\nnetid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`\nnetid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`\nwhile [ $c -gt 0 ]\ndo\n   echo \"Round: \" $c\n   echo -n \"Attaching two interfaces... \"\n   nova interface-attach --net-id $netid1 testvm\n   nova interface-attach --net-id $netid2 testvm\n   echo \"Done\"\n   echo \"Sleeping until both those show up in interfaces\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       if [ $count -eq 7 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   echo \"Waited for \" $waittime \" seconds\"\n   echo \"Detaching both... \"\n   nova interface-list testvm | grep $netid1 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   nova interface-list testvm | grep $netid2 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   echo \"Done; check interfaces are gone in a minute.\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       echo \"line count: \" $count\n       if [ $count -eq 5 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   if [ $waittime -ge 60 ]\n   then\n      echo \"bad case\"\n      exit 1\n   fi\n   echo \"Interfaces are gone\"\n   ((  c-- ))\ndone\n---------\n\nEventually the test will stop with a failure (\"bad case\") and the interface remaining either from testnet2 or testnet3 can not be detached at all.", 
    "tags": [
        "api", 
        "icehouse-backport-potential", 
        "in-stable-icehouse", 
        "network", 
        "neutron", 
        "nova"
    ], 
    "importance": "High", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1326183", 
    "owner": "https://api.launchpad.net/1.0/~hzwangpan", 
    "id": 1326183, 
    "index": 1504, 
    "created": "2014-06-04 00:39:56.858292+00:00", 
    "title": "detach interface fails as instance info cache is corrupted", 
    "comments": [
        {
            "content": "\nPerforming attach/detach interface on a VM sometimes results in an interface that can't be detached from the VM.\nI could triage it to the corrupted instance cache info due to non-atomic update of that information.\nDetails on how to reproduce the bug are as follows. Since this is due to a race condition, the test can take quite a bit of time before it hits the bug.\n\nSteps to reproduce:\n\n1) Devstack with trunk with the following local.conf:\ndisable_service n-net\nenable_service q-svc\nenable_service q-agt\nenable_service q-dhcp\nenable_service q-l3\nenable_service q-meta\nenable_service q-metering\nRECLONE=yes\n# and other options as set in the trunk's local\n\n2) Create few networks:\n$> neutron net-create testnet1\n$> neutron net-create testnet2\n$> neutron net-create testnet3\n$> neutron subnet-create testnet1 192.168.1.0/24\n$> neutron subnet-create testnet2 192.168.2.0/24\n$> neutron subnet-create testnet3 192.168.3.0/24\n\n2) Create a testvm in testnet1:\n$> nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm\n\n3) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:\n--------\n#! /bin/bash\nc=10000\nnetid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`\nnetid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`\nwhile [ $c -gt 0 ]\ndo\n   echo \"Round: \" $c\n   echo -n \"Attaching two interfaces... \"\n   nova interface-attach --net-id $netid1 testvm\n   nova interface-attach --net-id $netid2 testvm\n   echo \"Done\"\n   echo \"Sleeping until both those show up in interfaces\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       if [ $count -eq 7 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   echo \"Waited for \" $waittime \" seconds\"\n   echo \"Detaching both... \"\n   nova interface-list testvm | grep $netid1 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   nova interface-list testvm | grep $netid2 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   echo \"Done; check interfaces are gone in a minute.\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       echo \"line count: \" $count\n       if [ $count -eq 5 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   if [ $waittime -ge 60 ]\n   then\n      echo \"bad case\"\n      exit 1\n   fi\n   echo \"Interfaces are gone\"\n   ((  c-- ))\ndone\n---------\n\nEventually the test will stop with a failure (\"bad case\") and the interface remaining either from testnet2 or testnet3 can not be detached at all.", 
            "date_created": "2014-06-04 00:39:56.858292+00:00", 
            "author": "https://api.launchpad.net/1.0/~ypraveen-5"
        }, 
        {
            "content": "I could triage it to non-atomic instance info cache update. It seems heal_instance_info_cache periodic\ntask runs in parallel with these API calls and due to race condition between these two, the info\ncache is getting corrupted.\n\n\nI put in a debug statement to print stack track for each cache update and in the following failed case you could see that the \"_heal_instance_info_cache\" periodic task overwrote the info-cache incorrectly and hence corrupted it.\n\n------\naviuser@ubuntu-1204-server:~/devstack$ grep -A 1 \"Updating cache\" /opt/stack/logs/screen/screen-n-cpu.log | grep -A 4 1aea47a5-8677-4a68-aec6-74c8573dde52\n2014-06-03 17:13:27.870 DEBUG nova.network.base_api [req-8a4531a0-1b8c-4c2d-815d-0e126515d446 demo demo] Updating cache with info: [VIF({'ovs_interfaceid': u'9a697bd9-d2fb-4924-8298-b76c8d1c7c4f', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.1.2'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.1.3'}, 'dns': [], 'routes': [], 'cidr': u'192.168.1.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.1.1'})})], 'meta': {'injected': False, 'tenant_id': u'adeb5c20087c42ccb7f4561a7d9cba6e'}, 'id': u'a8159f6d-dc4a-4eab-a6e2-b9d3a626f4f2', 'label': u'testnet1'}), 'devname': u'tap9a697bd9-d2', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:d1:50:3f', 'active': True, 'type': u'ovs', 'id': u'9a697bd9-d2fb-4924-8298-b76c8d1c7c4f', 'qbg_params': None}), VIF({'ovs_interfaceid': u'1aea47a5-8677-4a68-aec6-74c8573dde52', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.2.143'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.2.3'}, 'dns': [], 'routes': [], 'cidr': u'192.168.2.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.2.1'})})], 'meta': {'injected': False, 'tenant_id': u'adeb5c20087c42ccb7f4561a7d9cba6e'}, 'id': u'1f411a5a-4664-4383-a44a-9d83bef7c1ca', 'label': u'testnet2'}), 'devname': u'tap1aea47a5-86', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:12:24:fc', 'active': False, 'type': u'ovs', 'id': u'1aea47a5-8677-4a68-aec6-74c8573dde52', 'qbg_params': None})] from (pid=5206) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/base_api.py:38\n2014-06-03 17:13:27.871 DEBUG nova.network.base_api [req-8a4531a0-1b8c-4c2d-815d-0e126515d446 demo demo] traceback: [('/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py', 194, 'main', 'result = function(*args, **kwargs)'), ('/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py', 129, '<lambda>', 'yield lambda: self._dispatch_and_reply(incoming)'), ('/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py', 134, '_dispatch_and_reply', 'incoming.message))'), ('/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py', 177, '_dispatch', 'return self._do_dispatch(endpoint, method, ctxt, args)'), ('/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py', 123, '_do_dispatch', 'result = getattr(endpoint, method)(ctxt, **new_args)'), ('/opt/stack/nova/nova/compute/manager.py', 401, 'decorated_function', 'return function(self, context, *args, **kwargs)'), ('/opt/stack/nova/nova/compute/manager.py', 4388, 'attach_interface', 'context, instance, port_id, network_id, requested_ip)'), ('/opt/stack/nova/nova/network/neutronv2/api.py', 436, 'allocate_port_for_instance', 'requested_networks=[(network_id, requested_ip, port_id)])'), ('/opt/stack/nova/nova/network/neutronv2/api.py', 360, 'allocate_for_instance', 'port_ids=ports_in_requested_order)'), ('/opt/stack/nova/nova/network/neutronv2/api.py', 471, 'get_instance_nw_info', 'result, update_cells=False)'), ('/opt/stack/nova/nova/network/base_api.py', 39, 'update_instance_cache_with_nw_info', \"LOG.debug('traceback: %s', traceback.extract_stack())\")] from (pid=5206) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/base_api.py:39\n--\n2014-06-03 17:13:27.936 DEBUG nova.network.base_api [-] Updating cache with info: [VIF({'ovs_interfaceid': u'9a697bd9-d2fb-4924-8298-b76c8d1c7c4f', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.1.2'})], 'version': 4, 'meta': {'dhcp_server': u'192.168.1.3'}, 'dns': [], 'routes': [], 'cidr': u'192.168.1.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.1.1'})})], 'meta': {'injected': False, 'tenant_id': u'adeb5c20087c42ccb7f4561a7d9cba6e'}, 'id': u'a8159f6d-dc4a-4eab-a6e2-b9d3a626f4f2', 'label': u'testnet1'}), 'devname': u'tap9a697bd9-d2', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': True, u'ovs_hybrid_plug': True}, 'address': u'fa:16:3e:d1:50:3f', 'active': True, 'type': u'ovs', 'id': u'9a697bd9-d2fb-4924-8298-b76c8d1c7c4f', 'qbg_params': None})] from (pid=5206) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/base_api.py:38\n2014-06-03 17:13:27.937 DEBUG nova.network.base_api [-] traceback: [('/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py', 194, 'main', 'result = function(*args, **kwargs)'), ('/opt/stack/nova/nova/openstack/common/loopingcall.py', 123, '_inner', 'idle = self.f(*self.args, **self.kw)'), ('/opt/stack/nova/nova/service.py', 304, 'periodic_tasks', 'return self.manager.periodic_tasks(ctxt, raise_on_error=raise_on_error)'), ('/opt/stack/nova/nova/manager.py', 81, 'periodic_tasks', 'return self.run_periodic_tasks(context, raise_on_error=raise_on_error)'), ('/opt/stack/nova/nova/openstack/common/periodic_task.py', 175, 'run_periodic_tasks', 'task(self, context)'), ('/opt/stack/nova/nova/compute/manager.py', 4910, '_heal_instance_info_cache', 'self._get_instance_nw_info(context, instance, use_slave=True)'), ('/opt/stack/nova/nova/compute/manager.py', 1142, '_get_instance_nw_info', 'instance)'), ('/opt/stack/nova/nova/network/neutronv2/api.py', 471, 'get_instance_nw_info', 'result, update_cells=False)'), ('/opt/stack/nova/nova/network/base_api.py', 39, 'update_instance_cache_with_nw_info', \"LOG.debug('traceback: %s', traceback.extract_stack())\")] from (pid=5206) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/base_api.py:39\n", 
            "date_created": "2014-06-04 00:40:48.021335+00:00", 
            "author": "https://api.launchpad.net/1.0/~ypraveen-5"
        }, 
        {
            "content": "Once corrupted, the info cache is never getting repaired and hence detach interface fails.\n\nUnfortunately, the code to refresh the instance cache is very hard to follow as the cache\nis getting updated from multiple points of the code in different ways. However, for this\nspecific bug, I could ensure that there is no race condition by putting in a lock in\nthe code that is obtaining the network information and updating the instance cache info.\n\nMy patch to get around this bug is at https://review.openstack.org/#/c/97662/", 
            "date_created": "2014-06-04 00:47:01.570267+00:00", 
            "author": "https://api.launchpad.net/1.0/~ypraveen-5"
        }, 
        {
            "content": "This issue is present in the Icehouse distribution (2014.1). I could not figure out how to add that as an affected version too. Can someone add it or let me know how to add it?", 
            "date_created": "2014-06-04 01:57:29.759364+00:00", 
            "author": "https://api.launchpad.net/1.0/~ypraveen-5"
        }, 
        {
            "content": "This patch is still active, and hopefully is something that should help a bunch of issues.", 
            "date_created": "2014-09-17 19:09:17.769521+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "If there any error message from detach_interface? If the interface detach failed, there should be some message", 
            "date_created": "2014-09-18 02:21:01.503648+00:00", 
            "author": "https://api.launchpad.net/1.0/~xuhj"
        }, 
        {
            "content": "Alex, There was no error message as the detach_interface is an asynchronous call.", 
            "date_created": "2014-09-18 18:07:16.739209+00:00", 
            "author": "https://api.launchpad.net/1.0/~ypraveen-5"
        }, 
        {
            "content": "Looks like bug 1357055 is related.", 
            "date_created": "2014-09-23 16:37:35.098984+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: stable/icehouse\nReview: https://review.openstack.org/123917", 
            "date_created": "2014-09-25 02:34:52.868080+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/97662\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=232cbfe67ffb7696f115830c711a960af5fa0828\nSubmitter: Jenkins\nBranch:    master\n\ncommit 232cbfe67ffb7696f115830c711a960af5fa0828\nAuthor: Praveen Yalagandula <email address hidden>\nDate:   Wed Jun 4 00:42:56 2014 +0000\n\n    Neutron: Atomic update of instance info cache\n    \n    In the Neutron network API implementation, there is a race condition\n    between a thread performing periodic task to read and heal instance\n    network info cache and another thread servicing interface-attach or\n    interface-detach calls. This patch ensures that instance info cache is\n    read and then updated in a synchronized block to ensure atomicity.\n    \n    Please see the bug report for more details.\n    \n    Change-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f\n    Closes-Bug: #1326183\n    Co-Authored-By: Dan Smith <email address hidden>\n", 
            "date_created": "2014-09-25 06:08:55.716499+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/123917\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=dfb0e0fc1604c0673fb9af8316dbf0e5d2c93cd1\nSubmitter: Jenkins\nBranch:    stable/icehouse\n\ncommit dfb0e0fc1604c0673fb9af8316dbf0e5d2c93cd1\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Sep 25 06:48:47 2014 -0700\n\n    Neutron: Atomic update of instance info cache\n    \n    In the Neutron network API implementation, there is a race condition\n    between a thread performing periodic task to read and heal instance\n    network info cache and another thread servicing interface-attach or\n    interface-detach calls. This patch ensures that instance info cache is\n    read and then updated in a synchronized block to ensure atomicity.\n    \n    Please see the bug report for more details.\n    \n    Conflicts:\n            nova/network/base_api.py\n            nova/network/neutronv2/api.py\n            nova/tests/network/test_neutronv2.py\n    \n    NOTE(mriedem): Notes on conflicts:\n    \n    1. base_api didn't exist until Juno so that's where refresh_cache\n       lives now.\n    2. nova.network.neutronv2.api is using oslo.i18n in Juno rather\n       than the old gettextutils import.\n    3. test_neutronv2 didn't start using mock until Juno and the change\n       that added the test class is probably something that we won't\n       be backporting, so the test class definition is brought back\n       with this change for the new test case.\n    \n    Change-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f\n    Closes-Bug: #1326183\n    Co-Authored-By: Dan Smith <email address hidden>\n    (cherry picked from commit 232cbfe67ffb7696f115830c711a960af5fa0828)\n", 
            "date_created": "2014-09-26 18:11:35.367638+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I seem to have found the same problem in Grizzly, is it possible?", 
            "date_created": "2014-10-21 19:42:41.458785+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "I believe this bug is still exists in master branch, the reproduce steps are:\n1) pull latest master branch\n2) add time.sleep(30) to _heal_instance_info_cache, like below:\ndiff --git a/nova/compute/manager.py b/nova/compute/manager.py\nindex f240709..9184e08 100644\n--- a/nova/compute/manager.py\n+++ b/nova/compute/manager.py\n@@ -5358,6 +5358,10 @@ class ComputeManager(manager.Manager):\n                     break\n \n         if instance:\n+            if instance['uuid'] == '61c1159c-3999-4bd5-9029-1757a55bce3d':   ### one of the new instance created in step 4(e.g. instanceB)\n+                LOG.debug(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------xxxxxxxxxxxxx\")\n+                time.sleep(30)\n+                LOG.debug(\"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy-------------------------xxxxxxxxxxxxx\")\n             # We have an instance now to refresh\n             try:\n                 # Call to network API to get instance info.. this will\n3) restart nova-compute\n4) create two new instances(e.g. instanceA & instanceB)\n5) create a new port\n6) attach the new port to instanceB when you see the debug log \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------xxxxxxxxxxxxx\"\n7) after attaching successfully, you can see two ips belong to instanceB through nova list CLI\n8) after you see the debug log \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy-------------------------xxxxxxxxxxxxx\"(may need a few seconds), you can see only ONE ip belong to instanceB through nova list CLI, but you can get two ports belong to instanceB through nova interface-list CLI\n", 
            "date_created": "2014-12-06 01:46:00.617524+00:00", 
            "author": "https://api.launchpad.net/1.0/~hzwangpan"
        }, 
        {
            "content": "this commit https://review.openstack.org/97662 is only add a lock for refresh_cache, but the info_cache contents while periodic task _heal_instance_info_cache running, may be the old dirty one(before attaching), so the new vif/port attached during this periodic task while be lost after this `_heal_instance_info_cache`.", 
            "date_created": "2014-12-06 01:54:00.495537+00:00", 
            "author": "https://api.launchpad.net/1.0/~hzwangpan"
        }, 
        {
            "content": "a new patch is here: https://review.openstack.org/#/c/139900", 
            "date_created": "2014-12-08 08:57:13.713264+00:00", 
            "author": "https://api.launchpad.net/1.0/~hzwangpan"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/164738", 
            "date_created": "2015-03-16 15:31:20.459671+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I'm going to mark this incomplete since I think we need a recreate with this logging to find out what's failing in the neutronv2 API before we can move forward with this, and verify it's still a problem in Kilo.\n\nhttps://review.openstack.org/#/c/164738/", 
            "date_created": "2015-03-16 16:46:38.361170+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/139900\nReason: I'm going to abandon this since we need a new bug filed (the bug was closed against other changes around locking while doing the nw info cache refresh), and we should have a recreate with the logging of the failure here:\n\nhttps://review.openstack.org/#/c/164738/", 
            "date_created": "2015-03-16 16:47:48.136017+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/164738\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=3244063a5cabd76a4651df8c0d8ff496ffc465d4\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3244063a5cabd76a4651df8c0d8ff496ffc465d4\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Mar 16 08:14:40 2015 -0700\n\n    Log exception from deallocate_port_for_instance for triage\n    \n    detach_interface is a cast operation and sometimes\n    NeutronClientExceptions slip through the neutronv2 API, so let's be sure\n    to log any exceptions that come up from the network API method so we can\n    triage them later.\n    \n    Related-Bug: #1326183\n    \n    Change-Id: I1e96128b8a502b32d1e651101d9bfd606ed4855b\n", 
            "date_created": "2015-03-19 16:21:11.523615+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
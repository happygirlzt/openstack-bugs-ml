{
    "status": "Confirmed", 
    "last_updated": "2017-06-23 16:29:42.015112+00:00", 
    "description": "    when nova-compute service is down, delete instance will call _local_delete in nova-api service, which will delete instance from DB,ternminate connection,detach volume and destroy bdm.\n    However,we set connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'} while call ternminate connection, which result an exception, leading the volume status still \"in used\", attached to the instance, but the instance and bdm are deleted in nova db.  all of this  make DB inconsistent state, bdm is deleted in nova, but volume still in use from cinder.\n    Because the nova compute service is down, we can't get the correct connector of host. If we record the connector in bdm while attaching volume, the connector can be get from bdm when local_delete, which will lead success of ,ternminate connection,detach volume and so on.", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 14, 
    "link": "https://bugs.launchpad.net/nova/+bug/1415778", 
    "owner": "None", 
    "id": 1415778, 
    "index": 4138, 
    "created": "2015-01-29 08:24:33.095217+00:00", 
    "title": "_local_delete results  inconsistent volume  state in DB", 
    "comments": [
        {
            "content": "    when nova-compute service is down, delete instance will call _local_delete in nova-api service, which will delete instance from DB,ternminate connection,detach volume and destroy bdm.\n    However,we set connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'} while call ternminate connection, which result an exception, leading the volume status still \"in used\", attached to the instance, but the instance and bdm are deleted in nova db.  all of this  make DB inconsistent state, bdm is deleted in nova, but volume still in use from cinder.\n    Because the nova compute service is down, we can't get the correct connector of host. If we record the connector in bdm while attaching volume, the connector can be get from bdm when local_delete, which will lead success of ,ternminate connection,detach volume and so on.", 
            "date_created": "2015-01-29 08:24:33.095217+00:00", 
            "author": "https://api.launchpad.net/1.0/~binzhou"
        }, 
        {
            "content": "Which version of OpenStack? Are there any stack traces?", 
            "date_created": "2015-02-01 01:33:39.585133+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "version: OpenStack Compute (nova) 2014.1 \"icehouse\"\nstack traces:\n2015-02-02 09:33:10.936 10739 ERROR oslo.messaging.rpc.dispatcher [req-33c22474-43a2-4d27-815c-ef6432ccfe82 4ae3fd1b211349498fbe3aaed423d653 71f90f295e8d40a4b1638049ce07c697 - - -] Exception during message handling: Bad or unexpected response from the storage volume backend API: Unable to terminate volume connection: _map_lun:get map group info fail.group name:HostGroup_R8949317243996453135 with ret code: 16916997\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 133, in _dispatch_and_reply\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 176, in _dispatch\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 122, in _do_dispatch\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib/python2.7/site-packages/cinder/volume/manager.py\", line 999, in terminate_connection\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher     raise exception.VolumeBackendAPIException(data=err_msg)\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to terminate volume connection: _map_lun:get map group info fail.group name:HostGroup_R8949317243996453135 with ret code: 16916997\n2015-02-02 09:33:10.936 10739 TRACE oslo.messaging.rpc.dispatcher \n2015-02-02 09:33:10.937 10739 ERROR oslo.messaging._drivers.common [req-33c22474-43a2-4d27-815c-ef6432ccfe82 4ae3fd1b211349498fbe3aaed423d653 71f90f295e8d40a4b1638049ce07c697 - - -] Returning exception Bad or unexpected response from the storage volume backend API: Unable to terminate volume connection: _map_lun:get map group info fail.group name:HostGroup_R8949317243996453135 with ret code: 16916997 to caller\n2015-02-02 09:33:10.937 10739 ERROR oslo.messaging._drivers.common [req-33c22474-43a2-4d27-815c-ef6432ccfe82 4ae3fd1b211349498fbe3aaed423d653 71f90f295e8d40a4b1638049ce07c697 - - -] ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 133, in _dispatch_and_reply\\n    incoming.message))\\n', '  File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 176, in _dispatch\\n    return self._do_dispatch(endpoint, method, ctxt, args)\\n', '  File \"/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 122, in _do_dispatch\\n    result = getattr(endpoint, method)(ctxt, **new_args)\\n', '  File \"/usr/lib/python2.7/site-packages/cinder/volume/manager.py\", line 999, in terminate_connection\\n    raise exception.VolumeBackendAPIException(data=err_msg)\\n', 'VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to terminate volume connection: _map_lun:get map group info fail.group name:HostGroup_R8949317243996453135 with ret code: 16916997\\n']", 
            "date_created": "2015-02-02 01:30:17.271485+00:00", 
            "author": "https://api.launchpad.net/1.0/~binzhou"
        }, 
        {
            "content": "This bug also appeared in Openstack juno and kilo version.", 
            "date_created": "2015-02-11 04:58:19.988514+00:00", 
            "author": "https://api.launchpad.net/1.0/~binzhou"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2015-04-13 04:17:13.663346+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "Change abandoned by Lee Yarwood (<email address hidden>) on branch: master\nReview: https://review.openstack.org/340951\nReason: https://review.openstack.org/#/c/257853/", 
            "date_created": "2016-08-08 10:15:57.044282+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I request the bug reporter to close this bug as it was fixed in Mitaka version and below is the detailed analysis and the delta between the Kilo and Mitaka version related to the bug.\n\nMy Analysis:\n\n1) Existing Code snippet in Kilo:\n\nIN _local_delete method:      (/opt/stack/nova/nova/compute/api.py)\n\nconnector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'}\n                try:\n                    self.volume_api.terminate_connection(context,bdm.volume_id,connector)\n                    self.volume_api.detach(elevated, bdm.volume_id)\n\n#Here, we are passing the connector ip as 127.0.0.1 to the terminate_connection and detach methods, as this is a loop back ip, it has no effect on the server side. So, the volume was not being detached from the instance and so status is remaining the same(which is not expected).\n\n\n2) Already fixed in Mitaka in the follwoing way:\n\n#A new method was written here to get the connector ip and the fake ip(127.0.0.1) given was removed here.\n\nIN _get_stashed_volume_connector method:   (/opt/stack/nova/nova/compute/api.py)\nconnector = jsonutils.loads(bdm.connection_info).get('connector')\n            if connector:\n                if connector.get('host') == instance.host:\n                    return connector\n\n# Gets the stashed connector dict out of the bdm.connection_info if set and the connector host matches the instance host. This method was called in the _local_cleanup_bdm_volumes method to perform terminate_connection and detach methods.\n\nREFFERED FILES:\n\t\n\t/opt/stack/nova/nova/compute/api.py\n\t/opt/stack/nova/nova/compute/manager.py", 
            "date_created": "2017-01-23 11:32:10.034857+00:00", 
            "author": "https://api.launchpad.net/1.0/~prameela"
        }
    ]
}
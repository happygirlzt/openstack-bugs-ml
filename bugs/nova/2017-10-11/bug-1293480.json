{
    "status": "Fix Released", 
    "last_updated": "2015-11-19 21:49:53.044299+00:00", 
    "description": "1. Libvirt driver can receive libvirt lifecycle events(registered in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L1004), then handle it in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L969 , that means  shutdown a domain  will  send out shutdown lifecycle event and nova compute will try to sync the instance's power_state.\n\n2. When reboot compute service ,  compute service is trying to reboot instance which were running before reboot.\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L911.  Compute service only checks the power_state in database. the value of power_state can be changed in 3.  That leads out  reboot host, some instances which were running before reboot can't be restarted.\n\n3. When reboot the host,  the code path like  1)libvirt-guests will shutdown all the domain,   2)then sendout  lifecycle event , 3)nova compute receive it and 4)save power_state 'shutoff' in db , 5)then try to stop it.   Compute service may be killed in any step,  In my test enviroment,  two running instances , only one instance was restarted succefully. another was set power_state with 'shutoff', task_state  with 'power off' in  step 4) .  So it can't pass the check in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L911. won't be restarted.\n\n\nNot sure this is a bug ,  wonder if there is solution for this .", 
    "tags": [
        "in-stable-juno", 
        "in-stable-kilo", 
        "libvirt"
    ], 
    "importance": "Medium", 
    "heat": 36, 
    "link": "https://bugs.launchpad.net/nova/+bug/1293480", 
    "owner": "https://api.launchpad.net/1.0/~toabctl", 
    "id": 1293480, 
    "index": 3835, 
    "created": "2014-03-17 10:11:02.450633+00:00", 
    "title": "Reboot host  didn't restart instances due to  libvirt lifecycle event change instance's power_stat as shutdown", 
    "comments": [
        {
            "content": "1. Libvirt driver can receive libvirt lifecycle events(registered in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L1004), then handle it in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L969 , that means  shutdown a domain  will  send out shutdown lifecycle event and nova compute will try to sync the instance's power_state.\n\n2. When reboot compute service ,  compute service is trying to reboot instance which were running before reboot.\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L911.  Compute service only checks the power_state in database. the value of power_state can be changed in 3.  That leads out  reboot host, some instances which were running before reboot can't be restarted.\n\n3. When reboot the host,  the code path like  1)libvirt-guests will shutdown all the domain,   2)then sendout  lifecycle event , 3)nova compute receive it and 4)save power_state 'shutoff' in db , 5)then try to stop it.   Compute service may be killed in any step,  In my test enviroment,  two running instances , only one instance was restarted succefully. another was set power_state with 'shutoff', task_state  with 'power off' in  step 4) .  So it can't pass the check in https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L911. won't be restarted.\n\n\nNot sure this is a bug ,  wonder if there is solution for this .", 
            "date_created": "2014-03-17 10:11:02.450633+00:00", 
            "author": "https://api.launchpad.net/1.0/~glongwave"
        }, 
        {
            "content": "I'm not familar with the  process of libvirt event detect/deliver , just think  stop compute service before  reboot host can avoid this .", 
            "date_created": "2014-03-17 11:06:03.686803+00:00", 
            "author": "https://api.launchpad.net/1.0/~glongwave"
        }, 
        {
            "content": "can you please indicate what platform you are using, as well as which versions of libvirt and OpenStack you are using?", 
            "date_created": "2014-04-08 16:01:57.074941+00:00", 
            "author": "https://api.launchpad.net/1.0/~sross-7"
        }, 
        {
            "content": "I met this issue several times. Nova compute maybe sync vm's power_state when rebooting. At the same time, if libvirt already shut down instances, vm_state can be synced to STOPPED.\n        elif vm_state == vm_states.ACTIVE:\n            # The only rational power state should be RUNNING\n            if vm_power_state in (power_state.SHUTDOWN,\n                                  power_state.CRASHED):\n                LOG.warn(_(\"Instance shutdown by itself. Calling \"\n                           \"the stop API.\"), instance=db_instance)\n                try:\n                    # Note(maoy): here we call the API instead of\n                    # brutally updating the vm_state in the database\n                    # to allow all the hooks and checks to be performed.\n                    self.compute_api.stop(context, db_instance)\n After reboot host, instances which started by libvirt can be shut down because of vm_state STOPPED.\n        elif vm_state == vm_states.STOPPED:\n            if vm_power_state not in (power_state.NOSTATE,\n                                      power_state.SHUTDOWN,\n                                      power_state.CRASHED):\n                LOG.warn(_(\"Instance is not stopped. Calling \"\n                           \"the stop API.\"), instance=db_instance)\n                try:\n                    # NOTE(russellb) Force the stop, because normally the\n                    # compute API would not allow an attempt to stop a stopped\n                    # instance.\n                    self.compute_api.force_stop(context, db_instance)\n\nNova version: Nova 2014.1 release\n", 
            "date_created": "2014-07-09 15:23:50.415252+00:00", 
            "author": "https://api.launchpad.net/1.0/~mark-xiett"
        }, 
        {
            "content": "I have the same problem using OpenStack Icehouse and Xen. Seems that a KVM VM doesn't emit lifecycle events during a reboot (reboot started just with \"reboot\" command inside of the VM). That's why a reboot with a KVM VM just works. A VM using Xen emits a VIR_DOMAIN_EVENT_STOPPED event which leads nova to shutdown the VM (see https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L5533 ). The nova-compute.log looks like this:\n\n2014-08-06 21:02:47.628 12222 INFO nova.compute.manager [-] Lifecycle event 1 on VM 6fab54d3-3131-460a-a025-66b937f71255\n2014-08-06 21:02:47.816 12222 WARNING nova.compute.manager [-] [instance: 6fab54d3-3131-460a-a025-66b937f71255] Instance shutdown by itself. Calling the stop API.\n2014-08-06 21:02:48.092 12222 INFO nova.virt.libvirt.driver [-] [instance: 6fab54d3-3131-460a-a025-66b937f71255] Instance destroyed successfully.", 
            "date_created": "2014-08-06 21:07:16.094803+00:00", 
            "author": "https://api.launchpad.net/1.0/~toabctl"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/112946", 
            "date_created": "2014-08-08 15:40:26.356415+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/112946\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=bd8329b34098436d18441a8129f3f20af53c2b91\nSubmitter: Jenkins\nBranch:    master\n\ncommit bd8329b34098436d18441a8129f3f20af53c2b91\nAuthor: Thomas Bechtold <email address hidden>\nDate:   Tue Aug 19 17:41:57 2014 +0200\n\n    Delay STOPPED lifecycle event for Xen domains\n    \n    When using libvirt, a reboot from inside of a kvm VM doesn't trigger\n    any libvirt lifecycle event. That's fine. But rebooting a Xen VM leads\n    to the events VIR_DOMAIN_EVENT_STOPPED and VIR_DOMAIN_EVENT_STARTED.\n    Nova compute manager catches these events and tries to sync the power\n    state of the VM with the power state in the database. In the case the VM\n    state is ACTIVE but the power state is something that doesn't fit, the\n    stop API call is executed to trigger all stop hooks. This leads to the\n    problem that a reboot of a Xen VM without using the API isn't possible.\n    To fix it, delay the emission of the STOPPED lifecycle event a couple of\n    seconds. If a VIR_DOMAIN_EVENT_STARTED event is received while the STOPPED\n    event is pending, cancel the pending STOPPED lifecycle event so the VM\n    can continue to run.\n    \n    Closes-Bug: #1293480\n    Change-Id: I690d3d700ab4d057554350da143ff77d78b509c6\n", 
            "date_created": "2014-10-29 19:30:30.235625+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/icehouse\nReview: https://review.openstack.org/131982", 
            "date_created": "2014-10-30 09:47:20.122563+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/163378", 
            "date_created": "2015-03-11 11:18:08.050177+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "We also have someone reporting this on icehouse and has a recreate in juno with some better logging that shows that during a soft reboot (non-xen domain in this case), the soft reboot is successful and shortly after that (8 seconds) the libvirt driver sends the stopped lifecycle event which, since the instance doesn't have a task_state set, triggers _sync_instance_power_state to call the force_stop() API to power off the instance.  1 second after the stopped event, we see the started event in the libvirt driver but b/c we're already in the process of stopping the instance from the previous event, we're left in shutdown state.", 
            "date_created": "2015-04-11 01:50:58.666327+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Attaching a juno log with a recreate, the instance uuid is 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78.", 
            "date_created": "2015-04-11 01:52:15.106426+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "There was a bug introduced with the original fix, so if we backport the first fix we also need this:\n\nhttps://review.openstack.org/#/c/166184/", 
            "date_created": "2015-04-11 02:13:11.136896+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Opened related bug 1443186 for kvm.", 
            "date_created": "2015-04-12 22:30:56.580810+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Hi, I hotfixed my Juno environment with latest code change in https://review.openstack.org/163378, but the issue can still be able to be reproduced.", 
            "date_created": "2015-04-14 15:31:47.992059+00:00", 
            "author": "https://api.launchpad.net/1.0/~bjzyang"
        }, 
        {
            "content": "@Yang, it didn't fix the problem because the patches for xen are (1) not handling kvm and (2) it's not a host reboot case, so the compute service is running when libvirt is sending lifecycle events.  I have a patch for kvm here:\n\nhttps://review.openstack.org/#/c/172775/\n\nThe original problem reported in this bug is when the compute host is being rebooted from under nova, so libvirt and nova are racing to shutdown and libvirt is sending events when shutting down guests and nova is trying to handle them, which races with the compute service itself going down, so the instances can be left in an inconsistent state and init_host might not clean them up properly on host reboot.\n\nOne issue here is I think we need to stop handling lifecycle events from the driver when the compute service (host) is going down, just like how we don't handle incoming neutron events on compute service shutdown.", 
            "date_created": "2015-04-15 16:12:58.044824+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reported bug 1444630 for comment 14.", 
            "date_created": "2015-04-15 18:14:31.223333+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/174069", 
            "date_created": "2015-04-15 20:00:15.371798+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/174069\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0\nSubmitter: Jenkins\nBranch:    master\n\ncommit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n", 
            "date_created": "2015-04-16 09:26:33.062427+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/174477", 
            "date_created": "2015-04-16 17:33:03.444773+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/163378\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=f60214b4fdbfb56c8428808a29b9cf911c0b1aa2\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit f60214b4fdbfb56c8428808a29b9cf911c0b1aa2\nAuthor: Thomas Bechtold <email address hidden>\nDate:   Tue Aug 19 17:41:57 2014 +0200\n\n    Delay STOPPED lifecycle event for Xen domains\n    \n    When using libvirt, a reboot from inside of a kvm VM doesn't trigger\n    any libvirt lifecycle event. That's fine. But rebooting a Xen VM leads\n    to the events VIR_DOMAIN_EVENT_STOPPED and VIR_DOMAIN_EVENT_STARTED.\n    Nova compute manager catches these events and tries to sync the power\n    state of the VM with the power state in the database. In the case the VM\n    state is ACTIVE but the power state is something that doesn't fit, the\n    stop API call is executed to trigger all stop hooks. This leads to the\n    problem that a reboot of a Xen VM without using the API isn't possible.\n    To fix it, delay the emission of the STOPPED lifecycle event a couple of\n    seconds. If a VIR_DOMAIN_EVENT_STARTED event is received while the STOPPED\n    event is pending, cancel the pending STOPPED lifecycle event so the VM\n    can continue to run.\n    \n    Closes-Bug: #1293480\n    (cherry picked from commit bd8329b34098436d18441a8129f3f20af53c2b91)\n    \n    ----\n    NOTE(mriedem): The fix for bug 1293480 introduced bug 1433049 so we\n    have to backport both together, hence the squashed commits.\n    ----\n    \n    libvirt: Delay only STOPPED event for Xen domain.\n    \n    This fix change bd8329b34098436d18441a8129f3f20af53c2b91 (Delay STOPPED\n    lifecycle event for Xen domains)\n    \n    Without this patch, a STOPPED event could be ignore if it was following a\n    STARTED event.\n    \n    A scenario that have the issue on tempest is\n    ServerActionsTestJSON:test_resize_server_confirm_from_stopped, and it\n    happens as follow:\n    - instance is stopped\n    nova start instance\n    - libvirt STARTED event received and delayed\n    nova stop instance\n    - libvirt STOPPED event received and ignored as there is a delayed event\n    nova resize instance 42\n    - resize finished\n    - the delayed STARTED event is emited\n    nova confirme-resize instance\n    nova show instance\n    - instance is show as ACTIVE, but should be SHUTOFF\n    \n    Also fix unit tests.\n    \n    Conflicts:\n            nova/tests/unit/virt/libvirt/test_host.py\n            nova/virt/libvirt/host.py\n    \n    NOTE(mriedem): The conflicts are due to the code being moved to the\n    nova.virt.libvirt.host module in Kilo.\n    \n    Closes-Bug: #1433049\n    Change-Id: If340f9b849b930c34238c5681018a29bc826798d\n    (cherry picked from commit b5a9c4e4d04d011c59fca5306be651906792f411)\n    \n    --\n    \n    Change-Id: I690d3d700ab4d057554350da143ff77d78b509c6\n", 
            "date_created": "2015-04-21 01:22:27.557321+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/174477\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=b19764d2c6a8160102a806c1d6811c4182a8bac8\nSubmitter: Jenkins\nBranch:    stable/kilo\n\ncommit b19764d2c6a8160102a806c1d6811c4182a8bac8\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n", 
            "date_created": "2015-04-21 11:17:46.653212+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/159275\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d09785b97a282e8538642f6f8bcdd8491197ed74\nSubmitter: Jenkins\nBranch:    master\n\ncommit d09785b97a282e8538642f6f8bcdd8491197ed74\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Feb 25 14:13:45 2015 -0800\n\n    Add config option to disable handling virt lifecycle events\n    \n    Historically the _sync_power_states periodic task has had the potential\n    for race conditions and several changes have been made to try and\n    tighten up this code:\n    \n    cc5388bbe81aba635fb757e202d860aeed98f3e8\n    aa1792eb4c1d10e9a192142ce7e20d37871d916a\n    baabab45e0ae0e9e35872cae77eb04bdb5ee0545\n    bd8329b34098436d18441a8129f3f20af53c2b91\n    \n    The handle_lifecycle_events method which gets power state change events\n    from the compute driver (currently only implemented by the libvirt\n    driver) and calls _sync_instance_power_state - the same method that the\n    _sync_power_states periodic task uses, except the periodic task at least\n    locks when it's running - expands the scope for race problems in the\n    compute manager so cloud providers should be able to turn it off. It is\n    also known to have races with reboot where rebooted instances are\n    automatically shutdown because of delayed lifecycle events that the\n    instance is stopped even though it's running.\n    \n    This is consistent with the view that Nova should manage it's own state\n    and not rely on external events telling it what to do about state\n    changes. For example, in _sync_instance_power_state, if the Nova\n    database thinks an instance is stopped but the hypervisor says it's\n    running, the compute manager issues a force-stop on the instance.\n    \n    Also, although not documented (at least from what I can find), Nova has\n    historically held a stance that it does not support out-of-band\n    discovery and management of instances, so allowing external events to\n    change state somewhat contradicts that stance and should be at least a\n    configurable deployment option.\n    \n    DocImpact: New config option \"handle_virt_lifecycle_events\" in the\n               DEFAULT group of nova.conf. By default the value is True\n               so there is no upgrade impact or change in functionality.\n    \n    Related-Bug: #1293480\n    Partial-Bug: #1443186\n    Partial-Bug: #1444630\n    \n    Change-Id: I26a1bc70939fb40dc38e9c5c43bf58ed1378bcc7\n", 
            "date_created": "2015-04-29 12:39:31.099360+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/179284", 
            "date_created": "2015-04-30 23:18:53.282748+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I'm finding that the delay patch is ineffective for larger VMs. On my servers, when the VM has more than about 16G of memory, it can take more than 15 seconds for the new one to start up on reboot.\n\nTurning off lifecycle event management doesn't work either ... because then if the VM is shutdown from inside (user types \"shutdown\", \"halt\", etc), nova doesn't know, and thinks it's still up and active.\n\nStill not sure what the right solution is .... maybe to not try and destroy the VM when it has already shutdown by itself... ?\n", 
            "date_created": "2015-05-04 15:21:11.101206+00:00", 
            "author": "https://api.launchpad.net/1.0/~dseven"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/179284\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=5228d4e418734164ffa5ccd91d2865d9cc659c00\nSubmitter: Jenkins\nBranch:    master\n\ncommit 906ab9d6522b3559b4ad36d40dec3af20397f223\nAuthor: He Jie Xu <email address hidden>\nDate:   Thu Apr 16 07:09:34 2015 +0800\n\n    Update rpc version aliases for kilo\n    \n    Update all of the rpc client API classes to include a version alias\n    for the latest version implemented in Kilo.  This alias is needed when\n    doing rolling upgrades from Kilo to Liberty.  With this in place, you can\n    ensure all services only send messages that both Kilo and Liberty will\n    understand.\n    \n    Closes-Bug: #1444745\n    \n    Conflicts:\n    \tnova/conductor/rpcapi.py\n    \n    NOTE(alex_xu): The conflict is due to there are some logs already added\n    into the master.\n    \n    Change-Id: I2952aec9aae747639aa519af55fb5fa25b8f3ab4\n    (cherry picked from commit 78a8b5802ca148dcf37c5651f75f2126d261266e)\n\ncommit f191a2147a21c7e50926b288768a96900cf4c629\nAuthor: Hans Lindgren <email address hidden>\nDate:   Fri Apr 24 13:10:39 2015 +0200\n\n    Add security group calls missing from latest compute rpc api version bump\n    \n    The recent compute rpc api version bump missed out on the security group\n    related calls that are part of the api.\n    \n    One possible reason is that both compute and security group client side\n    rpc api:s share a single target, which is of little value and only cause\n    mistakes like this.\n    \n    This change eliminates future problems like this by combining them into\n    one to get a 1:1 relationship between client and server api:s.\n    \n    Change-Id: I9207592a87fab862c04d210450cbac47af6a3fd7\n    Closes-Bug: #1448075\n    (cherry picked from commit bebd00b117c68097203adc2e56e972d74254fc59)\n\ncommit a2872a9262985bd0ee2c6df4f7593947e0516406\nAuthor: Dan Smith <email address hidden>\nDate:   Wed Apr 22 09:02:03 2015 -0700\n\n    Fix migrate_flavor_data() to catch instances with no instance_extra rows\n    \n    The way the query was being performed previously, we would not see any\n    instances that didn't have a row in instance_extra. This could happen if\n    an instance hasn't been touched for several releases, or if the data\n    set is old.\n    \n    The fix is a simple change to use outerjoin instead of join. This patch\n    includes a test that ensures that instances with no instance_extra rows\n    are included in the migration. If we query an instance without such a\n    row, we create it before doing a save on the instance.\n    \n    Closes-Bug: #1447132\n    Change-Id: I2620a8a4338f5c493350f26cdba3e41f3cb28de7\n    (cherry picked from commit 92714accc49e85579f406de10ef8b3b510277037)\n\ncommit e3a7b83834d1ae2064094e9613df75e3b07d77cd\nAuthor: OpenStack Proposal Bot <email address hidden>\nDate:   Thu Apr 23 02:18:41 2015 +0000\n\n    Updated from global requirements\n    \n    Change-Id: I5d4acd36329fe2dccb5772fed3ec55b442597150\n\ncommit 8c9b5e620eef3233677b64cd234ed2551e6aa182\nAuthor: Divya <email address hidden>\nDate:   Tue Apr 21 08:26:29 2015 +0200\n\n    Control create/delete flavor api permissions using policy.json\n    \n    The permissions of create/delete flavor api is currently broken\n    and expects the user to be always an admin, instead of controlling\n    the permissions by the rules defined in the nova policy.json.\n    \n    Change-Id: Ide3c9ec2fa674b4fe3ea9d935cd4f7848914b82e\n    Closes-Bug: 1445335\n    (cherry picked from commit ced60b1d1b1608dc8229741b207a95498bc0b212)\n\ncommit bf79742d26ae66886bcdc55eeaf27e1d7ce24be5\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Tue Apr 14 16:28:57 2015 +0100\n\n    Fix handling of pci_requests in consume_from_instance.\n    \n    Properly retrieve requests from pci_requests in consume_from_instance.\n    Without this the call to numa_fit_instance_to_host will fail because\n    it expects the request list.\n    And change the order in which apply_requests and numa_fit_instance_to_host\n    are called. Calling apply_requests first will remove devices from pools\n    and  may make numa_fit_instance_to_host fail.\n    \n    Change-Id: I41cf4e8e5c1dea5f91e5261a8f5e88f46c7994ef\n    Closes-bug: #1444021\n    (cherry picked from commit 0913e799e9ce3138235f5ea6f80159f468ad2aaa)\n\ncommit c2d7060b480608d9773340f51d6496fadf97b667\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Thu Apr 16 17:10:06 2015 +0100\n\n    Use list of requests in InstancePCIRequests.obj_from_db.\n    \n    InstancePCIRequests.obj_from_db assumes it's called with with a dict\n    of values from instances_extra table, but in some cases it's called\n    with just the value of pci_requests column.\n    This changes obj_from_db to be used with just the value of pci_requests column.\n    \n    Change-Id: I7bed733c845c365081719a70b8a2f0cc9a58370c\n    Closes-bug: #1445040\n    (cherry picked from commit a074d7b4465b45730a5171e024c5c39a66a9c927)\n\ncommit 7a609f153808f7cee1edbbb36accc292fa8df0d0\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Tue Apr 7 16:31:05 2015 +0100\n\n    Add numa_node field to PciDevicePool\n    \n    Without this field, PciDevicePool.from_dict will treat numa_node key in\n    the dict as a tag, which in turn means that the scheduler client will\n    drop it when converting stats to objects before reporting.\n    \n    Converting it back to dicts on the scheduler side thus will not have\n    access to the numa_node information which would cause any requests that\n    will look for the exact match between the device and instance NUMA nodes\n    in the NUMATopologyFilter to fail.\n    \n    Closes-Bug: #1441169\n    (cherry picked from commit 7db1ebc66c59205f78829d1e9cd10dcc1201d798)\n    \n    Conflicts:\n    \tnova/tests/unit/objects/test_objects.py\n    \n    Change-Id: I7381f909620e8e787178c0be9a362f8d3eb9ff7d\n\ncommit 880a356e40d327c0af4ce94b5a08fe0cd6fcab5d\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Tue Apr 7 20:53:32 2015 +0100\n\n    scheduler: re-calculate NUMA on consume_from_instance\n    \n    This patch narrows down the race window between the filter running and\n    the consumption of resources from the instance after the host has been\n    chosen.\n    \n    It does so by re-calculating the fitted NUMA topology just before consuming it\n    from the chosen host. Thus we avoid any locking, but also make sure that\n    the host_state is kept as up to date as possible for concurrent\n    requests, as there is no opportunity for switching threads inside a\n    consume_from_instance.\n    \n    Several things worth noting:\n      * Scheduler being lock free (and thus racy) does not really affect\n      resources other than PCI and NUMA topology this badly - this is due\n      to complexity of said resources. In order for scheduler decesions to not\n      be based on basically guessing, in case of those two we will likely need\n      to introduce either locking or special heuristics.\n    \n      * There is a lot of repeated code between the 'consume_from_instance'\n      method and the actual filters. This situation should really be fixed but\n      is out of scope for this bug fix (which is about preventing valid\n      requests failing because of races in the scheduler).\n    \n    Change-Id: If0c7ad20506c9dddf4dec1eb64c9d6dd4fb75633\n    Closes-bug: #1438238\n    (cherry picked from commit d6b3156a6c89ddff9b149452df34c4b32c50b6c3)\n\ncommit a4e9a146c3993f5775501716a21632f34a63a3ad\nAuthor: Rajesh Tailor <email address hidden>\nDate:   Wed Apr 15 06:59:04 2015 -0700\n\n    Fix kwargs['migration'] KeyError in @errors_out_migration decorator\n    \n    @errors_out_migration decorator is used in the compute manager on\n    resize_instance and finish_resize methods of ComputeManager class.\n    It is decorated via @utils.expects_func_args('migration') to check\n    'migration' is a parameter to the decorator method, however, that\n    only ensures there is a migration argument, not that it's in args or\n    kwargs (either is fine for what expects_func_args checks).\n    The errors_out_migration decorator can get a KeyError when checking\n    kwargs['migration'] and fails to set the migration status to 'error'.\n    \n    This fixes the KeyError in the decorator by normalizing the args/kwargs\n    list into a single dict that we can pull the migration from.\n    \n    Change-Id: I774ac9b749b21085f4fbcafa4965a78d68eec9c7\n    Closes-Bug: 1444300\n    (cherry picked from commit 3add7923fc16c050d4cfaef98a87886c6b6a589c)\n\ncommit 389368bcfe498323b369f68682babb92a5b0ca54\nAuthor: Gary Kotton <email address hidden>\nDate:   Wed Apr 15 05:14:42 2015 -0700\n\n    Resource tracker: unable to restart nova compute\n    \n    The resource tracker calculates its used resources. In certain cases\n    of failed migrations and an instance being deleted the resource tracker\n    causes an exception in nova compute. If this situation arises then nova\n    compute may not even be able to restart.\n    \n    Change-Id: I4a154e0cae3b8e22bd59ed05ba708e07eed8dea7\n    Closes-bug: #1444439\n    (cherry picked from commit ee7a7446cc6947a6bacacb6cb514934cc22e5782)\n\ncommit bd6a40fecde943a3ded0124481a12c27dbb167de\nAuthor: Andreas Jaeger <email address hidden>\nDate:   Mon Apr 20 11:01:22 2015 +0200\n\n    Release Import of Translations from Transifex\n    \n    Manual import of Translations from Transifex. This change also removes\n    all po files that are less than 66 per cent translated since such\n    partially translated files will not help users.\n    \n    This updates also recreates all pot (translation source files) to\n    reflect the state of the repository.\n    \n    This change needs to be done manually since the automatic import does\n    not handle the proposed branches and we need to sync with latest\n    translations.\n    \n    Change-Id: I0e9ef00182a2229602d23b8a67a02f0be62ee239\n\ncommit 8ebd515aa94ed399074a3b55bd36fd8cd579a499\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Apr 16 11:08:50 2015 -0700\n\n    Use kwargs from compute v4 proxy change_instance_metadata\n    \n    The args were passed to the compute manager method in the wrong order.\n    We noticed this in the gate with KeyError: 'uuid' in the logs because of\n    the LOG.debug statement in change_instance_metadata. Just use kwargs\n    like rpcapi would normally.\n    \n    There isn't a unit test for this since the v4 proxy code goes away in\n    liberty, this is for getting it into stable/kilo.\n    \n    Closes-Bug: #1444728\n    \n    Change-Id: Ic988f48d99e626ee5773c97904e09dbf00c5414a\n    (cherry picked from commit e55f746ea8590cce7c2b07a023197f369251a7ef)\n\ncommit b19764d2c6a8160102a806c1d6811c4182a8bac8\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n\ncommit 75e9de5d572578520c217b540aa2a40726f137f0\nAuthor: Roman Podoliaka <email address hidden>\nDate:   Wed Mar 4 17:27:06 2015 +0200\n\n    Forbid booting of QCOW2 images with virtual_size > root_gb\n    \n    Currently, it's possible to boot an instance from a QCOW2 image,\n    which has virtual_size bigger than one allowed by the given flavor\n    (root_gb).\n    \n    The issue is caused by two different problems in the code:\n    \n    1) typo in get_disk_size() has made it always return None and\n       effectively disabled verify_base_size() checks\n    \n    2) Rbd image backend skips the verify_base_size() step for\n       'cached' images (the one with base files), so it is possible to\n       boot an instance using a larger flavor once and then use smaller\n       flavors to boot the same image, even if allowed root_gb size is\n       smaller than the image virtual size\n    \n    Closes-Bug: #1429093\n    \n    Change-Id: I383130e5f8cc288f4b428ed43fe4d3aba7169473\n    (cherry picked from commit c1f9ed27af64e6893d9d0153a964df5aba99b8f0)\n\ncommit 33ba90240a2ad3165274d9e54ceb156273404c9a\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Apr 13 14:47:20 2015 -0700\n\n    Pass migrate_data to pre_live_migration\n    \n    Commit ebfa09fa197a1d88d1b3ab1f308232c3df7dc009 added an RPC proxy but\n    as part of that was passing migrate_data=None for pre_live_migration\n    which breaks live block migration when not using shared storage.\n    \n    Closes-Bug: #984996\n    \n    Change-Id: I2a83f1fb0e4468f9a6c67a188af725c3406139d1\n    (cherry picked from commit 4e515ec2269a1c3187ee9ffad3a6be059ec74b0b)\n\ncommit dea6116723f22632c2e478e00bb0aafcd2febdc9\nAuthor: Timofey Durakov <email address hidden>\nDate:   Fri Apr 10 19:38:33 2015 +0300\n\n    Fixed order of arguments during execution live_migrate()\n    \n    order of arguments that passed to\n    ComputeManager.live_,migration() differs in ComputeManager and\n    _ComputeV4Proxy classes\n    \n    Change-Id: I23c25d219e9cdd0673ae6a12250219680fb7bda9\n    Closes-Bug:#1442656\n    (cherry picked from commit ba521fa53711774e0718808fe333aca676de57ae)\n\ncommit 22d7547c6b62fb9dabd861e4941edd34eedabfc6\nAuthor: Doug Hellmann <email address hidden>\nDate:   Wed Apr 15 19:58:17 2015 +0000\n\n    update .gitreview for stable/kilo\n    \n    Change-Id: I6356513ac42b79402dbde8ee5e75cbbd1aee7eef\n\ncommit 68d6f924037f3b931add2ce5d0d433913e720ca6\nAuthor: Ken'ichi Ohmichi <email address hidden>\nDate:   Wed Apr 15 03:13:43 2015 +0000\n\n    Add min/max of API microversions to version API\n    \n    As nova-spec api-microversions, versions API needs to expose minimum\n    and maximum microversions to version API, because clients need to\n    know available microversions through the API. That is very important\n    for the interoperability.\n    This patch adds these versions as the nova-spec mentioned.\n    \n    Note:\n      As v2(not v2.1) API change manner, we have added new extensions if\n      changing API. However, this patch doesn't add a new extension even\n      if adding new parameters \"version\" and \"min_version\" because version\n      API is independent from both v2 and v2.1 APIs.\n    \n    Change-Id: Id464a07d624d0e228fe0aa66a04c8e51f292ba0c\n    Closes-Bug: #1443375\n    (cherry picked from commit 1830870718fe7472b47037f3331cfe59b5bdda07)\n    (cherry picked from commit 853671e912c6ad9a4605acad2575417911875cdd)\n\ncommit d82d492c67db546d01addc5fead9708760fb6abd\nAuthor: Sabari Kumar Murugesan <email address hidden>\nDate:   Mon Apr 6 20:36:48 2015 -0700\n\n    VMware: Fix attribute error in resize\n    \n    The class DatastorePath was recently removed from ds_util as it's\n    available in oslo.vmware. One of the reference was missed during\n    the refactor.\n    \n    Change-Id: Idc5825c304a99e83cbf36e93751148d6f995131a\n    Closes-Bug: #1440968\n    (cherry picked from commit ab4a5a5300179a79f7a67688f0e9f3fc280c0efa)\n\ncommit 3cff2c673c6cdf487c2a1eb2a5c6c89c6de80d11\nAuthor: jichenjc <email address hidden>\nDate:   Fri Mar 20 08:36:37 2015 +0800\n\n    Release bdm constraint source and dest type\n    \n    https://bugs.launchpad.net/nova/+bug/1377958 fixed a problem\n    that source_type: image, destination_type: local is not\n    supported for boot instance, exception should be raised to\n    reject the param otherwise it will lead to instance become\n    ERROR state.\n    \n    However the fix introduced a problem on nova client\n    https://bugs.launchpad.net/python-novaclient/+bug/1418484\n    The fix of the bug leads to following command become invalid\n    \n    nova boot test-vm --flavor m1.medium --image centos-vm-32\n    --nic net-id=c3f40e33-d535-4217-916b-1450b8cd3987 --block-device\n    id=26b7b917-2794-452a-95e5-2efb2ca6e32d,bus=sata,source=volume,bootindex=1\n    \n    So we need to release the original constraint to allow\n    the above special case pass the validation check then\n    we can revert the nova client exception\n    (https://review.openstack.org/#/c/165932/)\n    \n    This patch checks the boot_index and whether image param is\n    given after we found the bdm has source_type: image,\n    destination_type: local, if this is the special case, then\n    no exception will be raised.\n    \n    Closes-Bug: #1433609\n    \n    Change-Id: If43faae95169bc3864449a8364975f5c887aac14\n    (cherry picked from commit cadbcc440a2fcfb8532f38111999a06557fbafc2)\n\ncommit 97145ba175291d6522afa079860c72220e43024a\nAuthor: Dan Smith <email address hidden>\nDate:   Fri Apr 10 07:10:52 2015 -0700\n\n    Fix check_can_live_migrate_destination() in ComputeV4Proxy\n    \n    There was a mismatch in the V4 proxy in the call signatures of this\n    function. This was missed because the \"destination\" parameter is passed\n    in the rpcapi as the host to contact, which is consumed by the rpc\n    layer and not passed. Since it was not called one of the standard\n    names (either host if to be not passed, or host_param if to be passed),\n    this was missed.\n    \n    Change-Id: Idf2160934dade650ed02b672f3b64cb26247f8e6\n    Closes-Bug: #1442602\n    (cherry picked from commit 0c08f7f2ef070f7c6172d7742f9789e0a8bda91a)\n", 
            "date_created": "2015-05-06 13:44:55.644125+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: stable/icehouse\nReview: https://review.openstack.org/131982", 
            "date_created": "2015-05-28 20:29:48.165249+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: stable/juno\nReview: https://review.openstack.org/192244", 
            "date_created": "2015-06-16 15:08:51.778983+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/192244\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=7bc4be781564c6b9e7a519aecea84ddbee6bd935\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit 7bc4be781564c6b9e7a519aecea84ddbee6bd935\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Conflicts:\n    \tnova/compute/manager.py\n    \tnova/tests/unit/compute/test_compute_mgr.py\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n", 
            "date_created": "2015-06-18 01:51:17.256532+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
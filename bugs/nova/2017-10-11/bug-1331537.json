{
    "status": "Expired", 
    "last_updated": "2016-07-05 09:49:31.798642+00:00", 
    "description": "Nova compute services in Openstack Havana go down frequently as listed by \"nova service-list\" and requires to be restarted very frequently, multiple times every day. All the compute nodes have the ntp times in sync.\n\nWhen a node shows down, it is not able to use those compute nodes for launching new VMs and we quickly run out of compute resources. Hence our workaround is to restart the Compute nodes on those servers hourly.\n\nIn the nova-compute node I've found the following error and they did match with the \"Updated_at\" field from nova service-list.\n2014-06-07 00:21:15.690 511340 ERROR nova.servicegroup.drivers.db [-] model server went away\n2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db Traceback (most recent call last):\n2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db File \"/usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py\", l ine 92, in _report_state\n5804 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db report_count = service.service_ref['report_count'] + 1\n5805 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db TypeError: 'NoneType' object has no attribute '__getitem__'\n5806 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db\n\nIt looks like the ones that are shown as down haven't been able to update the database with the latest status and they did match with the Traceback seen above (2014-06-07 00:21:15.690) on at least two compute nodes that I have seen.\n+------------------+------------------------+--------------+----------+-------+----------------------------+-----------------+\n| Binary | Host | Zone | Status | State | Updated_at | Disabled Reason |\n+------------------+------------------------+--------------+----------+-------+----------------------------+-----------------+\n| nova-compute | nova1| blabla | enabled | up | 2014-06-07T00:37:42.000000 | None |\n| nova-compute | nova2 | blabla | enabled | down | 2014-06-07T00:21:05.000000 | None |", 
    "tags": [
        "compute", 
        "conductor"
    ], 
    "importance": "Undecided", 
    "heat": 66, 
    "link": "https://bugs.launchpad.net/nova/+bug/1331537", 
    "owner": "None", 
    "id": 1331537, 
    "index": 4931, 
    "created": "2014-06-18 16:00:37.804805+00:00", 
    "title": "nova service-list shows nova-compute as down and is required to be restarted frequently in order to provision new vms", 
    "comments": [
        {
            "content": "Nova compute services in Openstack Havana go down frequently as listed by \"nova service-list\" and requires to be restarted very frequently, multiple times every day. All the compute nodes have the ntp times in sync.\n\nWhen a node shows down, it is not able to use those compute nodes for launching new VMs and we quickly run out of compute resources. Hence our workaround is to restart the Compute nodes on those servers hourly.\n\nIn the nova-compute node I've found the following error and they did match with the \"Updated_at\" field from nova service-list.\n2014-06-07 00:21:15.690 511340 ERROR nova.servicegroup.drivers.db [-] model server went away\n2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db Traceback (most recent call last):\n2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db File \"/usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py\", l ine 92, in _report_state\n5804 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db report_count = service.service_ref['report_count'] + 1\n5805 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db TypeError: 'NoneType' object has no attribute '__getitem__'\n5806 2014-06-07 00:21:15.690 511340 TRACE nova.servicegroup.drivers.db\n\nIt looks like the ones that are shown as down haven't been able to update the database with the latest status and they did match with the Traceback seen above (2014-06-07 00:21:15.690) on at least two compute nodes that I have seen.\n+------------------+------------------------+--------------+----------+-------+----------------------------+-----------------+\n| Binary | Host | Zone | Status | State | Updated_at | Disabled Reason |\n+------------------+------------------------+--------------+----------+-------+----------------------------+-----------------+\n| nova-compute | nova1| blabla | enabled | up | 2014-06-07T00:37:42.000000 | None |\n| nova-compute | nova2 | blabla | enabled | down | 2014-06-07T00:21:05.000000 | None |", 
            "date_created": "2014-06-18 16:00:37.804805+00:00", 
            "author": "https://api.launchpad.net/1.0/~lborda"
        }, 
        {
            "content": "We've seen this issue recently. By viewing conductor's log, the  \"service_update\" message is received and there is no exceptions related to db in the log. The periodic task for doing service_update is not invoked  again until nova-compute is restarted.", 
            "date_created": "2014-07-10 09:51:52.083755+00:00", 
            "author": "https://api.launchpad.net/1.0/~zapchang"
        }, 
        {
            "content": "Setting these\nservice_down_time=60\nreport_interval=10\nwhich are the default values to nova.conf [DEFAULT] session, seems to solve the problem\nwhile i had to do 3-4 times nova-compute restart, now nova-compute is registered for 3-4 days without a problem", 
            "date_created": "2014-07-28 07:50:36.922997+00:00", 
            "author": "https://api.launchpad.net/1.0/~gkissand"
        }, 
        {
            "content": "Is anyone still experiencing this bug? The message \"model server went away\" indicates a connection problem with the database (message queue problem in your environment?). \r\n\r\nZap and George saw issues where there is no \"model server went away\" error message but instead the periodic task doesn't fire when it's supposed to. George's observation is strange given that setting default values in nova.conf fixes the problem? That doesn't make sense as those values are anyway set in the code that way, without requiring external configuration:\r\n\r\nhttps://github.com/openstack/nova/blob/65135e7/nova/service.py#L44\r\nhttps://github.com/openstack/nova/blob/65135e7/nova/service.py#L103", 
            "date_created": "2014-09-10 17:37:24.068162+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/122471", 
            "date_created": "2014-09-18 17:18:49.127741+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "There are 1450 hits of this in 7 days in the check/gate queues:\n\nmessage:\"model server went away\" AND tags:\"screen-n-cpu.txt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwibW9kZWwgc2VydmVyIHdlbnQgYXdheVwiIEFORCB0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDExMjMwMjYxNjI2LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9\n\nThere are only 5% of failing jobs when this shows up, so it's not causing us huge issues in the gate apparently.  I'd bet if we look at those failures there are other things failing due to messaging timeouts which are waiting on some response, like bug 1371677.", 
            "date_created": "2014-09-20 16:29:35.755737+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I used to encounter this problem one time.  Because no exact way to recreate it, I did not find out why conductor_api.service_update() returned None.", 
            "date_created": "2014-09-22 08:32:16.462883+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhaoqin"
        }, 
        {
            "content": "Which RPC backend is being used?", 
            "date_created": "2014-09-22 15:47:24.616521+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Is cells being used?", 
            "date_created": "2014-09-22 15:53:16.517581+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This has 0 hits:\n\nmessage:\"model server went away\" AND NOT message:\"Timeout\" AND tags:\"screen-n-cpu.txt\"\n\nWhich means every time we hit 'model server went away' in our Jenkins runs it's due to a messaging timeout.\n\nI wouldn't be surprised if there was a bug in an old kombu or qpid client that was swallowing something on an RPC call and returning None, but we'd need an example of that (log output), and it'd need to be reported against the offending library.", 
            "date_created": "2014-09-22 15:58:49.065852+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Marking as Incomplete because we need more info here.", 
            "date_created": "2014-09-22 15:59:16.843917+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Hi Matt,\n\nRabbitMQ is being used.", 
            "date_created": "2014-10-06 11:54:03.350971+00:00", 
            "author": "https://api.launchpad.net/1.0/~lborda"
        }, 
        {
            "content": "@Zhao qin\n     \nI worked on other commit and some one suggest to think about using cast instead of call for RPC here because \nthe cast is fine to this timely update function \nhttps://review.openstack.org/#/c/127876/5\n\nI knew this bug before and really don't know why it returns none ,it doesn't make sense from code analysis\n\nmaybe cast make sense as a workaround ? Thanks ", 
            "date_created": "2014-10-24 05:49:45.999192+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/122471\nReason: This review is > 4 weeks without comment and currently blocked by a core reviewer with a -2. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and contacting the reviewer with the -2 on this review to ensure you address their concerns.", 
            "date_created": "2014-11-20 15:18:21.293027+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "We also encountered this issue occasionally recently, and our code base is grizzely.\n\nAfter digging into some rpc related code, I noticed that there are at least 2 amqp message sent for returning the result of a rpc.call, one for the actual result, and the other for marking the ending.\nFor someone interested, the code is here: https://github.com/openstack/nova/blob/grizzly-eol/nova/openstack/common/rpc/amqp.py#L419\n\nI wonder if this issue is caused by the lost of the actual result  or the misorder  of the two messages.", 
            "date_created": "2015-02-28 07:27:48.027048+00:00", 
            "author": "https://api.launchpad.net/1.0/~zlji"
        }, 
        {
            "content": "https://bugs.launchpad.net/bugs/1407936 \nVladimir Eremin also reported occurrance of this issue", 
            "date_created": "2015-03-03 20:07:57.530487+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "After some experiments with rabbitmq and nova-conductor rpcapi, I noticed that this issue is caused by disorder of reply messages.\n\nAcoording to the code here: https://github.com/openstack/nova/blob/grizzly-eol/nova/openstack/common/rpc/amqp.py#L419, a service  have to send at least 2 rabbitmq message to compose a rpc.call response, and each message is sent by calling msg_reply.The problem here is, the connections used for sending these two messages may be different, and in such circumstance, message ordering won't be guaranteed according to the AMQP specification: 'Section 4.7 of the AMQP 0-9-1 core specification explains the conditions under which ordering is guaranteed: messages published in one channel, passing through one exchange and one queue and one outgoing channel will be received in the same order that they were sent', quoted from https://www.rabbitmq.com/semantics.html.", 
            "date_created": "2015-04-23 08:15:03.673194+00:00", 
            "author": "https://api.launchpad.net/1.0/~zlji"
        }, 
        {
            "content": "Continued from #16,  in latest oslo.messaging, reply messages were sent via a same connection.", 
            "date_created": "2015-04-23 08:31:59.937575+00:00", 
            "author": "https://api.launchpad.net/1.0/~zlji"
        }, 
        {
            "content": "Since icehouse, nova turned to use oslo.messaging, which should have fixed this issue. I made a patch to restricting using just one connection for replying messages during rpc.call. This patch should apply to grizzly, and apply to havana with little modification", 
            "date_created": "2015-04-28 01:59:43.621044+00:00", 
            "author": "https://api.launchpad.net/1.0/~zlji"
        }, 
        {
            "content": "Cleanup\n=======\n\nThis bug report has the status \"Incomplete\" since more than 30 days\nand it looks like that there are no open reviews for it. To keep\nthe bug list sane, I close this bug with \"won't fix\". This does not\nmean that it is not a valid bug report, it's more to acknowledge that\nno progress can be expected here anymore. You are still free to push a\nnew patch for this bug. If you could reproduce it on the current master\ncode or on a maintained stable branch, please switch it to \"Confirmed\".", 
            "date_created": "2016-02-10 15:51:16.510166+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Happened to me in Liberty:\n\nERROR nova.servicegroup.drivers.db [-] Failed reporting state, will try next interval (part of a looping call)\nERROR nova.servicegroup.drivers.db Traceback (most recent call last):\nERROR nova.servicegroup.drivers.db   File \"/usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py\", line 86, in _report_state\nERROR nova.servicegroup.drivers.db     service.service_ref.report_count += 1\nERROR nova.servicegroup.drivers.db AttributeError: 'NoneType' object has no attribute 'report_count'", 
            "date_created": "2016-04-05 09:26:29.252817+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "This sounds like its a new bug, have you looked at 1517926 and 1505471?\n\nWe might want to re-close this, and instead look at backporting the above fixes, if they have not already been backported.", 
            "date_created": "2016-04-05 11:22:54.563838+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "@speller can you provide the information above.\n\nWhat does your configuration look like\n\nWhich hypervisor? Is this cells? What base. \n\nThe very narrow stack trace isn't enough to be actionable.", 
            "date_created": "2016-04-05 11:23:14.139032+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "It would also be good to figure out what errors your conductor is having, because that seems to be the main trigger for this. ", 
            "date_created": "2016-04-05 11:24:53.292303+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "@sdague I don't know what questions you're asking about.\n\nhypervisor is libvirt, no cells, and Liberty is the base\nAlso, attached is the full nova-compute log with the exception\n\nAny other information I can provide?", 
            "date_created": "2016-04-05 11:37:03.294754+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "Attached the nova-conductor log as well.\nI don't see any errors", 
            "date_created": "2016-04-05 11:38:17.178539+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "Oh missed those errors in the end about the message handling. Do they mean anything that can cause this to happen?", 
            "date_created": "2016-04-05 11:39:57.198059+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "Attached the nova-conductor log as well.\nThere are these errors in the end, regarding message handling. Could this make this bug happen?", 
            "date_created": "2016-04-05 11:41:04.002723+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "@johngarbutt Both those bugs-fixes we're backported to nova 12.0.1., I'm using 12.0.0\nIll try to test with those fixes, so it seems this can be closed for now", 
            "date_created": "2016-04-05 11:48:40.809533+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "This happened to me again, after porting those 2 commits on top of version 12.0.0. My git log to show that:\n\n113750b servicegroup: stop zombie service due to exception\n736fa6a Handle DB failures in servicegroup DB driver\n6df6ad3 (tag: 12.0.0.0rc3, tag: 12.0.0) Omnibus stable/liberty fix\nb5b9f54 update dependencies based on rdopkg reqcheck\nc920b8f Update to upstream 12.0.0.0rc2\nba6fccc Merge \"Fix InstanceV1 backports to use context\" into stable/liberty\n6062043 Revert \"[libvirt] Move cleanup of imported files to imagebackend\"\n\nThe traceback I see in nova-compute:\n\n2016-04-10 19:54:52,014.014 25926 ERROR nova.servicegroup.drivers.db   File \"/usr/lib/python2.7/site-packages/nova/servicegroup/drivers/db.py\", line 87, in _report_state\n2016-04-10 19:54:52,014.014 25926 ERROR nova.servicegroup.drivers.db     service.service_ref.report_count += 1\n2016-04-10 19:54:52,014.014 25926 ERROR nova.servicegroup.drivers.db AttributeError: 'NoneType' object has no attribute 'report_count'\n\nAttached is the nova-compute with the traceback", 
            "date_created": "2016-04-11 08:53:02.322060+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "And attaching the nova-conductor log", 
            "date_created": "2016-04-11 08:53:28.296235+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "Hi Shoham Peller, which version oslo.messaging you are using.", 
            "date_created": "2016-04-12 02:40:28.745606+00:00", 
            "author": "https://api.launchpad.net/1.0/~zlji"
        }, 
        {
            "content": "@zlji oslo.messaging's version is 2.5.0", 
            "date_created": "2016-04-12 09:53:30.350695+00:00", 
            "author": "https://api.launchpad.net/1.0/~shoham-peller"
        }, 
        {
            "content": "Solving Inconsistency: Bug priority already set. Hence, changing status from \"new\" to \"confirmed\"", 
            "date_created": "2016-05-06 19:32:33.350533+00:00", 
            "author": "https://api.launchpad.net/1.0/~pushkar-umaranikar"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:49:31.284934+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2012-07-25 17:47:01.192228+00:00", 
    "description": "I used \"nova volume-list\" to find a volume is available status and attach no instance, but actually it is attached to a instance, and mount with /dev/vdb in that instance and can read/write. Then I detach this volume and want to attach again. But when I use \"nova attach server_id volume_id /dev/vdb\", it is errro show that /dev/vdb exists.  In instance, I use pvdisplay or fdisk -l , both show /dev/vdb error. \nI think where locked the /dev/vdb in instance, may it is a bug ?\n\n2011-11-01 15:24:28,938 ERROR nova.compute.manager [84abd913-684b-402c-b312-a986263e0d20 admin 1] instance 16: attach failed /dev/vdb, removing\n(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1359, in attach_volume\n(nova.compute.manager): TRACE:     mountpoint)\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/exception.py\", line 113, in wrapped\n(nova.compute.manager): TRACE:     return f(*args, **kw)\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/virt/libvirt/connection.py\", line 380, in attach_volume\n(nova.compute.manager): TRACE:     virt_dom.attachDevice(xml)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 298, in attachDevice\n(nova.compute.manager): TRACE:     if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\n(nova.compute.manager): TRACE: libvirtError: operation failed: target vdb already exists\n(nova.compute.manager): TRACE: \n2011-11-01 15:24:28,974 DEBUG nova.rpc [-] Making asynchronous call on volume.node2 ... from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:721\n2011-11-01 15:24:28,975 DEBUG nova.rpc [-] MSG_ID is b10ea92631694842a019fd7e29a0088a from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:724\n2011-11-01 15:24:29,043 ERROR nova.rpc [-] Exception during message handling\n(nova.rpc): TRACE: Traceback (most recent call last):\n(nova.rpc): TRACE:   File \"/data/nova/nova/rpc/impl_kombu.py\", line 620, in _process_data\n(nova.rpc): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 120, in decorated_function\n(nova.rpc): TRACE:     function(self, context, instance_id, *args, **kwargs)\n(nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1368, in attach_volume\n(nova.rpc): TRACE:     raise exc\n(nova.rpc): TRACE: TypeError: __init__() takes at least 2 arguments (1 given)\n(nova.rpc): TRACE:", 
    "tags": [
        "volume"
    ], 
    "importance": "Medium", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/884635", 
    "owner": "None", 
    "id": 884635, 
    "index": 2612, 
    "created": "2011-11-01 08:13:37.047448+00:00", 
    "title": "nova-compute cannot attach volume if mountpoint exists", 
    "comments": [
        {
            "content": "I used \"nova volume-list\" to find a volume is available status and attach no instance, but actually it is attached to a instance, and mount with /dev/vdb in that instance and can read/write. Then I detach this volume and want to attach again. But when I use \"nova attach server_id volume_id /dev/vdb\", it is errro show that /dev/vdb exists.  In instance, I use pvdisplay or fdisk -l , both show /dev/vdb error. \nI think where locked the /dev/vdb in instance, may it is a bug ?\n\n2011-11-01 15:24:28,938 ERROR nova.compute.manager [84abd913-684b-402c-b312-a986263e0d20 admin 1] instance 16: attach failed /dev/vdb, removing\n(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1359, in attach_volume\n(nova.compute.manager): TRACE:     mountpoint)\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/exception.py\", line 113, in wrapped\n(nova.compute.manager): TRACE:     return f(*args, **kw)\n(nova.compute.manager): TRACE:   File \"/data/nova/nova/virt/libvirt/connection.py\", line 380, in attach_volume\n(nova.compute.manager): TRACE:     virt_dom.attachDevice(xml)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 298, in attachDevice\n(nova.compute.manager): TRACE:     if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\n(nova.compute.manager): TRACE: libvirtError: operation failed: target vdb already exists\n(nova.compute.manager): TRACE: \n2011-11-01 15:24:28,974 DEBUG nova.rpc [-] Making asynchronous call on volume.node2 ... from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:721\n2011-11-01 15:24:28,975 DEBUG nova.rpc [-] MSG_ID is b10ea92631694842a019fd7e29a0088a from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:724\n2011-11-01 15:24:29,043 ERROR nova.rpc [-] Exception during message handling\n(nova.rpc): TRACE: Traceback (most recent call last):\n(nova.rpc): TRACE:   File \"/data/nova/nova/rpc/impl_kombu.py\", line 620, in _process_data\n(nova.rpc): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 120, in decorated_function\n(nova.rpc): TRACE:     function(self, context, instance_id, *args, **kwargs)\n(nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1368, in attach_volume\n(nova.rpc): TRACE:     raise exc\n(nova.rpc): TRACE: TypeError: __init__() takes at least 2 arguments (1 given)\n(nova.rpc): TRACE:", 
            "date_created": "2011-11-01 08:13:37.047448+00:00", 
            "author": "https://api.launchpad.net/1.0/~mwjpiero"
        }, 
        {
            "content": "I usually attach the volume by specifying the device as /dev/vdz even through on the VM it will be recognized as the next device in line, eg /dev/vdc || /dev/vdd but this way I avoid the above failure.\n\nHow should this be fixed? \nShould nova fail if it cannot set the device as it will be seen on the VM?\nOr.. something else?", 
            "date_created": "2012-02-28 01:58:16.094171+00:00", 
            "author": "https://api.launchpad.net/1.0/~chris-fattarsi"
        }, 
        {
            "content": "There isn't really any way for libvirt to know where the guest will stick the vm.  I think the only solution is to do some udev magic in the guest to figure out where it supposed to show the pci device by talking to the metadata server.\n\nOn Feb 27, 2012, at 5:58 PM, Chris Fattarsi wrote:\n\n> I usually attach the volume by specifying the device as /dev/vdz even\n> through on the VM it will be recognized as the next device in line, eg\n> /dev/vdc || /dev/vdd but this way I avoid the above failure.\n> \n> How should this be fixed? \n> Should nova fail if it cannot set the device as it will be seen on the VM?\n> Or.. something else?\n> \n> -- \n> You received this bug notification because you are subscribed to\n> OpenStack Compute (nova).\n> https://bugs.launchpad.net/bugs/884635\n> \n> Title:\n>  nova-compute cannot attach volume if mountpoint exists\n> \n> Status in OpenStack Compute (Nova):\n>  Confirmed\n> \n> Bug description:\n>  I used \"nova volume-list\" to find a volume is available status and attach no instance, but actually it is attached to a instance, and mount with /dev/vdb in that instance and can read/write. Then I detach this volume and want to attach again. But when I use \"nova attach server_id volume_id /dev/vdb\", it is errro show that /dev/vdb exists.  In instance, I use pvdisplay or fdisk -l , both show /dev/vdb error. \n>  I think where locked the /dev/vdb in instance, may it is a bug ?\n> \n>  2011-11-01 15:24:28,938 ERROR nova.compute.manager [84abd913-684b-402c-b312-a986263e0d20 admin 1] instance 16: attach failed /dev/vdb, removing\n>  (nova.compute.manager): TRACE: Traceback (most recent call last):\n>  (nova.compute.manager): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1359, in attach_volume\n>  (nova.compute.manager): TRACE:     mountpoint)\n>  (nova.compute.manager): TRACE:   File \"/data/nova/nova/exception.py\", line 113, in wrapped\n>  (nova.compute.manager): TRACE:     return f(*args, **kw)\n>  (nova.compute.manager): TRACE:   File \"/data/nova/nova/virt/libvirt/connection.py\", line 380, in attach_volume\n>  (nova.compute.manager): TRACE:     virt_dom.attachDevice(xml)\n>  (nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 298, in attachDevice\n>  (nova.compute.manager): TRACE:     if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\n>  (nova.compute.manager): TRACE: libvirtError: operation failed: target vdb already exists\n>  (nova.compute.manager): TRACE: \n>  2011-11-01 15:24:28,974 DEBUG nova.rpc [-] Making asynchronous call on volume.node2 ... from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:721\n>  2011-11-01 15:24:28,975 DEBUG nova.rpc [-] MSG_ID is b10ea92631694842a019fd7e29a0088a from (pid=6733) multicall /data/nova/nova/rpc/impl_kombu.py:724\n>  2011-11-01 15:24:29,043 ERROR nova.rpc [-] Exception during message handling\n>  (nova.rpc): TRACE: Traceback (most recent call last):\n>  (nova.rpc): TRACE:   File \"/data/nova/nova/rpc/impl_kombu.py\", line 620, in _process_data\n>  (nova.rpc): TRACE:     rval = node_func(context=ctxt, **node_args)\n>  (nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 120, in decorated_function\n>  (nova.rpc): TRACE:     function(self, context, instance_id, *args, **kwargs)\n>  (nova.rpc): TRACE:   File \"/data/nova/nova/compute/manager.py\", line 1368, in attach_volume\n>  (nova.rpc): TRACE:     raise exc\n>  (nova.rpc): TRACE: TypeError: __init__() takes at least 2 arguments (1 given)\n>  (nova.rpc): TRACE:\n> \n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/884635/+subscriptions\n\n", 
            "date_created": "2012-02-28 03:40:55+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "I'm not sure how we fix this bug. Ideas?", 
            "date_created": "2012-02-28 16:47:49.785997+00:00", 
            "author": "https://api.launchpad.net/1.0/~bcwaldon"
        }, 
        {
            "content": "I propose making the mountpoint an optional argument and handling the error in the above case so that it doesn't fail.\n\nIt is inconsistent that it fails when setting a point in use but quietly does something else when setting a higher than the next ordinal.", 
            "date_created": "2012-02-28 18:01:40.990027+00:00", 
            "author": "https://api.launchpad.net/1.0/~chris-fattarsi"
        }, 
        {
            "content": "I cannot reproduce it in latest master branch", 
            "date_created": "2012-03-25 02:11:35.082354+00:00", 
            "author": "https://api.launchpad.net/1.0/~gongysh"
        }, 
        {
            "content": "Aside from a small collateral bug (https://review.openstack.org/#/c/10283/), I believe this is something coming from libvirt/qemu itself. It seems that it's likely related to not running ACPI/PCI hotplugging in the guest, which means that a detach cannot actually fully clean up a given device, and thus an attempt to re-attach at the same point must fail for safety reasons. See this bug for details:\n\nhttps://bugs.launchpad.net/ubuntu/+source/qemu-kvm/+bug/897750\n\nI think that this one needs to be marked Invalid as a result of it not being related to nova.", 
            "date_created": "2012-07-25 17:44:58.502226+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2011-04-04 13:19:48.542875+00:00", 
    "description": "When booting a Linux VHD image on XenServer I'm getting an error related to pygrub checking for a kernel:\n\nhttp://paste.openstack.org/show/849/\n\nI'm using a Glance backend, with a VHD image created from a snapshot of a Linux instance booted with an ami.  This is happening on the current Nova trunk.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/732756", 
    "owner": "None", 
    "id": 732756, 
    "index": 2056, 
    "created": "2011-03-10 18:15:10.032263+00:00", 
    "title": "Nova fails to boot Linux VHD images under XenServer", 
    "comments": [
        {
            "content": "When booting a Linux VHD image on XenServer I'm getting an error related to pygrub checking for a kernel:\n\n2011-03-10 17:18:04,584 AUDIT nova.compute.manager [84N6U-MYHZ4G99GOED3O admin openstack] instance 36: starting...\n2011-03-10 17:18:04,772 DEBUG nova.rpc [-] Making asynchronous call... from (pid=29856) call /home/cwright/openstack/nova/nova/rpc.py:3142011-03-10 17:18:04,773 DEBUG nova.rpc [-] MSG_ID is 7a263f8f612940619f3225a9b838c1da from (pid=29856) call /home/cwright/openstack/nova/nova/rpc.py:317\n2011-03-10 17:18:05,251 DEBUG nova.virt.xenapi.vm_utils [-] Detected DISK_VHD format for image 15, instance 36 from (pid=29856) log_disk_format /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:467\n2011-03-10 17:18:05,263 DEBUG nova.virt.xenapi.vm_utils [-] Asking xapi to fetch vhd image 15 from (pid=29856) _fetch_image_glance_vhd /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:365\n2011-03-10 17:19:10,500 INFO nova.virt.xenapi [-] Task [Async.host.call_plugin] OpaqueRef:fcf5c798-bd2e-f61c-756a-66d724b2d716 status: success    <value>47994a14-ad30-4a98-933e-613ad7ee8f85</value>\n2011-03-10 17:19:10,504 DEBUG nova.virt.xenapi.vm_utils [-] Re-scanning SR OpaqueRef:e340e893-ca07-1647-b0ce-f0462003b700 from (pid=29856) scan_sr /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:652\n2011-03-10 17:19:11,911 INFO nova.virt.xenapi [-] Task [Async.SR.scan] OpaqueRef:0ad9a8bf-c123-ad6e-ff49-bc7f9da09c2b status: success\n2011-03-10 17:19:12,001 DEBUG nova.virt.xenapi.vm_utils [-] xapi 'download_vhd' returned VDI UUID 47994a14-ad30-4a98-933e-613ad7ee8f85 from (pid=29856) _fetch_image_glance_vhd /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:3922011-03-10 17:19:12,271 DEBUG nova.virt.xenapi.vm_utils [-] Detected DISK_VHD format for image 15, instance 36 from (pid=29856) log_disk_format /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:467\n2011-03-10 17:19:12,274 DEBUG nova.virt.xenapi.vm_utils [-] Created VM instance-00000024... from (pid=29856) create_vm /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:140\n2011-03-10 17:19:12,331 DEBUG nova.virt.xenapi.vm_utils [-] Created VM instance-00000024 as OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77. from (pid=29856) create_vm /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:143\n2011-03-10 17:19:12,331 DEBUG nova.virt.xenapi.vm_utils [-] Creating VBD for VM OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77, VDI OpaqueRef:40647537-6674-9a66-55f7-98d0fccfd5d2 ...  from (pid=29856) create_vbd /home/cwright/openstack/nova/nova/\nvirt/xenapi/vm_utils.py:175\n2011-03-10 17:19:12,388 DEBUG nova.virt.xenapi.vm_utils [-] Created VBD OpaqueRef:aba36ff9-6cc7-e84b-07bd-cfdb5dc8916d for VM OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77, VDI OpaqueRef:40647537-6674-9a66-55f7-98d0fccfd5d2. from (pid=29856) cre\nate_vbd /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:178\n2011-03-10 17:19:12,500 DEBUG nova [-] injecting network info to xenstore for vm: |OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77| from (pid=29856) inject_network_info /home/cwright/openstack/nova/nova/virt/xenapi/vmops.py:703\n2011-03-10 17:19:12,791 DEBUG nova [-] creating vif(s) for vm: |OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77| from (pid=29856) create_vifs /home/cwright/openstack/nova/nova/virt/xenapi/vmops.py:7512011-03-10 17:19:12,847 DEBUG nova.virt.xenapi.vm_utils [-] Creating VIF for VM OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77, network OpaqueRef:6198b4e4-9a74-d7ec-13c8-1b6ea2652450. from (pid=29856) create_vif /home/cwright/openstack/nova/nova/\nvirt/xenapi/vm_utils.py:229\n2011-03-10 17:19:12,897 DEBUG nova.virt.xenapi.vm_utils [-] Created VIF OpaqueRef:cf54599d-9646-1f3d-c79c-44a5d49a5a17 for VM OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77, network OpaqueRef:6198b4e4-9a74-d7ec-13c8-1b6ea2652450. from (pid=29856)\n create_vif /home/cwright/openstack/nova/nova/virt/xenapi/vm_utils.py:232\n2011-03-10 17:19:12,898 DEBUG nova.virt.xenapi.vmops [-] Starting VM OpaqueRef:ece95fe4-5dfc-8304-6ce3-97c06cffcb77... from (pid=29856) _spawn_with_disk /home/cwright/openstack/nova/nova/virt/xenapi/vmops.py:139\n2011-03-10 17:19:12,898 DEBUG nova.virt.xenapi.vmops [-] Starting instance instance-00000024 from (pid=29856) _start /home/cwright/openstack/nova/nova/virt/xenapi/vmops.py:72\n2011-03-10 17:19:16,964 ERROR nova.compute.manager [84N6U-MYHZ4G99GOED3O admin openstack] instance 36: Failed to spawn(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/compute/manager.py\", line 213, in run_instance\n(nova.compute.manager): TRACE:     self.driver.spawn(instance_ref)\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/virt/xenapi_conn.py\", line 159, in spawn\n(nova.compute.manager): TRACE:     self._vmops.spawn(instance)\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/virt/xenapi/vmops.py\", line 85, in spawn\n(nova.compute.manager): TRACE:     self._spawn_with_disk(instance, vdi_uuid=vdi_uuid)\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/virt/xenapi/vmops.py\", line 140, in _spawn_with_disk\n(nova.compute.manager): TRACE:     self._start(instance, vm_ref)\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/virt/xenapi/vmops.py\", line 73, in _start(nova.compute.manager): TRACE:     self._session.call_xenapi('VM.start', vm_ref, False, False)\n(nova.compute.manager): TRACE:   File \"/home/cwright/openstack/nova/nova/virt/xenapi_conn.py\", line 293, in call_xenapi(nova.compute.manager): TRACE:     return tpool.execute(f, *args)\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.6/eventlet/tpool.py\", line 123, in execute\n(nova.compute.manager): TRACE:     rv = erecv(e)\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.6/eventlet/tpool.py\", line 80, in tworker(nova.compute.manager): TRACE:     rv = meth(*args,**kwargs)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.6/dist-packages/XenAPI.py\", line 229, in __call__(nova.compute.manager): TRACE:     return self.__send(self.__name, args)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.6/dist-packages/XenAPI.py\", line 133, in xenapi_request\n(nova.compute.manager): TRACE:     result = _parse_result(getattr(self, methodname)(*full_params))\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.6/dist-packages/XenAPI.py\", line 203, in _parse_result\n(nova.compute.manager): TRACE:     raise Failure(result['ErrorDescription'])\n(nova.compute.manager): TRACE: Failure: ['Traceback (most recent call last):', '  File \"/usr/bin/pygrub\", line 746, in ?', '    raise RuntimeError, \"Unable to find partition containing kernel\"', 'RuntimeError: Unable to find partition containing \nkernel', '']\n(nova.compute.manager): TRACE:\n2011-03-10 17:19:17,284 INFO nova.virt.xenapi.vm_utils [-] (VM_UTILS) xenserver vm state -> |Halted|\n2011-03-10 17:19:17,285 INFO nova.virt.xenapi.vm_utils [-] (VM_UTILS) xenapi power_state -> |4|\n\nI'm using a Glance backend, with a VHD image created from a snapshot of a Linux instance booted with an ami.  This is happening on the current Nova trunk.", 
            "date_created": "2011-03-10 18:15:10.032263+00:00", 
            "author": "https://api.launchpad.net/1.0/~corywright"
        }, 
        {
            "content": "Lately I have not been able to reproduce this.  The issue may have been resolved.", 
            "date_created": "2011-03-18 12:31:36.501450+00:00", 
            "author": "https://api.launchpad.net/1.0/~corywright"
        }, 
        {
            "content": "Hi Cory, \n\nI had a look at the traceback you posted. \nMy impression is that this is a failure of the XenServer's backend rather than a compute service failure. \n\nIn your bug report, you said you created the VHD was created out of an ami image. \nI'm therefore assuming that you then launched your vhd instance with a separate kernel/ramdisk file, and that's the bit that puzzles me , as I think in that case pygrub should not be looking for a kernel in the vhd file. \n\nIf you still have the XenServer VM record for that particular VM, it would be interesting to look at the PV-* and HVM-* attributes to see whether nova compute instructed XenServer to boot it as a raw image.\n\nSalvatore", 
            "date_created": "2011-03-18 13:29:59.207018+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Let's close this per comment 1, and reopen (set back to \"New\") if you can reproduce and provide Salvatore the information he asked for...", 
            "date_created": "2011-04-04 13:19:46.885476+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }
    ]
}
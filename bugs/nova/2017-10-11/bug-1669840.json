{
    "status": "Expired", 
    "last_updated": "2017-06-04 04:17:27.347467+00:00", 
    "description": "I have configured Ocata release on Ubuntu 16.\n\nEverything seems to be working until I install Cinder volume. After configuring cinder, I started receiving an error under Instances option of Horizon.\n\nI get an error stating \"Unable to retrieve instances.\"\n\nAny guide in the above mentioned regard will be highly appreciated.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1669840", 
    "owner": "None", 
    "id": 1669840, 
    "index": 6708, 
    "created": "2017-03-03 16:21:06.765654+00:00", 
    "title": "OCATA: Error: Unable to retrieve instances.", 
    "comments": [
        {
            "content": "I have configured Ocata release on Ubuntu 16.\n\nEverything seems to be working until I install Cinder volume. After configuring cinder, I started receiving an error under Instances option of Horizon.\n\nI get an error stating \"Unable to retrieve instances.\"\n\nAny guide in the above mentioned regard will be highly appreciated.", 
            "date_created": "2017-03-03 16:21:06.765654+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "", 
            "date_created": "2017-03-03 16:23:04.069721+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "when i am trying to spawn an instance from cli, \n\nopenstack server create --image cirros --flavor m1.small --nic private_net testvm\n\n\nI get an error pasted below;\n\ndictionary update sequence element #0 has length 1; 2 is required", 
            "date_created": "2017-03-03 17:01:39.088572+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "After reviewing nova_api.log file I observed the following error message;\n\n2017-03-04 09:44:25.185 5369 ERROR nova.api.openstack.extensions OperationalError: (pymysql.err.OperationalError) (1044, u\"Access denied for user 'nova'@'%' to database 'nova_cell0'\"\n\nSolution:\n\nCreated nova_cell0 database and assigned nova user access to it. \n\nAfter completing the above step the error message under the horizon has disappeared.\n\n\nAny thoughts on the abovementioned will appreciated..", 
            "date_created": "2017-03-04 07:21:47.400254+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "\n\nJust to add on Step#3: after creating the database, did the database sync with the following command;\n\nnova-manage db sync\n\n\n\nWill wait for any feedback for Step#3 and Step#4....If this is amiss in the document or something else.", 
            "date_created": "2017-03-04 07:26:50.122205+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "I believe devstack already has step #3 and step #4 for you, if so, your problem is against some ubuntu doc instead of nova ,right? ", 
            "date_created": "2017-03-06 09:51:32.108375+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "I am following the documents published on openstack website for ubuntu flavour.", 
            "date_created": "2017-03-06 15:20:30.856723+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "Hello, kadtab.\n\nPlease be more explicit about your configuration - for instance if the installation was devstack, from packages or some other deployment tool and the steps that lead to your problem, that could help reproduce this problem.\n\nBecause of the above, I am marking this one as invalid. Please provide additional informations and after doing so, feel free to mark this bug as \"New\".\n\nOther than that\n\nCheers!", 
            "date_created": "2017-03-06 16:24:55.591886+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }, 
        {
            "content": "Deployment Scenario:\n\nDeploying ocata in all-in-one setup from openstack packages on Ubuntu 16.01 xenial. Have deployed the following modules; i.e. rabbitmq,keystone,neutron,nova,glance,cinder. All the modules were deployed following the documentation on openstack website.\n\nProblem #1 - Everything seems fine until I install Cinder. After I configure cinder service based on the documented steps I start getting error on the Horizon page under Instances option. Error: Unable to retrieve instances. \n\nSolution #1 - After changing the region to openstack under cinder option of nova.conf, it resolved my issue. No more error messages under horizon.\n\nProblem #2 - When I am trying to spawn the instance, it goes in a loop of spawning without any results. After reviewing the nova-conductor.log, i observed the following message stating : \"No cell mapping found for cell0 while trying to record scheduling failure. Setup is incomplete.\"\n\nAfter googling, found a solution to use the following command to resolve the above stated:\nnova-manage cell_v2 simple_cell_setup\n\nBut the above command resulted in the following error, which prompted me to create nova_cell0 database; \"Access denied for user 'nova'@'%' to database 'nova_cell0'\"\n\nNow when I am trying to spawn the instance getting an error message in horizon of \"No valid host found\".\n\nOutput of nova-conductor.log states as below;\n\ntail -f /var/log/nova/nova-conductor.log\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 98, in select_destinations\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager     dests = self.driver.select_destinations(ctxt, spec_obj)\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\", line 79, in select_destinations\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager     raise exception.NoValidHost(reason=reason)\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager NoValidHost: No valid host was found. There are not enough hosts available.\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager\n2017-03-06 19:18:40.968 3717 ERROR nova.conductor.manager\n2017-03-06 19:18:40.981 3717 ERROR nova.conductor.manager [req-1f5bedd7-c771-4873-8add-3ebcf59fdcd3 - - - - -] No cell mapping found for cell0 while trying to record scheduling failure. Setup is incomplete.\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager [req-96a934d1-fec0-414a-9967-91350fe27c87 - - - - -] Failed to schedule instances\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager Traceback (most recent call last):\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/conductor/manager.py\", line 866, in schedule_and_build_instances\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     request_specs[0].to_legacy_filter_properties_dict())\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/conductor/manager.py\", line 597, in _schedule_instances\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     hosts = self.scheduler_client.select_destinations(context, spec_obj)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/utils.py\", line 371, in wrapped\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return func(*args, **kwargs)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/client/__init__.py\", line 51, in select_destinations\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return self.queryclient.select_destinations(context, spec_obj)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/client/__init__.py\", line 37, in __run_method\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return getattr(self.instance, __name)(*args, **kwargs)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/client/query.py\", line 32, in select_destinations\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return self.scheduler_rpcapi.select_destinations(context, spec_obj)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/rpcapi.py\", line 129, in select_destinations\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return cctxt.call(ctxt, 'select_destinations', **msg_args)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 169, in call\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     retry=self.retry)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 97, in _send\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     timeout=timeout, retry=retry)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 458, in send\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     retry=retry)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 449, in _send\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     raise result\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager NoValidHost_Remote: No valid host was found. There are not enough hosts available.\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager Traceback (most recent call last):\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 218, in inner\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     return func(*args, **kwargs)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 98, in select_destinations\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     dests = self.driver.select_destinations(ctxt, spec_obj)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\", line 79, in select_destinations\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager     raise exception.NoValidHost(reason=reason)\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager NoValidHost: No valid host was found. There are not enough hosts available.\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:35.944 3715 ERROR nova.conductor.manager\n2017-03-06 19:29:36.104 3715 WARNING nova.scheduler.utils [req-96a934d1-fec0-414a-9967-91350fe27c87 - - - - -] Failed to compute_task_build_instances: No valid host was found. There are not enough hosts available.\nTraceback (most recent call last):\n\n  File \"/usr/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 218, in inner\n    return func(*args, **kwargs)\n\n  File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 98, in select_destinations\n    dests = self.driver.select_destinations(ctxt, spec_obj)\n\n  File \"/usr/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\", line 79, in select_destinations\n    raise exception.NoValidHost(reason=reason)\n\nNoValidHost: No valid host was found. There are not enough hosts available.\n\n2017-03-06 19:29:36.105 3715 WARNING nova.scheduler.utils [req-96a934d1-fec0-414a-9967-91350fe27c87 - - - - -] [instance: 80915c31-367e-480e-980d-c07562077478] Setting instance to ERROR state.\n\n\n\n\nWill appreciate any feedback for the issue being face in Ocata.                                                                            ", 
            "date_created": "2017-03-06 16:56:19.820197+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "kadtab,\n\nlet me explain what's going on in Ocata.\nPossibly, you already know that but just to be clear.\nIn Ocata new cells v2 functionality was enabled by default.\nCell is separate DB + message queue and again by default\nyou have two cells: cell1 is your main DB + MQ and cell0 is\nfallback DB without MQ. \n\nIn DB it looks like: \nhttp://ix.io/oDy \nhttp://ix.io/oDo\nAs you see database_connection is link to actual DB.\n\nI believe best way to solve your problem is to get \nnova-manage cell_v2 simple_cell_setup done successfully\nor at least to check if database_connection in cell_mappings\nfor cell0 is correct.", 
            "date_created": "2017-03-07 07:52:07.278083+00:00", 
            "author": "https://api.launchpad.net/1.0/~avolkov"
        }, 
        {
            "content": "Thanks Andrey for your explanation. Really appreciate it.", 
            "date_created": "2017-03-07 17:25:51.408372+00:00", 
            "author": "https://api.launchpad.net/1.0/~tabkad"
        }, 
        {
            "content": "Hi, i installed devstack master branch with magnum services on ubuntu 16.04, and nova gave same error \"Unable to retrieve instances.\" and i cant login on horizon.\n\n", 
            "date_created": "2017-04-04 06:35:28.665967+00:00", 
            "author": "https://api.launchpad.net/1.0/~yasemin-demiral"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2017-06-04 04:17:24.690609+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ]
}
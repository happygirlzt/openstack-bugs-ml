{
    "status": "Fix Released", 
    "last_updated": "2016-04-05 16:06:24.454728+00:00", 
    "description": "I was trying to figure out why creating 1,2,4 servers in parallel on an 8-core machine did not show any speedup. I found a\nproblem shown in this log snippet with 4 servers. The pair of calls producing the debug messages are separated only by\na single call. \n\n    def prepare_instance_filter(self, instance, network_info):\n        # make sure this is legacy nw_info\n        network_info = self._handle_network_info_model(network_info)\n\n        self.instances[instance['id']] = instance\n        self.network_infos[instance['id']] = network_info\n        self.add_filters_for_instance(instance)\n        LOG.debug(_('Filters added to instance'), instance=instance)\n        self.refresh_provider_fw_rules()\n        LOG.debug(_('Provider Firewall Rules refreshed'), instance=instance)\n        self.iptables.apply()\n\nNote the interleaving of the last two calls in this log snippet and how long they take:\n\n\nJun 22 10:52:09 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:09 DEBUG nova.virt.firewall [req-14689766-cc17-4d8d-85bb-c4c19a2fc88d demo demo] [instance: 4c5a43af-04fd-4aa0-818e-8e0c5384b279] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:10 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:10 DEBUG nova.virt.firewall [req-14689766-cc17-4d8d-85bb-c4c19a2fc88d demo demo] [instance: 4c5a43af-04fd-4aa0-818e-8e0c5384b279] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:18 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:18 DEBUG nova.virt.firewall [req-c9ed42e0-1eed-418a-ba37-132bcc26735c demo demo] [instance: df15e7d6-657e-4fd7-a4eb-6aab1bd63d5b] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:19 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:19 DEBUG nova.virt.firewall [req-c9ed42e0-1eed-418a-ba37-132bcc26735c demo demo] [instance: df15e7d6-657e-4fd7-a4eb-6aab1bd63d5b] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:19 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:19 DEBUG nova.virt.firewall [req-2daf4cb8-73c5-487a-9bf6-bea08125b461 demo demo] [instance: 765212a6-cc23-4d5a-b252-5fa6b5f8331e] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:25 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:25 DEBUG nova.virt.firewall [req-5618e93e-3af1-4c65-b826-9d38850a215d demo demo] [instance: fa6423ac-82b8-419b-a077-f2d44d081771] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:38 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:38 DEBUG nova.virt.firewall [req-2daf4cb8-73c5-487a-9bf6-bea08125b461 demo demo] [instance: 765212a6-cc23-4d5a-b252-5fa6b5f8331e] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:52 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:52 DEBUG nova.virt.firewall [req-5618e93e-3af1-4c65-b826-9d38850a215d demo demo] [instance: fa6423ac-82b8-419b-a077-f2d44d081771] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153", 
    "tags": [
        "performance"
    ], 
    "importance": "Medium", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1016633", 
    "owner": "https://api.launchpad.net/1.0/~hanlind", 
    "id": 1016633, 
    "index": 2944, 
    "created": "2012-06-22 16:14:08.092939+00:00", 
    "title": "Bad performance problem with nova.virt.firewall", 
    "comments": [
        {
            "content": "I was trying to figure out why creating 1,2,4 servers in parallel on an 8-core machine did not show any speedup. I found a\nproblem shown in this log snippet with 4 servers. The pair of calls producing the debug messages are separated only by\na single call. \n\n    def prepare_instance_filter(self, instance, network_info):\n        # make sure this is legacy nw_info\n        network_info = self._handle_network_info_model(network_info)\n\n        self.instances[instance['id']] = instance\n        self.network_infos[instance['id']] = network_info\n        self.add_filters_for_instance(instance)\n        LOG.debug(_('Filters added to instance'), instance=instance)\n        self.refresh_provider_fw_rules()\n        LOG.debug(_('Provider Firewall Rules refreshed'), instance=instance)\n        self.iptables.apply()\n\nNote the interleaving of the last two calls in this log snippet and how long they take:\n\n\nJun 22 10:52:09 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:09 DEBUG nova.virt.firewall [req-14689766-cc17-4d8d-85bb-c4c19a2fc88d demo demo] [instance: 4c5a43af-04fd-4aa0-818e-8e0c5384b279] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:10 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:10 DEBUG nova.virt.firewall [req-14689766-cc17-4d8d-85bb-c4c19a2fc88d demo demo] [instance: 4c5a43af-04fd-4aa0-818e-8e0c5384b279] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:18 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:18 DEBUG nova.virt.firewall [req-c9ed42e0-1eed-418a-ba37-132bcc26735c demo demo] [instance: df15e7d6-657e-4fd7-a4eb-6aab1bd63d5b] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:19 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:19 DEBUG nova.virt.firewall [req-c9ed42e0-1eed-418a-ba37-132bcc26735c demo demo] [instance: df15e7d6-657e-4fd7-a4eb-6aab1bd63d5b] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:19 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:19 DEBUG nova.virt.firewall [req-2daf4cb8-73c5-487a-9bf6-bea08125b461 demo demo] [instance: 765212a6-cc23-4d5a-b252-5fa6b5f8331e] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:25 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:25 DEBUG nova.virt.firewall [req-5618e93e-3af1-4c65-b826-9d38850a215d demo demo] [instance: fa6423ac-82b8-419b-a077-f2d44d081771] Filters added to instance from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:151\n\nJun 22 10:52:38 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:38 DEBUG nova.virt.firewall [req-2daf4cb8-73c5-487a-9bf6-bea08125b461 demo demo] [instance: 765212a6-cc23-4d5a-b252-5fa6b5f8331e] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153\n\nJun 22 10:52:52 xg06eth0 \u00ef\u00bb\u00bf2012-06-22 10:52:52 DEBUG nova.virt.firewall [req-5618e93e-3af1-4c65-b826-9d38850a215d demo demo] [instance: fa6423ac-82b8-419b-a077-f2d44d081771] Provider Firewall Rules refreshed from (pid=15704) prepare_instance_filter /opt/stack/nova/nova/virt/firewall.py:153", 
            "date_created": "2012-06-22 16:14:08.092939+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "I tried Vish's suggested fix http://paste.openstack.org/show/18764/ but the result was that the instances never make it out of the BUILD state.", 
            "date_created": "2012-06-25 20:04:31.231808+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Probably linked to the fact that your hosts are created on the same machine and the calls are synchronized.", 
            "date_created": "2012-06-26 10:24:12.589907+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "OK, so I grepped out the \"instance:\" lines for a 4-node run that took more than a minute and a 1-node run that took 10 seconds to create five files that can be compared. I noticed several strange things for the 4-node logs. \n\nFirst, there was a 14 second hiccup on all four between \"Starting instance...\" and \"Instance network info due to a call to heal_instance_info_cache. Since this fires every minute I assume it is not supposed to take 14 seconds. \n\nJust 2 of the 4, and not the one, had an \"iptables firewall:\" line, and one of them took 17 seconds to get to the next thing.\n\nI am attaching the files.", 
            "date_created": "2012-06-28 20:58:35.562184+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-06-28 20:59:17.703187+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-06-28 20:59:43.202168+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-06-28 21:00:05.782633+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-06-28 21:00:25.636520+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "", 
            "date_created": "2012-06-28 21:00:44.522614+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "I tried some runs with 8 instances on a host with 8 cores. There were several large gaps indicating serialization. The biggest was in the update_available_resource method in nova/virt/libvirt/connection.py. It was taking as much as 30 seconds. Most of that time seemed to be in getting vcpu_used, cpu_info, and disk_available_least.\n\nFor the multiple cpu case there is also always a 5 second gap between\n\nIPTablesManager.apply completed with success\n\nand\n\n Running cmd (subprocess): sudo /usr/local/bin/nova-rootwrap /etc/nova/rootwrap.conf tee /sys/class/net/vnet7/brport/hairpin_mode\n\n\nAll in all, the main problem seems to be that the periodic manager processes (green threads?) are doing slow operations and not sufficiently yielding to the \"real\" work that is trying to launch instances that truly run in parallel native processes. But I don't understand the nova threading/rpc/etc. architecture enough to really say.", 
            "date_created": "2012-06-29 15:03:12.758391+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Thierry, I don't agree that this is a wishlist item. At this point the bug title is not really right. The situation is that launching n instances in an overlapping way takes at least as long, and perhaps longer, than launching them serially. We may decide it can't be fixed for Folsom but it is a real bug. I understand that this may not seem important for the use case where you have large numbers of public cloud users randomly launching vms on a large cluster but many of us have more targeted applications that view a cluster as a large set of cores and may take advantage of the fact that certain groups of cores have certain locality properties.\n", 
            "date_created": "2012-06-29 15:12:03.428912+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Set to Medium instead, ping me if you disagree.", 
            "date_created": "2012-06-29 15:31:47.467664+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Hello \n\nI hit this issue when I'm deploying instances in parallel at one compute node with disabled allow_same_net_traffic. \n\nIt cause that instances are in error state due to openstack's RPC calls timeout errors... I have setup with 5minutes rpc limit ...:(\n\nJaroslav", 
            "date_created": "2012-10-25 14:07:19.114501+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "This needs to be verified with current trunk. A fix went in that should have minimized this:\n\nhttps://review.openstack.org/#/c/14326/", 
            "date_created": "2012-10-25 19:21:28.869062+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "I will use  attached patch until fix from #13 is not verified.", 
            "date_created": "2012-10-26 06:25:34.882681+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "Anyone could confirm that the issue is gone with the patch mentioned in comment 13 ?", 
            "date_created": "2012-12-13 10:49:31.506588+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Still to slow for me without patch from #14.", 
            "date_created": "2012-12-13 10:59:58.979517+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "The current situation on trunk is that creating n instances in parallel gets about 1/2 the speedup that would be hoped for. That is, starting up 4 instances takes twice as long as starting 1. That is a lot better than when I reported this bug. I don't know how much serialization is minimally required so don't know how good this is.", 
            "date_created": "2013-01-21 20:11:47.247448+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Is this still valid, this bug is fairly old and a lot has changed since this was last updated", 
            "date_created": "2014-08-22 05:35:37.240797+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Hello,\n\nthis issue is valid for all nova-network based deployment. I didn't check it at Neutron, but I expect same behavior if the concept of collecting IPs for each security group is based on 3x for cycles and running RPC call in each iteration (I didn't check it directly).", 
            "date_created": "2014-08-22 05:53:07.369798+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/184027\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=c5e2d4b6b4dfa64f898249d6ed871cf6d41c58a2\nSubmitter: Jenkins\nBranch:    master\n\ncommit c5e2d4b6b4dfa64f898249d6ed871cf6d41c58a2\nAuthor: Hans Lindgren <email address hidden>\nDate:   Thu Apr 23 22:43:24 2015 +0200\n\n    Remove unused provider firewall rules functionality in nova\n    \n    Provider firewall rules functionality is not in use and hasn't been\n    for a very long time. The api for this was removed in [1] and db api\n    methods for adding/removing rows in the associated db table have not\n    been used since.\n    \n    Stop refreshing those rules as it is essentially a no-op and indeed a\n    costly one that includes a rpc round trip to the conductor to get\n    back an always empty db result. This should have a positive impact on\n    instance boot performance since the conductor call happens to live\n    inside an externally syncronized block of code.\n    \n    Removes related compute rpcapi/manager code that were missed in a\n    recent cleanup[2]. Since this functionality hasn't been in use since\n    Havana timeframe(!), it should be fairly safe to remove without first\n    deprecating it.\n    \n    Also removes the now unused virtapi method provider_fw_rule_get_all()\n    and the virtapi itself from virt firewall driver initialization.\n    \n    [1] Commit: 62d5fae8d11b6403f9a63a709270ffafebb7ef09\n    [2] Commit: e6f7d8041783a0b3c740559d97b8d40b3568f214\n    \n    Change-Id: Ifbb2514b9bc1445eaa07dcfe172c7405fd1a58f7\n    Partial-Bug: #1016633\n", 
            "date_created": "2016-02-02 17:06:25.470799+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "@Hand Lindgren: What's missing to fix this bug completely?", 
            "date_created": "2016-04-01 15:28:48.139727+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Not sure. My change removed one bottleneck involving a synchronized RPC call but I don't know if there are others. Maybe someone reporting having this problem can verify things now work as expected and we can close this bug.", 
            "date_created": "2016-04-01 18:23:49.273082+00:00", 
            "author": "https://api.launchpad.net/1.0/~hanlind"
        }, 
        {
            "content": "@Hans Lindgren: Thanks for the feedback. This bug report is really old\nand I doubt that the current \"medium\" importance is still valid. I\nclose this as fix released, thanks for you patch.\n\n@David Kranz (+ other stakeholders): Please double-check if the \nissue is fixed from your perspective. Use the current master (Newton) \ncode for that. If it is not fixed, please reopen and provide some\ninformation how you tested this.", 
            "date_created": "2016-04-05 16:06:23.686317+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ]
}
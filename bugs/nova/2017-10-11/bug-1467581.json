{
    "status": "Fix Released", 
    "last_updated": "2016-06-07 20:54:19.070376+00:00", 
    "description": "Concurrently attaching multiple network interfaces to a single instance can often result in corruption of the instance's information cache in Nova. The result is that some network interfaces may be missing from 'nova list', and silently fail to detach when 'nova interface-detach' is run. The ports are listed in 'nova interface-list', however, and can be seen in 'neutron port-list'.\n\nInitially seen on CentOS7 running Juno. Reproduced on Ubuntu 14.04 running devstack (master branch).\n\nThis issue is similar (possibly identical) to bug 1326183, and the steps to reproduce it are similar also.\n\n1) Devstack with trunk with the following local.conf:\ndisable_service n-net\nenable_service q-svc\nenable_service q-agt\nenable_service q-dhcp\nenable_service q-meta\nRECLONE=yes\n# and other options as set in the trunk's local\n\n2) Create few networks:\n$> neutron net-create testnet1\n$> neutron net-create testnet2\n$> neutron net-create testnet3\n$> neutron subnet-create testnet1 192.168.1.0/24\n$> neutron subnet-create testnet2 192.168.2.0/24\n$> neutron subnet-create testnet3 192.168.3.0/24\n\n3) Create a testvm in testnet1:\n$> nova boot --flavor m1.tiny --image cirros-0.3.4-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm\n\n4) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:\n---------\n#! /bin/bash\nc=10000\nnetid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`\nnetid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`\nwhile [ $c -gt 0 ]\ndo\n   echo \"Round: \" $c\n   echo -n \"Attaching two interfaces concurrently... \"\n   nova interface-attach --net-id $netid1 testvm &\n   nova interface-attach --net-id $netid2 testvm &\n   wait\n   echo \"Done\"\n   echo \"Sleeping until both those show up in nova show\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova show testvm | grep testnet | wc -l`\n       if [ $count -eq 3 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   echo \"Waited for \" $waittime \" seconds\"\n   if [ $waittime -ge 60 ]\n   then\n      echo \"bad case\"\n      exit 1\n   fi\n   echo \"Detaching both... \"\n   nova interface-list testvm | grep $netid1 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   nova interface-list testvm | grep $netid2 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   echo \"Done; check interfaces are gone in a minute.\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       echo \"line count: \" $count\n       if [ $count -eq 5 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   if [ $waittime -ge 60 ]\n   then\n      echo \"failed to detach interfaces - raise another bug!\"\n      exit 1\n   fi\n   echo \"Interfaces are gone\"\n   (( c-- ))\ndone\n---------\n\nEventually the test will stop with a failure (\"bad case\") and the interface remaining either from testnet2 or testnet3 can not be detached at all.\n\nFor me, eventually is every time.\n\nBased on my analysis of the source code, the concurrent requests cause corruption of the instance network info cache. Each takes a copy of the info cache at the start of the request processing, which contains only the initial network. Each request thread then allocates a network port and adds it to the network info. This info object is then saved back to the DB. In each case, the info contains the initial network and the network that has been added by that thread. Therefore, the last thread to save wins, and the other network is lost.\n\nI have a patch that appears to fix the issue, by refreshing the info cache whilst holding the refresh-cache-<id> lock. However, I'm not intimately familiar with the nova networking code so would appreciate more experienced eyes on it. I will submit the change to gerrit for analysis and comments.", 
    "tags": [
        "db", 
        "in-stable-liberty", 
        "network"
    ], 
    "importance": "High", 
    "heat": 34, 
    "link": "https://bugs.launchpad.net/nova/+bug/1467581", 
    "owner": "https://api.launchpad.net/1.0/~mgoddard", 
    "id": 1467581, 
    "index": 1767, 
    "created": "2015-06-22 15:53:24.645157+00:00", 
    "title": "Concurrent interface attachment corrupts info cache", 
    "comments": [
        {
            "content": "Concurrently attaching multiple network interfaces to a single instance can often result in corruption of the instance's information cache in Nova. The result is that some network interfaces may be missing from 'nova list', and silently fail to detach when 'nova interface-detach' is run. The ports are listed in 'nova interface-list', however, and can be seen in 'neutron port-list'.\n\nInitially seen on CentOS7 running Juno. Reproduced on Ubuntu 14.04 running devstack (master branch).\n\nThis issue is similar (possibly identical) to bug 1326183, and the steps to reproduce it are similar also.\n\n1) Devstack with trunk with the following local.conf:\ndisable_service n-net\nenable_service q-svc\nenable_service q-agt\nenable_service q-dhcp\nenable_service q-meta\nRECLONE=yes\n# and other options as set in the trunk's local\n\n2) Create few networks:\n$> neutron net-create testnet1\n$> neutron net-create testnet2\n$> neutron net-create testnet3\n$> neutron subnet-create testnet1 192.168.1.0/24\n$> neutron subnet-create testnet2 192.168.2.0/24\n$> neutron subnet-create testnet3 192.168.3.0/24\n\n3) Create a testvm in testnet1:\n$> nova boot --flavor m1.tiny --image cirros-0.3.4-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm\n\n4) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:\n---------\n#! /bin/bash\nc=10000\nnetid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`\nnetid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`\nwhile [ $c -gt 0 ]\ndo\n   echo \"Round: \" $c\n   echo -n \"Attaching two interfaces concurrently... \"\n   nova interface-attach --net-id $netid1 testvm &\n   nova interface-attach --net-id $netid2 testvm &\n   wait\n   echo \"Done\"\n   echo \"Sleeping until both those show up in nova show\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova show testvm | grep testnet | wc -l`\n       if [ $count -eq 3 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   echo \"Waited for \" $waittime \" seconds\"\n   if [ $waittime -ge 60 ]\n   then\n      echo \"bad case\"\n      exit 1\n   fi\n   echo \"Detaching both... \"\n   nova interface-list testvm | grep $netid1 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   nova interface-list testvm | grep $netid2 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n   echo \"Done; check interfaces are gone in a minute.\"\n   waittime=0\n   while [ $waittime -lt 60 ]\n   do\n       count=`nova interface-list testvm | wc -l`\n       echo \"line count: \" $count\n       if [ $count -eq 5 ]\n       then\n           break\n       fi\n       sleep 2\n       (( waittime+=2 ))\n   done\n   if [ $waittime -ge 60 ]\n   then\n      echo \"failed to detach interfaces - raise another bug!\"\n      exit 1\n   fi\n   echo \"Interfaces are gone\"\n   (( c-- ))\ndone\n---------\n\nEventually the test will stop with a failure (\"bad case\") and the interface remaining either from testnet2 or testnet3 can not be detached at all.\n\nFor me, eventually is every time.\n\nBased on my analysis of the source code, the concurrent requests cause corruption of the instance network info cache. Each takes a copy of the info cache at the start of the request processing, which contains only the initial network. Each request thread then allocates a network port and adds it to the network info. This info object is then saved back to the DB. In each case, the info contains the initial network and the network that has been added by that thread. Therefore, the last thread to save wins, and the other network is lost.\n\nI have a patch that appears to fix the issue, by refreshing the info cache whilst holding the refresh-cache-<id> lock. However, I'm not intimately familiar with the nova networking code so would appreciate more experienced eyes on it. I will submit the change to gerrit for analysis and comments.", 
            "date_created": "2015-06-22 15:53:24.645157+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "I have submitted a patchset for review that appears to resolve this issue. Please see https://review.openstack.org/#/c/194597/.\n\nI would appreciate comments from developers with more experience of Nova networking than I have.", 
            "date_created": "2015-06-23 10:55:49.543596+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "http://martinfowler.com/bliki/TwoHardThings.html\n\n:)\n\nI'll review your patch shortly. Thanks!\n\n-jay", 
            "date_created": "2015-07-26 00:01:07.327393+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Thanks Jay, your help is much appreciated.\n\nMartin Fowler is on the money there - choosing the new function's name was as tough as fixing the bug. :)\n\nMark\n________________________________________\nFrom: <email address hidden> [<email address hidden>] on behalf of Jay Pipes [<email address hidden>]\nSent: 26 July 2015 01:01\nTo: Mark Goddard\nSubject: [Bug 1467581] Re: Concurrent interface attachment corrupts info cache\n\nhttp://martinfowler.com/bliki/TwoHardThings.html\n\n:)\n\nI'll review your patch shortly. Thanks!\n\n-jay\n\n--\nYou received this bug notification because you are subscribed to the bug\nreport.\nhttps://bugs.launchpad.net/bugs/1467581\n\nTitle:\n  Concurrent interface attachment corrupts info cache\n\nStatus in OpenStack Compute (nova):\n  In Progress\n\nBug description:\n  Concurrently attaching multiple network interfaces to a single\n  instance can often result in corruption of the instance's information\n  cache in Nova. The result is that some network interfaces may be\n  missing from 'nova list', and silently fail to detach when 'nova\n  interface-detach' is run. The ports are listed in 'nova interface-\n  list', however, and can be seen in 'neutron port-list'.\n\n  Initially seen on CentOS7 running Juno. Reproduced on Ubuntu 14.04\n  running devstack (master branch).\n\n  This issue is similar (possibly identical) to bug 1326183, and the\n  steps to reproduce it are similar also.\n\n  1) Devstack with trunk with the following local.conf:\n  disable_service n-net\n  enable_service q-svc\n  enable_service q-agt\n  enable_service q-dhcp\n  enable_service q-meta\n  RECLONE=yes\n  # and other options as set in the trunk's local\n\n  2) Create few networks:\n  $> neutron net-create testnet1\n  $> neutron net-create testnet2\n  $> neutron net-create testnet3\n  $> neutron subnet-create testnet1 192.168.1.0/24\n  $> neutron subnet-create testnet2 192.168.2.0/24\n  $> neutron subnet-create testnet3 192.168.3.0/24\n\n  3) Create a testvm in testnet1:\n  $> nova boot --flavor m1.tiny --image cirros-0.3.4-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm\n\n  4) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:\n  ---------\n  #! /bin/bash\n  c=10000\n  netid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`\n  netid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`\n  while [ $c -gt 0 ]\n  do\n     echo \"Round: \" $c\n     echo -n \"Attaching two interfaces concurrently... \"\n     nova interface-attach --net-id $netid1 testvm &\n     nova interface-attach --net-id $netid2 testvm &\n     wait\n     echo \"Done\"\n     echo \"Sleeping until both those show up in nova show\"\n     waittime=0\n     while [ $waittime -lt 60 ]\n     do\n         count=`nova show testvm | grep testnet | wc -l`\n         if [ $count -eq 3 ]\n         then\n             break\n         fi\n         sleep 2\n         (( waittime+=2 ))\n     done\n     echo \"Waited for \" $waittime \" seconds\"\n     if [ $waittime -ge 60 ]\n     then\n        echo \"bad case\"\n        exit 1\n     fi\n     echo \"Detaching both... \"\n     nova interface-list testvm | grep $netid1 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n     nova interface-list testvm | grep $netid2 | awk '{print \"deleting \",$4; system(\"nova interface-detach testvm \"$4 \" ; sleep 2\");}'\n     echo \"Done; check interfaces are gone in a minute.\"\n     waittime=0\n     while [ $waittime -lt 60 ]\n     do\n         count=`nova interface-list testvm | wc -l`\n         echo \"line count: \" $count\n         if [ $count -eq 5 ]\n         then\n             break\n         fi\n         sleep 2\n         (( waittime+=2 ))\n     done\n     if [ $waittime -ge 60 ]\n     then\n        echo \"failed to detach interfaces - raise another bug!\"\n        exit 1\n     fi\n     echo \"Interfaces are gone\"\n     (( c-- ))\n  done\n  ---------\n\n  Eventually the test will stop with a failure (\"bad case\") and the\n  interface remaining either from testnet2 or testnet3 can not be\n  detached at all.\n\n  For me, eventually is every time.\n\n  Based on my analysis of the source code, the concurrent requests cause\n  corruption of the instance network info cache. Each takes a copy of\n  the info cache at the start of the request processing, which contains\n  only the initial network. Each request thread then allocates a network\n  port and adds it to the network info. This info object is then saved\n  back to the DB. In each case, the info contains the initial network\n  and the network that has been added by that thread. Therefore, the\n  last thread to save wins, and the other network is lost.\n\n  I have a patch that appears to fix the issue, by refreshing the info\n  cache whilst holding the refresh-cache-<id> lock. However, I'm not\n  intimately familiar with the nova networking code so would appreciate\n  more experienced eyes on it. I will submit the change to gerrit for\n  analysis and comments.\n\nTo manage notifications about this bug go to:\nhttps://bugs.launchpad.net/nova/+bug/1467581/+subscriptions\n", 
            "date_created": "2015-07-26 17:08:25+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/194597\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=0fb97014689b1b9575cafae88447db7f86ff4292\nSubmitter: Jenkins\nBranch:    master\n\ncommit 0fb97014689b1b9575cafae88447db7f86ff4292\nAuthor: Mark Goddard <email address hidden>\nDate:   Fri Jun 19 15:53:17 2015 +0100\n\n    Refresh instance info cache within lock\n    \n    Fix interface attachment bug where multiple concurrent attachment\n    requests can cause corruption of the nova instance info cache. This\n    change refreshes the info cache object from the database whilst\n    holding the refresh-cache lock, ensuring that changes are\n    synchronised.\n    \n    Change-Id: I6ea2eda8a61f418b0c32f13a7ed6904352712857\n    Closes-Bug: #1467581\n", 
            "date_created": "2015-07-31 16:02:25.679590+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Hello!   I see this bug has been fixed in the master trunk.  Is there any potential for this to be backported to Juno and / or Icehouse in the near future? \n\nThanks!", 
            "date_created": "2015-08-27 13:47:42.584148+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "So, I've tried to take the fix that has been proposed to the master branch and applied the changes to our lab (running OpenStack Juno release on Ubuntu Trusty 14.04).  The changes seem to fit correctly into the Juno source code, but I'm running into problems which makes me believe that this current fix does NOT work properly.\n\nNow, I'm not really familiar with what has changed between Juno and the current master trunk, so its possible that this fix is alright for Liberty, but just not correctly for Juno.  However, I suspect that the problem applies to both releases.  What I am seeing is that with this patch installed, if I tried to boot 9 nova VM's with 10 interfaces each only 2 or 3 successfully boot.  The problem in this case is not that interfaces are disappearing, but rather extra ones are being added.  Specifically, the nova network instance cache is getting 2 copies of some of the interfaces.  This results in libvirt xml being generated that has interfaces with identical mac addresses and network devices.  Such an XML will not boot successfully.  \n\nI've been tracing through the code, but I am not 100% sure exactly where this happens.  I suspect that by adding the \"compute_utils.refresh_info_cache_for_instance(context, instance)\" into \"_get_instance_nw_info\" this results in the cache being updated from the database in an incorrect time and results in the interface getting added twice.  \n\nSpecifically, I can see that the refresh will be called when  \"allocate_for_instance\" in \"network/neutronv2/api.py\" runs the following:\n\n        nw_info = self.get_instance_nw_info(context, instance,\n                                            networks=nets_in_requested_order,\n                                            port_ids=ports_in_requested_order)\n\nI suspect that by refreshing the cache at this point it is leading to the double interfaces.  However, I'm not sure I have followed the logic correctly, so maybe this is not the correct corruption point.\n\nAny feedback or insight on how to fix this would be appreciated.  Thank you!", 
            "date_created": "2015-08-29 02:08:53.486403+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Hey, after posting the last comment, I decided to try modifying the patch to only run \"compute_utils.refresh_info_cache_for_instance(context, instance)\" within \"_get_instance_nw_info\" if networks is set to None.  This way the refresh would still be run in most circumstances, but not from \"allocate_for_instance\".  I'm not sure if this is really the final, solution, but this seemed to allow me to build 20 out of 20 VM's with 10 interfaces successfully with all interfaces present.   I have to test this more, and I would like to understand the root cause of this issue, but this is an improvement.", 
            "date_created": "2015-08-29 03:23:35.999072+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Hi SecurityFun23, thanks for looking into this.\n\nThat's a worrying problem you've found. Without closely looking into the differences between Juno and Liberty it's hard to say whether this patch should work in Juno. I think the thing to do to gain some confidence in this patch would be to try to reproduce the issue on Liberty. I won't have time to do this until mid next week. If you have time, it would be much appreciated but no problem if not.\n\nYour potential solution looks interesting. I'll take a look into why it might be working for you.", 
            "date_created": "2015-08-29 09:19:39.891833+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Mark,\n\nWe don't have a Liberty setup right here to test it on.  I'll see about the possibility of setting one up, but I'm not sure how long that will take to get going, and there may be other priorities first.   Thanks for looking into what we have found!", 
            "date_created": "2015-08-30 02:56:42.160572+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Mark,\n\nWe were able to get a single node Liberty setup with the latest master trunk code.  We found that this exhibited the same exact symptoms as applying the patch for this bug in Juno (i.e, instances with many instances would normally NOT boot at all).  When we tried modifying the api.py to only run \"compute_utils.refresh_info_cache_for_instance(context, instance)\" within \"_get_instance_nw_info\" if networks is set to None, the instance booted successfully.  However, we did not that we still see the case that the instance cache is not complete.  It occurs much less often.  When building 9 nodes with 10 interfaces, we were normally seeing one interface missing out of 90.  When we modify api.py to never run \"compute_utils.refresh_info_cache_for_instance(context, instance)\" within \"_get_instance_nw_info\" (i.e., the pre-bugfix behavior), we also see the instances boot successfully, but then the number of missing interfaces in the cache is much higher.  I did not measure exactly, but the number looked like something around 30 to 50 percent.\n\nBased on this behavior, I would think that modifying the api.py to only run \"compute_utils.refresh_info_cache_for_instance(context, instance)\" within \"_get_instance_nw_info\" if networks is set to None, would be an improvement to the current liberty trunk (and maybe worth backporting to Juno / Kilo).  However, there is still an underlying root cause that has not been fixed.\n\nIt is unclear to me exactly where the race condition is occurring.  In our case, we are trying to boot multiple VM's with many interfaces (7 or more).   However, while there are multiple interfaces being added my understanding of the code is that \"allocate_for_instance\" goes through the networks and ports serially, so I don't see where this would generate a race condition.  It could be related to booting multiple VM's at the same time.  However, since the each would have different info caches's in the database, I'm not sure how that would be a race condition either.  If you could explain better where you though the race condition was occurring in our scenario, it might help us to come to a full solution.\n\nHere is a example of the type of server we are creating in our heat template:\n\n  Server:\n    type: OS::Nova::Server\n    properties:\n      image: \"cirros-0.3.3-x86_64\"\n      flavor: \"m1.tiny\"\n      networks:\n        - port: { get_resource: ServerPort }\n        - port: { get_resource: ServerPort1 }\n        - port: { get_resource: ServerPort2 }\n        - port: { get_resource: ServerPort3 }\n        - port: { get_resource: ServerPort4 }\n        - port: { get_resource: ServerPort5 }\n        - port: { get_resource: ServerPort6 }\n        - port: { get_resource: ServerPort7 }\n        - port: { get_resource: ServerPort8 }\n        - port: { get_resource: ServerPort9 }\n\nHere is an excerpt from nova compute log file around when a VM fails to boot in Liberty.  The first info message about the cache not being found is not the main problem, but I'm not sure if its a hint as to what is going on.  The real key message is the \"Error launching a defined domain with XML\" entry.  That gives us the XML file that we generated for libvirt.  That XML file should contain 10 interfaces but it actually contains 20, and that 20 contains some interfaces twice (this example has 20 so it may have all interfaces repeated, but I've seen others where it was a number like 17, so its not always all interfaces).  Just search for a mac address like \"fa:16:3e:ee:34:33\" in the output and you will see it as 2 separate interfaces.  Since these interfaces are using the same tap device, the VM will fail to boot because the tap interface is already in use.\n\nI hope all this is useful.  Thanks!\n\n\n2015-08-31 15:56:28.385 INFO nova.compute.manager [req-9ec4c498-3929-4762-8d30-c505a2d730a4 nova service] [instance: da466d6d-adfe-473c-9214-306b9aeea5cb] Failed to process external instance event network-changed due to: Info cache for instance da466d6d-adfe-473c-9214-306b9aeea5cb could not be found.\nTraceback (most recent call last):\n\n  File \"/opt/stack/nova/nova/conductor/manager.py\", line 444, in _object_dispatch\n    return getattr(target, method)(*args, **kwargs)\n\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_versionedobjects/base.py\", line 213, in wrapper\n    return fn(self, *args, **kwargs)\n\n  File \"/opt/stack/nova/nova/objects/instance_info_cache.py\", line 113, in refresh\n    self.instance_uuid)\n\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_versionedobjects/base.py\", line 171, in wrapper\n    result = fn(cls, context, *args, **kwargs)\n\n  File \"/opt/stack/nova/nova/objects/instance_info_cache.py\", line 74, in get_by_instance_uuid\n    instance_uuid=instance_uuid)\n\nInstanceInfoCacheNotFound: Info cache for instance da466d6d-adfe-473c-9214-306b9aeea5cb could not be found.\n\n2015-08-31 15:56:30.769 INFO nova.virt.libvirt.driver [req-50bfa8d0-a4da-4122-b33a-1abdca0860c5 xxxxxxxx YYY] [instance: d8a30f26-a1c3-4c38-b7dc-f885c70c4fa2] Deleting instance files /opt/stack/data/nova/instances/d8a30f26-a1c3-4c38-b7dc-f885c70c4fa2_del\n2015-08-31 15:56:30.770 INFO nova.virt.libvirt.driver [req-50bfa8d0-a4da-4122-b33a-1abdca0860c5 xxxxxxxx YYY] [instance: d8a30f26-a1c3-4c38-b7dc-f885c70c4fa2] Deletion of /opt/stack/data/nova/instances/d8a30f26-a1c3-4c38-b7dc-f885c70c4fa2_del complete\n2015-08-31 15:56:34.134 INFO nova.compute.resource_tracker [req-8b3f3bf6-0070-42f1-b1bd-cb3118fa9235 None None] Auditing locally available compute resources for node ubuntu-liberty-test\n2015-08-31 15:56:34.878 INFO nova.compute.resource_tracker [req-8b3f3bf6-0070-42f1-b1bd-cb3118fa9235 None None] Total usable vcpus: 16, total allocated vcpus: 3\n2015-08-31 15:56:34.878 INFO nova.compute.resource_tracker [req-8b3f3bf6-0070-42f1-b1bd-cb3118fa9235 None None] Final resource view: name=ubuntu-liberty-test phys_ram=24100MB used_ram=5632MB phys_disk=91GB used_disk=42GB total_vcpus=16 used_vcpus=3 pci_stats=None\n2015-08-31 15:56:34.913 INFO nova.compute.resource_tracker [req-8b3f3bf6-0070-42f1-b1bd-cb3118fa9235 None None] Compute_service record updated for ubuntu-liberty-test:ubuntu-liberty-test\n2015-08-31 15:56:38.824 ERROR nova.virt.libvirt.guest [req-070e314e-edf5-4664-a279-5008dbc88902 xxxxxxxx YYY] Error launching a defined domain with XML: <domain type='kvm'>\n  <name>instance-0000004d</name>\n  <uuid>340ee4af-1567-46aa-8491-ef1076c0d20f</uuid>\n  <metadata>\n    <nova:instance xmlns:nova=\"http://openstack.org/xmlns/libvirt/nova/1.0\">\n      <nova:package version=\"12.0.0\"/>\n      <nova:name>TestInt</nova:name>\n      <nova:creationTime>2015-08-31 19:56:27</nova:creationTime>\n      <nova:flavor name=\"m1.tiny\">\n        <nova:memory>512</nova:memory>\n        <nova:disk>1</nova:disk>\n        <nova:swap>0</nova:swap>\n        <nova:ephemeral>0</nova:ephemeral>\n        <nova:vcpus>1</nova:vcpus>\n      </nova:flavor>\n      <nova:owner>\n        <nova:user uuid=\"84097fc87bec4e13aa69cea099a64c3f\">xxxxxxxx</nova:user>\n        <nova:project uuid=\"65493157c7f24bc8938aa24e93d0e5cd\">YYY</nova:project>\n      </nova:owner>\n      <nova:root type=\"image\" uuid=\"d0ff7d72-ccb7-4bce-9c24-71dc319f3d80\"/>\n    </nova:instance>\n  </metadata>\n  <memory unit='KiB'>524288</memory>\n  <currentMemory unit='KiB'>524288</currentMemory>\n  <vcpu placement='static'>1</vcpu>\n  <cputune>\n    <shares>1024</shares>\n  </cputune>\n  <sysinfo type='smbios'>\n    <system>\n      <entry name='manufacturer'>OpenStack Foundation</entry>\n      <entry name='product'>OpenStack Nova</entry>\n      <entry name='version'>12.0.0</entry>\n      <entry name='serial'>39333835-3431-5355-4531-31364e383558</entry>\n      <entry name='uuid'>340ee4af-1567-46aa-8491-ef1076c0d20f</entry>\n      <entry name='family'>Virtual Machine</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type arch='x86_64' machine='pc-i440fx-trusty'>hvm</type>\n    <boot dev='hd'/>\n    <smbios mode='sysinfo'/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <cpu>\n    <topology sockets='1' cores='1' threads='1'/>\n  </cpu>\n  <clock offset='utc'>\n    <timer name='pit' tickpolicy='delay'/>\n    <timer name='rtc' tickpolicy='catchup'/>\n    <timer name='hpet' present='no'/>\n  </clock>\n  <on_poweroff>destroy</on_poweroff>\n  <on_reboot>restart</on_reboot>\n  <on_crash>destroy</on_crash>\n  <devices>\n    <emulator>/usr/bin/kvm-spice</emulator>\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='qcow2' cache='none'/>\n      <source file='/opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f/disk'/>\n      <target dev='vda' bus='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x17' function='0x0'/>\n    </disk>\n    <disk type='file' device='cdrom'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source file='/opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f/disk.config'/>\n      <target dev='hdd' bus='ide'/>\n      <readonly/>\n      <address type='drive' controller='0' bus='1' target='0' unit='1'/>\n    </disk>\n    <controller type='usb' index='0'>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>\n    </controller>\n    <controller type='pci' index='0' model='pci-root'/>\n    <controller type='ide' index='0'>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>\n    </controller>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:65:b5:5b'/>\n      <source bridge='qbr6dd4f814-7d'/>\n      <target dev='tap6dd4f814-7d'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:23:bb:d9'/>\n      <source bridge='qbr9fb2258d-c1'/>\n      <target dev='tap9fb2258d-c1'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:b7:2e:af'/>\n      <source bridge='qbr6ee7c04c-83'/>\n      <target dev='tap6ee7c04c-83'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:4b:a3:a1'/>\n      <source bridge='qbrfa6f0d20-43'/>\n      <target dev='tapfa6f0d20-43'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:a3:61:34'/>\n      <source bridge='qbrdb4707b1-83'/>\n      <target dev='tapdb4707b1-83'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:f4:72:b4'/>\n      <source bridge='qbr3a434ae9-f2'/>\n      <target dev='tap3a434ae9-f2'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:0f:a7:d3'/>\n      <source bridge='qbrf4b5202e-48'/>\n      <target dev='tapf4b5202e-48'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:b5:63:00'/>\n      <source bridge='qbr3df1d3ea-e3'/>\n      <target dev='tap3df1d3ea-e3'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:89:db:68'/>\n      <source bridge='qbr0a64210a-47'/>\n      <target dev='tap0a64210a-47'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0b' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:ee:34:33'/>\n      <source bridge='qbrf8009be3-51'/>\n      <target dev='tapf8009be3-51'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0c' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:a3:61:34'/>\n      <source bridge='qbrdb4707b1-83'/>\n      <target dev='tapdb4707b1-83'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0d' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:65:b5:5b'/>\n      <source bridge='qbr6dd4f814-7d'/>\n      <target dev='tap6dd4f814-7d'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0e' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:b7:2e:af'/>\n      <source bridge='qbr6ee7c04c-83'/>\n      <target dev='tap6ee7c04c-83'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x0f' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:4b:a3:a1'/>\n      <source bridge='qbrfa6f0d20-43'/>\n      <target dev='tapfa6f0d20-43'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x10' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:23:bb:d9'/>\n      <source bridge='qbr9fb2258d-c1'/>\n      <target dev='tap9fb2258d-c1'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x11' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:b5:63:00'/>\n      <source bridge='qbr3df1d3ea-e3'/>\n      <target dev='tap3df1d3ea-e3'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x12' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:89:db:68'/>\n      <source bridge='qbr0a64210a-47'/>\n      <target dev='tap0a64210a-47'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x13' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:f4:72:b4'/>\n      <source bridge='qbr3a434ae9-f2'/>\n      <target dev='tap3a434ae9-f2'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x14' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:0f:a7:d3'/>\n      <source bridge='qbrf4b5202e-48'/>\n      <target dev='tapf4b5202e-48'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x15' function='0x0'/>\n    </interface>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:ee:34:33'/>\n      <source bridge='qbrf8009be3-51'/>\n      <target dev='tapf8009be3-51'/>\n      <model type='virtio'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x16' function='0x0'/>\n    </interface>\n    <serial type='file'>\n      <source path='/opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f/console.log'/>\n      <target port='0'/>\n    </serial>\n    <serial type='pty'>\n      <target port='1'/>\n    </serial>\n    <console type='file'>\n      <source path='/opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f/console.log'/>\n      <target type='serial' port='0'/>\n    </console>\n    <input type='mouse' bus='ps2'/>\n    <input type='keyboard' bus='ps2'/>\n    <graphics type='vnc' port='-1' autoport='yes' listen='127.0.0.1' keymap='en-us'>\n      <listen type='address' address='127.0.0.1'/>\n    </graphics>\n    <video>\n      <model type='cirrus' vram='9216' heads='1'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>\n    </video>\n    <memballoon model='virtio'>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x18' function='0x0'/>\n      <stats period='10'/>\n    </memballoon>\n  </devices>\n</domain>\n\n2015-08-31 15:56:38.825 ERROR nova.compute.manager [req-070e314e-edf5-4664-a279-5008dbc88902 xxxxxxxx YYY] [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Instance failed to spawn\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Traceback (most recent call last):\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2138, in _build_resources\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     yield resources\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2008, in _build_and_run_instance\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     block_device_info=block_device_info)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 2455, in spawn\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     block_device_info=block_device_info)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 4529, in _create_domain_and_network\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     xml, pause=pause, power_on=power_on)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 4459, in _create_domain\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     guest.launch(pause=pause)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/virt/libvirt/guest.py\", line 141, in launch\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     self._encoded_xml, errors='ignore')\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 195, in __exit__\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     six.reraise(self.type_, self.value, self.tb)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/opt/stack/nova/nova/virt/libvirt/guest.py\", line 136, in launch\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     return self._domain.createWithFlags(flags)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     rv = execute(f, *args, **kwargs)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     six.reraise(c, e, tb)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     rv = meth(*args, **kwargs)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 900, in createWithFlags\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] libvirtError: Unable to create tap device tapdb4707b1-83: Device or resource busy\n2015-08-31 15:56:38.825 TRACE nova.compute.manager [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f]\n2015-08-31 15:56:38.826 INFO nova.compute.manager [req-070e314e-edf5-4664-a279-5008dbc88902 xxxxxxxx YYY] [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Terminating instance\n2015-08-31 15:56:38.837 INFO nova.virt.libvirt.driver [-] [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Instance destroyed successfully.\n2015-08-31 15:56:43.060 INFO nova.virt.libvirt.driver [req-070e314e-edf5-4664-a279-5008dbc88902 xxxxxxxx YYY] [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Deleting instance files /opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f_del\n2015-08-31 15:56:43.061 INFO nova.virt.libvirt.driver [req-070e314e-edf5-4664-a279-5008dbc88902 xxxxxxxx YYY] [instance: 340ee4af-1567-46aa-8491-ef1076c0d20f] Deletion of /opt/stack/data/nova/instances/340ee4af-1567-46aa-8491-ef1076c0d20f_del complete\n", 
            "date_created": "2015-09-01 03:17:32.082817+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Hi, thanks for your excellent work on investigating this problem.\n\nIt looks like your heat template attaches the interfaces all in one go when the instance is booted, rather than attaching them one by one after boot which is what the original reproducer was. Are those interfaces pre-created ports in Neutron, managed outside of Nova? I wonder whether the sheer number of interfaces is exposing a race condition that is not seen with fewer? When requesting networks at boot time, they should all be allocated in one go.\n\nWould it be possible for you to obtain logs from your system with debug=True in nova.conf? Ideally, logs for a single failing instance. No problem if not.\n\nThe first exception looks like it's due to the info cache not existing at the time when the refresh is called. The request being processed is actually a callback from Neutron with an updated set of networks. There is definitely scope for interaction here between the Neutron callback and the original boot request.\n\nOne thing to try off the top of my head - ignore the InstanceInfoCacheNotFound exception when refreshing the info cache in nova.compute.utils.refresh_info_cache_for_instance.", 
            "date_created": "2015-09-01 18:20:00.705051+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Mark,\n\nDoing a little more testing, I may have been mistaken about the case of missing one interface out of 90 with the \"modified\" patch.  I think I was not looking closely enough.  What I was doing was looking at \"nova list\" to see if any interfaces were missing.  However, since I was using the whole dynamic range of 192.168.0.2 through 192.168.0.254 (and similar subnets for the other interfaces), sometimes I would get a VM that would have slightly smaller line due to just have small numbers at the end of its IP addresses. \n\nNow, that I'm aware of that I'm paying more attention (and starting the range at 100 so all the IP's line up), I have not seen any interfaces missing with the \"modified\" patch.  So it does appear that the \"modified\" patch works, but I'm still not exactly clear of why the race condition occurs in the original patch during VM creation.  I understand that trying to refresh the cache during the creating is probably a bad idea, but I'm not sure why it would result in race condition type behavior instead of a consistent failure.\n\nAs for your question about the neutron ports.   The heat template that I'm using is creating the ports first and passing them into the server object.  Basically, in this case we have 10 ports like the following the feed into the nova server being created.\n\n  ServerPort1:\n    type: OS::Neutron::Port\n    properties:\n      name: \"TestIntPort\"\n      network: { get_param: ServerNet1 }\n\nI'm not sure if I was clear above, the log file excerpt in my last message was for the current master liberty trunk (where we can hardly ever boot a VM with 10 interfaces.  Where you asking for a more detail debug log of that failure?  Or were you looking for a debug log when I just lost one interface (which I now think was due to me misreading the screen)?\n\nThe InstanceInfoCacheNotFound exception is not really surprising in the standard liberty case, since it is trying to read the cache during the creation process (before the interfaces have been all provisioned)\n\nThanks!", 
            "date_created": "2015-09-01 19:31:13.391357+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Mark,\n\n       We have been running in our main lab for several weeks with the modified patch backported to Juno.  It seem to have been working fine.   Is there any update on getting this changed in Liberty going forward and or backporting this to Juno / Kilo?\n\nThanks!", 
            "date_created": "2015-09-22 15:49:13.709111+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Mark,\n\n     Any updates?\n\nThanks!", 
            "date_created": "2015-09-28 16:06:11.313463+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "Hi,\n\nI'm afraid I just don't have the time to work on this currently as I have internal deadlines to meet. I'm hoping to get some time towards the end of this week. It's also getting very near to the end of the Liberty cycle which makes me a little nervous about changing things. Sorry I can't be any more help at this time.", 
            "date_created": "2015-09-28 16:24:21.818372+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Mark,\n\n     I understand your constraints.  Its very difficult to fit everything in.  However, I do think its important to remember that the current fix that has been added to the trunk to fix the original issue will cause critical issue in Liberty for system that have multiple interfaces.  That seems to me to make it important to fix this before Liberty is released.  Hopefully, it is as simple as taking the simple modification that I proposed.\n\nThanks!", 
            "date_created": "2015-09-28 16:39:35.904528+00:00", 
            "author": "https://api.launchpad.net/1.0/~securityfun23"
        }, 
        {
            "content": "I've started to look into this. I have a theory on what may be happening but I need to investigate further. I will keep you posted.", 
            "date_created": "2015-09-29 17:10:38.492799+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "I've managed to reproduce the issue, and it looks as though my hunch was right. Neutron sends notifications to Nova when port state changes (nova.compute.manager.external_instance_event), and it's possible for this to become interleaved between the Neutron API call in allocate_for_instance that triggers it and the call to get_instance_nw_info that updates the cache. You can make the problem more reproducible by inserting a time.sleep(10) just before the call to get_instance_nw_info in allocate_for_instance.\n\nThe problem is that allocate_for_instance calls get_instance_nw_info with the new network info, which unconditionally adds that network info into the cache. If the Neutron notification (or indeed any other operation that refreshes the cache), then you'll end up with duplicate entries in the instance info cache.\n\nI don't have time to submit a patch today, but I'll get something in tomorrow after a bit more investigation. In the mean time, if you'd like to try my first bash at a fix, here's the patch, including the reproducing sleep.\n\ndiff --git a/nova/network/neutronv2/api.py b/nova/network/neutronv2/api.py\nindex ec1174b..f689516 100644\n--- a/nova/network/neutronv2/api.py\n+++ b/nova/network/neutronv2/api.py\n@@ -725,6 +725,10 @@ class API(base_api.NetworkAPI):\n                                        preexisting_port_ids,\n                                        neutron, port_client)\n                     self._delete_ports(neutron, instance, created_port_ids)\n+\n+        import time\n+        time.sleep(10)\n+\n         nw_info = self.get_instance_nw_info(\n             context, instance, networks=nets_in_requested_order,\n             port_ids=ports_in_requested_order,\n@@ -954,6 +961,9 @@ class API(base_api.NetworkAPI):\n         else:\n             # Since networks does not contain the existing networks on the\n             # instance we use their values from the cache and add it.\n+            net_ids = [iface['network']['id'] for iface in ifaces]\n+            networks = [network for network in networks\n+                        if network not in net_ids]\n             networks = networks + [\n                 {'id': iface['network']['id'],\n                  'name': iface['network']['label'],\n@@ -961,7 +971,10 @@ class API(base_api.NetworkAPI):\n                 for iface in ifaces]\n\n             # Include existing interfaces so they are not removed from the db.\n-            port_ids = [iface['id'] for iface in ifaces] + port_ids\n+            existing_port_ids = [iface['id'] for iface in ifaces]\n+            port_ids = [port_id for port_id in port_ids\n+                        if port_id not in existing_port_ids]\n+            port_ids = existing_port_ids + port_ids\n\n         return networks, port_ids", 
            "date_created": "2015-09-30 18:25:58.218884+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Bug for new issue raised as https://bugs.launchpad.net/nova/+bug/1501735. Please post further comments on there.", 
            "date_created": "2015-10-01 11:59:52.843690+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/230919", 
            "date_created": "2015-10-05 09:37:28.954838+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/233611", 
            "date_created": "2015-10-12 12:43:48.520428+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/252565", 
            "date_created": "2015-12-02 19:20:49.465990+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/260462", 
            "date_created": "2015-12-22 12:11:25.025543+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/265363", 
            "date_created": "2016-01-08 17:16:49.525442+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by sahid (<email address hidden>) on branch: master\nReview: https://review.openstack.org/265363", 
            "date_created": "2016-01-12 07:48:29.771306+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by sahid (<email address hidden>) on branch: master\nReview: https://review.openstack.org/260462", 
            "date_created": "2016-01-12 07:48:39.952684+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by sahid (<email address hidden>) on branch: master\nReview: https://review.openstack.org/260462", 
            "date_created": "2016-01-13 16:44:40.015849+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by sahid (<email address hidden>) on branch: master\nReview: https://review.openstack.org/265363", 
            "date_created": "2016-01-13 16:44:47.217114+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: stable/kilo\nReview: https://review.openstack.org/233611\nReason: This is not a security fix, and the later changes make this all confusing. ndipanov explained in irc this morning that basically if a branch doesn't have the locking change https://review.openstack.org/#/c/194597/ then we shouldn't backport it unless we were to also backport https://review.openstack.org/#/c/230919/ which we aren't going to do for kilo at this point, so I'm abandoning this.", 
            "date_created": "2016-01-14 15:02:17.514677+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/252565\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=3031adb857993d8196b4c9febca51ac82cf35fd6\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3031adb857993d8196b4c9febca51ac82cf35fd6\nAuthor: David Edery <email address hidden>\nDate:   Wed Dec 2 19:56:54 2015 +0100\n\n    ports & networks gather should validate existance\n    \n    _gather_port_ids_and_networks assumes that the input networks variable\n    doesn't contain the networks in ifaces. This is a wrong assumption\n    ever since the introduction of the \"refresh_cache-<instance id>\" locks to\n    the process in the Liberty cycle (see related bugs) and the\n    \"Refactor network API 'get_instance_nw_info'\" patchset\n    (https://review.openstack.org/#/c/146036/).\n    \n    The fix validates that the networks stated in ifaces doen't exist in the\n    gotten networks list.\n    \n    Duplicate networks were observed at the following closed/related bugs.\n    \n    Change-Id: I8c2c9e3c89babbe5e48c5129b9854013690b38f6\n    Closes-Bug: #1522112\n    Related-Bug: #1467581\n    Related-Bug: #1501735\n", 
            "date_created": "2016-01-18 09:22:45.029567+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/270686", 
            "date_created": "2016-01-21 09:23:47.024759+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/270814", 
            "date_created": "2016-01-21 13:59:06.353215+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/230919\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=8694c1619d774bb8a6c23ed4c0f33df2084849bc\nSubmitter: Jenkins\nBranch:    master\n\ncommit 8694c1619d774bb8a6c23ed4c0f33df2084849bc\nAuthor: Mark Goddard <email address hidden>\nDate:   Thu Oct 1 17:37:45 2015 +0100\n\n    network: Don't repopulate instance info cache from Neutron ports\n    \n    Allocation of network interfaces for an instance can result in\n    corruption of the instance info cache. The result is that the cache\n    may contain duplicate entries for network interfaces. This can cause\n    instance boot failure. This bug appears to be attributable to the\n    combined effects of the fixes for bugs #1467581 and #1407664.\n    \n    This change reverts the fix for bug #1407664, whilst keeping a\n    modified version of the unit test that was added with it. It also\n    adds a second unit test.\n    \n    Change-Id: I53d5284907d44ae8b5546993f8fd461b385c39e6\n    Closes-bug: #1501735\n    Related-bug: #1467581\n    Related-bug: #1407664\n", 
            "date_created": "2016-01-22 10:32:39.001190+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/270686\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=adc21bb0c62db822c3d656c4721f7b7c3ad41b7a\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit adc21bb0c62db822c3d656c4721f7b7c3ad41b7a\nAuthor: David Edery <email address hidden>\nDate:   Wed Dec 2 19:56:54 2015 +0100\n\n    ports & networks gather should validate existance\n    \n    _gather_port_ids_and_networks assumes that the input networks variable\n    doesn't contain the networks in ifaces. This is a wrong assumption\n    ever since the introduction of the \"refresh_cache-<instance id>\" locks to\n    the process in the Liberty cycle (see related bugs) and the\n    \"Refactor network API 'get_instance_nw_info'\" patchset\n    (https://review.openstack.org/#/c/146036/).\n    \n    The fix validates that the networks stated in ifaces doen't exist in the\n    gotten networks list.\n    \n    Duplicate networks were observed at the following closed/related bugs.\n    \n    Change-Id: I8c2c9e3c89babbe5e48c5129b9854013690b38f6\n    Closes-Bug: #1522112\n    Related-Bug: #1467581\n    Related-Bug: #1501735\n    (cherry picked from commit 3031adb857993d8196b4c9febca51ac82cf35fd6)\n", 
            "date_created": "2016-02-10 19:51:15.350327+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/270814\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=ce392292353c2b289aa3247b3c401f1cf7e1506c\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit ce392292353c2b289aa3247b3c401f1cf7e1506c\nAuthor: Mark Goddard <email address hidden>\nDate:   Thu Oct 1 17:37:45 2015 +0100\n\n    network: Don't repopulate instance info cache from Neutron ports\n    \n    Allocation of network interfaces for an instance can result in\n    corruption of the instance info cache. The result is that the cache\n    may contain duplicate entries for network interfaces. This can cause\n    instance boot failure. This bug appears to be attributable to the\n    combined effects of the fixes for bugs #1467581 and #1407664.\n    \n    This change reverts the fix for bug #1407664, whilst keeping a\n    modified version of the unit test that was added with it. It also\n    adds a second unit test.\n    \n    Conflicts:\n     nova/tests/unit/network/test_neutronv2.py\n    \n    This is because If0a1f3850d92d011aae32ae34e8c8664f2ee4313 isn't in\n    stable/liberty\n    \n    \n    Change-Id: I53d5284907d44ae8b5546993f8fd461b385c39e6\n    Closes-bug: #1501735\n    Related-bug: #1467581\n    Related-bug: #1407664\n    (cherry picked from commit 8694c1619d774bb8a6c23ed4c0f33df2084849bc)\n", 
            "date_created": "2016-06-07 20:54:18.085521+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
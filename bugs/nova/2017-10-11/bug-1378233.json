{
    "status": "Opinion", 
    "last_updated": "2014-12-04 10:11:37.161545+00:00", 
    "description": "It would be very useful for our case scenario to be able to have an option that enables not counting suspended machines as consuming resources. The use case is having little memory available and still being able to launch new VMs when old VMs are in suspended mode. We understand that once the compute node's memory is full we won't be able to resume these machines, but that is OK with the way we're using our cloud.\n\nFor example a compute node with 8G of RAM, launch 1 VM with 4G and another with 2G, then suspend them both, one could then launch a new VM with 4G of RAM (the actual memory on the compute node is free).\n\nOn essex we had the following patch for this scenario to work : \n\nIndex: nova/nova/scheduler/host_manager.py\n===================================================================\n--- nova.orig/nova/scheduler/host_manager.py\n+++ nova/nova/scheduler/host_manager.py\n@@ -337,6 +337,8 @@ class HostManager(object):\n             if not host:\n                 continue\n             host_state = host_state_map.get(host, None)\n+            if instance.get('power_state', 1) != 1: # power_state.RUNNING\n+                continue\n             if not host_state:\n                 continue\n             host_state.consume_from_instance(instance)\n\nWe're looking into patching icehouse for the same behaviour but would like to add it as an option this time.", 
    "tags": [], 
    "importance": "Wishlist", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1378233", 
    "owner": "None", 
    "id": 1378233, 
    "index": 1617, 
    "created": "2014-10-07 08:59:35.260884+00:00", 
    "title": "Provide an option to ignore suspended VMs in the resource count", 
    "comments": [
        {
            "content": "It would be very useful for our case scenario to be able to have an option that enables not counting suspended machines as consuming resources. The use case is having little memory available and still being able to launch new VMs when old VMs are in suspended mode. We understand that once the compute node's memory is full we won't be able to resume these machines, but that is OK with the way we're using our cloud.\n\nFor example a compute node with 8G of RAM, launch 1 VM with 4G and another with 2G, then suspend them both, one could then launch a new VM with 4G of RAM (the actual memory on the compute node is free).\n\nOn essex we had the following patch for this scenario to work : \n\nIndex: nova/nova/scheduler/host_manager.py\n===================================================================\n--- nova.orig/nova/scheduler/host_manager.py\n+++ nova/nova/scheduler/host_manager.py\n@@ -337,6 +337,8 @@ class HostManager(object):\n             if not host:\n                 continue\n             host_state = host_state_map.get(host, None)\n+            if instance.get('power_state', 1) != 1: # power_state.RUNNING\n+                continue\n             if not host_state:\n                 continue\n             host_state.consume_from_instance(instance)\n\nWe're looking into patching icehouse for the same behaviour but would like to add it as an option this time.", 
            "date_created": "2014-10-07 08:59:35.260884+00:00", 
            "author": "https://api.launchpad.net/1.0/~arthur-lutz-logilab"
        }, 
        {
            "content": "Taking a look at the new code, does anyone think this can be done using scheduler/weights/ram.py or scheduler/weights/metrics.py ? ", 
            "date_created": "2014-10-07 09:31:47.192142+00:00", 
            "author": "https://api.launchpad.net/1.0/~arthur-lutz-logilab"
        }, 
        {
            "content": "Perhaps the best way to kick of discussion about this would be a post to the openstack-dev mailing list or if you have a pretty clear plan submit a blueprint and nova spec for approval.", 
            "date_created": "2014-10-07 22:41:44.696767+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyeoh-0"
        }, 
        {
            "content": "No response on the mailing list http://lists.openstack.org/pipermail/openstack-dev/2014-October/048160.html \n\nWhere does that leave us ? Should we work on a patch ? Should we find a way to do this through some sort of extension/plugin ? ", 
            "date_created": "2014-11-13 14:12:19.266082+00:00", 
            "author": "https://api.launchpad.net/1.0/~arthur-lutz-logilab"
        }, 
        {
            "content": "Although this may be fine for your cloud, I don't think this is desirable in the general case.", 
            "date_created": "2014-11-19 19:09:56.675550+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "It seems that removing the RamFilter from the scheduler_default_filters gets the desired effect. \n\nDefault (on icehouse) is : \n\n#scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter\n\nUsing : \n\nscheduler_default_filters=RetryFilter,AvailabilityZoneFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter", 
            "date_created": "2014-12-04 10:11:36.326233+00:00", 
            "author": "https://api.launchpad.net/1.0/~arthur-lutz-logilab"
        }
    ]
}
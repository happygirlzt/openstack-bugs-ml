{
    "status": "Fix Released", 
    "last_updated": "2014-03-20 18:29:30.844968+00:00", 
    "description": "Simple to reproduce this with libvirt.  Let's assume there' s a single VM running on the hypervisor in question.  Gracefully shutdown the instance in question via the guest OS and wait for the power stat to reflect that it is shutdown.  Then reboot the hypervisor, then attempt to call 'nova start <uuid>'.  This operation will fail, because the VIFs, block devices, etc are all missing.\n\nThis comes down to us calling _create_domain() rather than _create_domain_and_network() within the libvirt driver.  The compute manager needs to pass more information into driver.power_on, so that we can rebuild the VIFs and block connections.  Basically, similar treatment to what we've already done for driver.resume(), driver.reboot(), etc.", 
    "tags": [
        "libvirt"
    ], 
    "importance": "Medium", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1105445", 
    "owner": "None", 
    "id": 1105445, 
    "index": 3210, 
    "created": "2013-01-25 21:12:02.452997+00:00", 
    "title": "Instance 'start' will fail after hypervisor reboot", 
    "comments": [
        {
            "content": "Simple to reproduce this with libvirt.  Let's assume there' s a single VM running on the hypervisor in question.  Gracefully shutdown the instance in question via the guest OS and wait for the power stat to reflect that it is shutdown.  Then reboot the hypervisor, then attempt to call 'nova start <uuid>'.  This operation will fail, because the VIFs, block devices, etc are all missing.\n\nThis comes down to us calling _create_domain() rather than _create_domain_and_network() within the libvirt driver.  The compute manager needs to pass more information into driver.power_on, so that we can rebuild the VIFs and block connections.  Basically, similar treatment to what we've already done for driver.resume(), driver.reboot(), etc.", 
            "date_created": "2013-01-25 21:12:02.452997+00:00", 
            "author": "https://api.launchpad.net/1.0/~rkhardalian"
        }, 
        {
            "content": "I tried this scenario out using devstack and the latest code.  \n\nMy environment was setup as follows:\n- node1 (controller & compute):  Running glance, keystone, nova (n-api, n-sch, n-cpu), neutron, rabbit, and mysql\n- node2 (compute): Running nova (n-cpu) and neutron.\n\nFollowing the steps above in my environment:\n1.  Spawned an instance on node2.\n2.  Gracefully shutdown the instance on node2.\n3.  Waited for the power state to show the instance is shutdown.\n4.  Rebooted the hypervisor.\n5.  Called \"nova start <instance>\", which properly started the instance.\n\nI did not observe the same behavior as documented in this bug.  I also looked at  nova/nova/compute/manager.py and notice the driver.power_on has the proper network_info and block_device_info passed to it (similar how it is done in the resume_instance and reboot_instance methods):\n\ndef _power_on(self, context, instance):\n        network_info = self._get_instance_nw_info(context, instance)\n        block_device_info = self._get_instance_volume_block_device_info(\n                                context, instance)\n        self.driver.power_on(context, instance,\n                             network_info,\n                             block_device_info)\n\nWhat does the environment look like?  Anything in the logs?", 
            "date_created": "2014-03-20 17:55:32.920007+00:00", 
            "author": "https://api.launchpad.net/1.0/~thang-pham"
        }, 
        {
            "content": "This was fixed during the Havana cycle:\n\nhttps://github.com/openstack/nova/commit/db3989586a8d5bbbcf857b9294a124ecc5fc57e8\n\nWe can close out the bug.", 
            "date_created": "2014-03-20 18:13:10.981979+00:00", 
            "author": "https://api.launchpad.net/1.0/~rkhardalian"
        }
    ]
}
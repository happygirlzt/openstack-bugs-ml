{
    "status": "Expired", 
    "last_updated": "2017-07-17 04:17:34.049211+00:00", 
    "description": "I have a devstack change up which runs tempest with nova + the lxc backend and it fails some instances on boot because the nbd device is in use or the rootfs mount is busy:\n\nhttps://review.openstack.org/#/c/219448/\n\nhttp://logs.openstack.org/48/219448/1/check/gate-tempest-dsvm-full/bc0fc2a/logs/screen-n-cpu.txt.gz?level=TRACE\n\nIt looks like we're leaking nbd devices and failing to tear down instances properly, e.g.:\n\n2015-09-01 22:10:50.141 ERROR nova.virt.disk.api [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] Failed to mount container filesystem '<nova.virt.disk.api._DiskImage object at 0x7f94ec58cd90>' on '/opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs': \n--\nFailed to mount filesystem: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\nExit code: 32\nStdout: u''\nStderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n'\n2015-09-01 22:10:50.142 ERROR nova.compute.manager [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Instance failed to spawn\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Traceback (most recent call last):\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2138, in _build_resources\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     yield resources\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2008, in _build_and_run_instance\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info=block_device_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2449, in spawn\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info=block_device_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4521, in _create_domain_and_network\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info, disk_info):\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     return self.gen.next()\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4430, in _lxc_disk_handler\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info, disk_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4382, in _create_domain_setup_lxc\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     container_dir=container_dir)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/disk/api.py\", line 452, in setup_container\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     raise exception.NovaException(img.errors)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] NovaException: \n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] --\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Failed to mount filesystem: Unexpected error while running command.\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Exit code: 32\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Stdout: u''\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Stderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n'\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] \n\n\n\n\n2015-09-01 22:10:50.025 DEBUG nova.virt.disk.mount.api [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] Failed to mount filesystem: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\nExit code: 32\nStdout: u''\nStderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n' mnt_dev /opt/stack/new/nova/nova/virt/disk/mount/api.py:237\n\netc", 
    "tags": [
        "libvirt", 
        "lxc", 
        "nbd"
    ], 
    "importance": "High", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1491563", 
    "owner": "None", 
    "id": 1491563, 
    "index": 1804, 
    "created": "2015-09-02 19:43:36.614622+00:00", 
    "title": "Failed to spawn lxc instance due to nbd mount in use or rootfs busy", 
    "comments": [
        {
            "content": "I have a devstack change up which runs tempest with nova + the lxc backend and it fails some instances on boot because the nbd device is in use or the rootfs mount is busy:\n\nhttps://review.openstack.org/#/c/219448/\n\nhttp://logs.openstack.org/48/219448/1/check/gate-tempest-dsvm-full/bc0fc2a/logs/screen-n-cpu.txt.gz?level=TRACE\n\nIt looks like we're leaking nbd devices and failing to tear down instances properly, e.g.:\n\n2015-09-01 22:10:50.141 ERROR nova.virt.disk.api [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] Failed to mount container filesystem '<nova.virt.disk.api._DiskImage object at 0x7f94ec58cd90>' on '/opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs': \n--\nFailed to mount filesystem: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\nExit code: 32\nStdout: u''\nStderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n'\n2015-09-01 22:10:50.142 ERROR nova.compute.manager [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Instance failed to spawn\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Traceback (most recent call last):\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2138, in _build_resources\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     yield resources\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2008, in _build_and_run_instance\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info=block_device_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2449, in spawn\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info=block_device_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4521, in _create_domain_and_network\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info, disk_info):\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     return self.gen.next()\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4430, in _lxc_disk_handler\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     block_device_info, disk_info)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4382, in _create_domain_setup_lxc\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     container_dir=container_dir)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]   File \"/opt/stack/new/nova/nova/virt/disk/api.py\", line 452, in setup_container\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78]     raise exception.NovaException(img.errors)\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] NovaException: \n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] --\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Failed to mount filesystem: Unexpected error while running command.\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Exit code: 32\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Stdout: u''\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] Stderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n'\n2015-09-01 22:10:50.142 4548 ERROR nova.compute.manager [instance: a9317161-ad31-4948-9c2d-3d4899718a78] \n\n\n\n\n2015-09-01 22:10:50.025 DEBUG nova.virt.disk.mount.api [req-7e5384a8-34f3-4e3a-98cd-b4bb544804a7 tempest-ServersAdminTestJSON-276960936 tempest-ServersAdminTestJSON-718419646] Failed to mount filesystem: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd4 /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs\nExit code: 32\nStdout: u''\nStderr: u'mount: /dev/nbd4 already mounted or /opt/stack/data/nova/instances/a9317161-ad31-4948-9c2d-3d4899718a78/rootfs busy\\n' mnt_dev /opt/stack/new/nova/nova/virt/disk/mount/api.py:237\n\netc", 
            "date_created": "2015-09-02 19:43:36.614622+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/219859", 
            "date_created": "2015-09-02 19:54:29.653915+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Some notes from a failed delete in\n\nhttp://logs.openstack.org/48/219448/1/check/gate-tempest-dsvm-full/bc0fc2a/logs/screen-n-cpu.txt.gz\n\nMounted /dev/nbd13 here:\n\n2015-09-01 22:10:36.697 DEBUG oslo_concurrency.processutils [req-5a3718b5-5042-4bd3-b82f-6868097f6f47 tempest-\n\nAggregatesAdminTestJSON-1589136606 tempest-AggregatesAdminTestJSON-1584222786] CMD \"sudo nova-rootwrap \n\n/etc/nova/rootwrap.conf mount /dev/nbd13 /opt/stack/data/nova/instances/87a8fc91-5d8b-474c-aedf-36bf8300282e/rootfs\" \n\nreturned: 0 in 0.066s execute /usr/local/lib/python2.7/dist-packages/oslo_concurrency/processutils.py:260\n\nUnmounted /dev/nbd13 here (should be from clean_lxc_namespace):\n\n2015-09-01 22:10:42.569 DEBUG oslo_concurrency.processutils [req-5a3718b5-5042-4bd3-b82f-6868097f6f47 tempest-\n\nAggregatesAdminTestJSON-1589136606 tempest-AggregatesAdminTestJSON-1584222786] CMD \"sudo nova-rootwrap \n\n/etc/nova/rootwrap.conf umount /dev/nbd13\" returned: 0 in 0.555s execute /usr/local/lib/python2.7/dist-\n\npackages/oslo_concurrency/processutils.py:260\n\nReleased the /dev/nbd13 device here (should be from _teardown_container):\n\n2015-09-01 22:10:49.043 DEBUG oslo_concurrency.processutils [req-811e7df5-a137-41cb-8c3a-918712f61e46 tempest-\n\nAggregatesAdminTestJSON-1589136606 tempest-AggregatesAdminTestJSON-1584222786] CMD \"sudo nova-rootwrap \n\n/etc/nova/rootwrap.conf qemu-nbd -d /dev/nbd13\" returned: 0 in 0.103s execute /usr/local/lib/python2.7/dist-\n\npackages/oslo_concurrency/processutils.py:260\n\nDeleting the instance files fails here, is it still mounted?\n\n2015-09-01 22:10:49.449 ERROR nova.virt.libvirt.driver [req-811e7df5-a137-41cb-8c3a-918712f61e46 tempest-\n\nAggregatesAdminTestJSON-1589136606 tempest-AggregatesAdminTestJSON-1584222786] [instance: 87a8fc91-5d8b-474c-aedf-\n\n36bf8300282e] Failed to cleanup directory /opt/stack/data/nova/instances/87a8fc91-5d8b-474c-aedf-36bf8300282e_del: [Errno \n\n16] Device or resource busy: '/opt/stack/data/nova/instances/87a8fc91-5d8b-474c-aedf-36bf8300282e_del/rootfs'\n\n\ndestroy calls:\n\t_destroy calls:\n\t\t_teardown_container executes:\n\t\t\tqemu-nbd -d\n\tcleanup calls:\n\t\tdelete_instance_files:\n\t\t\tThis is what fails saying ", 
            "date_created": "2015-09-02 20:31:50.783206+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "https://lists.nongnu.org/archive/html/qemu-devel/2011-12/msg00734.html seems to suggest that qemu-nbd is asynchronous, which could mean that qemu-nbd -d is not done by the time we try to delete the rootfs.", 
            "date_created": "2015-09-02 20:36:48.819693+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I wonder if this is related?\n\nhttps://bugs.launchpad.net/ubuntu/+source/qemu/+bug/1435428", 
            "date_created": "2015-09-02 21:16:23.261324+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "There is also quite a bit of history here:\n\nhttps://bugs.launchpad.net/nova/+bug/973413", 
            "date_created": "2015-09-02 21:20:23.789381+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This one is confusing to me. I can't think of the exact problems that would cause the errors shown in this log.\n\nWill add some extra extra logging and re-run to try and track it down.\n\nThe 'Failed to cleanup directory' messages where the instance directory is being deleted shouldn't be caused by a failure to shut down the container or remove the nbd device. The rootfs is only temporarily mounted in the instance directory and is unmounted from there once the container is running and the device in mounted in the container namespace. (See virt/disk/api.py clean_lxc_namespace). Its possible I guess that there is a race condition here (Not accounted for) that means we are trying to delete the instance directory before the container is fully set up.\n\nThe '/dev/nbd4 already mounted' error shouldn't be related to a failure to disconnect previous nbd devices as the code in virt/disk/mount/nbd.py _find_unused should only get free nbd devices or error out.\n\n", 
            "date_created": "2015-09-11 16:51:52.361868+00:00", 
            "author": "https://api.launchpad.net/1.0/~mjdoffma"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/223125", 
            "date_created": "2015-09-14 14:32:12.094482+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Added logging to try and see what is happening in this directory that is causing it to be busy.\n\nLogs available at http://logs.openstack.org/48/219448/4/check/gate-tempest-dsvm-full/a3552a1/logs/screen-n-cpu.txt.gz.\n\nAdded 'lsof' and 'mountpoint' commands. \n\nUnfortunately / Fortunately neither show any cause for the busy error. The directory is not a mountpoint and doesn't seem to have any processes with open files. At least, not in the top level of the directory. Need to check if I have the 'mountpoint' command pointed at the wrong place. \n\nInstead of 'instance_del/' perhaps 'instance_del/rootfs' is the correct directory to check if still mounted?", 
            "date_created": "2015-10-01 16:35:36.601359+00:00", 
            "author": "https://api.launchpad.net/1.0/~mjdoffma"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/219859\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=b9e997617fd21e260bd532670b3e70a4f66dbf2a\nSubmitter: Jenkins\nBranch:    master\n\ncommit b9e997617fd21e260bd532670b3e70a4f66dbf2a\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Sep 2 12:46:13 2015 -0700\n\n    libvirt: add debug logging for lxc teardown paths\n    \n    There are races to setup and teardown lxc-backed instances using nbd\n    mounts when running tempest which result in mount failing.\n    \n    This change adds some debug logging to help figure out what's going on\n    to see where the race is happening.\n    \n    Related-Bug: #1491563\n    \n    Change-Id: Ief8edef0df8da1a6ee94aafa9ceaf96da5e3211b\n", 
            "date_created": "2015-10-29 07:17:17.477460+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/223125\nReason: The DNM testing changes I had are abandoned so let's drop this.", 
            "date_created": "2015-12-02 03:13:42.908121+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "@johnthetubaguy: Any reason this is set to High? Seems like an LXC-specific bug, which IMHO, should make it nothing higher than Medium...\n\nAlso, is the bug condition even reproducible any more?\n\nGoing to set the bug to Incomplete until we can get a reproduction on a modern Nova.\n\n-jay", 
            "date_created": "2017-05-17 06:20:12.983081+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2017-07-17 04:17:30.163762+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ]
}
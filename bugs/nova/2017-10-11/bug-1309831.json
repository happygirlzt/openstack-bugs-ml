{
    "status": "Expired", 
    "last_updated": "2017-07-04 04:18:03.160844+00:00", 
    "description": "http://logs.openstack.org/65/88165/2/check/check-tempest-dsvm-full/da0fa73/console.html\n\n....\n\nTraceback (most recent call last):\n  File \"tempest/api/compute/v3/servers/test_server_actions.py\", line 209, in test_resize_server_revert\n    self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE')\n  File \"tempest/services/compute/v3/json/servers_client.py\", line 168, in wait_for_server_status\n    raise_on_error=raise_on_error)\n  File \"tempest/common/waiters.py\", line 89, in wait_for_server_status\n    raise exceptions.TimeoutException(message)\nTimeoutException: Request timed out\nDetails: Server 65867fad-6671-4912-8f1c-28977ad404d1 failed to reach VERIFY_RESIZE status and task state \"None\" within the required time (196 s). Current status: RESIZE. Current task state: resize_finish.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1309831", 
    "owner": "None", 
    "id": 1309831, 
    "index": 4781, 
    "created": "2014-04-19 03:04:28.919047+00:00", 
    "title": "failed to reach VERIFY_RESIZE", 
    "comments": [
        {
            "content": "http://logs.openstack.org/65/88165/2/check/check-tempest-dsvm-full/da0fa73/console.html\n\n....\n\nTraceback (most recent call last):\n  File \"tempest/api/compute/v3/servers/test_server_actions.py\", line 209, in test_resize_server_revert\n    self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE')\n  File \"tempest/services/compute/v3/json/servers_client.py\", line 168, in wait_for_server_status\n    raise_on_error=raise_on_error)\n  File \"tempest/common/waiters.py\", line 89, in wait_for_server_status\n    raise exceptions.TimeoutException(message)\nTimeoutException: Request timed out\nDetails: Server 65867fad-6671-4912-8f1c-28977ad404d1 failed to reach VERIFY_RESIZE status and task state \"None\" within the required time (196 s). Current status: RESIZE. Current task state: resize_finish.", 
            "date_created": "2014-04-19 03:04:28.919047+00:00", 
            "author": "https://api.launchpad.net/1.0/~harlowja"
        }, 
        {
            "content": "I saw \"failed to reach VERIFY_RESIZE status and task state\" fall out of \"tempest.api.compute.servers.test_disk_config.ServerDiskConfigTestXML.test_resize_server_from_manual_to_auto\".  I don't guess the test ever saw an error, it just gave up \"within the required time (196 s).\"\n\nThere was a conflict (409) api request in the remaining traceback \"Cannot 'resize' while instance is in task_state resize_migrating\" :\n\nhttp://logs.openstack.org/87/87387/7/gate/gate-tempest-dsvm-postgres-full/fecf28d/console.html\n\n^ I guess that was after the timeout?\n\nThe nova compute logs show that it seems to think it lost track of the instance, some sort of libvirt communication error maybe:\n\nhttp://logs.openstack.org/87/87387/7/gate/gate-tempest-dsvm-postgres-full/fecf28d/logs/screen-n-cpu.txt.gz?level=WARNING\n\nBut ,maybe related, there was a terminate instance request (req-f160d8e5-ca3b-48d7-b3e3-90374bf6758d) that was processed while this was going on:\n\n2014-05-05 09:48:40.147 INFO nova.osapi_compute.wsgi.server [req-f160d8e5-ca3b-48d7-b3e3-90374bf6758d ServerDiskConfigTestXML-661532059 ServerDiskConfigTestXML-342531198] 127.0.0.1 \"DELETE /v2/cb0a7aaf62f7428c86514d7aa75ca520/servers/3f29caac-09d6-4f36-8095-d2ad3a75699b HTTP/1.1\" status: 204 len: 197 time: 0.2444570\n\n... and in nova compute:\n\n2014-05-05 09:48:40.235 AUDIT nova.compute.manager [req-f160d8e5-ca3b-48d7-b3e3-90374bf6758d ServerDiskConfigTestXML-661532059 ServerDiskConfigTestXML-342531198] [instance: 3f29caac-09d6-4f36-8095-d2ad3a75699b] Terminating instance\n\nIt's only later then that you see the libvirt error pop:\n\n2014-05-05 09:48:43.310 WARNING nova.virt.libvirt.driver [req-2585193a-c0b5-437d-8bf3-302a83e9c62f ServerDiskConfigTestXML-661532059 ServerDiskConfigTestXML-342531198] Error from libvirt while getting description of instance-00000032: [Error Code 38] Unable to read from monitor: Connection reset by peer\n2014-05-05 09:48:43.310 ERROR nova.compute.manager [req-2585193a-c0b5-437d-8bf3-302a83e9c62f ServerDiskConfigTestXML-661532059 ServerDiskConfigTestXML-342531198] [instance: 3f29caac-09d6-4f36-8095-d2ad3a75699b] Setting instance vm_state to ERROR\n\n^ but that may have all been after the resize already timed out, and then the test cleanup was just failing.\n\nI didn't really see anything indicating why the resize may have failed.", 
            "date_created": "2014-05-05 19:16:33.182194+00:00", 
            "author": "https://api.launchpad.net/1.0/~clay-gerrard"
        }, 
        {
            "content": "Hrmm... so it's a little more obvious after looking at another state change timeout failure on another test, that the errant DELETE request coming through during the time tempest seemed to be in a poll loop waiting on status is just the test cleanup\n\nreq-8b3684a4-33af-4136-9b80-e4ba1570fe48 - http://logs.openstack.org/65/87265/2/check/check-tempest-dsvm-postgres-full/3e4f954/logs/screen-n-api.txt.gz\n\n2014-05-05 07:42:42.626 INFO nova.osapi_compute.wsgi.server [req-8b3684a4-33af-4136-9b80-e4ba1570fe48 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] 127.0.0.1 \"DELETE /v2/d541b859eb53445798138a740acc73a6/servers/11f120f8-3fcb-4767-9d6e-5c2daeebe0cc HTTP/1.1\" status: 204 len: 197 time: 0.2575510\n\nI think this is telling me that tempest had actually just given up:\n\n2014-05-05 07:53:05.996 | 2014-05-05 07:42:42.334 9644 TRACE tempest.api.compute.base Details: Server 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc failed to reach ACTIVE status and task state \"None\" within the required time (196 s). Current status: ACTIVE. Current task state: pausing.\n2014-05-05 07:53:05.996 | 2014-05-05 07:42:42.334 9644 TRACE tempest.api.compute.base \n2014-05-05 07:53:05.996 | 2014-05-05 07:42:42,604 Request (ServersNegativeTestXML:tearDown): 204 DELETE http://127.0.0.1:8774/v2/d541b859eb53445798138a740acc73a6/servers/11f120f8-3fcb-4767-9d6e-5c2daeebe0cc 0.237s\n\nAnd then after processing the DELETE nova compute blows up a few seconds later (i think I was mistakingly comparing the when the test run logged the test output on the far left, instead of when the request was made)\n\n2014-05-05 07:42:46.393 31628 WARNING nova.compute.manager [-] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Instance shutdown by itself. Calling the stop API.\n2014-05-05 07:42:46.864 ERROR nova.virt.libvirt.driver [req-44901be6-30e8-4560-aa72-a5b3763468b2 None None] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Error from libvirt during destroy. Code=55 Error=Requested operation is not valid: domain is not running\n\n\nBut again, I don't really see a *cause* for the error, request comes in:\n\n2014-05-05 07:36:10.884 AUDIT nova.compute.manager [req-7b1cc76b-bb6c-48ab-a311-6c8c80eef354 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Pausing\n\nThen after a long wait with nothing interesting that I could see in between, the destroy blows things up and seems to unwedge it, then the pause fails:\n\n2014-05-05 07:42:42.683 AUDIT nova.compute.manager [req-8b3684a4-33af-4136-9b80-e4ba1570fe48 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Terminating instance\n2014-05-05 07:42:45.916 31628 DEBUG nova.virt.driver [-] Emitting event <nova.virt.event.LifecycleEvent object at 0x4076150> emit_event /opt/stack/new/nova/nova/virt/driver.py:1210\n2014-05-05 07:42:45.917 31628 INFO nova.compute.manager [-] Lifecycle event 1 on VM 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc\n2014-05-05 07:42:45.971 ERROR nova.virt.libvirt.driver [req-8b3684a4-33af-4136-9b80-e4ba1570fe48 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Error from libvirt during destroy. Code=55 Error=Requested operation is not valid: domain is not running\n2014-05-05 07:42:45.971 DEBUG nova.compute.manager [req-8b3684a4-33af-4136-9b80-e4ba1570fe48 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] [instance: 11f120f8-3fcb-4767-9d6e-5c2daeebe0cc] Deallocating network for instance _deallocate_network /opt/stack/new/nova/nova/compute/manager.py:1810\n2014-05-05 07:42:46.276 DEBUG nova.openstack.common.lockutils [req-7b1cc76b-bb6c-48ab-a311-6c8c80eef354 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] Got semaphore \"compute_resources\" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168\n2014-05-05 07:42:46.276 DEBUG nova.openstack.common.lockutils [req-7b1cc76b-bb6c-48ab-a311-6c8c80eef354 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] Got semaphore / lock \"update_usage\" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248\n2014-05-05 07:42:46.298 DEBUG nova.openstack.common.lockutils [req-7b1cc76b-bb6c-48ab-a311-6c8c80eef354 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] Semaphore / lock released \"update_usage\" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252\n2014-05-05 07:42:46.304 ERROR oslo.messaging.rpc.dispatcher [req-7b1cc76b-bb6c-48ab-a311-6c8c80eef354 ServersNegativeTestXML-102762525 ServersNegativeTestXML-175555124] Exception during message handling: Unable to read from monitor: Connection reset by peer\n\nIdk, maybe there was some related libvirt errors:\n\n2014-05-05 07:39:48.000+0000: 20000: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:40:16.007+0000: 16158: error : virNetDevGetIndex:656 : Unable to get index for interface vnet1: No such device\n2014-05-05 07:40:18.002+0000: 20000: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:40:50.000+0000: 16152: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:40:52.394+0000: 16153: error : virNetDevGetIndex:656 : Unable to get index for interface vnet3: No such device\n2014-05-05 07:40:58.557+0000: 16153: error : virNetDevGetIndex:656 : Unable to get index for interface vnet2: No such device\n2014-05-05 07:41:14.478+0000: 16152: error : virNetDevGetIndex:656 : Unable to get index for interface vnet2: No such device\n2014-05-05 07:41:21.000+0000: 16154: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:41:44.806+0000: 16161: error : virNetDevGetIndex:656 : Unable to get index for interface vnet1: No such device\n2014-05-05 07:41:51.000+0000: 16152: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:42:21.000+0000: 16152: error : qemuDomainObjBeginJobInternal:789 : Timed out during operation: cannot acquire state change lock\n2014-05-05 07:42:24.405+0000: 16151: error : qemuMonitorIO:603 : internal error End of file from monitor\n2014-05-05 07:42:24.407+0000: 16151: error : virNetDevGetIndex:656 : Unable to get index for interface vnet1: No such device\n2014-05-05 07:42:44.355+0000: 16151: error : qemuMonitorIORead:513 : Unable to read from monitor: Connection reset by peer\n\n", 
            "date_created": "2014-05-05 20:11:41.293248+00:00", 
            "author": "https://api.launchpad.net/1.0/~clay-gerrard"
        }, 
        {
            "content": "Can You help me reproducing the error!", 
            "date_created": "2014-07-09 05:39:25.621352+00:00", 
            "author": "https://api.launchpad.net/1.0/~amitpp23"
        }, 
        {
            "content": "This bug seems to be appeared very rarely recently.(Only two times in 3 months.)\nWe need more information for investigation.\n\nmessage:\"failed to reach VERIFY_RESIZE status\"\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiZmFpbGVkIHRvIHJlYWNoIFZFUklGWV9SRVNJWkUgc3RhdHVzXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImFsbCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTI5MDY1OTMyOTB9\n", 
            "date_created": "2014-10-10 02:11:01.107447+00:00", 
            "author": "https://api.launchpad.net/1.0/~igawa"
        }, 
        {
            "content": "Guys, any updates for this bug? Has anyone encountered the issue recently?", 
            "date_created": "2015-06-30 14:15:14.741800+00:00", 
            "author": "https://api.launchpad.net/1.0/~ylobankov"
        }, 
        {
            "content": "hello, this is happening in our side.. when we try to run whole compute api tests, all tests passed except this :\n\n\"\"\"\nTest Result (1 failure / -10)\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto[id-414e7e93-45b5-44bc-8e03-55159c6bfc97]\nStacktrace\n\nTraceback (most recent call last):\ntesttools.testresult.real._StringException: Empty attachments:\n  pythonlogging:''\n  stderr\n  stdout\n\nTraceback (most recent call last):\n  File \"/root/jenkins/workspace/tempest_cloud_compute_keystone_v3_as_non-admin_1.0.0.1/tempest-repo/tempest/tempest/api/compute/servers/test_disk_config.py\", line 107, in test_resize_server_from_manual_to_auto\n    'VERIFY_RESIZE')\n  File \"/root/jenkins/workspace/tempest_cloud_compute_keystone_v3_as_non-admin_1.0.0.1/tempest-repo/tempest/tempest/common/waiters.py\", line 95, in wait_for_server_status\n    raise exceptions.TimeoutException(message)\ntempest.exceptions.TimeoutException: Request timed out\nDetails: (ServerDiskConfigTestJSON:test_resize_server_from_manual_to_auto) Server 81ecbb17-ff3e-4b09-b099-8a6af8430d76 failed to reach VERIFY_RESIZE status and task state \"None\" within the required time (600 s). Current status: ACTIVE. Current task state: None.\n\"\"\"\n\n\nhowever when i manually run this single test with ./run_tempest.sh it passes instantly:\n\n`\ntempest-repo/tempest # ./run_tempest.sh tempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\n\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON\n    test_resize_server_from_manual_to_auto[id-414e7e93-45b5-44bc-8e03-55159c6bfc97]OK  32.91\n\nSlowest 1 tests took 32.91 secs:\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON\n    test_resize_server_from_manual_to_auto[id-414e7e93-45b5-44bc-8e03-55159c6bfc97]  32.91\n\nRan 1 test in 97.502s\n\nOK\n`\n\n", 
            "date_created": "2016-06-14 05:17:05.385669+00:00", 
            "author": "https://api.launchpad.net/1.0/~hayderimran7"
        }, 
        {
            "content": "This issue still exists like http://logs.openstack.org/45/350645/1/check/gate-tempest-dsvm-multinode-full/defb83c/\n\nIn most cases I checked today, the issue happened in gate-tempest-dsvm-multinode-full", 
            "date_created": "2016-08-16 20:05:21.005002+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "http://logs.openstack.org/45/350645/1/check/gate-tempest-dsvm-multinode-full/defb83c/\n\nTraceback (most recent call last):\n  File \"tempest/api/compute/servers/test_server_actions.py\", line 296, in test_resize_server_confirm\n    self._test_resize_server_confirm(stop=False)\n  File \"tempest/api/compute/servers/test_server_actions.py\", line 274, in _test_resize_server_confirm\n    'VERIFY_RESIZE')\n  File \"tempest/common/waiters.py\", line 95, in wait_for_server_status\n    raise exceptions.TimeoutException(message)\ntempest.exceptions.TimeoutException: Request timed out\nDetails: (ServerActionsTestJSON:test_resize_server_confirm) Server c07855e4-1064-4964-a354-170565069d8b failed to reach VERIFY_RESIZE status and task state \"None\" within the required time (196 s). Current status: RESIZE. Current task state: resize_finish.\n\nNova compute output the error log related to the instance (c07855e4-..)\n\n2016-08-12 17:44:53.701 18249 ERROR nova.compute.resource_tracker [req-b81ccfed-e879-4b9e-a738-f4fba78d0670 - -] Migration for instance c07855e4-1064-4964-a354-170565069d8b refers to another host's instance!\n\nThe error is not expected according to the code:\n\n516     def _pair_instances_to_migrations(self, migrations, instances):\n517         instance_by_uuid = {inst.uuid: inst for inst in instances}\n518         for migration in migrations:\n519             try:\n520                 migration.instance = instance_by_uuid[migration.instance_uuid]\n521             except KeyError:\n522                 # NOTE(danms): If this happens, we don't set it here, and\n523                 # let the code either fail or lazy-load the instance later\n524                 # which is what happened before we added this optimization.\n525                 # This _should_ not be possible, of course.\n526                 LOG.error(_LE('Migration for instance %(uuid)s refers to '\n527                               'another host\\'s instance!'),\n528                           {'uuid': migration.instance_uuid})\n\n", 
            "date_created": "2016-08-16 20:52:18.749732+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "--- nova-api ---\n\n2016-08-12 17:44:32.856 16812 DEBUG nova.api.openstack.wsgi [req-a686c8c2-4453-4462-8b4e-792329e9de5d tempest-ServerActionsTestJSON-1690841472 tempest-ServerActionsTestJSON-1690841472] Action: 'action', calling method: <bound method ServersController._action_resize of <nova.api.openstack.compute.servers.ServersController object at 0x7f925e6e1b90>>, body: {\"resize\": {\"flavorRef\": \"84\"}} _process_stack /opt/stack/new/nova/nova/api/openstack/wsgi.py:633\n\n2016-08-12 17:44:33.198 16812 INFO nova.osapi_compute.wsgi.server [req-a686c8c2-4453-4462-8b4e-792329e9de5d tempest-ServerActionsTestJSON-1690841472 tempest-ServerActionsTestJSON-1690841472] 10.210.3.123 \"POST /v2.1/servers/c07855e4-1064-4964-a354-170565069d8b/action HTTP/1.1\" status: 202 len: 332 time: 0.4828379\n\nThe error started happening after the above resize.\nThe resize itself succeeded as 202.\n\n\n--- nova-cpu ---\n\n2016-08-12 17:44:33.777 18249 DEBUG nova.virt.libvirt.driver [req-a686c8c2-4453-4462-8b4e-792329e9de5d tempest-ServerActionsTestJSON-1690841472 tempest-ServerActionsTestJSON-1690841472] [instance: c07855e4-1064-4964-a354-170565069d8b] Starting migrate_disk_and_power_off migrate_disk_and_power_off /opt/stack/new/nova/nova/virt/libvirt/driver.py:7025\n\n2016-08-12 17:44:37.407 18249 INFO nova.virt.libvirt.driver [req-a686c8c2-4453-4462-8b4e-792329e9de5d tempest-ServerActionsTestJSON-1690841472 tempest-ServerActionsTestJSON-1690841472] [instance: c07855e4-1064-4964-a354-170565069d8b] Instance shutdown successfully after 3 seconds.\n\n2016-08-12 17:44:52.511 18249 INFO nova.compute.manager [req-a554b12c-69b9-4259-ad3a-75f695bc8f0e - -] [instance: c07855e4-1064-4964-a354-170565069d8b] During the sync_power process the instance has moved from host ubuntu-trusty-2-node-rax-ord-3442057-150667 to host ubuntu-trusty-2-node-rax-ord-3442057\n\n^^^: https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L6215\n\n2016-08-12 17:44:53.701 18249 ERROR nova.compute.resource_tracker [req-b81ccfed-e879-4b9e-a738-f4fba78d0670 - -] Migration for instance c07855e4-1064-4964-a354-170565069d8b refers to another host's instance!\n\n^^^: https://github.com/openstack/nova/blob/master/nova/compute/resource_tracker.py#L526", 
            "date_created": "2016-08-16 21:43:19.506373+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "--- nova-sched ---\n\nnova-sched decided the instance is scheduled on ubuntu-trusty-2-node-rax-ord-3442057 (The host is new/dest one on the above nova-cpu log.)\n\n2016-08-12 17:44:33.133 17645 DEBUG nova.scheduler.host_manager [req-a686c8c2-4453-4462-8b4e-792329e9de5d tempest-ServerActionsTestJSON-1690841472 tempest-ServerActionsTestJSON-1690841472]\nUpdate host state with instances: {'1cc597b0-619b-4f98-8602-7989e7a66947': Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=False,availability_zone=None,cell_\nname=None,cleaned=False,config_drive='',created_at=2016-08-12T17:42:43Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,device_metadata=None,disable_ter\nminate=False,display_description='tempest.common.compute-instance-711090908',display_name='tempest.common.compute-instance-711090908',ec2_ids=EC2Ids,ephemeral_gb=0,ephemeral_key_uuid=None,f\nault=<?>,flavor=Flavor(11),host='ubuntu-trusty-2-node-rax-ord-3442057',hostname='tempest.common.compute-instance-711090908',id=85,image_ref='d8b47f62-bf17-410e-8229-1fb8e4f2844e',info_cache\n=InstanceInfoCache,instance_type_id=11,kernel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',key_data=None,key_name=None,keypairs=KeyPairList,launch_index=0,launched_at=2016-08-12T17:42:47Z,laun\nched_on='ubuntu-trusty-2-node-rax-ord-3442057',locked=False,locked_by=None,memory_mb=64,metadata={},migration_context=<?>,new_flavor=None,node='ubuntu-trusty-2-node-rax-ord-3442057',numa_to\npology=None,old_flavor=None,os_type=None,pci_devices=PciDeviceList,pci_requests=InstancePCIRequests,power_state=1,progress=0,project_id='2a8702b2008242c785ee1d93b57232c8',ramdisk_id='ffe1ae\nfc-e06f-4c4e-9bed-8b7aef1e9c83',reservation_id='r-xq05khpo',root_device_name='/dev/vda',root_gb=0,security_groups=SecurityGroupList,services=<?>,shutdown_terminate=False,system_metadata={im\nage_base_image_ref='d8b47f62-bf17-410e-8229-1fb8e4f2844e',image_container_format='ami',image_disk_format='ami',image_kernel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',image_min_disk='0',imag\ne_min_ram='0',image_ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83'},tags=<?>,task_state=None,terminated_at=None,updated_at=2016-08-12T17:42:44Z,user_data=None,user_id='b559d9121cd2454f99\n6d8721b002b003',uuid=1cc597b0-619b-4f98-8602-7989e7a66947,vcpu_model=VirtCPUModel,vcpus=1,vm_mode=None,vm_state='active'), '099f7fc6-0715-473e-919d-00b3101408c0': Instance(access_ip_v4=None\n,access_ip_v6=None,architecture=None,auto_disk_config=False,availability_zone=None,cell_name=None,cleaned=False,config_drive='',created_at=2016-08-12T17:43:45Z,default_ephemeral_device=None\n,default_swap_device=None,deleted=False,deleted_at=None,device_metadata=None,disable_terminate=False,display_description='tempest.common.compute-instance-2117336986',display_name='tempest.c\nommon.compute-instance-2117336986',ec2_ids=EC2Ids,ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,flavor=Flavor(11),host='ubuntu-trusty-2-node-rax-ord-3442057',hostname='tempest.common.com\npute-instance-2117336986',id=91,image_ref='d8b47f62-bf17-410e-8229-1fb8e4f2844e',info_cache=InstanceInfoCache,instance_type_id=11,kernel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',key_data=N\none,key_name=None,keypairs=KeyPairList,launch_index=0,launched_at=2016-08-12T17:43:49Z,launched_on='ubuntu-trusty-2-node-rax-ord-3442057',locked=False,locked_by=None,memory_mb=64,metadata={\n},migration_context=<?>,new_flavor=None,node='ubuntu-trusty-2-node-rax-ord-3442057',numa_topology=None,old_flavor=None,os_type=None,pci_devices=PciDeviceList,pci_requests=InstancePCIRequest\ns,power_state=1,progress=0,project_id='c962b6465915478690a97f3aaa405bec',ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83',reservation_id='r-bguj5nfb',root_device_name='/dev/vda',root_gb=0,\nsecurity_groups=SecurityGroupList,services=<?>,shutdown_terminate=False,system_metadata={image_base_image_ref='d8b47f62-bf17-410e-8229-1fb8e4f2844e',image_container_format='ami',image_disk_\nformat='ami',image_kernel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',image_min_disk='0',image_min_ram='0',image_ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83'},tags=<?>,task_state=None,te\nrminated_at=None,updated_at=2016-08-12T17:43:46Z,user_data=None,user_id='b308ad3f08cc4aa58c6a2072ca31f562',uuid=099f7fc6-0715-473e-919d-00b3101408c0,vcpu_model=VirtCPUModel,vcpus=1,vm_mode=\nNone,vm_state='active'), 'a55b2ab5-e3cd-4173-ad12-dadd42eeee10': Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=False,availability_zone=None,cell_name=None,\ncleaned=False,config_drive='',created_at=2016-08-12T17:42:43Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,device_metadata=None,disable_terminate=Fal\nse,display_description='tempest.common.compute-instance-1045938226',display_name='tempest.common.compute-instance-1045938226',ec2_ids=EC2Ids,ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>\n,flavor=Flavor(11),host='ubuntu-trusty-2-node-rax-ord-3442057',hostname='tempest.common.compute-instance-1045938226',id=84,image_ref='',info_cache=InstanceInfoCache,instance_type_id=11,kern\nel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',key_data=None,key_name=None,keypairs=KeyPairList,launch_index=0,launched_at=2016-08-12T17:42:50Z,launched_on='ubuntu-trusty-2-node-rax-ord-34420\n57',locked=False,locked_by=None,memory_mb=64,metadata={},migration_context=<?>,new_flavor=None,node='ubuntu-trusty-2-node-rax-ord-3442057',numa_topology=None,old_flavor=None,os_type=None,pc\ni_devices=PciDeviceList,pci_requests=InstancePCIRequests,power_state=1,progress=0,project_id='385c99b71cbc4525b287dc2331bf5cf6',ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83',reservation\n_id='r-ppp0c7du',root_device_name='/dev/vda',root_gb=0,security_groups=SecurityGroupList,services=<?>,shutdown_terminate=False,system_metadata={image_base_image_ref='',image_kernel_id='e1b0\nfd12-d97e-496d-ba2b-62fa9e893c6c',image_min_disk='0',image_min_ram='0',image_ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83'},tags=<?>,task_state=None,terminated_at=None,updated_at=2016-0\n8-12T17:42:45Z,user_data=None,user_id='36dce452766a4846bc9e9eed2b4d384e',uuid=a55b2ab5-e3cd-4173-ad12-dadd42eeee10,vcpu_model=VirtCPUModel,vcpus=1,vm_mode=None,vm_state='active'), 'c07855e4\n-1064-4964-a354-170565069d8b': Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=False,availability_zone=None,cell_name=None,cleaned=True,config_drive='',creat\ned_at=2016-08-12T17:38:44Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,device_metadata=None,disable_terminate=False,display_description='tempest.com\nmon.compute-instance-72989124',display_name='tempest-server-592658720',ec2_ids=<?>,ephemeral_gb=0,ephemeral_key_uuid=None,fault=<?>,flavor=Flavor(11),host='ubuntu-trusty-2-node-rax-ord-3442\n057',hostname='tempest.common.compute-instance-72989124',id=80,image_ref='d8b47f62-bf17-410e-8229-1fb8e4f2844e',info_cache=InstanceInfoCache,instance_type_id=11,kernel_id='e1b0fd12-d97e-496\nd-ba2b-62fa9e893c6c',key_data=None,key_name=None,keypairs=<?>,launch_index=0,launched_at=2016-08-12T17:44:21Z,launched_on='ubuntu-trusty-2-node-rax-ord-3442057',locked=False,locked_by=None,\nmemory_mb=64,metadata={rebuild='server'},migration_context=None,new_flavor=None,node='ubuntu-trusty-2-node-rax-ord-3442057',numa_topology=None,old_flavor=None,os_type=None,pci_devices=PciDe\nviceList,pci_requests=InstancePCIRequests,power_state=1,progress=0,project_id='2a8702b2008242c785ee1d93b57232c8',ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83',reservation_id='r-3pb442vv\n',root_device_name='/dev/vda',root_gb=0,security_groups=SecurityGroupList,services=<?>,shutdown_terminate=False,system_metadata={clean_attempts='3',image_container_format='ami',image_disk_f\normat='ami',image_kernel_id='e1b0fd12-d97e-496d-ba2b-62fa9e893c6c',image_min_disk='0',image_min_ram='0',image_ramdisk_id='ffe1aefc-e06f-4c4e-9bed-8b7aef1e9c83'},tags=<?>,task_state=None,ter\nminated_at=None,updated_at=2016-08-12T17:44:16Z,user_data=None,user_id='b559d9121cd2454f996d8721b002b003',uuid=c07855e4-1064-4964-a354-170565069d8b,vcpu_model=VirtCPUModel,vcpus=1,vm_mode=N\none,vm_state='active')} _locked_update /opt/stack/new/nova/nova/scheduler/host_manager.py:177\n", 
            "date_created": "2016-08-16 22:00:33.239568+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "There was not any activity in long-term, so it would be nice to drop this from our queue.", 
            "date_created": "2017-05-04 21:00:54.683508+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2017-07-04 04:17:58.997731+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ]
}
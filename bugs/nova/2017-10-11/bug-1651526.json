{
    "status": "Confirmed", 
    "last_updated": "2017-06-27 15:54:57.660571+00:00", 
    "description": "Description\n===========\nAfter we upgraded our openstack deployment to Mitaka we started noticing more and more VMs in an error and/or shutoff state. We found out that although nova service-list was reporting all nova-compute as \"up\" nova-compute was logging :\n\nERROR nova.compute.manager OSError: [Errno 24] Too many open files\n\nour deployment uses ceph as storage backend for ephemeral/cinder/glance with currently around 900 osd installed.\n\nSteps to reproduce\n==================\n\nAsk a compute node to perform several actions at once such as:\n-take a live snapshot\n-delete a VM\n-boot a VM\n\nDepending on the task the compute will allow for a certain number of jobs before complaining about the number of open files.\n\nExpected behaviour\n=================\nNova-compute should be more robust in handling requests and should report when in error state.\n\nEnvironment\n===========\nWe are currently running:\n\nnova-common                          2:13.1.2-0ubuntu2~cloud0  \nnova-compute                         2:13.1.2-0ubuntu2~cloud0\n\nhypervisor: Libvirt + KVM \nnova-compute-kvm                     2:13.1.2-0ubuntu2~cloud0  \nnova-compute-libvirt                 2:13.1.2-0ubuntu2~cloud0\n\nstorage: \nceph                                     0.94.9-1trusty\n\nnetworking: neutron + openvswitch\nneutron-common                           2:8.3.0-0ubuntu1.1~cloud0\n\nLogs & Configs\n==============", 
    "tags": [
        "ceph", 
        "compute", 
        "openstack-version.mitaka"
    ], 
    "importance": "Low", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1651526", 
    "owner": "None", 
    "id": 1651526, 
    "index": 870, 
    "created": "2016-12-20 17:11:34.019100+00:00", 
    "title": "nova-compute reports OSError: [Errno 24] Too many open files", 
    "comments": [
        {
            "content": "Description\n===========\nAfter we upgraded our openstack deployment to Mitaka we started noticing more and more VMs in an error and/or shutoff state. We found out that although nova service-list was reporting all nova-compute as \"up\" nova-compute was logging :\n\nERROR nova.compute.manager OSError: [Errno 24] Too many open files\n\nour deployment uses ceph as storage backend for ephemeral/cinder/glance with currently around 900 osd installed.\n\nSteps to reproduce\n==================\n\nAsk a compute node to perform several actions at once such as:\n-take a live snapshot\n-delete a VM\n-boot a VM\n\nDepending on the task the compute will allow for a certain number of jobs before complaining about the number of open files.\n\nExpected behaviour\n=================\nNova-compute should be more robust in handling requests and should report when in error state.\n\nEnvironment\n===========\nWe are currently running:\n\nnova-common                          2:13.1.2-0ubuntu2~cloud0  \nnova-compute                         2:13.1.2-0ubuntu2~cloud0\n\nhypervisor: Libvirt + KVM \nnova-compute-kvm                     2:13.1.2-0ubuntu2~cloud0  \nnova-compute-libvirt                 2:13.1.2-0ubuntu2~cloud0\n\nstorage: \nceph                                     0.94.9-1trusty\n\nnetworking: neutron + openvswitch\nneutron-common                           2:8.3.0-0ubuntu1.1~cloud0\n\nLogs & Configs\n==============", 
            "date_created": "2016-12-20 17:11:34.019100+00:00", 
            "author": "https://api.launchpad.net/1.0/~mattia-belluco"
        }, 
        {
            "content": "", 
            "date_created": "2016-12-20 17:11:34.019100+00:00", 
            "author": "https://api.launchpad.net/1.0/~mattia-belluco"
        }, 
        {
            "content": "The compute logs are all wrapped so it's hard to read, so I've reformatted it and the error is here:\n\nhttp://paste.openstack.org/show/594211/\n\n2016-12-20 17:02:11.554 102936 DEBUG oslo_service.periodic_task [req-442fe967-9b86-4ddc-9548-e569f226b508 - - - - -] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.7/dist-packages/oslo_service/periodic_task.py:215\n2016-12-20 17:02:11.576 102936 INFO nova.compute.resource_tracker [req-442fe967-9b86-4ddc-9548-e569f226b508 - - - - -] Auditing locally available compute resources for node node-l4-21-16\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager [req-442fe967-9b86-4ddc-9548-e569f226b508 - - - - -] Error updating resources for node node-l4-21-16.\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager Traceback (most recent call last): \n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 6487, in update_available_resource\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager rt.update_available_resource(context)\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py\", line 508, in update_available_resource\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 5365, in get_available_resource\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 5006, in _get_local_gb_info\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/storage/rbd_utils.py\", line 380, in get_pool_info\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/storage/rbd_utils.py\", line 107, in __init__\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/storage/rbd_utils.py\", line 136, in _connect_to_rados\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/dist-packages/rados.py\", line 212, in __init__\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/ctypes/util.py\", line 253, in find_library\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager File \"/usr/lib/python2.7/ctypes/util.py\", line 242, in _findSoname_ldconfig\n2016-12-20 17:02:11.577 102936 ERROR nova.compute.manager OSError: [Errno 24] Too many open files\n\nWe end up failing here:\n\nhttps://github.com/openstack/nova/blob/stable/mitaka/nova/compute/manager.py#L6498\n\nThe resource tracker doesn't change the service record state, that's handled in the service group API based on whether or not the service is reporting in on a regular interval, which is different from what's failing here which is getting local used disk info.\n\nThe compute node record itself doesn't have a state or status, that's all determined through the associated services record.\n\nThe libvirt driver could disable the service itself, like it does when libvirt is disconnected. That happens via event callback here:\n\nhttps://github.com/openstack/nova/blob/stable/mitaka/nova/virt/libvirt/driver.py#L3445\n\nSo when we hit too many files error here:\n\nhttps://github.com/openstack/nova/blob/stable/mitaka/nova/virt/libvirt/driver.py#L5006\n\nWe could disable the service for that host...but I'm not sure how we re-enable the service...I guess we could set a flag in memory when disabling the service because of that failure and on the next check if it passes and the flag was set, we then enable the service again if get_pool_info() passes. It doesn't seem like an awesome solution but it could work to automatically disable/enable the service during the update available resource periodic task, and disabling the service might trigger an alarm for the operator to see things are going haywire in the cluster.", 
            "date_created": "2017-01-07 17:55:50.742567+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I've also posted to the dev and operators mailing lists about this issue and to get some feedback on how to fix it:\n\nhttp://lists.openstack.org/pipermail/openstack-dev/2017-January/109780.html", 
            "date_created": "2017-01-07 18:05:11.229723+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Sorry, but disabling the compute node doesn't solve the issue, although you might want to change the behavior, this is a solution for a different problem.\n\nThe problem here is twofold:\n* Mitaka+ version of nova now creates multiple threads to delete VMs in parallel (and create snapshots etc), and as a consequence is more connection-hungry\n* nova is not dealing properly with fd starvation.\n\nSince the number of connections created is a function of the size of the ceph cluster, I would expect either:\na) nova is limiting the amount of parallel operations based on the max number of files he can create (man getrlimit)\nb) (ugly but easier) a configuration option is provided to limit the amount of parallel connections nova will make\n\nOption a) has the advantage that in some cases the limits are too low to be able to do anything and this issue might be spotted *before* actually failing, and a nice error might be printed in the log file.\n\nSo far we were able to *mitigate* the issue by:\n\n1) setting EVENTLET_THREADPOOL_SIZE to a lower value in upstart script\n2) increasing nfile (ulimit in upstart script)\n", 
            "date_created": "2017-01-09 11:19:56.196205+00:00", 
            "author": "https://api.launchpad.net/1.0/~arcimboldo"
        }, 
        {
            "content": "Automatically discovered version mitaka in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:54:57.220935+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
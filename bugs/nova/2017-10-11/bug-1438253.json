{
    "status": "Confirmed", 
    "last_updated": "2017-06-27 15:53:04.039307+00:00", 
    "description": "Using a CPU policy of dedicated ('hw:cpu_policy=dedicated') results in vCPUs being pinned to pCPUs, per the original blueprint:\n\n\u00a0\u00a0\u00a0\u00a0http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-cpu-pinning.html\n\nWhen combined with the 'vcpu_pin_set' nova configuration, it should be possible to get very good control over which pCPUs are used by instances. This works as expected when creating a multi-vCPU instance pinned to pCPUs on a single NUMA node. However, if these pCPUs are spread across multiple NUMA nodes then it fails. This behavior is not documented in the original blueprint and therefore should not function like so.\n\n---\n\n# Testing Configuration\n\nTesting was conducted on a single-node, Fedora 21-based (3.17.8-300.fc21.x86_64) OpenStack instance (built with devstack). The system is a dual-socket, 10 core, HT-enabled system (2 sockets * 10 cores * 2 threads = 40 \"pCPUs\". 0-9,20-29 = node0, 10-19,30-39 = node1). Two flavors were used:\n\n\u00a0\u00a0\u00a0\u00a0openstack flavor create --ram 4096 --disk 20 --vcpus 10 demo.no-pinning\n\n\u00a0\u00a0\u00a0\u00a0openstack flavor create --ram 4096 --disk 20 --vcpus 10 demo.pinning\n\u00a0\u00a0\u00a0\u00a0nova flavor-key demo.pinning set hw:cpu_policy=dedicated hw:cpu_thread_policy=isolate\n\n# Results\n\nPassing (same NUMA socket, contiguous)\n======================================\n\n\u00a0vcpu_pin_set = 10-19\n\nPinned\n------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 68150's current affinity list: 10-19\n\u00a0$ sudo -E virsh vcpucount 13\n\u00a0maximum      config        10\n\u00a0maximum      live          10\n\u00a0current      config        10\n\u00a0current      live          10\n\nUnpinned\n--------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 70249's current affinity list: 10-19\n\nPassing (same NUMA socket, non-contiguous)\n==========================================\n\n\u00a0vcpu_pin_set = 5-9,25-29\n\nPinned\n------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 91186's current affinity list: 5-9,25-29\n\nUnpinned\n--------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 92212's current affinity list: 5-9,25-29\n\nPassing (multiple NUMA sockets, non-contiguous)\n===============================================\n\n\u00a0vcpu_pin_set = 5-10,25-29\n\nPinned\n------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 93884's current affinity list: 5-9,25-29\n\nUnpinned\n--------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print $2}' | head -1 | xargs taskset -pc\n\u00a0pid 94468's current affinity list: 5-9,25-29\n\nAdditional combinations\n-----------------------\n\n\u00a0vcpu_pin_set = 5-14,25-29\n\nFailing (different NUMA socket, contiguous)\n===========================================\n\n\u00a0vcpu_pin_set = 9-18\n\nPinned\n------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\u00a0ERROR: openstackclient.compute.v2.server.CreateServer Error creating server: test1\n\nUnpinned\n--------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 71194's current affinity list: 9-18\n\nFailing (different NUMA socket, non-contiguous)\n===============================================\n\n\u00a0vcpu_pin_set = 5-9,15-19\n\nPinned\n------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0ERROR: openstackclient.compute.v2.server.CreateServer Error creating server: test1\n\u00a0$ openstack server show test1 | grep fault\n\u00a0| fault                                | {u'message': u'No valid host was found. There are not enough hosts available.', u'code': 500,\n\nUnpinned\n--------\n\n\u00a0$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\u00a0$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\u00a0pid 88703's current affinity list: 5-9,15-19", 
    "tags": [
        "libvirt", 
        "numa", 
        "openstack-version.kilo"
    ], 
    "importance": "Medium", 
    "heat": 46, 
    "link": "https://bugs.launchpad.net/nova/+bug/1438253", 
    "owner": "None", 
    "id": 1438253, 
    "index": 4192, 
    "created": "2015-03-30 15:09:50.437058+00:00", 
    "title": "nova libvirt pinning won't work across numa nodes", 
    "comments": [
        {
            "content": "Using a CPU policy of dedicated ('hw:cpu_policy=dedicated') results in vCPUs being pinned to pCPUs, per the original blueprint:\n\n    http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-cpu-pinning.html\n\nWhen combined with the 'vcpu_pin_set' nova configuration, it should be possible to get very good control over which pCPUs are used by instances. This works as expected when creating a multi-vCPU instance pinned to pCPUs on a single NUMA node. However, if these pCPUs are spread across multiple NUMA nodes then it fails. This behavior is not documented in the original blueprint and therefore should not function like so.\n\n---\n\n# Testing Configuration\n\nTesting was conducted on a single-node, Fedora 21-based (3.17.8-300.fc21.x86_64) OpenStack instance (built with devstack). Two flavors were used:\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 10 demo.no-pinning\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 10 demo.pinning\n    nova flavor-key demo.pinning set hw:cpu_policy=dedicated hw:cpu_threads_policy=separate\n\n# Results\n\nPassing (same NUMA socket, contiguous)\n======================================\n\n\tvcpu_pin_set = 10-19\n\nPinned\n------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 68150's current affinity list: 10-19\n\t$ sudo -E virsh vcpucount 13\n\tmaximum      config        10\n\tmaximum      live          10\n\tcurrent      config        10\n\tcurrent      live          10\n\nUnpinned\n--------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 70249's current affinity list: 10-19\n\nPassing (same NUMA socket, non-contiguous)\n==========================================\n\n\tvcpu_pin_set = 5-9,25-29\n\nPinned\n------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 91186's current affinity list: 5-9,25-29\n\nUnpinned\n--------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 92212's current affinity list: 5-9,25-29\n\nPassing (multiple NUMA sockets, non-contiguous)\n===============================================\n\n\tvcpu_pin_set = 5-10,25-29\n\nPinned\n------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 93884's current affinity list: 5-9,25-29\n\nUnpinned\n--------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print $2}' | head -1 | xargs taskset -pc\n\tpid 94468's current affinity list: 5-9,25-29\n\nAdditional combinations\n-----------------------\n\n\tvcpu_pin_set = 5-14,25-29\n\n\nFailing (different NUMA socket, contiguous)\n===========================================\n\n\tvcpu_pin_set = 9-18\n\nPinned\n------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.pinning test1 --wait\n\tERROR: openstackclient.compute.v2.server.CreateServer Error creating server: test1\n\nUnpinned\n--------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 71194's current affinity list: 9-18\n\nFailing (different NUMA socket, non-contiguous)\n===============================================\n\n\tvcpu_pin_set = 5-9,15-19\n\nPinned\n------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\tERROR: openstackclient.compute.v2.server.CreateServer Error creating server: test1\n\t$ openstack server show test1 | grep fault\n\t| fault                                | {u'message': u'No valid host was found. There are not enough hosts available.', u'code': 500,\n\nUnpinned\n--------\n\n\t$ openstack server create --image Fedora-x86_64-20-20140618-sda --flavor demo.no-pinning test1 --wait\n\t$ ps aux | grep qemu | awk '{print }' | head -1 | xargs taskset -pc\n\tpid 88703's current affinity list: 5-9,15-19", 
            "date_created": "2015-03-30 15:09:50.437058+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "Is the parameter \"hw:cpu_threads_policy\" error? I cant find this in the M version source code", 
            "date_created": "2016-06-27 12:22:32.374924+00:00", 
            "author": "https://api.launchpad.net/1.0/~licayun"
        }, 
        {
            "content": "It is: it should read 'hw:cpu_thread_policy'. Fixed.", 
            "date_created": "2016-06-27 12:50:02.246315+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "In the M version source code I cant find the cpu_thread_policy with option  'separate'.In the  BP 'http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-cpu-pinning.html' it only hava 'hw:cpu_policy=shared|dedicated'.  In other BP 'https://blueprints.launchpad.net/nova/+spec/virt-driver-cpu-thread-pinning' it have 'hw:cpu_thread_policy=prefer|isolate|require'. Would you tell me the BP about 'hw:cpu_thread_policy=separate'?", 
            "date_created": "2016-06-28 02:20:43.388429+00:00", 
            "author": "https://api.launchpad.net/1.0/~licayun"
        }, 
        {
            "content": "\uff1f\uff1f\uff1f the source code don't have 'hw:cpu_thread_policy=separate',and the the BP including \"hw:cpu_thread_policy=separate\" is not approved.", 
            "date_created": "2016-06-30 00:15:29.743371+00:00", 
            "author": "https://api.launchpad.net/1.0/~licayun"
        }, 
        {
            "content": "I'm not sure where you're reading 'separate' from? This was in an earlier version of the spec, but it was removed in the final version. The final version of the blueprint and specs are linked below:\n\n    * https://blueprints.launchpad.net/nova/+spec/virt-driver-cpu-thread-pinning\n    * http://specs.openstack.org/openstack/nova-specs/specs/mitaka/implemented/virt-driver-cpu-thread-pinning.html\n\nBesides, thread policies don't really have anything to do with this?", 
            "date_created": "2016-08-10 13:49:35.716042+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "The problem in method _add_cpu_pinning_constraint()\nhttps://github.com/openstack/nova/blob/master/nova/virt/hardware.py#L1174-L1185\n\nIf flavor has *no* information about requested NUMA topology (user don't care about NUMA topology, he just want to use hw:cpu_policy=dedicated) nova creates a fake NUMA requirements. As we see from code this is a single NUMA cell with CPU and RAM requirements from flavor. That is why we get an exception if this single NUMA node couldn't be fitted into free host NUMA nodes. The problem is in this fake NUMA requirement. Maybe new VM could be fitted into 2 or 3 host NUMA nodes. But nova requiren only one node if there is no NUMA topology requirements in flavor. \n\nRoot of this problem is that cpu_policy is based on NUMA topology in Nova. We even store information about used (pinned) CPUs in numa topology table. To fix this bug we need some magic algorithm which will be fit VM (without NUMA requirements) to NUMA nodes with enough *summarily* number of free CPUs.", 
            "date_created": "2016-10-04 11:12:27.872529+00:00", 
            "author": "https://api.launchpad.net/1.0/~snikitin"
        }, 
        {
            "content": "Automatically discovered version kilo in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:53:03.428039+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
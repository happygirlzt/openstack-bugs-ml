{
    "status": "Fix Released", 
    "last_updated": "2014-03-20 22:10:42.923965+00:00", 
    "description": "Distributed Setup with:\n2x Compute nodes with Debian Wheezy + XCP installed from repositories (Kronos). The VM controlling XCP is installed on Ubuntu Precise.\n1x Controller node with Keystone and Nova (except Network and Compute) on Ubuntu Precise with OpenStack Grizzly installed from cloud archive.\n1x Network node running Nova network with FlatDHCP (no quantum is used because it is not supported for XCP yet - I think it will starting with Havana release). The network node has 3 interfaces. 1x Public, 1x Management, 1x Tenant.\n1x Storage node running Cinder, Glance and NFSv3 for shared storage to support live migration\n\n\nI experiment with XCP and live migration these days so after I configured everything else, I tried to configure floating IP addresses as well. The configuration of the floating IP's was trivial but when I booted a VM, I instantly migrated it (that's what I am mostly testing) and then assigned a floating IP. Then I tried to ping it and connect to it using ssh and everything worked fine.\n\nI boot a second VM and this time I do not migrate it. I assign a floating IP address and no ping or ssh connection is possible to be made on this one even though the iptables have been setup correctly (the SNAT and DNAT). I migrate the VM and then I can connect to it using SSH without any problems.\n\nIn the beginning I thought it is a bug and for some reason when you boot even though you should be able to connect, you cannot. After looking in the documentation I found this: http://docs.openstack.org/essex/openstack-compute/admin/content/configuring-openstack-compute-basics.html#enabling-access-to-vms-on-the-compute-node\n\nWhat I understood from this is that it is the other way around and I should NOT be able to ping or connect to the VMs using SSH by default if I don't explicitly add the secgroup rules to allow such actions.\n\nAfter adding these two rules everything works fine (I can access any vm, migrated or non-migrated):\n$ nova secgroup-add-rule default  icmp -1 -1 0.0.0.0/0\n$ nova secgroup-add-rule default  tcp 22 22 0.0.0.0/0\n\nAfter removing them again, I cannot access the non-migrated VM's (correct) but I can still access those that they were migrated once.\n\nEven when I migrate them back to the hypervisor originally booted on, the secgroups still do not apply and I can access those VM's.", 
    "tags": [
        "xenserver"
    ], 
    "importance": "High", 
    "heat": 272, 
    "link": "https://bugs.launchpad.net/nova/+bug/1202266", 
    "owner": "https://api.launchpad.net/1.0/~johngarbutt", 
    "id": 1202266, 
    "index": 1163, 
    "created": "2013-07-17 15:49:32.822875+00:00", 
    "title": "[OSSA 2013-030] xenapi: secgroups are not in place after live-migration (CVE-2013-4497)", 
    "comments": [
        {
            "content": "Distributed Setup with:\n2x Compute nodes with Debian Wheezy + XCP installed from repositories (Kronos). The VM controlling XCP is installed on Ubuntu Precise.\n1x Controller node with Keystone and Nova (except Network and Compute) on Ubuntu Precise with OpenStack Grizzly installed from cloud archive.\n1x Network node running Nova network with FlatDHCP (no quantum is used because it is not supported for XCP yet - I think it will starting with Havana release). The network node has 3 interfaces. 1x Public, 1x Management, 1x Tenant.\n1x Storage node running Cinder, Glance and NFSv3 for shared storage to support live migration\n\n\nI experiment with XCP and live migration these days so after I configured everything else, I tried to configure floating IP addresses as well. The configuration of the floating IP's was trivial but when I booted a VM, I instantly migrated it (that's what I am mostly testing) and then assigned a floating IP. Then I tried to ping it and connect to it using ssh and everything worked fine.\n\nI boot a second VM and this time I do not migrate it. I assign a floating IP address and no ping or ssh connection is possible to be made on this one even though the iptables have been setup correctly (the SNAT and DNAT). I migrate the VM and then I can connect to it using SSH without any problems.\n\nIn the beginning I thought it is a bug and for some reason when you boot even though you should be able to connect, you cannot. After looking in the documentation I found this: http://docs.openstack.org/essex/openstack-compute/admin/content/configuring-openstack-compute-basics.html#enabling-access-to-vms-on-the-compute-node\n\nWhat I understood from this is that it is the other way around and I should NOT be able to ping or connect to the VMs using SSH by default if I don't explicitly add the secgroup rules to allow such actions.\n\nAfter adding these two rules everything works fine (I can access any vm, migrated or non-migrated):\n$ nova secgroup-add-rule default  icmp -1 -1 0.0.0.0/0\n$ nova secgroup-add-rule default  tcp 22 22 0.0.0.0/0\n\nAfter removing them again, I cannot access the non-migrated VM's (correct) but I can still access those that they were migrated once.\n\nEven when I migrate them back to the hypervisor originally booted on, the secgroups still do not apply and I can access those VM's.", 
            "date_created": "2013-07-17 15:49:32.822875+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyberang3l"
        }, 
        {
            "content": "Adding Nova PTL for sanity check", 
            "date_created": "2013-07-18 08:34:38.853487+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Just to mention as well that I am using XenAPI pools and shared storage for the live migrations as described here: http://docs.openstack.org/trunk/openstack-compute/admin/content/configuring-migrations.html#configuring-migrations-xenserver-shared-storage\n\nMore information on my setup here:\nhttps://answers.launchpad.net/nova/+question/232484", 
            "date_created": "2013-07-18 08:52:22.188675+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyberang3l"
        }, 
        {
            "content": "I think the first thing I'd like to do here is try to replicate this behavior on another setup.  I'm adding John Garbutt, nova-core and xenapi driver expert, to this bug to see if he can help try this out.", 
            "date_created": "2013-07-18 15:47:09.367285+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "I thought this sounded familiar:\nhttps://bugs.launchpad.net/nova/+bug/1073306\n\nBut I agree, this is a security issue, for those using Security Groups.", 
            "date_created": "2013-07-18 15:52:20.266760+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "On re-reading, this might be a new case.\n\nThe old bug covers migration not live-migration. But its likely the cause.", 
            "date_created": "2013-07-18 15:55:39.334594+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "So this is Xen-specific ? \n\nI think it would qualify as a vulnerability because people would expect port filtering (at least default DROP/REJECT rules) to be reapplied ?", 
            "date_created": "2013-07-19 10:32:39.330200+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Sorry, lost the email notification, but this is XenServer specific, its in the XenAPI driver code.\n\nThere is quite a bit of work to fix up this feature around:\n* resize, migrate, live-migrate\n\nBasically, the security groups is currently totally untested, in many respects. However, I don't think this has really been communicated at all well (if at all).\n\nThere are deeper issues here too, because the feature was written for linux bridge, but XenServer now uses OVS by default, so the iptables rules are not good enough. Will need some digging around neutron vs nova here too. I know BobBall at Citrix was talking a look at the above deeper issues, its probably worth brining him in here.", 
            "date_created": "2013-07-24 16:49:48.387175+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Just to confirm, my code inspection confirms that this bug happens with XenAPI.\n\nI have not checked if this bug is present in any other drivers.\n\nI am also, not sure we have tests for this in tempest, as I heard XenAPI is very close to passing full tempest (on volume related test currently failing).", 
            "date_created": "2013-07-24 16:55:02.325131+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "My take is that we'll need to issue an OSSA on this one.", 
            "date_created": "2013-07-29 10:21:41.669244+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@Gabe -- do you have a resource who might be able to help out with this one?", 
            "date_created": "2013-07-29 10:28:44.362831+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "John: so in essence, we're talking about applying new flows once the VM has moved to the destination, correct? \n\nAs you point out, OVS is the default behavior here, and to my knowledge, no real implementation exists for applying OVS flows today. The implementation in Neutron (last I checked) was only a basic OVS pass, and actually utilized IPTables rules in addition. OVS was there more as a proof of concept than actual useful implementation. Ensuring that resize et al also attempt to apply security groups is insufficient since there's nothing (again, to the best of my knowledge) capable of applying those flows. ", 
            "date_created": "2013-07-29 22:36:56.649922+00:00", 
            "author": "https://api.launchpad.net/1.0/~cerberus"
        }, 
        {
            "content": "So, not sure the best way to fix this stuff, advice needed.\n\nI almost want to say we should just issue an advisory to clarify the state of the security groups feature as \"experimental\" with more work required before it is production ready, and the work around is not to rely on security groups. But that doesn't feel like the right response. However a proper fix will require this feature to (effectively) be implemented.\n\nSo lets try summarize the issues:\n* nova has missing calls to the firewall driver (there are open public bugs on this one, and there are fixes in progress, in the public, which is probably bad) - I am happy to look into getting this fixed, but do we need to backport these? Will need a networking expert to check the fixes.\n* the firewall driver in nova doesn't work with OVS - I could do with a hand fixing that\n* I don't know the state of the various neutron drivers and how they interact, we don't yet have the equivalent VIF drivers for XenAPI, but that might not matter - again, not something I really know how to fix\n* MAC and IP address spoofing should also be checked\n\nGoing into the firewall driver issues, it was written when XenServer used bridge networking, back in 5.6.  The OVS case has always been avoided, because until recently, the version of OVS shipping with XenServer (apparently) did not have the bit masking operation that would allow you to avoid some of the worst bits of rule explosion in the number of rules. You need to take care, because there is a massive OVS slowdown once the rules don't fit in your processors L2/L3 cache, or something like that, which would give users a sort of DoS attach on the other VMs the host their VM is present.", 
            "date_created": "2013-07-30 09:10:02.453666+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "@John: if \"fixing\" it for stable/grizzly amounts to implementing the full feature there, then I would agree that we should issue a security note about it being non-usable and move on. The issue is, it looks implemented enough so that people would trust it... in particular, security groups seem to work properly until you do a migration ?\n\nSitting on the fence on this one.", 
            "date_created": "2013-08-01 13:12:53.141362+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@John: what would it take to plug the security hole (people run with security groups and they kinda work, migrate and expect them to be restored but poof they are gone) ? We don't necessarily need to fix all security group bugs :)", 
            "date_created": "2013-08-08 15:44:52.451927+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@Thierry, sorry for the delay, screwed up my email filters, and just back from holiday.\n\nI agree, its probably best to fix this up.\n\nNot sure of the best way to phase this, but it would be good to alert people to the fact this \n\nA quick fix for non-live migration is probably just a cut and paste of this code:\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L444\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L462\nAnd put it either side of the boot command here (for migration):\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L311\n\nIn terms of the public bug, around migration not setting up security groups, you can see my attempt at a \"proper\" fix here:\nhttps://review.openstack.org/#/c/38455/ \n\n\nI am not really sure how to fix it for live-migration, need someone with more neutron skills than me really.\n\nThe problem is the VM domain and networking is created on the destination by XenAPI, auto-magically, in a single live-migrate operation:\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L1881\n\nWe could add a \"post_live_migration_at_destination\", that does the firewall setup, but that would be after the VM has become active on the destination, so there is likely to be a small period when the traffic is not filtered, unless we can some how block all the traffic during that time (which I think might happen when you have an OVS controller present, but I am not sure). The missing code that would go in post_live_migration, must be something like this:\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L444\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/vmops.py#L462\n\nIn libvirt, I think the VIFs are plugged before the live-migrate happens, so its covered by this method call: \nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L3794\nXenAPI has a rather dodgy implementation of that:\nhttps://github.com/openstack/nova/blob/master/nova/virt/xenapi/driver.py#L418\nI see familar code in the libvirt driver, but I don't think that will work when the VIFs have not been created, and the VIFs are not created in time:\nhttps://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L3332\n\nAs far as getting a networking guru, I know Salvatore Orlando worked on this first implementation of these bits, maybe he would be a good person to cast his eye across the above ideas. But there might be someone on the security team who can help out, not sure who is on the list.", 
            "date_created": "2013-08-20 14:04:49.870126+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "I see no point in keeping this private, since the live migration case is mentioned on the public bug. Unless someone complains, I'll open this up, which should facilitate the fixing, since this is actually non-trivial.", 
            "date_created": "2013-08-20 15:16:20.503489+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "CCing Salvatore: we might need your help around here.", 
            "date_created": "2013-08-20 15:17:23.094471+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Agreed, with bug 1073306 already mentioning that xenapi migrations don't apply security group filters and that it also affects live migration, this is now public knowledge. Opening it up the discussion to the wider developer community will hopefully also get us a fix sooner.", 
            "date_created": "2013-08-20 16:15:57.014750+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Shall be a common OSSA with 1073306", 
            "date_created": "2013-08-21 18:50:28.854404+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Potentially making this a little wider, my current understanding of the OVS is that the OVS does not call the netfilter code when it is forwarding traffic to VMs.  In summary my belief is that only bridge-based systems support security groups, and if you configure a host to use libvirt and OVS (which I believe is possible?) then that would suffer from the same issue.\n\nAgreed that there is a question about how to handle the live migration case with XAPI doing most of the work.  There is a hook we can use in XAPI - but I'm not sure this is the best solution.  I'd prefer to create the VM with fully blocked ports and then apply the correct security groups.", 
            "date_created": "2013-08-21 19:21:22.288554+00:00", 
            "author": "https://api.launchpad.net/1.0/~bob-ball"
        }, 
        {
            "content": "Bob, seems like a good option. Can you take on fixing the live migrate issue?", 
            "date_created": "2013-08-30 14:07:47.233437+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "In response to Thierry's comment #19, I'm unsure how we'll be able to issue a common OSSA if the proposed fix for bug 1073306 does not address this issue. Should we hold the advisory until such time as fixes for both are ready, or do they need to diverge?", 
            "date_created": "2013-09-09 00:06:13.986484+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I have a suggested partial fix for this issue.\n\nI have split the issue to include some additional work in this bug:\nhttps://bugs.launchpad.net/nova/+bug/1224587", 
            "date_created": "2013-09-12 17:10:15.722945+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "@Vangelis I would really appreciate help testing this, if you still have a setup you can check this on. The fix is still a little work in progress, but I wanted to get peoples opinions, on if this would be an acceptable way forward, in the short term.", 
            "date_created": "2013-09-12 17:17:11.978101+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/46313", 
            "date_created": "2013-09-12 17:17:13.073727+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "John, I still have the setup so definitely I can help on testing. I'll report back early next week as I'm away at the moment.", 
            "date_created": "2013-09-12 18:00:00.780818+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyberang3l"
        }, 
        {
            "content": "Vangelis, did you have a chance to confirm whether John's patch above mitigates the issue on your setup?", 
            "date_created": "2013-09-17 17:27:44.395397+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I just and it's not working for me.\nHowever, I get the following error in the nova-compute.log:\n\n2013-09-18 14:46:10.972 INFO nova.compute.manager [req-629ff9d2-fb6c-4c83-a672-e5aecd36cb20 8ca2f0ccd55f4d9ba53847755f2e0b18 10c16ce5ffb54c56925152b7d331a8d2] [instance: f521d7f2-d27f-40ff-81da-d1a25a5b69ac] Post operation of migration started\n2013-09-18 14:46:12.221 ERROR nova.openstack.common.rpc.amqp [req-629ff9d2-fb6c-4c83-a672-e5aecd36cb20 8ca2f0ccd55f4d9ba53847755f2e0b18 10c16ce5ffb54c56925152b7d331a8d2] Exception during message handling\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 430, in _process_data\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp     rval = self.proxy.dispatch(ctxt, version, method, **args)\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py\", line 133, in dispatch\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 3273, in post_live_migration_at_destination\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp     block_migration, block_device_info)\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/virt/xenapi/driver.py\", line 521, in post_live_migration_at_destination\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp     self._vmops.post_live_migration_at_destination(ctxt, instance_ref,\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp AttributeError: 'VMOps' object has no attribute 'post_live_migration_at_destination'\n2013-09-18 14:46:12.221 1418 TRACE nova.openstack.common.rpc.amqp \n2013-09-18 14:46:12.230 ERROR nova.openstack.common.rpc.common [req-629ff9d2-fb6c-4c83-a672-e5aecd36cb20 8ca2f0ccd55f4d9ba53847755f2e0b18 10c16ce5ffb54c56925152b7d331a8d2] Returning exception 'VMOps' object has no attribute 'post_live_migration_at_destination' to caller\n2013-09-18 14:46:12.230 ERROR nova.openstack.common.rpc.common [req-629ff9d2-fb6c-4c83-a672-e5aecd36cb20 8ca2f0ccd55f4d9ba53847755f2e0b18 10c16ce5ffb54c56925152b7d331a8d2] ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 430, in _process_data\\n    rval = self.proxy.dispatch(ctxt, version, method, **args)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py\", line 133, in dispatch\\n    return getattr(proxyobj, method)(ctxt, **kwargs)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 3273, in post_live_migration_at_destination\\n    block_migration, block_device_info)\\n', '  File \"/usr/lib/python2.7/dist-packages/nova/virt/xenapi/driver.py\", line 521, in post_live_migration_at_destination\\n    self._vmops.post_live_migration_at_destination(ctxt, instance_ref,\\n', \"AttributeError: 'VMOps' object has no attribute 'post_live_migration_at_destination'\\n\"]\n\n\nI have tried to patch the files \"nova/virt/xenapi/driver.py\" and \"nova/virt/xenapi/vmops.py\" according to John's commit in the openstack nova compute XCP virtual machines in both nodes participating in the live migration.\n\nAm I doing anything wrong? Maybe I need to patch something more since I am on Grizzly and not trunk?", 
            "date_created": "2013-09-18 12:59:43.164733+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyberang3l"
        }, 
        {
            "content": "@Vangelis sorry, it will need to be against trunk, a lot of that code has changed recently :(\n\nI could try a backport to grizzly.", 
            "date_created": "2013-09-20 17:41:19.770449+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "@John Garbutt: Which other versions are affected ? Looks like Grizzly is ? What about Folsom ?", 
            "date_created": "2013-09-23 15:24:32.158893+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/46313\nCommitted: http://github.com/openstack/nova/commit/5cced7a6dd32d231c606e25dbf762d199bf9cca7\nSubmitter: Jenkins\nBranch:    master\n\ncommit 5cced7a6dd32d231c606e25dbf762d199bf9cca7\nAuthor: John Garbutt <email address hidden>\nDate:   Thu Sep 12 18:11:49 2013 +0100\n\n    xenapi: enforce filters after live-migration\n    \n    Currently and network filters, including security groups, are\n    lost after a server has been live-migrated.\n    \n    This partially fixes the issue by ensuring that security groups are\n    re-applied to the VM once it reached the destination, and been started.\n    \n    This leaves a small amount of time during the live-migrate where the VM\n    is not protected. There is a further bug raised to close the rest of\n    this whole, but this helps keep the VM protected for the majority of the\n    time.\n    \n    Fixes bug 1202266\n    \n    Change-Id: I84fdb6e2a8ee38d75f243aadbe79945af1d6849d\n", 
            "date_created": "2013-09-27 18:46:33.999261+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Can we have backports to nova's stable/grizzly branch (and stable/folsom if affected similarly)?", 
            "date_created": "2013-09-27 20:56:55.815591+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Any information on which stable release branches are/were affected by this (if any)? We'll want bug tasks and backports for them as far back as folsom if possible.", 
            "date_created": "2013-10-09 22:05:26.157101+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "This one is since folsom, when live-migrate landed, totally my bad:\nhttps://blueprints.launchpad.net/nova/+spec/xenapi-live-migration\n\nThe backport should be more straight forward for this one, although there has been quite a lot of rework around live-migrate recently, it shouldn't fundamentally change things.\n\nSorry for the delay, seem to have lost the updates on this in the regular email soup.\n\n\n", 
            "date_created": "2013-10-21 13:47:00.010334+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Great--thanks! I'll work on the combined impact description in bug 1073306 for now.", 
            "date_created": "2013-10-21 15:24:01.298659+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I have a first attempt at some backports here:\n\nhttps://review.openstack.org/#/c/52987/\n\nhttps://review.openstack.org/#/c/52989/", 
            "date_created": "2013-10-21 18:46:23.014048+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Vangelis: do you have an affiliation with any employer you want mentioned as part of your reporter credit on the security advisory for this issue?", 
            "date_created": "2013-10-21 21:34:32.502096+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Jeremy: No employer needs to be mentioned, thanks :)", 
            "date_created": "2013-10-22 23:45:28.011367+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyberang3l"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/52987\nCommitted: http://github.com/openstack/nova/commit/df2ea2e3acdede21b40d47b7adbeac04213d031b\nSubmitter: Jenkins\nBranch:    stable/grizzly\n\ncommit df2ea2e3acdede21b40d47b7adbeac04213d031b\nAuthor: John Garbutt <email address hidden>\nDate:   Thu Sep 12 18:11:49 2013 +0100\n\n    xenapi: enforce filters after live-migration\n    \n    Currently and network filters, including security groups, are\n    lost after a server has been live-migrated.\n    \n    This partially fixes the issue by ensuring that security groups are\n    re-applied to the VM once it reached the destination, and been started.\n    \n    This leaves a small amount of time during the live-migrate where the VM\n    is not protected. There is a further bug raised to close the rest of\n    this whole, but this helps keep the VM protected for the majority of the\n    time.\n    \n    Fixes bug 1202266\n    \n    (Cherry picked from commit: 5cced7a6dd32d231c606e25dbf762d199bf9cca7)\n    \n    Change-Id: I66bc7af1c6da74e18dce47180af0cb6020ba2c1a\n", 
            "date_created": "2013-10-31 09:20:18.063544+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "[OSSA 2013-030] ", 
            "date_created": "2013-11-15 10:46:55.446566+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }
    ]
}
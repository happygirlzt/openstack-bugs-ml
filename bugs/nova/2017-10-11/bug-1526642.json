{
    "status": "Opinion", 
    "last_updated": "2016-02-17 16:28:22.895686+00:00", 
    "description": "Let's say we have a setup with 3 compute nodes (CN1, CN2 and CN3) and 3 controllers (in HA mode). There are 2 VMs with anti-affinity policy (the same server group) running in the environment:\n\n* CN1 - VM A (anti-affinity)\n* CN2 - VM B (anti-affinity)\n* CN3 - empty\n\nIf we trigger live migration of VM A and then trigger live migration of VM B without waiting for scheduling phase of VM A to complete we will end up with anti-affinity policy violated:\n\n* CN1 - empty\n* CN2 - empty\n* CN3 - VM A, VM B (both with anti-affinity policy)\n\nWorkaround is to wait few seconds and let scheduler finish the job for the first VM.", 
    "tags": [
        "anti-affinity", 
        "live-migration", 
        "scheduler"
    ], 
    "importance": "Wishlist", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1526642", 
    "owner": "None", 
    "id": 1526642, 
    "index": 1771, 
    "created": "2015-12-16 07:36:33.244726+00:00", 
    "title": "Simultaneous live migrations break anti-affinity policy", 
    "comments": [
        {
            "content": "Let's say we have a setup with 3 compute nodes (CN1, CN2 and CN3) and 3 controllers (in HA mode). There are 2 VMs with anti-affinity policy (the same server group) running in the environment:\n\n* CN1 - VM A (anti-affinity)\n* CN2 - VM B (anti-affinity)\n* CN3 - empty\n\nIf we trigger live migration of VM A and then trigger live migration of VM B without waiting for scheduling phase of VM A to complete we will end up with anti-affinity policy violated:\n\n* CN1 - empty\n* CN2 - empty\n* CN3 - VM A, VM B (both with anti-affinity policy)\n\nWorkaround is to wait few seconds and let scheduler finish the job for the first VM.", 
            "date_created": "2015-12-16 07:36:33.244726+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-koniszewski"
        }, 
        {
            "content": "The service group policies are checked in two places: in the scheduler filters and in _build_and_run_instance(). The scheduler check chooses a host based on the last information reported back from a host - which takes time to come up to date. As a result, two select_destinations() calls to the scheduler will give completely independent answers and may not comply with the anti-affinity policy. The subsequent check at the compute manager is expected to enforce the anti-affinity policy by rejecting the second request, but this is not done for live migration. Moreover, boot commands go through a retry loop with the scheduler, but live migration commands don't, so a check in the live migration path can only fail the live migration, it can't result in another host being chosen.\n\nSo adding support for anti-affinity in live migration is not straight forward. Adding the check will enforce the policy by failing migrations.\n\nAnother consideration is that sometimes the service group policies should be violated for live migration. This is not true for anti-affinity, but affinity will need to be broken to allow two vms that have affinity to be migrated one at a time. ", 
            "date_created": "2016-02-09 16:03:43.108291+00:00", 
            "author": "https://api.launchpad.net/1.0/~pmurray"
        }, 
        {
            "content": "This is definitely a feature rather than a bug, given the complexity. We should track it as a spec or blueprint instead. ", 
            "date_created": "2016-02-17 16:28:21.204601+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
{
    "status": "Won't Fix", 
    "last_updated": "2016-12-09 15:17:53.374807+00:00", 
    "description": "We are now facing a nova operation issue about setting different ceph rbd pool to each corresponding nova compute node in one available zone. For instance:\n(1) compute-node-1  in az1 and set images_rbd_pool=pool1\n(2) compute-node-2  in az1 and set images_rbd_pool=pool2\nThis setting can normally work fine.\n\nBut problem encountered when doing resize/migrate instance. For instance, when try to resize an instance-1 originally in compute-node-1, then nova will do schedule procedure, assuming that nova-scheduler get the chosen compute node is compute-node-2. Then the nova will get the following error:\nhttp://paste.openstack.org/show/585540/.\n\nThis exception is because that in compute-node-2 nova can't find pool1 vm1 disk. So is there a way nova can handle this? Similar thing in cinder, you may see a cinder volume has host attribute like:\nhost_name@pool_name#ceph.\n\nWhy we use such setting is because that while doing storage capacity expansion we want to avoid the influence of ceph rebalance.\n\nOne solution we found is AggregateInstanceExtraSpecsFilter, this can coordinate working with Host Aggregates metadata and flavor metadata.\nWe try to create Host Aggregates like:\naz1-pool1 with hosts compute-node-1, and metadata {ceph_pool: pool1};\naz1-pool2 with hosts compute-node-2, and metadata {ceph_pool: pool2};\nand create flavors like:\nflavor1-pool1 with metadata {ceph_pool: pool1};\nflavor2-pool1 with metadata {ceph_pool: pool1};\nflavor1-pool2 with metadata {ceph_pool: pool2};\nflavor2-pool2 with metadata {ceph_pool: pool2};\n\nBut this may introduce a new issue about the create_instance. Which flavor should be used? The business/application layer seems need to add it's own flavor scheduler. And this can also\ncause a compute service capacity issue. If choice one flavor to resize, the scheduler will use the AggregateInstanceExtraSpecsFilter to limit the destination host to the same rbd pool, what if there is no available compute host? what if there has no enough memory or CPU? So this is not the best solution.\n\nSo here finally, I want to ask, if there is a best practice about using multiple ceph rbd pools in one available zone.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1633990", 
    "owner": "None", 
    "id": 1633990, 
    "index": 6485, 
    "created": "2016-10-17 06:05:17.792284+00:00", 
    "title": "resize or migrate instance will get failed if two compute hosts are set in different rbd pool", 
    "comments": [
        {
            "content": "We are now facing a nova operation issue about setting different ceph rbd pool to each corresponding nova compute node in one available zone. For instance:\n(1) compute-node-1  in az1 and set images_rbd_pool=pool1\n(2) compute-node-2  in az1 and set images_rbd_pool=pool2\nThis setting can normally work fine.\n\nBut problem encountered when doing resize/migrate instance. For instance, when try to resize an instance-1 originally in compute-node-1, then nova will do schedule procedure, assuming that nova-scheduler get the chosen compute node is compute-node-2. Then the nova will get the following error:\nhttp://paste.openstack.org/show/585540/.\n\nThis exception is because that in compute-node-2 nova can't find pool1 vm1 disk. So is there a way nova can handle this? Similar thing in cinder, you may see a cinder volume has host attribute like:\nhost_name@pool_name#ceph.\n\nWhy we use such setting is because that while doing storage capacity expansion we want to avoid the influence of ceph rebalance.\n\nOne solution I found is AggregateInstanceExtraSpecsFilter, this can coordinate working with Host Aggregates metadata and flavor metadata.\nWe try to create Host Aggregates like:\naz1-pool1 with hosts compute-node-1, and metadata {ceph_pool: pool1};\naz1-pool2 with hosts compute-node-2, and metadata {ceph_pool: pool2};\nand create flavors like:\nflavor1-pool1 with metadata {ceph_pool: pool1};\nflavor2-pool1 with metadata {ceph_pool: pool1};\nflavor1-pool2 with metadata {ceph_pool: pool2};\nflavor2-pool2 with metadata {ceph_pool: pool2};\n\nBut this may introduce a new issue about the create_instance. Which flavor should be used? The business/application layer seems need to add it's own flavor scheduler.\n\nSo here finally, I want to ask, if there is a best practice about using multiple ceph rbd pools in one available zone.", 
            "date_created": "2016-10-17 06:05:17.792284+00:00", 
            "author": "https://api.launchpad.net/1.0/~dragon889"
        }, 
        {
            "content": "If use cinder to boot the VMs, then the following bug will be an issue:\nhttps://bugs.launchpad.net/nova/+bug/1474253", 
            "date_created": "2016-10-17 07:43:24.242845+00:00", 
            "author": "https://api.launchpad.net/1.0/~dragon889"
        }, 
        {
            "content": "I think this is basically going to have to be fixed with the generic resource providers work that's going on, i.e. you have different sets of aggregates (compute hosts) that are using different shared storage pools and when you migrate an instance using shared storage in one of the hosts then you migrate to another host in that same aggregate.", 
            "date_created": "2016-10-26 10:40:19.053561+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Until the resource providers work is done, there is no fix for this. Under current architecture this is a Won't Fix", 
            "date_created": "2016-12-09 15:17:52.897954+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
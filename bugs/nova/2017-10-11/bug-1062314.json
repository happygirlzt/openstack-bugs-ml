{
    "status": "Fix Released", 
    "last_updated": "2013-05-16 17:39:38.775394+00:00", 
    "description": "This is a bug against stable essex. I have made no attempt to determine if this is still a problem in Folsom at this stage.\n\nDuring a sprint this week we took a nova region which was previously relatively idle and started turning up large numbers of instances using juju. We started to experience very slow instance starts, which I dug into. I should note that juju seems to trigger this behaviour by refreshing security groups when ports are exposed, but other openstack users will probably experience problems if they are trying to do non-trivial things with security groups.\n\nIt appears that do_refresh_security_group_rules can sometimes take a very long time to run, and it holds the \"iptables\" lock while doing this. This is a problem because launching a new instance needs to take the iptables lock, and can end up being blocked. An example slow instance start (nova logs editted for clarity):\n\n(logs from scheduler node)\n2012-10-05 08:06:28 run_instance\n2012-10-05 08:06:29 cast to <<compute node>>\n\n(logs from compute node)\n2012-10-05 08:07:21 Starting instance...\n2012-10-05 08:07:34 Starting toXML method\n2012-10-05 08:07:43 Finished toXML method\n2012-10-05 08:07:43 Called setup_basic_filtering in nwfilter\n2012-10-05 08:07:43 Ensuring static filters\n\n2012-10-05 08:08:48 Attempting to grab semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"...\n2012-10-05 08:24:00 Got semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"...\n\n2012-10-05 08:24:01 Creating image\n2012-10-05 08:24:06 Instance is running\n2012-10-05 08:25:28 Checking state\n2012-10-05 08:25:30 Instance spawned successfully.\n\nI instrumented utils.synchronized to include lock wait and help times like this (patch against essex):\n\ndiff --git a/nova/utils.py b/nova/utils.py\nindex 6535b06..2e01a15 100644\n--- a/nova/utils.py\n+++ b/nova/utils.py\n@@ -926,10 +926,16 @@ def synchronized(name, external=False):\n             LOG.debug(_('Attempting to grab semaphore \"%(lock)s\" for method '\n                         '\"%(method)s\"...') % {'lock': name,\n                                               'method': f.__name__})\n+            started_waiting = time.time()\n+\n             with sem:\n                 LOG.debug(_('Got semaphore \"%(lock)s\" for method '\n-                            '\"%(method)s\"...') % {'lock': name,\n-                                                  'method': f.__name__})\n+                            '\"%(method)s\" after %(wait)f second wait...'),\n+                          {'lock': name,\n+                           'method': f.__name__,\n+                           'wait': time.time() - started_waiting})\n+                started_working = time.time()\n+\n                 if external and not FLAGS.disable_process_locking:\n                     LOG.debug(_('Attempting to grab file lock \"%(lock)s\" for '\n                                 'method \"%(method)s\"...') %\n@@ -945,6 +951,12 @@ def synchronized(name, external=False):\n                 else:\n                     retval = f(*args, **kwargs)\n \n+            LOG.debug(_('Released semaphore \"%(lock)s\" for method '\n+                        '\"%(method)s\" after %(wait)f seconds of use...'),\n+                      {'lock': name,\n+                       'method': f.__name__,\n+                       'wait': time.time() - started_working})\n+\n             # If no-one else is waiting for it, delete it.\n             # See note about possible raciness above.\n             if not sem.balance < 1:\n\nTaking a look at the five longest lock holds in my logs after this patch is applied, I get:\n\n# grep \"Released semaphore\" /var/log/nova/nova-compute.log | grep iptables | awk '{print$15, $13}' | sort -n | tail -5\n192.134270 \"do_refresh_security_group_rules\"\n194.140478 \"do_refresh_security_group_rules\"\n194.153729 \"do_refresh_security_group_rules\"\n201.135854 \"do_refresh_security_group_rules\"\n297.725837 \"do_refresh_security_group_rules\"\n\nSo I then instrumented do_refresh_security_group_rules to try and see what was slow. I used this patch (which I know is horrible):\n\ndiff --git a/nova/virt/firewall.py b/nova/virt/firewall.py\nindex f0f1594..99f580a 100644\n--- a/nova/virt/firewall.py\n+++ b/nova/virt/firewall.py\n@@ -17,6 +17,8 @@\n #    License for the specific language governing permissions and limitations\n #    under the License.\n \n+import time\n+\n from nova import context\n from nova import db\n from nova import flags\n@@ -167,16 +169,35 @@ class IptablesFirewallDriver(FirewallDriver):\n                 self.iptables.ipv6['filter'].add_rule(chain_name, rule)\n \n     def add_filters_for_instance(self, instance):\n+        start_time = time.time()\n         network_info = self.network_infos[instance['id']]\n+        LOG.debug(_('Get network info took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         chain_name = self._instance_chain_name(instance)\n+        LOG.debug(_('Get chain name took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         if FLAGS.use_ipv6:\n             self.iptables.ipv6['filter'].add_chain(chain_name)\n         self.iptables.ipv4['filter'].add_chain(chain_name)\n+        LOG.debug(_('Add chain took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         ipv4_rules, ipv6_rules = self._filters_for_instance(chain_name,\n                                                             network_info)\n+        LOG.debug(_('Filters for instance took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         self._add_filters('local', ipv4_rules, ipv6_rules)\n         ipv4_rules, ipv6_rules = self.instance_rules(instance, network_info)\n         self._add_filters(chain_name, ipv4_rules, ipv6_rules)\n+        LOG.debug(_('Add filters took %f seconds'),\n+                  time.time() - start_time)\n \n     def remove_filters_for_instance(self, instance):\n         chain_name = self._instance_chain_name(instance)\n@@ -362,9 +383,17 @@ class IptablesFirewallDriver(FirewallDriver):\n \n     @utils.synchronized('iptables', external=True)\n     def do_refresh_security_group_rules(self, security_group):\n+        # TODO(mikal): why is security group passed in if its not used?\n         for instance in self.instances.values():\n+            remove_start = time.time()\n             self.remove_filters_for_instance(instance)\n+            add_start = time.time()\n             self.add_filters_for_instance(instance)\n+            LOG.debug(_('Refreshing security groups for instance (removal '\n+                        'took %(remove)f seconds, add took %(add)f seconds)'),\n+                      {'remove': add_start - remove_start,\n+                       'add': time.time() - add_start},\n+                      instance=instance)\n \n     def refresh_provider_fw_rules(self):\n         \"\"\"See :class:`FirewallDriver` docs.\"\"\"\n\nThis gives me log entries like this (not all are this slow):\n\n2012-10-05 13:27:43 DEBUG nova.virt.firewall [req-051d71e4-1a71-43f9-87a4-36686b5445c6 None None] Add filters took 46.641673 seconds from (pid=8052) add_filters_for_instance /usr/lib/python2.7/dist-packages/nova/virt/firewall.py:200\n2012-10-05 13:27:43 DEBUG nova.virt.firewall [req-051d71e4-1a71-43f9-87a4-36686b5445c6 None None] [instance: 06ea0120-ed5e-4379-90b1-322e1de68950] Refreshing security groups for instance (removal took 0.000958 seconds, add took 46.643727 seconds) from (pid=8052) do_refresh_security_group_rules /usr/lib/python2.7/dist-packages/nova/virt/firewall.py:396\n\nI'm still looking at this, but it looks like nova.virt.firewall.py IptablesFirewallDriver.instance_rules() is the problem. This method makes both database and rpc calls while holding the iptables lock.", 
    "tags": [
        "canonistack", 
        "ops", 
        "verification-done"
    ], 
    "importance": "High", 
    "heat": 26, 
    "link": "https://bugs.launchpad.net/nova/+bug/1062314", 
    "owner": "https://api.launchpad.net/1.0/~mikal", 
    "id": 1062314, 
    "index": 841, 
    "created": "2012-10-05 13:58:52.577829+00:00", 
    "title": "do_refresh_security_group_rules in nova.virt.firewall is very slow", 
    "comments": [
        {
            "content": "This is a bug against stable essex. I have made no attempt to determine if this is still a problem in Folsom at this stage.\n\nDuring a sprint this week we took a nova region which was previously relatively idle and started turning up large numbers of instances using juju. We started to experience very slow instance starts, which I dug into. I should note that juju seems to trigger this behaviour by refreshing security groups when ports are exposed, but other openstack users will probably experience problems if they are trying to do non-trivial things with security groups.\n\nIt appears that do_refresh_security_group_rules can sometimes take a very long time to run, and it holds the \"iptables\" lock while doing this. This is a problem because launching a new instance needs to take the iptables lock, and can end up being blocked. An example slow instance start (nova logs editted for clarity):\n\n(logs from scheduler node)\n2012-10-05 08:06:28 run_instance\n2012-10-05 08:06:29 cast to <<compute node>>\n\n(logs from compute node)\n2012-10-05 08:07:21 Starting instance...\n2012-10-05 08:07:34 Starting toXML method\n2012-10-05 08:07:43 Finished toXML method\n2012-10-05 08:07:43 Called setup_basic_filtering in nwfilter\n2012-10-05 08:07:43 Ensuring static filters\n\n2012-10-05 08:08:48 Attempting to grab semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"...\n2012-10-05 08:24:00 Got semaphore \"iptables\" for method \"_do_refresh_provider_fw_rules\"...\n\n2012-10-05 08:24:01 Creating image\n2012-10-05 08:24:06 Instance is running\n2012-10-05 08:25:28 Checking state\n2012-10-05 08:25:30 Instance spawned successfully.\n\nI instrumented utils.synchronized to include lock wait and help times like this (patch against essex):\n\ndiff --git a/nova/utils.py b/nova/utils.py\nindex 6535b06..2e01a15 100644\n--- a/nova/utils.py\n+++ b/nova/utils.py\n@@ -926,10 +926,16 @@ def synchronized(name, external=False):\n             LOG.debug(_('Attempting to grab semaphore \"%(lock)s\" for method '\n                         '\"%(method)s\"...') % {'lock': name,\n                                               'method': f.__name__})\n+            started_waiting = time.time()\n+\n             with sem:\n                 LOG.debug(_('Got semaphore \"%(lock)s\" for method '\n-                            '\"%(method)s\"...') % {'lock': name,\n-                                                  'method': f.__name__})\n+                            '\"%(method)s\" after %(wait)f second wait...'),\n+                          {'lock': name,\n+                           'method': f.__name__,\n+                           'wait': time.time() - started_waiting})\n+                started_working = time.time()\n+\n                 if external and not FLAGS.disable_process_locking:\n                     LOG.debug(_('Attempting to grab file lock \"%(lock)s\" for '\n                                 'method \"%(method)s\"...') %\n@@ -945,6 +951,12 @@ def synchronized(name, external=False):\n                 else:\n                     retval = f(*args, **kwargs)\n \n+            LOG.debug(_('Released semaphore \"%(lock)s\" for method '\n+                        '\"%(method)s\" after %(wait)f seconds of use...'),\n+                      {'lock': name,\n+                       'method': f.__name__,\n+                       'wait': time.time() - started_working})\n+\n             # If no-one else is waiting for it, delete it.\n             # See note about possible raciness above.\n             if not sem.balance < 1:\n\nTaking a look at the five longest lock holds in my logs after this patch is applied, I get:\n\n# grep \"Released semaphore\" /var/log/nova/nova-compute.log | grep iptables | awk '{print$15, $13}' | sort -n | tail -5\n192.134270 \"do_refresh_security_group_rules\"\n194.140478 \"do_refresh_security_group_rules\"\n194.153729 \"do_refresh_security_group_rules\"\n201.135854 \"do_refresh_security_group_rules\"\n297.725837 \"do_refresh_security_group_rules\"\n\nSo I then instrumented do_refresh_security_group_rules to try and see what was slow. I used this patch (which I know is horrible):\n\ndiff --git a/nova/virt/firewall.py b/nova/virt/firewall.py\nindex f0f1594..99f580a 100644\n--- a/nova/virt/firewall.py\n+++ b/nova/virt/firewall.py\n@@ -17,6 +17,8 @@\n #    License for the specific language governing permissions and limitations\n #    under the License.\n \n+import time\n+\n from nova import context\n from nova import db\n from nova import flags\n@@ -167,16 +169,35 @@ class IptablesFirewallDriver(FirewallDriver):\n                 self.iptables.ipv6['filter'].add_rule(chain_name, rule)\n \n     def add_filters_for_instance(self, instance):\n+        start_time = time.time()\n         network_info = self.network_infos[instance['id']]\n+        LOG.debug(_('Get network info took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         chain_name = self._instance_chain_name(instance)\n+        LOG.debug(_('Get chain name took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         if FLAGS.use_ipv6:\n             self.iptables.ipv6['filter'].add_chain(chain_name)\n         self.iptables.ipv4['filter'].add_chain(chain_name)\n+        LOG.debug(_('Add chain took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         ipv4_rules, ipv6_rules = self._filters_for_instance(chain_name,\n                                                             network_info)\n+        LOG.debug(_('Filters for instance took %f seconds'),\n+                  time.time() - start_time)\n+\n+        start_time = time.time()\n         self._add_filters('local', ipv4_rules, ipv6_rules)\n         ipv4_rules, ipv6_rules = self.instance_rules(instance, network_info)\n         self._add_filters(chain_name, ipv4_rules, ipv6_rules)\n+        LOG.debug(_('Add filters took %f seconds'),\n+                  time.time() - start_time)\n \n     def remove_filters_for_instance(self, instance):\n         chain_name = self._instance_chain_name(instance)\n@@ -362,9 +383,17 @@ class IptablesFirewallDriver(FirewallDriver):\n \n     @utils.synchronized('iptables', external=True)\n     def do_refresh_security_group_rules(self, security_group):\n+        # TODO(mikal): why is security group passed in if its not used?\n         for instance in self.instances.values():\n+            remove_start = time.time()\n             self.remove_filters_for_instance(instance)\n+            add_start = time.time()\n             self.add_filters_for_instance(instance)\n+            LOG.debug(_('Refreshing security groups for instance (removal '\n+                        'took %(remove)f seconds, add took %(add)f seconds)'),\n+                      {'remove': add_start - remove_start,\n+                       'add': time.time() - add_start},\n+                      instance=instance)\n \n     def refresh_provider_fw_rules(self):\n         \"\"\"See :class:`FirewallDriver` docs.\"\"\"\n\nThis gives me log entries like this (not all are this slow):\n\n2012-10-05 13:27:43 DEBUG nova.virt.firewall [req-051d71e4-1a71-43f9-87a4-36686b5445c6 None None] Add filters took 46.641673 seconds from (pid=8052) add_filters_for_instance /usr/lib/python2.7/dist-packages/nova/virt/firewall.py:200\n2012-10-05 13:27:43 DEBUG nova.virt.firewall [req-051d71e4-1a71-43f9-87a4-36686b5445c6 None None] [instance: 06ea0120-ed5e-4379-90b1-322e1de68950] Refreshing security groups for instance (removal took 0.000958 seconds, add took 46.643727 seconds) from (pid=8052) do_refresh_security_group_rules /usr/lib/python2.7/dist-packages/nova/virt/firewall.py:396\n\nI'm still looking at this, but it looks like nova.virt.firewall.py IptablesFirewallDriver.instance_rules() is the problem. This method makes both database and rpc calls while holding the iptables lock.", 
            "date_created": "2012-10-05 13:58:52.577829+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Status changed to 'Confirmed' because the bug affects multiple users.", 
            "date_created": "2012-10-05 15:16:02.512211+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "I think the issue here is that nova.virt.firewall.py IptablesFirewallDriver.instance_rules() is calling get_instance_nw_info() which is causing rpcs to be fired off _while_still_holding_the_iptables_lock. I suspect that the rpcs need to happen outside the lock.\n\nFrom yet more instrumented code:\n\nA synchronous RPC call is being made while a lock is held. This is probably a bug. Please report it. Include lines following this that start with ** please.\n** multicall\n** call\n** call\n** call\n** get_instance_nw_info\n** instance_rules\n** add_filters_for_instance\n** do_refresh_security_group_rules\n** inner_while_holding_lock\n** refresh_security_group_members\n** refresh_security_group_members\n** refresh_security_group_members\n** wrapped\n** _process_data\n** wrapped\n** _spawn_n_impl\n** end of stack trace", 
            "date_created": "2012-10-05 17:04:30.132522+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/14326", 
            "date_created": "2012-10-11 04:51:56.803461+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/14326\nCommitted: http://github.com/openstack/nova/commit/ba585524e32965697c1a44c8fd743dea060bb1af\nSubmitter: Jenkins\nBranch:    master\n\ncommit ba585524e32965697c1a44c8fd743dea060bb1af\nAuthor: Michael Still <email address hidden>\nDate:   Thu Oct 11 15:46:11 2012 +1100\n\n    Avoid RPC calls while holding iptables lock.\n    \n    This exhibitied itself as very slow instance starts on a Canonical\n    test cluster. This was because do_referesh_security_group_rules()\n    was making rpc calls while holding the iptables lock. This refactor\n    avoids that while making no functional changes (I hope).\n    \n    This should resolve bug 1062314.\n    \n    Change-Id: I36f805bd72f7bd06082cfe96c58d637203bcffb7\n", 
            "date_created": "2012-10-11 12:44:31.873670+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/folsom\nReview: https://review.openstack.org/14367", 
            "date_created": "2012-10-11 23:37:48.138276+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/essex\nReview: https://review.openstack.org/14368", 
            "date_created": "2012-10-11 23:47:54.275788+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/14367\nCommitted: http://github.com/openstack/nova/commit/eee4dbb07e88e19ca891275f5ba1d381bd98b4e5\nSubmitter: Jenkins\nBranch:    stable/folsom\n\ncommit eee4dbb07e88e19ca891275f5ba1d381bd98b4e5\nAuthor: Michael Still <email address hidden>\nDate:   Thu Oct 11 15:46:11 2012 +1100\n\n    Avoid RPC calls while holding iptables lock.\n    \n    This exhibitied itself as very slow instance starts on a Canonical\n    test cluster. This was because do_referesh_security_group_rules()\n    was making rpc calls while holding the iptables lock. This refactor\n    avoids that while making no functional changes (I hope).\n    \n    Fixes bug: 1062314\n    Change-Id: I36f805bd72f7bd06082cfe96c58d637203bcffb7\n    Cherry-picked: ba585524e32965697c1a44c8fd743dea060bb1af\n", 
            "date_created": "2012-10-12 16:02:36.401466+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Upstream has chosen not to backport this fix to essex. Can we please consider carrying this patch ourselves?", 
            "date_created": "2012-10-12 19:27:41.020509+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "To clarify, it's not in 2012.1.3 but will probably be added to the 2012.1.4 branch", 
            "date_created": "2012-10-12 19:39:02.961566+00:00", 
            "author": "https://api.launchpad.net/1.0/~p-draigbrady"
        }, 
        {
            "content": "This bug was fixed in the package nova - 2012.2-0ubuntu5\n\n---------------\nnova (2012.2-0ubuntu5) quantal-proposed; urgency=low\n\n  [ Adam Gandelman ]\n  * Move management of /var/lib/nova/volumes from nova-common to\n    nova-volume.  Ensure it has proper permissions. (LP: #1065320)\n  * debian/patches/avoid_setuptools_git_dependency.patch:  Remove\n    setuptools_git from tools/pip-requires to avoid it being automatically\n    added to python-nova's runtime dependencies. (LP: #1059907)\n\n  [ Chuck Short ]\n  * debian/patches/rbd-security.patch: Support override of ceph rbd\n    user and secret in nova-compute. (LP: #1065883)\n  * debian/patches/ubuntu/fix-libvirt-firewall-slowdown.patch: Fix\n    refreshing of security groups in libvirt not to block on RPC calls.\n    (LP: #1062314)\n  * debian/patches/ubuntu/fix-ec2-volume-id-mappings.patch: Read deleted\n    snapshot and volume id mappings. (LP: #1065785)\n -- Chuck Short <email address hidden>   Fri, 12 Oct 2012 12:35:01 -0500", 
            "date_created": "2012-10-15 09:45:38.642784+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/14368\nCommitted: http://github.com/openstack/nova/commit/86a59376c6d4d2ec5a05c8b63a419d0374e9d9c9\nSubmitter: Jenkins\nBranch:    stable/essex\n\ncommit 86a59376c6d4d2ec5a05c8b63a419d0374e9d9c9\nAuthor: Michael Still <email address hidden>\nDate:   Thu Oct 11 15:46:11 2012 +1100\n\n    Avoid RPC calls while holding iptables lock.\n    \n    This exhibitied itself as very slow instance starts on a Canonical\n    test cluster. This was because do_referesh_security_group_rules()\n    was making rpc calls while holding the iptables lock. This refactor\n    avoids that while making no functional changes (I hope).\n    \n    Fixes bug: 1062314\n    Change-Id: I36f805bd72f7bd06082cfe96c58d637203bcffb7\n    Cherry-picked: ba585524e32965697c1a44c8fd743dea060bb1af\n    Conflicts:\n    \tnova/virt/firewall.py\n", 
            "date_created": "2012-10-31 07:36:24.375680+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Hello Michael, or anyone else affected,\n\nAccepted nova into quantal-proposed. The package will build now and be available at http://launchpad.net/ubuntu/+source/nova/2012.2.1+stable-20121212-a99a802e-0ubuntu1 in a few hours, and then in the -proposed repository.\n\nPlease help us by testing this new package.  See https://wiki.ubuntu.com/Testing/EnableProposed for documentation how to enable and use -proposed.  Your feedback will aid us getting this update out to other Ubuntu users.\n\nIf this package fixes the bug for you, please add a comment to this bug, mentioning the version of the package you tested, and change the tag from verification-needed to verification-done. If it does not fix the bug for you, please add a comment stating that, and change the tag to verification-failed.  In either case, details of your testing will help us make a better decision.\n\nFurther information regarding the verification process can be found at https://wiki.ubuntu.com/QATeam/PerformingSRUVerification .  Thank you in advance!", 
            "date_created": "2012-12-28 16:27:38.663270+00:00", 
            "author": "https://api.launchpad.net/1.0/~clint-fewbar"
        }, 
        {
            "content": "This bug was fixed in the package nova - 2012.2.1+stable-20121212-a99a802e-0ubuntu1\n\n---------------\nnova (2012.2.1+stable-20121212-a99a802e-0ubuntu1) quantal-proposed; urgency=low\n\n  * Ubuntu updates:\n    - debian/control: Ensure novaclient is upgraded with nova,\n      require python-keystoneclient >= 1:2.9.0. (LP: #1073289)\n    - d/p/avoid_setuptools_git_dependency.patch: Refresh.\n  * Dropped patches, applied upstream:\n    - debian/patches/CVE-2012-5625.patch: [a99a802]\n  * Resynchronize with stable/folsom (b55014ca) (LP: #1085255):\n    - [a99a802] create_lvm_image allocates dirty blocks (LP: #1070539)\n    - [670b388] RPC exchange name defaults to 'openstack' (LP: #1083944)\n    - [3ede373] disassociate_floating_ip with multi_host=True fails\n      (LP: #1074437)\n    - [22d7c3b] libvirt imagecache should handle shared image storage\n      (LP: #1075018)\n    - [e787786] Detached and deleted RBD volumes remain associated with insance\n      (LP: #1083818)\n    - [9265eb0] live_migration missing migrate_data parameter in Hyper-V driver\n      (LP: #1066513)\n    - [3d99848] use_single_default_gateway does not function correctly\n      (LP: #1075859)\n    - [65a2d0a] resize does not migrate DHCP host information (LP: #1065440)\n    - [102c76b] Nova backup image fails (LP: #1065053)\n    - [48a3521] Fix config-file overrides for nova-dhcpbridge\n    - [69663ee] Cloudpipe in Folsom: no such option: cnt_vpn_clients\n      (LP: #1069573)\n    - [6e47cc8] DisassociateAddress can cause Internal Server Error\n      (LP: #1080406)\n    - [22c3d7b] API calls to dis-associate an auto-assigned floating IP should\n      return proper warning (LP: #1061499)\n    - [bd11d15] libvirt: if exception raised during volume_detach, volume state\n      is inconsistent (LP: #1057756)\n    - [dcb59c3] admin can't describe all images in ec2 api (LP: #1070138)\n    - [78de622] Incorrect Exception raised during Create server when metadata\n      over 255 characters  (LP: #1004007)\n    - [c313de4] Fixed IP isn't released before updating DHCP host file\n      (LP: #1078718)\n    - [f4ab42d] Enabling Return Reservation ID with XML create server request\n      returns no body  (LP: #1061124)\n    - [3db2a38] 'BackupCreate' should accept rotation parameter greater than or\n      equal to zero (LP: #1071168)\n    - [f7e5dde] libvirt reboot sometimes fails to reattach volumes\n      (LP: #1073720)\n    - [ff776d4] libvirt: detaching volume may fail while terminating other\n      instances on the same host concurrently (LP: #1060836)\n    - [85a8bc2] Used instance uuid rather than id in remove-fixed-ip\n    - [42a85c0] Fix error on invalid delete_on_termination value\n    - [6a17579] xenapi migrations fail w/ swap (LP: #1064083)\n    - [97649b8] attach-time field for volumes is not updated for detach volume\n      (LP: #1056122)\n    - [8f6a718] libvirt: rebuild is not using kernel and ramdisk associated with\n      the new image (LP: #1060925)\n    - [fbe835f] live-migration and volume host assignement (LP: #1066887)\n    - [c2a9150] typo prevents volume_tmp_dir flag from working (LP: #1071536)\n    - [93efa21] Instances deleted during spawn leak network allocations\n      (LP: #1068716)\n    - [ebabd02] After restarting an instance volume is lost (LP: #1071069)\n    - [a369303] xen volume auto device selection always picks xvdb\n      (LP: #1061944)\n    - [8d1095c] Calls to to_xml() to generate XML for a soft deleted flavor fail\n      (LP: #1073736)\n    - [1857821] nova-manage doesn't validate the key value supplied to update\n      the quota (LP: #1064359)\n    - [6ae32f0] Compute manager doesn't update 'host' field when it tries to run\n      a VM (LP: #1073600)\n    - [284f6ea] Host field set too early during builds (LP: #1060255)\n    - [395511f] finish_resize failures result in NoneType exception\n      (LP: #1071595)\n    - [85ccf80] confirm_resize mgr call requires admin context (LP: #1071600)\n    - [2dceffa] Only return the last N lines of the console log (LP: #1081436)\n    - [9c7a711] console auth does not work with memcache, unicode error\n      (LP: #1057279)\n    - [b27f7ef] disk path not exists when using LXC with libvirt_images_type=lvm\n      (LP: #1079113)\n    - [1351c6b] nova-api now requires quantumclient (LP: #1070509)\n    - [612f404] nova-api now requires quantumclient (LP: #1070509)\n    - [7e8a166] nova-compute (folsom) fails to start, compute_driver is None\n      (LP: #1081836)\n    - [182ca80] Nova API does not work with QuantumV2 API subclasses\n      (LP: #1070045)\n    - [55d1412] 413 error code doesn't always provide Retry-After (LP: #1079387)\n    - [1581505] Snapshotting LXC instance fails (LP: #1058273)\n    - [197398f] Stop network.api import on network import\n    - [b874d21] Scheduler Race Condition at high volume (LP: #1073956)\n    - [3316e1f] Nic Ordering not guaranteed with Quantum API (LP: #1064524)\n    - [ab7e37e] Stable oslo (aka common) update\n    - [3f7788c] update nova to report quantum floating IPs (LP: #1023169)\n    - [d3fd05b] metadata service throws 500 - NoSuchOptError (LP: #1063851)\n    - [97542c9] libvirt imagecache still runs even if disabled (LP: #1075017)\n    - [b31f528] OS API: XML Namespace Handling Broken (LP: #887191)\n    - [76b44d9] nova-api crashes if it is run with nobody account.\n      (LP: #1073858)\n    - [d59f6ad] nova-compute will assign the same device name if volume-attach\n      continuously  (LP: #1062033)\n    - [8e11181] Nova does not delete the LV on LVM backed VMs (LP: #1078085)\n    - [9bf2c6a] Fixed instance deletion issue from Nova API.\n    - [c0e1247] forget to release resource when terminate an instance from a\n      failed compute node (LP: #1067214)\n    - [49397a4] ensure_default_security_group() does not call sgh (LP: #1050982)\n    - [47ff8a5] trigger_instance[add/remove]_security_group_refresh are never\n      called (LP: #1057069)\n    - [c9cade2] Resource reservation isn't rolled back properly for certain\n      failures during Instance Create (LP: #1065092)\n    - [34c3845] Resource tracker uses regex DB query too often (LP: #1060363)\n    - [92eddd2] Logging CPU incompatibility when attempting live migration fails\n      (LP: #1076308)\n    - [8b4896b] hostname in metadata ends with . if dhcp_domain flag is empty\n      (LP: #1064713)\n    - [ded0473] deletes fail when instance in RESIZED (LP: #1056601)\n    - [d015be5] libvirt: cannot detach volume from stopped domain (LP: #1057730)\n    - [d5888f1] Resizing a Xen instance with attached volumes fails\n      (LP: #1028092)\n    - [fb88827] resize leave leftover libvirt configs (LP: #1015731)\n    - [5ccd691] nova-network cannot re-generate MAC address if collision happen\n      (LP: #1059366)\n    - [e3d7f8c] After folsom upgrade, instances can no longer access existing\n      volumes. (LP: #1065702)\n    - [804f858] Jenkins jobs fail because of incompatibility between sqlalchemy-\n      migrate and the newest sqlalchemy-0.8.0b1 (LP: #1073569)\n    - [f67a5f9] block device mappings for deleted instances are leaked\n      (LP: #1069099)\n    - [32d8722] volume and snapshot IDs do not correctly map to UUIDs after\n      folsom upgrade (LP: #1065785)\n    - [9613643] Xenserver cannot boot vm_mode=xen type images (LP: #1055431)\n    - [863c767] Cloudpipe extension xml serialization doesn't return the\n      instance(s) data (LP: #1056242)\n    - [724adcf] deleting security group does not mark rules as deleted\n      (LP: #1056380)\n    - [84a996c] IP Protocol for security group should be returned in lower case\n      to be compliant with the ec2 api (LP: #1057196)\n    - [e1ed06a] db tests fail with sqlalchemy 0.7.4 (LP: #1057145)\n    - [bddb06d] Fail to boot raw image on XenServer (LP: #1055413)\n    - [d4d1665] Add SIGPIPE handler to subprocess execution in rootwrap and\n      utils.execute (LP: #1053364)\n    - [0af4dd0] libvirt: concurrent detach_volume and terminate fails\n      (LP: #1057719)\n    - [ebbfa9e] Instances in vm state DELETED are preventing compute restart\n      (LP: #1053441)\n    - [db516a2] ComputeManager does not provide block_device_info on destroy\n      call in revert_resize (LP: #1056285)\n    - [4223ebf] Set defaultbranch in .gitreview to stable/folsom\n    - [eee4dbb] do_refresh_security_group_rules in nova.virt.firewall is very\n      slow (LP: #1062314)\n    - [b7e509a] Set read_deleted='yes' for instance_id_mappings.\n    - [9e20735] Tests fail on 32bit machines (_get_hash_str is platform\n      dependent) (LP: #1050359)\n -- Adam Gandelman <email address hidden>   Wed, 12 Dec 2012 16:27:51 -0400", 
            "date_created": "2013-01-29 13:10:46.783756+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "Hello Michael, or anyone else affected,\n\nAccepted nova into precise-proposed. The package will build now and be available at http://launchpad.net/ubuntu/+source/nova/2012.1.3+stable-20130423-e52e6912-0ubuntu1 in a few hours, and then in the -proposed repository.\n\nPlease help us by testing this new package.  See https://wiki.ubuntu.com/Testing/EnableProposed for documentation how to enable and use -proposed.  Your feedback will aid us getting this update out to other Ubuntu users.\n\nIf this package fixes the bug for you, please add a comment to this bug, mentioning the version of the package you tested, and change the tag from verification-needed to verification-done. If it does not fix the bug for you, please add a comment stating that, and change the tag to verification-failed.  In either case, details of your testing will help us make a better decision.\n\nFurther information regarding the verification process can be found at https://wiki.ubuntu.com/QATeam/PerformingSRUVerification .  Thank you in advance!", 
            "date_created": "2013-05-09 23:14:41.871351+00:00", 
            "author": "https://api.launchpad.net/1.0/~brian-murray"
        }, 
        {
            "content": "Please find the attached test log from the Ubuntu Server Team's CI infrastructure.  As part of the verification process for this bug, Nova has been deployed and configured across multiple nodes using precise-proposed as an installation source.  After successful bring-up and configuration of the cluster, a number of exercises and smoke tests have be invoked to ensure the updated package did not introduce any regressions. A number of test iterations were carried out to catch any possible transient errors.\n\nPlease Note the list of installed packages at the top and bottom of the report.\n\nFor records of upstream test coverage of this update, please see the Jenkins links in the comments of the relevant  upstream code-review(s):\n\nTrunk review: https://review.openstack.org/14326\nStable review: https://review.openstack.org/14368\n\nAs per the provisional Micro Release Exception granted to this package by the Technical Board, we hope this contributes toward verification of this update.", 
            "date_created": "2013-05-16 07:30:36.764207+00:00", 
            "author": "https://api.launchpad.net/1.0/~yolanda.robla"
        }, 
        {
            "content": "Test coverage log.", 
            "date_created": "2013-05-16 07:30:41.026301+00:00", 
            "author": "https://api.launchpad.net/1.0/~yolanda.robla"
        }, 
        {
            "content": "Please find the attached test log from the Ubuntu Server Team's CI infrastructure.  As part of the verification process for this bug, Nova has been deployed and configured across multiple nodes using precise-proposed as an installation source.  After successful bring-up and configuration of the cluster, a number of exercises and smoke tests have be invoked to ensure the updated package did not introduce any regressions. A number of test iterations were carried out to catch any possible transient errors.\n\nPlease Note the list of installed packages at the top and bottom of the report.\n\nFor records of upstream test coverage of this update, please see the Jenkins links in the comments of the relevant  upstream code-review(s):\n\nTrunk review: https://review.openstack.org/14326\nStable review: https://review.openstack.org/14368\n\nAs per the provisional Micro Release Exception granted to this package by the Technical Board, we hope this contributes toward verification of this update.", 
            "date_created": "2013-05-16 07:32:56.360009+00:00", 
            "author": "https://api.launchpad.net/1.0/~yolanda.robla"
        }, 
        {
            "content": "Test coverage log.", 
            "date_created": "2013-05-16 07:33:01.020180+00:00", 
            "author": "https://api.launchpad.net/1.0/~yolanda.robla"
        }, 
        {
            "content": "The verification of this Stable Release Update has completed successfully and the package has now been released to -updates.  Subsequently, the Ubuntu Stable Release Updates Team is being unsubscribed and will not receive messages about this bug report.  In the event that you encounter a regression using the package from -updates please report a new bug using ubuntu-bug and tag the bug report regression-update so we can easily find any regresssions.", 
            "date_created": "2013-05-16 17:29:29.127138+00:00", 
            "author": "https://api.launchpad.net/1.0/~kitterman"
        }
    ]
}
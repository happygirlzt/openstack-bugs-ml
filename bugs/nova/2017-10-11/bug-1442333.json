{
    "status": "Invalid", 
    "last_updated": "2015-08-18 22:52:40.933442+00:00", 
    "description": "This issue is being treated as a potential security risk under embargo. Please do not make any public mention of embargoed (private) security vulnerabilities before their coordinated publication by the OpenStack Vulnerability Management Team in the form of an official OpenStack Security Advisory. This includes discussion of the bug or associated fixes in public forums such as mailing lists, code review systems and bug trackers. Please also avoid private disclosure to other individuals not already approved for access to this information, and provide this same reminder to those who are made aware of the issue prior to publication. All discussion should remain confined to this private bug report, and any proposed fixes should be added as to the bug as attachments.\n\nNot quite sure if this is a security issue and I unfortunately already mentioned it in #openstack-nova (http://eavesdrop.openstack.org/irclogs/%23openstack-nova/%23openstack-nova.2015-04-09.log starts at 2015-04-09T18:39:39), but figured I would be nice and report this privately then you all can make it more public if that is desireable.\n\nI am debugging the upstream gate's multinode test environment with nova network. This env has two nodes, a controller with compute and a compute node. They run in multi host mode so each compute node is its own gateway.\n\nIf I create two VMs, nodeA and nodeB this schedules nodeA on the controller and nodeB on the compute node. Then I attach a floating ip to each VM. nodeA gets fipA and nodeB gets fipB. Now delete nodeB, shelve nodeA, unshelve nodeA. We end up with nodeA moved to the compute node from the controller. The issue is that nodeA has both fipA and fipB attached to it according to nova show nodeA. I was able to confirm that sshing to both floating IPs connects me to nodeA.\n\nWhile I only tested this with a single tenant, my understanding of nova net's floating ip implementation is that this can also affect nodeA and nodeB if they belong to different tenants. In any case the potential to grab someone elses floating ip when they have potentially tried to remove its connectivity is why I am filing this as a security bug. At the very least it is a bug and shelve unshelve should not steal floating ips.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1442333", 
    "owner": "None", 
    "id": 1442333, 
    "index": 5499, 
    "created": "2015-04-09 19:20:08.444183+00:00", 
    "title": "Floating IP is reused after nodeB deletion and nodeA shelve unshelve", 
    "comments": [
        {
            "content": "Not quite sure if this is a security issue and I unfortunately already mentioned it in #openstack-nova (http://eavesdrop.openstack.org/irclogs/%23openstack-nova/%23openstack-nova.2015-04-09.log starts at 2015-04-09T18:39:39), but figured I would be nice and report this privately then you all can make it more public if that is desireable.\n\nI am debugging the upstream gate's multinode test environment with nova network. This env has two nodes, a controller with compute and a compute node. They run in multi host mode so each compute node is its own gateway.\n\nIf I create two VMs, nodeA and nodeB this schedules nodeA on the controller and nodeB on the compute node. Then I attach a floating ip to each VM. nodeA gets fipA and nodeB gets fipB. Now delete nodeB, shelve nodeA, unshelve nodeA. We end up with nodeA moved to the compute node from the controller. The issue is that nodeA has both fipA and fipB attached to it according to nova show nodeA. I was able to confirm that sshing to both floating IPs connects me to nodeA.\n\nWhile I only tested this with a single tenant, my understanding of nova net's floating ip implementation is that this can also affect nodeA and nodeB if they belong to different tenants. In any case the potential to grab someone elses floating ip when they have potentially tried to remove its connectivity is why I am filing this as a security bug. At the very least it is a bug and shelve unshelve should not steal floating ips.", 
            "date_created": "2015-04-09 19:20:08.444183+00:00", 
            "author": "https://api.launchpad.net/1.0/~cboylan"
        }, 
        {
            "content": "Since this report concerns a possible security risk, an incomplete security advisory task has been added while the core security reviewers for the affected project or projects confirm the bug and discuss the scope of any vulnerability along with potential solutions. ", 
            "date_created": "2015-04-09 19:22:46.531742+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "The key question here is, does it reproduce cross-tenant? Else I can't see why we would keep this under embargo.", 
            "date_created": "2015-05-26 19:17:02.448355+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "I'm looking for a nova-core to take a look at this at the moment. I will report my progress back once I've made some.", 
            "date_created": "2015-06-03 05:01:09.401059+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "For what it's worth, I have been trying to reproduce the bug locally using a multinode devstack lab without success. Same tenant or cross-tenant, I don't see floating ips taken over by unshelved instances moving to the compute node where another instance has been deleted. I don't have the same config and conditions as the multinode environment in infra, though, so I'm not sure it means much. ", 
            "date_created": "2015-06-11 11:57:11.558270+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Clark, does this bug reproduced or was it a one time failure ?\n\nIs there a specifics infra-related settings that could help Melanie to debunk it ?", 
            "date_created": "2015-06-11 19:26:54.429923+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "My recollection is that this was discovered while going down one of many blind alleys of potential configurations for what we eventually ended up with for multi-node devstack job workers, so there's a good chance we don't _currently_ have any configured in a way that would easily exhibit this. Once Clark is around again, he'll hopefully remember what other possibly relevant details might be missing from the bug for proper reducibility. Until then I don't think it's urgent to continue trying to reproduce.", 
            "date_created": "2015-06-11 20:00:49.799780+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "To be specific, I was able to gather a lot of information from an example multinode job [1] run, so I did see the localrc and nova.conf for both nodes and made mine as close as possible. I created two Vagrant Virtualbox VMs each with 3 interfaces, one NAT eth0, two host-only eth1 and eth2. I configured the fixed range on eth1 and the floating range on eth2. Bridge br100 was attached to eth1, and that was the only bridge.\n\nInitially, I had problems where after the nodeA instance moved to the compute node after unshelve, I could no longer reach (destination unreachable) the floating ip attached to it, so I changed to using the NoopFirewallDriver. At no point did I see two floating ips attached in the 'nova show nodeA', IptablesFirewallDriver or NoopFirewallDriver.\n\nWith the NoopFirewallDriver, after nodeA moves, I could still access its floating ip, but only its original floating ip. I never saw the released floating ip from nodeB attached to nodeA (from a 'nova show' point of view), nor could I ping it or access it. I got messages like this trying to ping nodeB's old floating ip:\n\n$ ping 192.168.42.130\nPING 192.168.42.130 (192.168.42.130) 56(84) bytes of data.\nFrom 192.168.42.11: icmp_seq=2 Redirect Host(New nexthop: 192.168.42.11)\nFrom 192.168.42.11: icmp_seq=3 Redirect Host(New nexthop: 192.168.42.11)\nFrom 192.168.42.11 icmp_seq=1 Destination Host Unreachable\nFrom 192.168.42.11: icmp_seq=5 Redirect Host(New nexthop: 192.168.42.11)\nFrom 192.168.42.11: icmp_seq=6 Redirect Host(New nexthop: 192.168.42.11)\nFrom 192.168.42.11 icmp_seq=4 Destination Host Unreachable\n\nI didn't know if I could be missing anything network config related, that makes my environment different than infra's.\n\n[1] http://logs.openstack.org/70/173170/6/check/check-tempest-dsvm-multinode-full/0eb5f9a/logs/subnode-2/", 
            "date_created": "2015-06-11 20:20:05.752582+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "This seemed to be 100% reproduceable on our multinode test envs.\n\nThe basic process is make nova net setup with two compute nodes and enable nova net multihost so that each compute node is its own router. Now spin up two VMs such that VMa is on hypvervisorA and VMb is on hypervisorB. Now attach fipA to VMa. Now delete VMa. At this point the NAT rules appear to still be in place on hypversorA for fipA (I believe this to be the underlying cause of the bug and if we can at least confirm and fix that the shelve/unshelve stuff may just be noise). Finally shelve unshelve VMb such that it moves from hypervisorB to hypervisorA, at this point because the fipA NAT rules are still in place on hypervisorA we end up with fipA \"attached\" to VMb.\n\nThis was not tested cross tenant when I did it, but from my understanding of how nova net and iptables works there is no tenant awareness of the NAT rules, its just based on l3 rewrites between IP addresses in two different address ranges.\n\nAs far as reproducing this setup locally the entire configuration for the multinode envs should be published in the logs for those job runs. This includes the nova configuration and general network setup in localrc.", 
            "date_created": "2015-06-23 01:12:39.477119+00:00", 
            "author": "https://api.launchpad.net/1.0/~cboylan"
        }, 
        {
            "content": "@Clark, thanks for those details. Since the bug has to do with the iptables rules, what I was doing with the firewall disabled wouldn't have triggered the behavior. I'll need to get my environment working with the firewall on in order to have a chance at reproducing this bug. So I'll try it again.", 
            "date_created": "2015-07-02 00:11:28.129619+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "Also, as you mentioned at a basic level I can check the state of the NAT rules after deleting an instance with floating ip attached.", 
            "date_created": "2015-07-02 00:17:19.109892+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "I just did a test to check the state of the NAT rules after deleting an instance. I have attached the output of 'iptables -L -nv -t nat' before the floating ip associate, after the floating ip associate, and after the instance deletion.\n\n1. Boot an instance\n2. Attach floating ip\n3. Delete instance\n\nI observed there are no NAT rules for the floating ip after deleting the instance.", 
            "date_created": "2015-07-09 21:39:08.249312+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "I finally have the multinode node test envs working properly again in the upstream test infra so attempted to try reproducing again and I cannot. I can confirm that the NAT rules are removed when deleting and shelving an instance. Either this bug has been fixed or I was misreading things and there was never an issue (though I am fairly certain that the observed problem was actually present).\n\nReproducing is relatively complicated so I won't try bisecting to find out. We could use the openstack/openstack repo to checkout the state of the world when I first filed this bug and run devstack against that, but considering this appears to be working now wouldn't worry about it.\n\nI will let someone else close this bug as others may want to track down if this was an issue a few months back.", 
            "date_created": "2015-07-24 18:17:27.839136+00:00", 
            "author": "https://api.launchpad.net/1.0/~cboylan"
        }, 
        {
            "content": "Given that we are having issues reproducing this I propose that we open the bug next week unless anybody disagrees?\n", 
            "date_created": "2015-07-27 16:59:57.228549+00:00", 
            "author": "https://api.launchpad.net/1.0/~gmurphy"
        }, 
        {
            "content": "I'm good with making this public.", 
            "date_created": "2015-07-28 01:04:46.369994+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "I've asked Clark to see if he can reproduce with stable/kilo since it's entirely possible the issue he witnessed got solved quietly in master after the release.", 
            "date_created": "2015-08-03 14:45:12.890019+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Have poked at a kilo multinode environment and I am not able to reproduce there either. Sorry for the noise.\n\nI do note that shelve unshelve doesn't as reliably move the VM from one compute node to the other on kilo. Seems like on master if there is only one VM eating resources it will consistently switch hypervisors on each unshelve. Probably a small difference in the schedulers and not really a bug just noticed it.", 
            "date_created": "2015-08-18 22:22:00.313712+00:00", 
            "author": "https://api.launchpad.net/1.0/~cboylan"
        }, 
        {
            "content": "Thanks for following up, Clark!", 
            "date_created": "2015-08-18 22:52:40.192597+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }
    ]
}
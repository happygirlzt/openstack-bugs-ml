{
    "status": "Invalid", 
    "last_updated": "2016-03-07 09:17:24.006578+00:00", 
    "description": "last night our ops team live migrated (nova live-migration --block-migrate $vm) a group of vm to do hw  maintenance. \n\nthe vm ended on a different AZ making the vm unusable (we have different upstream network connectivity on each AZ)\nit never happened before, i tested \n\n\nof course, i have setup AZ filter\n\n\nscheduler_available_filters=nova.scheduler.filters.all_filters\nscheduler_default_filters=RetryFilter,AggregateInstanceExtraSpecsFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ImagePropertiesFilter\n\ni'm using icehouse 2014.1.2-0ubuntu1.1~cloud0\n\ni will clean and upload logs right away", 
    "tags": [
        "live-migration", 
        "scheduler"
    ], 
    "importance": "Medium", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1442024", 
    "owner": "https://api.launchpad.net/1.0/~roman-dobosz", 
    "id": 1442024, 
    "index": 4212, 
    "created": "2015-04-09 09:04:25.931552+00:00", 
    "title": "AvailabilityZoneFilter does not  filter when doing live migration", 
    "comments": [
        {
            "content": "last night our ops team live migrated (nova live-migration --block-migrate $vm) a group of vm to do hw  maintenance. \n\nthe vm ended on a different AZ making the vm unusable (we have different upstream network connectivity on each AZ)\nit never happened before, i tested \n\n\nof course, i have setup AZ filter\n\n\nscheduler_available_filters=nova.scheduler.filters.all_filters\nscheduler_default_filters=RetryFilter,AggregateInstanceExtraSpecsFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ImagePropertiesFilter\n\ni'm using icehouse 2014.1.2-0ubuntu1.1~cloud0\n\ni will clean and upload logs right away", 
            "date_created": "2015-04-09 09:04:25.931552+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "2015-04-09 05:53:28.628 32204 AUDIT nova.scheduler.host_manager [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Host filter ignori\nng hosts: ip-10-21-176-44.ds.example.com  ----> host disabled\n\n2015-04-09 05:53:28.629 32204 DEBUG nova.filters [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Starting with 137 host(s) get_fil\ntered_objects /usr/lib/python2.7/dist-packages/nova/filters.py:70\n2015-04-09 05:53:28.629 32204 DEBUG nova.scheduler.filters.retry_filter [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Host [u'ip\n-10-21-176-210.ds.example.com', u'ip-10-21-176-210.ds.example.com'] passes.  Previously tried hosts: [] host_passes /usr/lib/python2.7/dist-packages/nova/scheduler/filters/retry_filter.py:4\n5\nother 136 hosts\n\n2015-04-09 05:53:28.665 32204 DEBUG nova.filters [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Filter RetryFilter returned 137 host(s) get_filtered_objects /usr/lib/python2.\n7/dist-packages/nova/filters.py:88\n2015-04-09 05:53:29.226 32204 DEBUG nova.filters [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Filter AggregateInstanceExtraSpecsFilter returned 137 host(s)\nget_filtered_objects /usr/lib/python2.7/dist-packages/nova/filters.py:88\n2015-04-09 05:53:29.226 32204 DEBUG nova.filters [req-1a035767-e1c7-4885-9e14-0924afe9a42a 1632e93a212e4221a8ffa442b4da39af 4869754a3f75444dbdac5e5b7bb48bb0] Filter AvailabilityZoneFilter returned 137 host(s) get_filtered_objects /usr/\nlib/python2.7/dist-packages/nova/filters.py:88\n\n\nip-10-21-176-210.ds.example.com is the hypervisor where all the 5 vm ended, which was on a different AZ than the origin AZ.\n\ni have the complete logs for both nova-scheduler and nova-api instances and compute nodes, but they are huge, let me know what else is needed and i will search.\n\n\ni will attach a file which is the result of the req-id for the live migration against nova-scheduler log file, is not that big \n\nthe req to watch is req-1a035767-e1c7-4885-9e14-0924afe9a42a and the vm being moved is 912144fa-a86a-489e-81bb-33b076897234 \n\n\nmy scheduler config is \n\nscheduler_available_filters=nova.scheduler.filters.all_filters\nscheduler_default_filters=RetryFilter,AggregateInstanceExtraSpecsFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ImagePropertiesFilter\n\n", 
            "date_created": "2015-04-09 10:55:17.276947+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "i want to add, when we boot vm they always go to the right AZ, it never fails. only live migration failed and only it failed today.  i will ask ops how many live migrations have been done since we have AZ to have an idea on how often did it fail.", 
            "date_created": "2015-04-09 11:16:49.306904+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "about the version, we use 2014.1.2-0ubuntu1.1~cloud0 with local patches. but none of our patches  touches the scheduler", 
            "date_created": "2015-04-10 01:54:32.363825+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "I am not sure you are talking about this \n\nhttps://blueprints.launchpad.net/nova/+spec/persist-scheduler-hints\n\nlooks to me it's not landed not ", 
            "date_created": "2015-04-10 15:42:31.224562+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "I mean the bp is talked but the code and implementation are not done yet ...", 
            "date_created": "2015-04-10 15:42:57.813876+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "ohh, i though AZ was different, i knew about hints but i expected the AZ to be something *diferent*.\n\nin test environment live migration worked as expected. but in prod did not. that could be explained like this,\nin test environment (where both AZ have the same load/capacity) the scheduler did the right thing while in prod where one AZ is almost unused, the scheduler pick nodes on the empty AZ, not the right thing :(\n\n\n:(\n\nplease set this bug as whishlist ", 
            "date_created": "2015-04-10 16:31:49.815195+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "I don't know why 'test' and 'prod' behavior different \n\nI don't have env to test but from code \n/nova/nova/conductor/tasks/live_migrate.py:_find_destination\n\n156         request_spec = scheduler_utils.build_request_spec(self.context, image,\n157                                                           [self.instance])\n158\n159         host = None\n160         while host is None:\n161             self._check_not_over_max_retries(attempted_hosts)\n162             filter_properties = {'ignore_hosts': attempted_hosts}\n\n\nwe will build request spec and filter prop based on current instance and image \nI think we don't persistent az in the db which means it won't be able to be used during live migration ....\n", 
            "date_created": "2015-04-10 20:34:29.900052+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "I have performed the test, which I hoped will shed some light on this \n(potential) behavior, however turns out it's not.\n\nThe idea was to prepare two AZ which will separate the two groups of \ncomputes (in my case it was simply 3-node devstack), so that first AZ would \nhave one compute and the second AZ would have the other one. There is also \none Host Aggregate which contain all the computes. With this approach it \nmight happen, that Host Aggregate will take a precedence over the AZ.\n\nThe actors:\n\n1. ctrl (controller node)\n2. Alter the nova.conf:\n   scheduler_available_filters=nova.scheduler.filters.all_filters\n   scheduler_default_filters=RetryFilter,AggregateInstanceExtraSpecsFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ImagePropertiesFilter\n3. cpu1 and cpu2 (compute nodes)\n4. availability zone az1 which include cpu1 and have metadata set to\n   some.hw=true\n5. availability zone az2 which include cpu2\n6. host aggregate aggr3 which include cpu and cpu2\n7. flavor aztest with the extra spec set to some.hw=true\n\nThe action:\n\nCreate the vms with aztest - all of them should be spawned on cpu1. Note,\ncirrosXXX has to be avialable; i've used image for i386 to be able to \nsuccessfully perform live migration on my devstack setup.\n\n$ nova boot --flavor aztest --image cirrosXXX --min-count 4 vm\n$ nova list --fields host,name,status                                \n+--------------------------------------+------+------+--------+\n| ID                                   | Host | Name | Status |\n+--------------------------------------+------+------+--------+\n| 1569be1a-1289-4d52-b3d1-c3008f7c865f | cpu1 | vm-4 | ACTIVE |\n| 217cb74e-74c6-4e46-abbc-3582d7e5fb4d | cpu1 | vm-3 | ACTIVE |\n| 7dc98646-db5a-4433-b000-fd0ae671f3c7 | cpu1 | vm-2 | ACTIVE |\n| a6ddd4d8-d05f-45c3-9e6a-4c9fa33da2ea | cpu1 | vm-1 | ACTIVE |\n+--------------------------------------+------+------+--------+\n\nNow, try live migrate the vm-1:\n\n$ nova live-migration --block-migrate vm-1\nERROR (BadRequest): No valid host was found. There are not enough hosts available. (HTTP 400) (Request-ID: req-2b1cd8d2-2316-40f2-8600-98c748ae565d)\n\nAfter adding another compute to the cluster, and adding it to the az1, live \nmigration works as expected:\n\n$ nova aggregate-add-host aggr1 cpu3\n$ nova live-migration --block-migrate vm-1\n\nSo I've failed to reproduce the reported behaviour, which might be a result \nof not enough data provided, and might be an configuration issue on the \nproduction.\n", 
            "date_created": "2016-02-16 14:08:12.202741+00:00", 
            "author": "https://api.launchpad.net/1.0/~roman-dobosz"
        }, 
        {
            "content": "unfortunately I cannot show you logs as I left the place were I had this problem. I'm no longer running production clouds so I cannot reproduce it.\n\nbut I can add a few data points\n\n- the environment where the problem occur had 300 hypervisors at the time, about half of them where in a AZ and the rest in another AZ, each AZ also had 3 - 4 aggregates \n\n- it did't have any aggregate which span over all the hypervisors\n\n- i wasn't able to reproduce the behavior on smaller test clouds,  neither migrating a single vm on the production cloud.\n\nhope this information is useful in case you get a bigger test bed. thanks\n\n\n", 
            "date_created": "2016-03-03 08:03:37.578983+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "Hi Gustavo,\n\nThanks for the feedback. Unfortunately, I don't have an access for such big \ncloud (couple of computes it all I can get). My understanding of what could go \nsouth was, that you have setups similarly to the one I have described, \naggregate filter might pull the hosts out of AZ (but it wasn't happen anyway).\n\nWhat bugs me however, is that you wrote on comment #2 that it failed on \nproduction only once (is this correct?), while never on test environment. \nAlso, I can't see any reason why the issue reveal itself on large scale only.\n", 
            "date_created": "2016-03-03 09:35:27.529599+00:00", 
            "author": "https://api.launchpad.net/1.0/~roman-dobosz"
        }, 
        {
            "content": "Roman\n\nYes, it only happened in prod, it happened more than once but not many times as it was very bad for us so we moved to a procedure where the operator selected the destination hv using an script instead of letting nova to select the destination.\n\nI just found out all the logs, nova-scheduler, nova-api and nova-computes. would you like me to search for something there? \ni can sanitize the logs and upload somewhere, but be aware that I have almost 500M of logs.\n\n\nthe issue was _very_  real, but we were never able to reproduce it.  looking now at the logs, i see that many vm were live migrated at the same time\n\n\nthanks\n\n", 
            "date_created": "2016-03-07 09:04:40.485969+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }, 
        {
            "content": "having said that, i think the bug can be closed, both of us weren't able to reproduce it, icehouse is an old release and i don't have access to the environment where the problem manifested.\n\n\n\nthanks", 
            "date_created": "2016-03-07 09:17:23.180719+00:00", 
            "author": "https://api.launchpad.net/1.0/~gfa"
        }
    ]
}
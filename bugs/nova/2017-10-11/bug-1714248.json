{
    "status": "In Progress", 
    "last_updated": "2017-09-29 16:14:38.757736+00:00", 
    "description": "Description\n===========\nIn an environment where there are multiple compute nodes with ironic driver,\nwhen a compute node goes down, another compute node cannot take over ironic nodes.\n\nSteps to reproduce\n==================\n1. Start multiple compute nodes with ironic driver.\n2. Register one node to ironic.\n2. Stop a compute node which manages the ironic node.\n3. Create an instance.\n\nExpected result\n===============\nThe instance is created.\n\nActual result\n=============\nThe instance creation is failed.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running.\nopenstack-nova-scheduler-15.0.6-2.el7.noarch\nopenstack-nova-console-15.0.6-2.el7.noarch\npython2-novaclient-7.1.0-1.el7.noarch\nopenstack-nova-common-15.0.6-2.el7.noarch\nopenstack-nova-serialproxy-15.0.6-2.el7.noarch\nopenstack-nova-placement-api-15.0.6-2.el7.noarch\npython-nova-15.0.6-2.el7.noarch\nopenstack-nova-novncproxy-15.0.6-2.el7.noarch\nopenstack-nova-api-15.0.6-2.el7.noarch\nopenstack-nova-conductor-15.0.6-2.el7.noarch\n\n2. Which hypervisor did you use?\nironic\n\nDetails\n=======\nWhen a nova-compute goes down, another nova-compute will take over ironic nodes managed by the failed nova-compute by re-balancing a hash-ring. Then the active nova-compute tries creating a\nnew resource provider with a new ComputeNode object UUID and the hypervisor name (ironic node UUID)[1][2][3]. This creation fails with a conflict(409) since there is a resource provider with the same name created by the failed nova-compute.\n\nWhen a new instance is requested, the scheduler gets only an old resource provider for the ironic node[4]. Then, the ironic node is not selected:\n\nWARNING nova.scheduler.filters.compute_filter [req-a37d68b5-7ab1-4254-8698-502304607a90 7b55e61a07304f9cab1544260dcd3e41 e21242f450d948d7af2650ac9365ee36 - - -] (compute02, 8904aeeb-a35b-4ba3-848a-73269fdde4d3) ram: 4096MB disk: 849920MB io_ops: 0 instances: 0 has not been heard from in a while\n\n[1] https://github.com/openstack/nova/blob/stable/ocata/nova/compute/resource_tracker.py#L464\n[2] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/client/report.py#L630\n[3] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/client/report.py#L410\n[4] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/filter_scheduler.py#L183", 
    "tags": [
        "ironic", 
        "placement"
    ], 
    "importance": "High", 
    "heat": 22, 
    "link": "https://bugs.launchpad.net/nova/+bug/1714248", 
    "owner": "https://api.launchpad.net/1.0/~johngarbutt", 
    "id": 1714248, 
    "index": 2142, 
    "created": "2017-08-31 13:30:42.881363+00:00", 
    "title": "Compute node HA for ironic doesn't work due to the name duplication of Resource Provider ", 
    "comments": [
        {
            "content": "Description\n===========\nIn an environment where there are multiple compute nodes with ironic driver, \nwhen a compute node goes down, another compute node cannot take over ironic nodes.\n\nSteps to reproduce\n==================\n1. Start multiple compute nodes with ironic driver.\n2. Register one node to ironic.\n2. Stop a compute node which manages the ironic node.\n3. Create an instance.\n\nExpected result\n===============\nThe instance creation is failed.\n\nActual result\n=============\nThe instance is created.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running.\nopenstack-nova-scheduler-15.0.6-2.el7.noarch\nopenstack-nova-console-15.0.6-2.el7.noarch\npython2-novaclient-7.1.0-1.el7.noarch\nopenstack-nova-common-15.0.6-2.el7.noarch\nopenstack-nova-serialproxy-15.0.6-2.el7.noarch\nopenstack-nova-placement-api-15.0.6-2.el7.noarch\npython-nova-15.0.6-2.el7.noarch\nopenstack-nova-novncproxy-15.0.6-2.el7.noarch\nopenstack-nova-api-15.0.6-2.el7.noarch\nopenstack-nova-conductor-15.0.6-2.el7.noarch\n\n2. Which hypervisor did you use?\nironic\n\nDetails\n=======\nWhen a nova-compute goes down, another nova-compute will take over ironic nodes managed by the failed nova-compute by re-balancing a hash-ring. Then the active nova-compute tries creating a\nnew resource provider with a new ComputeNode object UUID and the hypervisor name (ironic node name)[1][2][3]. This creation fails with a conflict(409) since there is a resource provider with the same name created by the failed nova-compute.\n\nWhen a new instance is requested, the scheduler gets only an old resource provider for the ironic node[4]. Then, the ironic node is not selected:\n\nWARNING nova.scheduler.filters.compute_filter [req-a37d68b5-7ab1-4254-8698-502304607a90 7b55e61a07304f9cab1544260dcd3e41 e21242f450d948d7af2650ac9365ee36 - - -] (compute02, 8904aeeb-a35b-4ba3-848a-73269fdde4d3) ram: 4096MB disk: 849920MB io_ops: 0 instances: 0 has not been heard from in a while\n\n[1] https://github.com/openstack/nova/blob/stable/ocata/nova/compute/resource_tracker.py#L464\n[2] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/client/report.py#L630\n[3] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/client/report.py#L410\n[4] https://github.com/openstack/nova/blob/stable/ocata/nova/scheduler/filter_scheduler.py#L183", 
            "date_created": "2017-08-31 13:30:42.881363+00:00", 
            "author": "https://api.launchpad.net/1.0/~shiina-hironori"
        }, 
        {
            "content": "This isn't the first time we've seen something like this. I wonder if we should think about what the impact would be if we removed the uniq requirement on the name field of a resource provider. It seems like it will inevitably cause problems as people/services start doing things with placement that span arbitrary boundaries (like time in this case) that matter to the client side, but are meaningless to placement.", 
            "date_created": "2017-09-01 15:52:06.375655+00:00", 
            "author": "https://api.launchpad.net/1.0/~cdent"
        }, 
        {
            "content": "I believe I am also seeing this issue. First, a little about the environment.\n\nThe control plane is containerised, and deployed using an Ocata release of kolla-ansible. The base OS and container OS are both CentOS 7.3. The RDO nova compute package is openstack-nova-compute-15.0.6-2.el7.noarch. There are 3 OpenStack controllers, each with a nova compute service for ironic. There are 4 ironic baremetal nodes.\n\nI have seen the issue twice now, and as Hironori described, the main user visible symptom is that one of the ironic nodes becomes unschedulable. Digging into the logs, the compute service to which the ironic node has been mapped shows the following messages occurring every minute:\n\n2017-09-13 09:49:42.618 7 INFO nova.scheduler.client.report [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Another thread already created a resource provider with the UUID 22787651-ab4a-4c8b-b72b-5e20bb3fad2c. Grabbing that record from the placement API.\n2017-09-13 09:49:42.631 7 WARNING nova.scheduler.client.report [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Unable to refresh my resource provider record\n2017-09-13 09:49:42.689 7 DEBUG nova.compute.resource_tracker [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Total usable vcpus: 64, total allocated vcpus: 0 _report_final_resource_view /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:688\n2017-09-13 09:49:42.690 7 INFO nova.compute.resource_tracker [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Final resource view: name=5d1535b1-0984-42b3-a574-a62afddd9307 phys_ram=262144MB used_ram=0MB phys_disk=222GB used_disk=0GB total_vcpus=64 used_vcpus=0 pci_stats=[]\n2017-09-13 09:49:42.691 7 DEBUG nova.compute.resource_tracker [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Compute_service record updated for kef1p-phycon0003-ironic:5d1535b1-0984-42b3-a574-a62afddd9307 _update_available_resource /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:626\n\nThe placement logs are fairly lacking in useful information, even with logging set to debug. Picking out the relevant logs:\n\n2017-09-13 09:51:43.604 20 DEBUG nova.api.openstack.placement.requestlog [req-298e44a2-5944-4322-87b2-e1b28d9fbc6a ac342c8d47c8416580ec6f3affcd287f 4970f0b152ca41dc968b4473bb8a48d9 - default default] Starting request: 10.105.1.3 \"POST /resource_providers\" __call__ /usr/lib/python2.7/site-packages/nova/api/openstack/placement/requestlog.py:38\n2017-09-13 09:51:43.612 20 INFO nova.api.openstack.placement.requestlog [req-298e44a2-5944-4322-87b2-e1b28d9fbc6a ac342c8d47c8416580ec6f3affcd287f 4970f0b152ca41dc968b4473bb8a48d9 - default default] 10.105.1.3 \"POST /resource_providers\" status: 409 len: 675 microversion: 1.0\n\nWe can see here that the scheduler client first tries to GET the resource_provider for compute node 22787651-ab4a-4c8b-b72b-5e20bb3fad2c, but fails with a 404 not found. Following this, it tries to create a resource provider for the compute node, but fails with a 409, presumably because a resource provider exists with the same name (the ironic node UUID) but a different UUID.\n\nLooking at the DB for further info, here's the troublesome RP:\n\n+---------------------+---------------------+-------+--------------------------------------+--------------------------------------+------------+----------+\n| created_at          | updated_at          | id    | uuid                                 | name                                 | generation | can_host |\n+---------------------+---------------------+-------+--------------------------------------+--------------------------------------+------------+----------+\n| 2017-09-01 18:10:43 | 2017-09-12 15:44:53 |    88 | 2f786d5d-169f-49b7-880f-d63cea9e4906 | 5d1535b1-0984-42b3-a574-a62afddd9307 |         19 |        0 |\n+---------------------+---------------------+-------+--------------------------------------+--------------------------------------+------------+----------+\n\nThe compute node with UUID 2f786d5d-169f-49b7-880f-d63cea9e4906 was actually deleted about the same time as the 'Another thread...' log started appearing:\n\n+---------------------+---------------------+---------------------+----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n| created_at          | updated_at          | deleted_at          | id | service_id | vcpus | memory_mb | local_gb | vcpus_used | memory_mb_used | local_gb_used | hypervisor_type | hypervisor_version | cpu_info | disk_available_least | free_ram_mb | free_disk_gb | current_workload | running_vms | hypervisor_hostname                  | deleted | host_ip    | supported_instances              | pci_stats                                                                                                                                                                         | metrics | extra_resources | stats                                                                                                                                                                                                                                                                                                                                                | numa_topology | host                    | ram_allocation_ratio | cpu_allocation_ratio | uuid                                 | disk_allocation_ratio |\n+---------------------+---------------------+---------------------+----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n| 2017-09-01 18:10:43 | 2017-09-12 15:51:17 | 2017-09-12 15:51:23 | 70 |       NULL |     0 |         0 |        0 |        128 |         524288 |           444 | ironic          |                  1 |          |                 -222 |     -524288 |         -444 |                0 |           2 | 5d1535b1-0984-42b3-a574-a62afddd9307 |      70 | 10.105.1.6 | [[\"x86_64\", \"baremetal\", \"hvm\"]] | {\"nova_object.version\": \"1.1\", \"nova_object.changes\": [\"objects\"], \"nova_object.name\": \"PciDevicePoolList\", \"nova_object.data\": {\"objects\": []}, \"nova_object.namespace\": \"nova\"} | []      | NULL            | {\"num_task_None\": \"2\", \"cpu_arch\": \"x86_64\", \"io_workload\": \"1\", \"num_instances\": \"2\", \"num_proj_12539bef11ff48d2a04e9cf8c13ac7c3\": \"2\", \"cpu_txt\": \"true\", \"num_vm_active\": \"1\", \"num_vm_building\": \"1\", \"cpu_hugepages\": \"true\", \"cpu_vt\": \"true\", \"boot_option\": \"local\", \"num_os_type_None\": \"2\", \"cpu_hugepages_1g\": \"true\", \"cpu_aes\": \"true\"} | NULL          | kef1p-phycon0003-ironic |                    1 |                    0 | 2f786d5d-169f-49b7-880f-d63cea9e4906 |                     0 |\n+---------------------+---------------------+---------------------+----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n\nThe active compute node entry is:\n\n+---------------------+---------------------+------------+-----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n| created_at          | updated_at          | deleted_at | id  | service_id | vcpus | memory_mb | local_gb | vcpus_used | memory_mb_used | local_gb_used | hypervisor_type | hypervisor_version | cpu_info | disk_available_least | free_ram_mb | free_disk_gb | current_workload | running_vms | hypervisor_hostname                  | deleted | host_ip    | supported_instances              | pci_stats                                                                                                                                                                         | metrics | extra_resources | stats                                                                                                                                                       | numa_topology | host                    | ram_allocation_ratio | cpu_allocation_ratio | uuid                                 | disk_allocation_ratio |\n+---------------------+---------------------+------------+-----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n| 2017-09-12 15:53:26 | 2017-09-13 10:26:26 | NULL       | 101 |       NULL |    64 |    262144 |      222 |          0 |              0 |             0 | ironic          |                  1 |          |                  222 |      262144 |          222 |                0 |           0 | 5d1535b1-0984-42b3-a574-a62afddd9307 |       0 | 10.105.1.6 | [[\"x86_64\", \"baremetal\", \"hvm\"]] | {\"nova_object.version\": \"1.1\", \"nova_object.changes\": [\"objects\"], \"nova_object.name\": \"PciDevicePoolList\", \"nova_object.data\": {\"objects\": []}, \"nova_object.namespace\": \"nova\"} | []      | NULL            | {\"cpu_vt\": \"true\", \"cpu_arch\": \"x86_64\", \"cpu_hugepages\": \"true\", \"boot_option\": \"local\", \"cpu_txt\": \"true\", \"cpu_aes\": \"true\", \"cpu_hugepages_1g\": \"true\"} | NULL          | kef1p-phycon0003-ironic |                    1 |                    0 | 22787651-ab4a-4c8b-b72b-5e20bb3fad2c |                     0 |\n+---------------------+---------------------+------------+-----+------------+-------+-----------+----------+------------+----------------+---------------+-----------------+--------------------+----------+----------------------+-------------+--------------+------------------+-------------+--------------------------------------+---------+------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-------------------------+----------------------+----------------------+--------------------------------------+-----------------------+\n\nThey are both mapped to the same nova compute service, kef1p-phycon0003-ironic. Picking out some events from the logs in the run up to the compute node switchover:\n\n* Creation of an instance on the ironic node under question was aborted.\n* Cleaning up the instance failed due to maxing out the ironic API retries.\n\n2017-09-12 15:51:17.846 7 WARNING nova.compute.manager [req-3dbcf213-dddd-4462-a1aa-6cfa697449fd 7dc9cbb312404469b3d2ea983387181f 12539bef11ff48d2a04e9cf8c13ac7c3 - - -] Could not clean up failed build, not rescheduling. Error: Node 5d1535b1-0984-42b3-a574-a62afddd9307 is locked by host kef1p-phycon0002, please retry after the current operation is completed. (HTTP 409)\n\n* Nova compute deletes the compute node, claiming it is orphaned.\n\n2017-09-12 15:51:23.894 7 INFO nova.compute.manager [req-569e86cc-a2c6-4043-8efa-ea31e14d86dc - - - - -] Deleting orphan compute node 70 hypervisor host is 5d1535b1-0984-42b3-a574-a62afddd9307, nodes are set([u'60c1ee36-b49d-4350-9e4e-e1995a289b2b', u'eb9e200f-0702-4668-849c-6e46a2864e9c'])", 
            "date_created": "2017-09-13 11:05:59.532296+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Staring at the nova code a little longer, I think I've pieced together what happened.\n\n* An instance was aborted during creation.\n* Destroying the instance failed because the node was locked (possibly due to a long running neutron port update), and the retry mechanism maxed out.\n* Shortly afterwards, during the compute service's update_available_resource periodic task, the compute node was determined to be an orphan, and deleted.\n* Deleting the resource provider for the compute node failed because allocations still existed from the instance that wasn't cleaned up.\n\nThis raises a question, why was the compute node seen to be orphaned? This happened because the ironic virt driver did not include the node in the list returned by get_available_nodes(). I suspect this is because the ironic node still had an instance_uuid set, but that instance was not mapped to the compute host.\n\nAnother point worth mentioning is that I ended up deleting the stale resource provider in the DB, and the compute service created another, allowing things to return to normal.", 
            "date_created": "2017-09-13 11:34:54.184917+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Actually, looking at the ironic logs, the node was being provisioned at the time of the abort and the conductor had locked the node as it was handling the IPA callback.", 
            "date_created": "2017-09-13 13:20:16.784896+00:00", 
            "author": "https://api.launchpad.net/1.0/~mgoddard"
        }, 
        {
            "content": "Pursuant to comment #3, why doesn't the new compute host react to the 409 by deleting the old RP?", 
            "date_created": "2017-09-28 16:21:31.453350+00:00", 
            "author": "https://api.launchpad.net/1.0/~efried"
        }, 
        {
            "content": "Oh, I think I understand: at the moment, this code is out of the control of ironic itself; it's all handled by the resource tracker.\n\nSo... why does the new compute host try to create the node RP with a *different* UUID?  Doesn't the node have an immutable UUID that ought to be used no matter which host is doing the registration, or when, or why?", 
            "date_created": "2017-09-28 17:32:17.106208+00:00", 
            "author": "https://api.launchpad.net/1.0/~efried"
        }, 
        {
            "content": "The ironic driver just passes the node uuids up to Nova:\nhttps://github.com/openstack/nova/blob/master/nova/virt/ironic/driver.py#L718\n\nIts the resource tracker that decides to create resource providers from that. In production pike cloud, I see the RP name matching the ironic node uuid. Not sure where the other uuid came from :S", 
            "date_created": "2017-09-29 12:27:04.099816+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Oh wait, its totally the compute node uuid. RP uuid=computeNode and name=ironic node uuid\n\nNova creates a new compute node uuid for the same ironic node uuid, because its on a different nova-compute node. That works fine see:\nhttps://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/models.py#L117\n\nThe ComputeNode unique constraint simply checks ('host', 'hypervisor_hostname', 'deleted') is unique, so we totally allow the ironic node uuid to be registered twice in the ComputeNode's table.\n\nIf you restarted the dead n-cpu node (yet, I know, that is stupid), it would delete the orphan compute nodes, but in this case it is dead, so that left in the DB.\n\nBut on placement side, we have the (new-uuid, existing-node-uuid) pair, and it fails the unique constraint on the name:\nhttps://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/api_models.py#L295\n\n... So someone needs to delete the old resource provider, I guess the logic that creates the new one could check to see if the name is used by some other node, and delete it as required, and re-assign all the allocations to the new node, etc... Yuck.", 
            "date_created": "2017-09-29 12:43:58.138305+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/508508", 
            "date_created": "2017-09-29 13:02:43.484549+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "So idea for brute force fix...\n\nOn above change, we detect the conflict. We see if the name is duplicated, if it is we delete the old resource provider, to do that we probably have to delete all its allocations first, possible also its inventory, etc.\n\nOnce that is all done, we create a new resource provider, then add the appropriate allocations.\n\n... But now we have a race where another instance will come in an try to get allocations on the new thing, just after we create it, but before we add back in the allocations. Boom, dang, the sky is falling down.\n\nNow in the virt driver we know the node has the instance_uuid is set, so probably not a total disaster, but it seems a nasty race.\n\n... not sure where to go with this.", 
            "date_created": "2017-09-29 13:17:33.570696+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "... another possible fix is to change the ComputeNode.uuid to match the ironic_node.uuid, but we hit the same race above on the first restart after we make this change, as we delete all the auto-generated ComputeNode.uuids and update them to ironic_node.uuids. Although at least we only hit the nasty re-calculate race once... Yuck again.", 
            "date_created": "2017-09-29 13:48:18.409068+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Better idea: after rebalance, we must create compute nodes, lets check other compute nodes don't already exist, and either create or re-purpose depending on what happens.", 
            "date_created": "2017-09-29 14:00:20.391150+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/508555", 
            "date_created": "2017-09-29 16:10:16.434932+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by John Garbutt (<email address hidden>) on branch: master\nReview: https://review.openstack.org/508508", 
            "date_created": "2017-09-29 16:14:38.064411+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
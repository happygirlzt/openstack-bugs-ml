{
    "status": "Confirmed", 
    "last_updated": "2017-06-27 19:23:28.216116+00:00", 
    "description": "I am seeing this weird behavior in our production environment. Right now, we are seeing an issue where launching of an instance is failing since the compute node and neutron is not cleaning up the qbr or qvo it had created even after we try to terminate the failed instance. Here are the logs from nova-conductor:-\n2016-03-08 01:35:49.478 14041 ERROR nova.scheduler.utils [req-6ec7ee4b-9663-4f1b-910a-a87d99ac941c c665814ae07a4f71b666d04fcb99c2e9 a0288bedbb884e07bc0c602e7a343de8 - - -] [instance: fa9c27b4-06dd-4c04-9647-44e1fb8c1a81] Error from last host: compute-42 (node compute-42): [u'Traceback (most recent call last):\\n', u'  File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2254, in _do_build_and_run_instance\\n    filter_properties)\\n', u'  File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2400, in _build_and_run_instance\\n    instance_uuid=instance.uuid, reason=six.text_type(e))\\n', u\"RescheduledException: Build of instance fa9c27b4-06dd-4c04-9647-44e1fb8c1a81 was re-scheduled: Error during following call to agent: ['ovs-vsctl', '--timeout=120', '--', '--if-exists', 'del-port', u'qvo3e44fa11-05', '--', 'add-port', 'br-int', u'qvo3e44fa11-05', '--', 'set', 'Interface', u'qvo3e44fa11-05', u'external-ids:iface-id=3e44fa11-05b5-44dc-8c0c-6b937fe7abe0', 'external-ids:iface-status=active', u'external-ids:attached-mac=fa:16:3e:60:aa:5e', 'external-ids:vm-uuid=fa9c27b4-06dd-4c04-9647-44e1fb8c1a81']\\n\"]\n\nThis qvo still exists on the compute node:-\n[root@compute-42 rahul]# ifconfig | grep qvo3e44fa11-05\nqvo3e44fa11-05: flags=4419<UP,BROADCAST,RUNNING,PROMISC,MULTICAST>  mtu 9000   <----- this still exists\n[root@compute-42 rahul]# ifconfig | grep qvo | wc -l\n392                   <------------------------ there are about 350+ such entries\n[root@compute-42 rahul]# ifconfig | grep tap | wc -l\n8                        <----------------------- the compute is running only 8 instances, still more than 350+ entries for qvo-XX alone\n[root@compute-42 rahul]#\n\nI am running Kilo release and RHEL 7 Openstack rpms.\n\nExpected:-\nShouldn't the qvo and qvb be deleted if creation of instance has failed?", 
    "tags": [
        "libvirt", 
        "needs-attention", 
        "ovs"
    ], 
    "importance": "Medium", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1556549", 
    "owner": "None", 
    "id": 1556549, 
    "index": 4482, 
    "created": "2016-03-13 06:10:08.856709+00:00", 
    "title": "too many qbr or qvo entries on compute node even though I have 7-8 instances on that compute node", 
    "comments": [
        {
            "content": "I am seeing this wierd behavior in our production environment. Right now, we are seeing an issue where launching of an instance is failing since the compute node and neutron is not cleaning up the qbr or qvo it had created even after we try to terminate the failed instance. Here are the logs from nova-conductor:-\n2016-03-08 01:35:49.478 14041 ERROR nova.scheduler.utils [req-6ec7ee4b-9663-4f1b-910a-a87d99ac941c c665814ae07a4f71b666d04fcb99c2e9 a0288bedbb884e07bc0c602e7a343de8 - - -] [instance: fa9c27b4-06dd-4c04-9647-44e1fb8c1a81] Error from last host: compute-42 (node compute-42): [u'Traceback (most recent call last):\\n', u'  File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2254, in _do_build_and_run_instance\\n    filter_properties)\\n', u'  File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2400, in _build_and_run_instance\\n    instance_uuid=instance.uuid, reason=six.text_type(e))\\n', u\"RescheduledException: Build of instance fa9c27b4-06dd-4c04-9647-44e1fb8c1a81 was re-scheduled: Error during following call to agent: ['ovs-vsctl', '--timeout=120', '--', '--if-exists', 'del-port', u'qvo3e44fa11-05', '--', 'add-port', 'br-int', u'qvo3e44fa11-05', '--', 'set', 'Interface', u'qvo3e44fa11-05', u'external-ids:iface-id=3e44fa11-05b5-44dc-8c0c-6b937fe7abe0', 'external-ids:iface-status=active', u'external-ids:attached-mac=fa:16:3e:60:aa:5e', 'external-ids:vm-uuid=fa9c27b4-06dd-4c04-9647-44e1fb8c1a81']\\n\"]\n\nThis qvo still exists on the compute node:-\n[root@compute-42 rahul]# ifconfig | grep qvo3e44fa11-05\nqvo3e44fa11-05: flags=4419<UP,BROADCAST,RUNNING,PROMISC,MULTICAST>  mtu 9000   <----- this still exists\n[root@compute-42 rahul]# ifconfig | grep qvo | wc -l\n392                   <------------------------ there are about 350+ such entries\n[root@compute-42 rahul]# ifconfig | grep tap | wc -l\n8                        <----------------------- the compute is running only 8 instances, still more than 350+ entries for qvo-XX alone\n[root@compute-42 rahul]# \n\nI am running Kilo release and RHEL 7 Openstack rpms.\n\nExpected:-\nShouldn't the qvo and qvb be deleted if creation of instance has failed?", 
            "date_created": "2016-03-13 06:10:08.856709+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "I am not sure why Neutron is involved here: hybrid bridge, as well as tap devices are managed by Nova solely.", 
            "date_created": "2016-03-13 20:41:35.872704+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "Please at least provide debug logs for neutron-server and l2 agent used.", 
            "date_created": "2016-03-13 20:42:09.327398+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "Is this RHEL OSP (the product)?  If so, you probably need to open a bugzilla against Red Hat to start since I don't know what they have in their product that might not be in upstream stable/kilo.\n\nAlso, are you able to recreate this on stable/liberty or mitaka?", 
            "date_created": "2016-03-16 16:52:39.513743+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The full stacktrace should be getting logged:\n\nhttps://github.com/openstack/nova/blob/stable/kilo/nova/network/linux_net.py#L1363\n\nWhich should include the stdout/stderr from the command to explain why it failed.  That should be in the nova-compute logs, please provide that stacktrace/error message.", 
            "date_created": "2016-03-16 16:56:10.947631+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Hi Matt,\n\nAs I debugged further, the issue occurred because the openvswitch service was failing on that particular node with the error:\novs-vsctl: unix:/var/run/openvswitch/db.sock: database connection failed (Protocol error)\\n\n\nHowever, since new instance creation requests were coming in again and again, it kept on creating qvb and qvo interfaces successfully but failed after that I guess. For example, in logs it states that it failed for this request:-\n\n2016-03-08 01:34:45.083 4444 INFO nova.virt.libvirt.driver [req-c78b194d-6fa0-4cc2-8751-0c9d3fc43bea c665814ae07a4f71b666d04fcb99c2e9 a0288bedbb884e07bc0c602e7a343de8 - - -] [instance: ce125391-b07f-4100-8046-51b982c17553] Creating image\n2016-03-08 01:35:03.595 4444 ERROR nova.network.linux_net [req-c78b194d-6fa0-4cc2-8751-0c9d3fc43bea c665814ae07a4f71b666d04fcb99c2e9 a0288bedbb884e07bc0c602e7a343de8 - - -] Unable to execute ['ovs-vsctl', '--timeout=120', '--', '--if-exists', 'del-port', u'qvo2188d93e-29', '--', 'add-port', 'br-int', u'qvo2188d93e-29', '--', 'set', 'Interface', u'qvo2188d93e-29', u'external-ids:iface-id=2188d93e-2945-4f11-80d8-525e8d81957b', 'external-ids:iface-status=active', u'external-ids:attached-mac=fa:16:3e:2d:51:19', 'external-ids:vm-uuid=ce125391-b07f-4100-8046-51b982c17553']. Exception: Unexpected error while running command.\nCommand: sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 -- --if-exists del-port qvo2188d93e-29 -- add-port br-int qvo2188d93e-29 -- set Interface qvo2188d93e-29 external-ids:iface-id=2188d93e-2945-4f11-80d8-525e8d81957b external-ids:iface-status=active external-ids:attached-mac=fa:16:3e:2d:51:19 external-ids:vm-uuid=ce125391-b07f-4100-8046-51b982c17553\nExit code: 1\nStdout: u''\nStderr: u'ovs-vsctl: unix:/var/run/openvswitch/db.sock: database connection failed (Protocol error)\\n'\n2016-03-08 01:35:03.596 4444 ERROR nova.compute.manager [req-c78b194d-6fa0-4cc2-8751-0c9d3fc43bea c665814ae07a4f71b666d04fcb99c2e9 a0288bedbb884e07bc0c602e7a343de8 - - -] [instance: ce125391-b07f-4100-8046-51b982c17553] Instance failed to spawn\n\n\nHowever, if I check for qvo2188d93e-29, it is still present:-\n[root@compute-42 rahul]# ifconfig qvo2188d93e-29\nqvo2188d93e-29: flags=4419<UP,BROADCAST,RUNNING,PROMISC,MULTICAST>  mtu 9000\n        inet6 fe80::dcf4:caff:fef0:8e5  prefixlen 64  scopeid 0x20<link>\n        ether de:f4:ca:f0:08:e5  txqueuelen 1000  (Ethernet)\n        RX packets 15  bytes 1206 (1.1 KiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 8  bytes 648 (648.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\nDue to this, the compute node ended up having more than 350+ qvo/qvb pairs. I added this bug since this behavior seems to mess up the compute node, though the openvswitch is unable to connect to the database. Also, neutron-agent is seen as up in this case.\n\nPlease find logs for nova-compute attached.", 
            "date_created": "2016-03-16 17:52:51.706484+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "Logs for openvswitch-agent on compute node.", 
            "date_created": "2016-03-16 18:12:35.266443+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "Neutron server logs.", 
            "date_created": "2016-03-17 17:42:41.427867+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "I can reproduce this easily on devstack.\n\nubuntu@devstack:~/devstack$ git branch -a\n* master\n  remotes/origin/HEAD -> origin/master\n  remotes/origin/master\n  remotes/origin/stable/kilo\n  remotes/origin/stable/liberty\nubuntu@devstack:~/devstack$ \n\n\nHere is the script to make openvswitch to fail:-\n\n#!/bin/bash\n\nfor i in `seq 1 350`;\ndo\n    /usr/bin/ovsdb-client monitor Interface name,ofport,external_ids --format=json &\ndone\n\nOnce it spawns more than 330 ovsdb-client processes, openvswitch starts failing. Now, if you try to launch an instance, it errors out after some time with \"no valid host\" error. Even if you terminate the instance, you can see a leftover pair with two ends as qvo and qvb respectively. The more instances you launch, you will end up more number of veth pairs on your compute node.\n\nroot@devstack:/home/ubuntu# ifconfig | grep qvo\nqvo4f07f296-d2 Link encap:Ethernet  HWaddr 1e:bc:17:e4:3c:b8  \nroot@devstack:/home/ubuntu# \n\nReboot of compute node is the only solution to get rid of those pairs, or removing them manually but it would be good if those stale entries are not left after the instance has failed and its removed.\n\nCan you please let me know your views on the same?", 
            "date_created": "2016-03-19 05:45:39.360347+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "The bug reporter provided a reproducer script. Switching back to \"new\".", 
            "date_created": "2016-03-21 09:21:28.003339+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "What version of openvswitch are you running?  We should probably re-open the neutron part of this since it seems odd the ovs agent in neutron would still be up even if we can't connect to the database. It also seems like a bug in ovs if it's creating the interface but the command fails and it doesn't cleanup after itself.  People on the neutron side might know more about that.", 
            "date_created": "2016-03-21 19:48:30.951254+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I'm wondering why the ovs db connection failure is happening, I saw something related here, in this case it looks like the compute node is running out of memory, are you having any issues like that on this node?\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1158701", 
            "date_created": "2016-03-21 19:53:22.973524+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "ubuntu@devstack:~$ ovs-vsctl --version\novs-vsctl (Open vSwitch) 2.4.0\nCompiled Oct 16 2015 09:22:33\nDB Schema 7.12.1\nubuntu@devstack:~$ \n", 
            "date_created": "2016-03-21 21:06:37.758884+00:00", 
            "author": "https://api.launchpad.net/1.0/~rahulsharmaait"
        }, 
        {
            "content": "There is a reproduce script: Confirmed", 
            "date_created": "2016-04-18 18:03:40.860888+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "This seems to be an OVS limitation. See http://openvswitch.org/pipermail/dev/2016-February/066030.html\r\nMore specifically, the patch described on OVS mailing list removed the OVSDB limitation of 330 connections:\r\n@@ -130,7 +130,6 @@ ovsdb_jsonrpc_server_create(void)\r\n {\r\n     struct ovsdb_jsonrpc_server *server = xzalloc(sizeof *server);\r\n     ovsdb_server_init(&server->up);\r\n-    server->max_sessions = 330;   /* Random limit. */\r\n     shash_init(&server->remotes);\r\n     return server;\r\n }", 
            "date_created": "2016-04-18 18:25:55.288998+00:00", 
            "author": "https://api.launchpad.net/1.0/~ichukhnakov"
        }, 
        {
            "content": "Hi, all, any updates for this bug?\n\n\nWe met this issue too, something like this:\n\n[root@compute-108-18 nova]# ip a|grep qvo|wc -l\n38\n[root@compute-108-18 nova]# ip a|grep qvb|wc -l\n38\n[root@compute-108-18 nova]# ip a|grep tab|wc -l\n0", 
            "date_created": "2016-11-14 07:01:07.889945+00:00", 
            "author": "https://api.launchpad.net/1.0/~dragon889"
        }, 
        {
            "content": "There are no currently open reviews on this bug, changing the status back to the previous state and unassigning. If there are active reviews related to this bug, please include links in comments. ", 
            "date_created": "2017-06-27 19:23:22.971116+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
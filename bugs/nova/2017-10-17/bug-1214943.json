{
    "status": "Fix Released", 
    "last_updated": "2016-10-19 20:31:22.528744+00:00", 
    "description": "I encounter an issue  when live migrate an instance specified  the target host,  i think the operation will be successes ,  but it is failed  for below reason:\n\nMigrationPreCheckError: Migration pre-check error: Unable to migrate a34f9b88-1e07-4798-af46-ca3b3dbaceda to hchenos2: Lack of memory(host:336 <= instance:512)\n\n  1 .  My OpenStack cluster information :\n\n1). There are two compute nodes in my cluster,  and i created 4  instance(1vcpu/512Mmemory) on these hosts\n\n-----------\nmysql> select hypervisor_hostname,vcpus,vcpus_used,running_vms,memory_mb,memory_mb_used,free_ram_mb,deleted from compute_nodes where deleted=0;\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hypervisor_hostname              | vcpus | vcpus_used | running_vms | memory_mb | memory_mb_used | free_ram_mb | deleted |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hchenos1.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n| hchenos2.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n2 rows in set (0.00 sec)\n\nmysql> \n------------------------\n[root@hchenos ~]# nova list\n+--------------------------------------+------+--------+----------+\n| ID                                   | Name | Status | Networks |\n+--------------------------------------+------+--------+----------+\n| a34f9b88-1e07-4798-af46-ca3b3dbaceda | vm1  | ACTIVE |          |      >>> on host 'hchenos1'\n| f6aaeff9-2220-4693-8e5a-710f4c52b774 | vm2  | ACTIVE |          |        >>>> on host 'hchenos2'\n| bbee57a2-81cd-4933-a943-1c2272f7f550 | vm4  | ACTIVE |          |      >>>> on host  'hchenos1'\n| 74fe26ec-919c-4fa7-890f-f59abe09ef4f | vm5  | ACTIVE |          |        >>>> on host 'hchenos2'\n+--------------------------------------+------+--------+----------+\n[root@hchenos ~]# \n\n 2\uff09.  I  also enable the  ComputeFilter,RamFilter and CoreFilter in nova.conf,  but don't config over commit ratio for both vcpu and memory, so the default ratio will be used. \n\n2.   In the above conditions, live migrate instance vm1  to hchenos2 failed:\n\n[root@hchenos ~]# nova live-migration vm1 hchenos2\nERROR: Live migration of instance a34f9b88-1e07-4798-af46-ca3b3dbaceda to host hchenos2 failed (HTTP 400) (Request-ID: req-68244b99-e438-4000-8bdb-cc43b275c018)\n\n conductor log:\n...\nckages/nova/conductor/tasks/live_migrate.py\", line 87, in _check_requested_destination\\n    self._check_destination_has_enough_memory()\\n\\n  File \"/usr/lib/python2.6/site-packages/nova/conductor/tasks/live_migrate.py\", line 108, in _check_destination_has_enough_memory\\n    mem_inst=mem_inst))\\n\\nMigrationPreCheckError: Migration pre-check error: Unable to migrate a34f9b88-1e07-4798-af46-ca3b3dbaceda to hchenos2: Lack of memory(host:336 <= instance:512)\\n\\n']\n\nI think the reason for above as below:\n\nthe free_ram_mb  for 'hchenos2 ' is 336M,  the request memory is 512M, so the operation is failed. \n\nfree_ram_mb = memory_mb (1872) - 512(reserved_host_memory_mb) - 2*512(instance consume) = 336\n\n\n3.  But successfully  boot an instance  on 'hchenos2' \n\n[root@hchenos ~]# nova boot --image cirros-0.3.0-x86_64 --flavor 1 --availability-zone nova:hchenos2 xhu\n\n[root@hchenos ~]# nova list\n+--------------------------------------+------+--------+----------+\n| ID                                   | Name | Status | Networks |\n+--------------------------------------+------+--------+----------+\n| a34f9b88-1e07-4798-af46-ca3b3dbaceda | vm1  | ACTIVE |          |\n| f6aaeff9-2220-4693-8e5a-710f4c52b774 | vm2  | ACTIVE |          |\n| bbee57a2-81cd-4933-a943-1c2272f7f550 | vm4  | ACTIVE |          |\n| 74fe26ec-919c-4fa7-890f-f59abe09ef4f | vm5  | ACTIVE |          |\n| 364d1a01-67ed-4966-bbfd-d21b6bc3067c | xhu  | ACTIVE |          |   >>>>  is active\n+--------------------------------------+------+--------+----------+\n[root@hchenos ~]#\n\nmysql> select hypervisor_hostname,vcpus,vcpus_used,running_vms,memory_mb,memory_mb_used,free_ram_mb,deleted from compute_nodes where deleted=0;\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hypervisor_hostname              | vcpus | vcpus_used | running_vms | memory_mb | memory_mb_used | free_ram_mb | deleted |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hchenos1.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n| hchenos2.eng.platformlab.ibm.com |     2 |          3 |           3 |      1872 |           2048 |        -176 |       0 |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n2 rows in set (0.00 sec)\n\nmysql> \n\nSo,  I'm very confused  for above test result, why boot an instance is OK on 'hchenos2', but live migration an instance to this host failed due to \"not enough memory\" ?\n\nAfter  carefully go through NOVA source code (live_migrate.py: execute()) , i think  below will cause this issue:\n\n1).  The function '_check_destination_has_enough_memory' doesn't consider the ram allocation ratio(default value is 1.5) when calculate host  free memory('free_ram_mb'), it is inconsistent with  'RamFilter' for memory check when boot instance.\n\nI think the free memory of host  'hchenos2' should be:\n\nfree_ram_mb = memory_mb (1872)  *  ram_allocation_ratio(1.5)  -  memory_mb_used('1536')  = 1272 \n\n2)  why not check vcpu  for live migration target host, only check memory is enough?\n\nlive_migrate.py: execute\n\n        self._check_instance_is_running()\n        self._check_host_is_up(self.source)\n\n        if not self.destination:\n            self.destination = self._find_destination()    \n        else:\n            self._check_requested_destination()    >>>> \n\n\n    def _check_requested_destination(self):\n        self._check_destination_is_not_source()\n        self._check_host_is_up(self.destination)\n        self._check_destination_has_enough_memory()              >>>>   Only check memory, why not check vcpu   together?\n        self._check_compatible_with_source_hypervisor(self.destination)\n        self._call_livem_checks_on_host(self.destination)\n\n3)  The VM status need to be considering  as well,  for example, if the instance is off, it doesn't consume compute node resource anymore on KVM platform(is different form IBM PowerVM), but in resource_tracker.py:_update_usage_from_instances() , only instance 'deleted' flag\nis taken into account  when calculate resource usage.", 
    "tags": [
        "compute", 
        "live-migration"
    ], 
    "importance": "High", 
    "heat": 68, 
    "link": "https://bugs.launchpad.net/nova/+bug/1214943", 
    "owner": "https://api.launchpad.net/1.0/~sylvain-bauza", 
    "id": 1214943, 
    "index": 1186, 
    "openned": "2013-08-21 14:33:01.792137+00:00", 
    "created": "2013-08-21 14:33:01.792137+00:00", 
    "title": "Live migration should use the same memory over subscription logic as instance boot", 
    "comments": [
        {
            "content": "I encounter an issue  when live migrate an instance specified  the target host,  i think the operation will be successes ,  but it is failed  for below reason:\n\nMigrationPreCheckError: Migration pre-check error: Unable to migrate a34f9b88-1e07-4798-af46-ca3b3dbaceda to hchenos2: Lack of memory(host:336 <= instance:512)\n\n  1 .  My OpenStack cluster information :\n\n1). There are two compute nodes in my cluster,  and i created 4  instance(1vcpu/512Mmemory) on these hosts\n\n-----------\nmysql> select hypervisor_hostname,vcpus,vcpus_used,running_vms,memory_mb,memory_mb_used,free_ram_mb,deleted from compute_nodes where deleted=0;\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hypervisor_hostname              | vcpus | vcpus_used | running_vms | memory_mb | memory_mb_used | free_ram_mb | deleted |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hchenos1.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n| hchenos2.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n2 rows in set (0.00 sec)\n\nmysql> \n------------------------\n[root@hchenos ~]# nova list\n+--------------------------------------+------+--------+----------+\n| ID                                   | Name | Status | Networks |\n+--------------------------------------+------+--------+----------+\n| a34f9b88-1e07-4798-af46-ca3b3dbaceda | vm1  | ACTIVE |          |      >>> on host 'hchenos1'\n| f6aaeff9-2220-4693-8e5a-710f4c52b774 | vm2  | ACTIVE |          |        >>>> on host 'hchenos2'\n| bbee57a2-81cd-4933-a943-1c2272f7f550 | vm4  | ACTIVE |          |      >>>> on host  'hchenos1'\n| 74fe26ec-919c-4fa7-890f-f59abe09ef4f | vm5  | ACTIVE |          |        >>>> on host 'hchenos2'\n+--------------------------------------+------+--------+----------+\n[root@hchenos ~]# \n\n 2\uff09.  I  also enable the  ComputeFilter,RamFilter and CoreFilter in nova.conf,  but don't config over commit ratio for both vcpu and memory, so the default ratio will be used. \n\n2.   In the above conditions, live migrate instance vm1  to hchenos2 failed:\n\n[root@hchenos ~]# nova live-migration vm1 hchenos2\nERROR: Live migration of instance a34f9b88-1e07-4798-af46-ca3b3dbaceda to host hchenos2 failed (HTTP 400) (Request-ID: req-68244b99-e438-4000-8bdb-cc43b275c018)\n\n conductor log:\n...\nckages/nova/conductor/tasks/live_migrate.py\", line 87, in _check_requested_destination\\n    self._check_destination_has_enough_memory()\\n\\n  File \"/usr/lib/python2.6/site-packages/nova/conductor/tasks/live_migrate.py\", line 108, in _check_destination_has_enough_memory\\n    mem_inst=mem_inst))\\n\\nMigrationPreCheckError: Migration pre-check error: Unable to migrate a34f9b88-1e07-4798-af46-ca3b3dbaceda to hchenos2: Lack of memory(host:336 <= instance:512)\\n\\n']\n\nI think the reason for above as below:\n\nthe free_ram_mb  for 'hchenos2 ' is 336M,  the request memory is 512M, so the operation is failed. \n\nfree_ram_mb = memory_mb (1872) - 512(reserved_host_memory_mb) - 2*512(instance consume) = 336\n\n\n3.  But successfully  boot an instance  on 'hchenos2' \n\n[root@hchenos ~]# nova boot --image cirros-0.3.0-x86_64 --flavor 1 --availability-zone nova:hchenos2 xhu\n\n[root@hchenos ~]# nova list\n+--------------------------------------+------+--------+----------+\n| ID                                   | Name | Status | Networks |\n+--------------------------------------+------+--------+----------+\n| a34f9b88-1e07-4798-af46-ca3b3dbaceda | vm1  | ACTIVE |          |\n| f6aaeff9-2220-4693-8e5a-710f4c52b774 | vm2  | ACTIVE |          |\n| bbee57a2-81cd-4933-a943-1c2272f7f550 | vm4  | ACTIVE |          |\n| 74fe26ec-919c-4fa7-890f-f59abe09ef4f | vm5  | ACTIVE |          |\n| 364d1a01-67ed-4966-bbfd-d21b6bc3067c | xhu  | ACTIVE |          |   >>>>  is active\n+--------------------------------------+------+--------+----------+\n[root@hchenos ~]#\n\nmysql> select hypervisor_hostname,vcpus,vcpus_used,running_vms,memory_mb,memory_mb_used,free_ram_mb,deleted from compute_nodes where deleted=0;\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hypervisor_hostname              | vcpus | vcpus_used | running_vms | memory_mb | memory_mb_used | free_ram_mb | deleted |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n| hchenos1.eng.platformlab.ibm.com |     2 |          2 |           2 |      1872 |           1536 |         336 |       0 |\n| hchenos2.eng.platformlab.ibm.com |     2 |          3 |           3 |      1872 |           2048 |        -176 |       0 |\n+----------------------------------+-------+------------+-------------+-----------+----------------+-------------+---------+\n2 rows in set (0.00 sec)\n\nmysql> \n\nSo,  I'm very confused  for above test result, why boot an instance is OK on 'hchenos2', but live migration an instance to this host failed due to \"not enough memory\" ?\n\nAfter  carefully go through NOVA source code (live_migrate.py: execute()) , i think  below will cause this issue:\n\n1).  The function '_check_destination_has_enough_memory' doesn't consider the ram allocation ratio(default value is 1.5) when calculate host  free memory('free_ram_mb'), it is inconsistent with  'RamFilter' for memory check when boot instance.\n\nI think the free memory of host  'hchenos2' should be:\n\nfree_ram_mb = memory_mb (1872)  *  ram_allocation_ratio(1.5)  -  memory_mb_used('1536')  = 1272 \n\n2)  why not check vcpu  for live migration target host, only check memory is enough?\n\nlive_migrate.py: execute\n\n        self._check_instance_is_running()\n        self._check_host_is_up(self.source)\n\n        if not self.destination:\n            self.destination = self._find_destination()    \n        else:\n            self._check_requested_destination()    >>>> \n\n\n    def _check_requested_destination(self):\n        self._check_destination_is_not_source()\n        self._check_host_is_up(self.destination)\n        self._check_destination_has_enough_memory()              >>>>   Only check memory, why not check vcpu   together?\n        self._check_compatible_with_source_hypervisor(self.destination)\n        self._call_livem_checks_on_host(self.destination)\n\n3)  The VM status need to be considering  as well,  for example, if the instance is off, it doesn't consume compute node resource anymore on KVM platform(is different form IBM PowerVM), but in resource_tracker.py:_update_usage_from_instances() , only instance 'deleted' flag\nis taken into account  when calculate resource usage.", 
            "date_created": "2013-08-21 14:33:01.792137+00:00", 
            "author": "https://api.launchpad.net/1.0/~xlhuxa"
        }, 
        {
            "content": "There is no need to check memory for live migration, since it live migration failed, nova compute will rollback the live migration operation.\n\nAlso some hypervisors does support resource overcommit natively such as KVM and VMWare, we can remove the memory checking and let nova compute handle this case.", 
            "date_created": "2013-08-21 23:05:05.461674+00:00", 
            "author": "https://api.launchpad.net/1.0/~jake-liu"
        }, 
        {
            "content": "I am agree with you, remove memory check from nova-conductor, thanks.", 
            "date_created": "2013-08-21 23:41:11.121590+00:00", 
            "author": "https://api.launchpad.net/1.0/~xlhuxa"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/43213", 
            "date_created": "2013-08-22 00:11:25.437690+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I don't think removing the check is appropriate; live migration and scheduler decisions should be in sync with each other, and this makes them be out of sync : a cloud configured for no over-commit could end up overcommitted this way, couldn't it?", 
            "date_created": "2013-08-22 01:21:47.610128+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Thanks all for the input here. \n\nRobert, I think that we should only do the checking in nova scheduler but not in both scheduler and conductor as it might make the code difficult to maintain (we need to check if RamFilter was enabled in nova conductor and then get ram allocation ratio in conductor).\n\nIt is better that we keep the logic same as Grizzly, do not check memory in conductor but leave it to nova compute.", 
            "date_created": "2013-08-22 05:45:44.296303+00:00", 
            "author": "https://api.launchpad.net/1.0/~jay-lau-513"
        }, 
        {
            "content": "Why not migrate the instance to another host?\nI don't think migrating/booting an instance to/on a host with high memory usage is  a good practice.\n\nThe cost of doing live migration first and rolling back live migration later is high.\n\nI think we need to keep the check.\n\n", 
            "date_created": "2013-09-02 03:37:49.762202+00:00", 
            "author": "https://api.launchpad.net/1.0/~wenjianhn"
        }, 
        {
            "content": "why not delete the 'host' parameter once for all? I think user always do not know anything about host information below, why not leave the decision to the system itself, then I think you will success more times.", 
            "date_created": "2013-09-03 07:50:16.533747+00:00", 
            "author": "https://api.launchpad.net/1.0/~kong"
        }, 
        {
            "content": "The same issue existed in Grizzly and was fixed to align with the scheduler behavior here: https://review.openstack.org/#/c/19369/\nIt probably broke when the migration path was moved from the scheduler to the conductor.", 
            "date_created": "2013-09-03 07:52:51.087776+00:00", 
            "author": "https://api.launchpad.net/1.0/~hanlind"
        }, 
        {
            "content": "@Lingxian, yes, we are using openstack nova scheduler to select the host for migration. But the problem here is that the host selected by scheduler does not meet the requirement of memory checking in conductor. So my fix was remove the checking but let leave the decision to hypervisor itself as nova scheduler already selected the best host for such case.\n\n@Hans, just review the code https://review.openstack.org/#/c/19369/ and noticed that live_migrate.py was using same logic as your fix. But I think that your fix might still not resolve this issue.\n\ndef _check_destination_has_enough_memory(self):\n        avail = self._get_compute_info(self.destination)['free_ram_mb'] << This is get from DB, free_ram_mb = memory_mb - reserved_host_memory_mb - n*instance consume, the avail also did not consinder ram allocation ratio if RamFilter enabled.\n        mem_inst = self.instance.memory_mb\n\n        if not mem_inst or avail <= mem_inst:\n            instance_uuid = self.instance.uuid\n            dest = self.destination\n            reason = _(\"Unable to migrate %(instance_uuid)s to %(dest)s: \"\n                       \"Lack of memory(host:%(avail)s <= \"\n                       \"instance:%(mem_inst)s)\")\n            raise exception.MigrationPreCheckError(reason=reason % dict(\n                    instance_uuid=instance_uuid, dest=dest, avail=avail,\n                    mem_inst=mem_inst))\n", 
            "date_created": "2013-09-03 12:04:38.508442+00:00", 
            "author": "https://api.launchpad.net/1.0/~jake-liu"
        }, 
        {
            "content": "This bug means that in order to support evacuation from a given (possible overprovisioned) host I need to keep TWO empty hosts at the ready.  Expensive!", 
            "date_created": "2014-10-14 21:26:13.261315+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrewbogott"
        }, 
        {
            "content": "I can't quite believe that this bug is still open and only triaged as medium certainly seems 'high' to me.\n\nMy users typically demand more memory than they actually use so while I may be allocating at 1.5:1 or even 2:1 actual utilization is usually more like 25% or 50% so if my cluster is evenly loaded and all systems are at 1:1 RAM allocation I still have lots of head room, but cannot do any migrations.\n\nI also want to keep the host option for testing purposes though agree best practice is to let the scheduler schedule.\n", 
            "date_created": "2014-10-15 16:07:14.851746+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "I don't understand, the bug Status is Abandoned, what it mean?\nI have the same problem.\n\nAdrian", 
            "date_created": "2014-12-18 19:50:13.056048+00:00", 
            "author": "https://api.launchpad.net/1.0/~gherasim-a"
        }, 
        {
            "content": "This is  painful bug. We add these lines to nova-compute.conf on each compute node in order to allow migrations:\n\nreserved_host_disk_mb=-2097152\nreserved_host_memory_mb=-32768\n\nSo we are cheating the scheduler telling them we have -2TB of storage and -32GB of RAM. Then we can migrate more VMs to each node. \n\nIt's dangerous if you don't monitor RAM and storage in your compute nodes... \n\nAnd yes, I can't understand how this bug can still be open after more than 2 years...", 
            "date_created": "2015-02-09 11:46:37.508984+00:00", 
            "author": "https://api.launchpad.net/1.0/~diego-parrilla-santamaria"
        }, 
        {
            "content": "To be clear,  the bug report states that it fails when a destination host is failed. That's due to the fact that when specifying a destination host, it totally bypasses the scheduler logic and directly talks to the nova-compute service.\n\nUnfortunately, overcommit ratios are at the moment scheduler config flags which are defined and used by RAMFilter.\n\nConceptually, it would make sense to consider those flags as n-cpu ones so that it would be specified per compute host.\nFollowing that path, it would imply that compute hosts report their allocation ratios to the scheduler as a resource item (so the scheduler can still scheduler based on that logic). \nComputes then could check instance claims based on those ratios and trigger an Exception if either the scheduler was wrong (potentially due to a race condition or whatever else) or if the operator was wrong when explicitely giving a destination host in the request.\n\nThat's heavily tied to an approved spec https://review.openstack.org/#/c/98664/ but I would prefer a approach where allocation ratios are a compute metric, not something that the scheduler can just import_opt.", 
            "date_created": "2015-03-19 11:22:56.687359+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "By thinking of it, until we fix the allocation ratio logic to be moved to the computes, an easy workaround would be to validate the destination host by calling the scheduler even if the destination host is specified.\nProvided that the destination host would be in the result set, we could consider that the host is valid.", 
            "date_created": "2015-03-19 11:49:40.637674+00:00", 
            "author": "https://api.launchpad.net/1.0/~sylvain-bauza"
        }, 
        {
            "content": "I can still observing the bug and took me some time to figure out that the logic of migration when destination host is specified and when is not is different. is it so difficult to take the logic from host_passes() method in ram_filter.py (scheduler) and place it to _check_destination_is_not_source() method from live_migrate.py in nova compute?", 
            "date_created": "2015-07-28 14:46:16.965376+00:00", 
            "author": "https://api.launchpad.net/1.0/~mkrcmari"
        }, 
        {
            "content": "Marian, blueprint that will move allocation ratios from scheduler to resource tracker has been accepted. Implementation of this blueprint should finally fix memory oversubscription problem. Please refer to https://review.openstack.org/#/c/173252/", 
            "date_created": "2015-08-10 09:53:21.416313+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-koniszewski"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/180151\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=f5e35dcfe8ee586106438bcfa551426babc75bf6\nSubmitter: Jenkins\nBranch:    master\n\ncommit f5e35dcfe8ee586106438bcfa551426babc75bf6\nAuthor: Sylvain Bauza <email address hidden>\nDate:   Thu Sep 17 17:51:06 2015 +0200\n\n    Correct memory validation for live migration\n    \n    Since live migration has been moved to the conductor, there was no\n    possibility for the conductor to verify if the destination had\n    enough RAM just because it didn't know the allocation ratios given\n    by the scheduler.\n    \n    Now that ComputeNodes provide a ram_allocation_field, we can fix\n    that check and provide the same validation than RAMFilter to make\n    sure that the destination is good.\n    \n    Closes-Bug: #1451831\n    Closes-Bug: #1214943\n    \n    Change-Id: Ie6c768fc915553da73160ea51961078bfbacec77\n", 
            "date_created": "2015-11-16 13:51:33.772248+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.0.0.0b1 development milestone.", 
            "date_created": "2015-12-02 16:17:57.413851+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Has there been any discussion about solving this same problem for disk? I have disk_allocation_ratio set to >1 in my nova configuration, but this same type of issue occurs when trying a live migration. There is plenty of space on the actual disk where the instance data is stored, but it wont let me migrate because disk_available_least is far below that:\n\n+----------------------+-------+\n| Property             | Value |\n+----------------------+-------+\n| count                | 1     |\n| current_workload     | 0     |\n| disk_available_least | 12    |\n| free_disk_gb         | 57    |\n| free_ram_mb          | 15612 |\n| local_gb             | 399   |\n| local_gb_used        | 342   |  <- also even this isnt right because according to df i have 193G used.\n| memory_mb            | 64252 |\n| memory_mb_used       | 48640 |\n| running_vms          | 14    |\n| vcpus                | 24    |\n| vcpus_used           | 25    |\n", 
            "date_created": "2016-10-19 20:31:21.196246+00:00", 
            "author": "https://api.launchpad.net/1.0/~ctrlaltdel121"
        }
    ], 
    "closed": "2015-12-03 21:32:55.982829+00:00"
}
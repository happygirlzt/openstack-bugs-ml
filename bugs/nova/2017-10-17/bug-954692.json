{
    "status": "Fix Released", 
    "last_updated": "2012-04-05 11:05:07.464326+00:00", 
    "description": "Version: 2012.1~e4~20120217.12709-0ubuntu1\n\nI attached a volume to an instance via iSCSI, shut down the instance, and then attempted to detach the volume.  The result is the following in nova-compute.log, and the volume remains \"in-use\".  I also tried \"euca-detach-volume --force\" with the same result.\n\nVOLUME  vol-00000009     10             nova    in-use (pjdc_project, zucchini, i-00001a05[ankaa], /dev/vdc)      2012-03-13T20:49:28Z\n\n2012-03-14 03:04:08,486 DEBUG nova.rpc.common [-] received {u'_context_roles': [u'cloudadmin', u'netadmin', u'projectmanager', u'admin'], u'_context_request_id': u'req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3', u'_context_read_deleted': u'no', u'args': {u'instance_uuid': u'f7620968-686d-4a3d-a1b3-2d0881e1656d', u'volume_id': 9}, u'_context_auth_token': None, u'_context_strategy': u'noauth', u'_context_is_admin': True, u'_context_project_id': u'pjdc_project', u'_context_timestamp': u'2012-03-14T03:03:59.303517', u'_context_user_id': u'pjdc', u'method': u'detach_volume', u'_context_remote_address': u'XXX.XXX.XXX.XXX'} from (pid=8590) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:144\n2012-03-14 03:04:08,487 DEBUG nova.rpc.common [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] unpacked context: {'request_id': u'req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3', 'user_id': u'pjdc', 'roles': [u'cloudadmin', u'netadmin', u'projectmanager', u'admin'], 'timestamp': '2012-03-14T03:03:59.303517', 'is_admin': True, 'auth_token': None, 'project_id': u'pjdc_project', 'remote_address': u'XXX.XXX.XXX.XXX', 'read_deleted': u'no', 'strategy': u'noauth'} from (pid=8590) unpack_context /usr/lib/python2.7/dist-packages/nova/rpc/amqp.py:186\n2012-03-14 03:04:08,515 INFO nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] check_instance_lock: decorating: |<function detach_volume at 0x1f591b8>|\n2012-03-14 03:04:08,516 INFO nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] check_instance_lock: arguments: |<nova.compute.manager.ComputeManager object at 0x1c6b1d0>| |<nova.rpc.amqp.RpcContext object at 0x4987a50>| |f7620968-686d-4a3d-a1b3-2d0881e1656d|\n2012-03-14 03:04:08,516 DEBUG nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] instance f7620968-686d-4a3d-a1b3-2d0881e1656d: getting locked state from (pid=8590) get_lock /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1508\n2012-03-14 03:04:08,668 ERROR nova.rpc.common [-] Exception during message handling\n(nova.rpc.common): TRACE: Traceback (most recent call last):\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/rpc/amqp.py\", line 250, in _process_data\n(nova.rpc.common): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 112, in wrapped\n(nova.rpc.common): TRACE:     return f(*args, **kw)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 139, in decorated_function\n(nova.rpc.common): TRACE:     locked = self.get_lock(context, instance_uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 112, in wrapped\n(nova.rpc.common): TRACE:     return f(*args, **kw)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 168, in decorated_function\n(nova.rpc.common): TRACE:     return function(self, context, instance_uuid, *args, **kwargs)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1509, in get_lock\n(nova.rpc.common): TRACE:     instance_ref = self.db.instance_get_by_uuid(context, instance_uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/api.py\", line 586, in instance_get_by_uuid\n(nova.rpc.common): TRACE:     return IMPL.instance_get_by_uuid(context, uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 119, in wrapper\n(nova.rpc.common): TRACE:     return f(*args, **kwargs)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 1452, in instance_get_by_uuid\n(nova.rpc.common): TRACE:     raise exception.InstanceNotFound(instance_id=uuid)\n(nova.rpc.common): TRACE: InstanceNotFound: Instance f7620968-686d-4a3d-a1b3-2d0881e1656d could not be found.\n(nova.rpc.common): TRACE:", 
    "tags": [
        "canonistack", 
        "essec-rc-potential"
    ], 
    "importance": "Medium", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/954692", 
    "owner": "https://api.launchpad.net/1.0/~gandelman-a", 
    "id": 954692, 
    "index": 2807, 
    "openned": "2012-03-15 00:08:45.955885+00:00", 
    "created": "2012-03-14 03:25:41.533536+00:00", 
    "title": "cannot detach volume from terminated instance", 
    "comments": [
        {
            "content": "Version: 2012.1~e4~20120217.12709-0ubuntu1\n\nI attached a volume to an instance via iSCSI, shut down the instance, and then attempted to detach the volume.  The result is the following in nova-compute.log, and the volume remains \"in-use\".  I also tried \"euca-detach-volume --force\" with the same result.\n\nVOLUME  vol-00000009     10             nova    in-use (pjdc_project, zucchini, i-00001a05[ankaa], /dev/vdc)      2012-03-13T20:49:28Z\n\n2012-03-14 03:04:08,486 DEBUG nova.rpc.common [-] received {u'_context_roles': [u'cloudadmin', u'netadmin', u'projectmanager', u'admin'], u'_context_request_id': u'req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3', u'_context_read_deleted': u'no', u'args': {u'instance_uuid': u'f7620968-686d-4a3d-a1b3-2d0881e1656d', u'volume_id': 9}, u'_context_auth_token': None, u'_context_strategy': u'noauth', u'_context_is_admin': True, u'_context_project_id': u'pjdc_project', u'_context_timestamp': u'2012-03-14T03:03:59.303517', u'_context_user_id': u'pjdc', u'method': u'detach_volume', u'_context_remote_address': u'XXX.XXX.XXX.XXX'} from (pid=8590) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:144\n2012-03-14 03:04:08,487 DEBUG nova.rpc.common [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] unpacked context: {'request_id': u'req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3', 'user_id': u'pjdc', 'roles': [u'cloudadmin', u'netadmin', u'projectmanager', u'admin'], 'timestamp': '2012-03-14T03:03:59.303517', 'is_admin': True, 'auth_token': None, 'project_id': u'pjdc_project', 'remote_address': u'XXX.XXX.XXX.XXX', 'read_deleted': u'no', 'strategy': u'noauth'} from (pid=8590) unpack_context /usr/lib/python2.7/dist-packages/nova/rpc/amqp.py:186\n2012-03-14 03:04:08,515 INFO nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] check_instance_lock: decorating: |<function detach_volume at 0x1f591b8>|\n2012-03-14 03:04:08,516 INFO nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] check_instance_lock: arguments: |<nova.compute.manager.ComputeManager object at 0x1c6b1d0>| |<nova.rpc.amqp.RpcContext object at 0x4987a50>| |f7620968-686d-4a3d-a1b3-2d0881e1656d|\n2012-03-14 03:04:08,516 DEBUG nova.compute.manager [req-da8a3819-70b2-4d0b-a9fb-0dfefa85f9f3 pjdc pjdc_project] instance f7620968-686d-4a3d-a1b3-2d0881e1656d: getting locked state from (pid=8590) get_lock /usr/lib/python2.7/dist-packages/nova/compute/manager.py:1508\n2012-03-14 03:04:08,668 ERROR nova.rpc.common [-] Exception during message handling\n(nova.rpc.common): TRACE: Traceback (most recent call last):\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/rpc/amqp.py\", line 250, in _process_data\n(nova.rpc.common): TRACE:     rval = node_func(context=ctxt, **node_args)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 112, in wrapped\n(nova.rpc.common): TRACE:     return f(*args, **kw)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 139, in decorated_function\n(nova.rpc.common): TRACE:     locked = self.get_lock(context, instance_uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 112, in wrapped\n(nova.rpc.common): TRACE:     return f(*args, **kw)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 168, in decorated_function\n(nova.rpc.common): TRACE:     return function(self, context, instance_uuid, *args, **kwargs)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1509, in get_lock\n(nova.rpc.common): TRACE:     instance_ref = self.db.instance_get_by_uuid(context, instance_uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/api.py\", line 586, in instance_get_by_uuid\n(nova.rpc.common): TRACE:     return IMPL.instance_get_by_uuid(context, uuid)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 119, in wrapper\n(nova.rpc.common): TRACE:     return f(*args, **kwargs)\n(nova.rpc.common): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py\", line 1452, in instance_get_by_uuid\n(nova.rpc.common): TRACE:     raise exception.InstanceNotFound(instance_id=uuid)\n(nova.rpc.common): TRACE: InstanceNotFound: Instance f7620968-686d-4a3d-a1b3-2d0881e1656d could not be found.\n(nova.rpc.common): TRACE:", 
            "date_created": "2012-03-14 03:25:41.533536+00:00", 
            "author": "https://api.launchpad.net/1.0/~pjdc"
        }, 
        {
            "content": "Steps to reproduce:\n1. create volume\n2. boot an instance (tested with cirros)\n3. attach volume\n4. in instance: 'sudo poweroff'\n5. after kvm machine has stopped on compute node, terminate instance. results in traceback on compute.log:\n  \n  (nova.rpc.amqp): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 719, in _delete_instance\n  (nova.rpc.amqp): TRACE:     self._shutdown_instance(context, instance, 'Terminating')\n  (nova.rpc.amqp): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 681, in _shutdown_instance\n  (nova.rpc.amqp): TRACE:     raise exception.Invalid(_msg % instance_uuid)\n  (nova.rpc.amqp): TRACE: Invalid: trying to destroy already destroyed instance: b929bf81-65ee-46e0-8c07-5aae49a0213c\n\n6. instance is destroyed, volume is still 'in-use'\n7. detach volume, results in traceback in original bug report\n\n\n\n\n\n\n\n\n", 
            "date_created": "2012-03-15 00:08:27.068266+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "fwiw following recent fix might have changed the issue:\n\ncommit 22b484a6d0b65c2a41fd4c730a5ebddf98c70c84\nAuthor: Vishvananda Ishaya <email address hidden>\nDate: Wed Mar 14 09:26:40 2012 -0700\n\n    Allow errored volumes to be deleted\n\n     * Allow volumes with no host set to be deleted from db\n     * Allow volumes in state error to be deleted\n     * Replicates code from nova-manage command\n     * Fixes bug 953594\n", 
            "date_created": "2012-03-15 11:15:41.811190+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "we discussed on irc.  This is a separate bug", 
            "date_created": "2012-03-15 15:46:30.239122+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "Bogged down with other things ATM, but spent some time looking at this last night.  \n\nnova.compute.manager._shutdown_instance() raises an exception if the instance is already in  POWEROFF state.  It looks like this conditional has existed forever:\n\n        if current_power_state == power_state.SHUTOFF:\n            self.db.instance_destroy(context, instance_id)\n            _msg = _('trying to destroy already destroyed instance: %s')\n            raise exception.Invalid(_msg % instance_uuid)\n\n\nIt currently does nothing to cleanup bdms and inform nova-volume that the volume is free.   We can certainly do that from the compute manager when the condition is met,  volumes are freed up to be used elsewhere.  The problem there is the iSCSI sessions are never cleaned up from the compute host.  Reattaching the volume to another instance on the same compute node works okay since https://review.openstack.org/#change,4611, but having dangling iSCSI sessions hanging around seems dirty.\n\nLooking at the libvirt compute driver, it appears the l_shutdown_instance()'s later call to driver.destroy() handles terminating an already SHUTOFF'd instance just fine, and also properly cleans up its iscsi connections, among other things.  It would appear that, in teh case of libvirt, the condition raised above is obsolete.  But I'm unsure if this is true for other compute drivers and hesitant to propose dropping it without confirmation.", 
            "date_created": "2012-03-15 17:40:04.084370+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "I think removing that block of code makes sense.\n\nHowever, a few lines below, I would add a try/except around the call to self.driver.destroy().   It looks like it could raise if the instance is not created at the driver level, which could be the case for an instance that's in ERROR state.", 
            "date_created": "2012-03-15 20:32:10.248599+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbehrens"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/5437", 
            "date_created": "2012-03-16 01:11:43.493792+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Chris-\n\nI'm a little uncomfortable wrapping self.driver.destroy in a try/except block until there is a common exception language virt drivers can use to raise specific, fatal exceptions.  I'm thinking of a scenario where libvirt/xs cannot shutdown instance, raises, and compute continues on with volume disassociation.  The stuck instance may still be powered on and scribbling all over its disk, meanwhile the volume has been attached to another instance. ", 
            "date_created": "2012-03-16 01:15:17.333032+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "Makes sense.  Thanks.", 
            "date_created": "2012-03-16 02:30:42.425684+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbehrens"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/5437\nCommitted: http://github.com/openstack/nova/commit/6657f70ee3e792b39e45a2a96fb5d4b380f0ae91\nSubmitter: Jenkins\nBranch:    master\n\ncommit 6657f70ee3e792b39e45a2a96fb5d4b380f0ae91\nAuthor: Adam Gandelman <email address hidden>\nDate:   Thu Mar 15 18:08:35 2012 -0700\n\n    Allow proper instance cleanup if state == SHUTOFF\n    \n    Removes an obsolete check for instance's power_state\n    on shutdown_instance().  With it in place, volume detachment\n    and disassociation never takes place.  Compute should instead rely\n    on virt drivers to handle this case and raise accordingly. libvirt's\n    destroy() currently handles powered off instances fine, and properly\n    detaches any existing volume connections.\n    \n    Fixes bug 954692\n    \n    Change-Id: I200d5b2073e5b52a9733d8324d016b14bdc96067\n", 
            "date_created": "2012-03-16 02:55:37.067935+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2012-03-20 08:46:50.782607+00:00"
}
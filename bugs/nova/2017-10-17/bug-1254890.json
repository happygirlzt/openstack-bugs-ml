{
    "status": "Invalid", 
    "last_updated": "2014-10-16 18:09:30.352293+00:00", 
    "description": "Separate out bug from:\nhttps://bugs.launchpad.net/neutron/+bug/1250168/comments/23\n\nLogstash query from elastic-recheck:\nmessage:\"Details: Timed out waiting for thing\" AND message:\"to become ACTIVE\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogVGltZWQgb3V0IHdhaXRpbmcgZm9yIHRoaW5nXCIgQU5EIG1lc3NhZ2U6XCJ0byBiZWNvbWUgQUNUSVZFXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTA4NDc1MzM2MDR9", 
    "tags": [
        "gate-failure", 
        "in-stable-havana", 
        "testing"
    ], 
    "importance": "Undecided", 
    "heat": 114, 
    "link": "https://bugs.launchpad.net/nova/+bug/1254890", 
    "owner": "None", 
    "id": 1254890, 
    "index": 5586, 
    "openned": "2013-11-25 21:50:29.666235+00:00", 
    "created": "2013-11-25 21:50:29.666235+00:00", 
    "title": "'Timed out waiting for thing ... to become ACTIVE' causes tempest-dsvm-* failures", 
    "comments": [
        {
            "content": "Separate out bug from:\nhttps://bugs.launchpad.net/neutron/+bug/1250168/comments/23\n\nLogstash query:\nmessage:\"Details: Timed out waiting for thing\" AND build_name:gate-tempest-devstack-vm-neutron-large-ops\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogVGltZWQgb3V0IHdhaXRpbmcgZm9yIHRoaW5nXCIgQU5EIGJ1aWxkX25hbWU6Z2F0ZS10ZW1wZXN0LWRldnN0YWNrLXZtLW5ldXRyb24tbGFyZ2Utb3BzIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI0MzIwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzODU0MDQ5Mzg5MjZ9", 
            "date_created": "2013-11-25 21:50:29.666235+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Looking at\nhttp://logs.openstack.org/08/59108/1/gate/gate-tempest-dsvm-neutron-large-ops/694acc7/logs/\n\nhttp://paste.openstack.org/show/54362/ - \"Waiting for\" messages stop at 14:17:38.668 (should be one every second). Looks like stuck in call_until_true loop. then it wakes up after 5 mins after the sleep and decides to get out of the loop w/o checking one last time if the condition is true. curiously server becomes \"ACTIVE\" at \"2013-12-03T14:22:07Z\"\n\n", 
            "date_created": "2013-12-03 15:41:34.398469+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/59769", 
            "date_created": "2013-12-03 16:13:58.808799+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Some more investigation in bug 1232303.", 
            "date_created": "2013-12-03 17:13:57.016254+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "http://bit.ly/1gwADji is the logstash url I am using", 
            "date_created": "2013-12-14 00:04:52.908271+00:00", 
            "author": "https://api.launchpad.net/1.0/~anteaya"
        }, 
        {
            "content": "seems to also cause gate-tempest-dsvm-full to fail\n\nhttp://logs.openstack.org/32/58332/2/gate/gate-tempest-dsvm-full/25f0684/console.html\n\n", 
            "date_created": "2013-12-17 07:53:29.774166+00:00", 
            "author": "https://api.launchpad.net/1.0/~notmyname"
        }, 
        {
            "content": "This is currently the 2nd highest reset problem after the SSH bug - http://status.openstack.org/elastic-recheck/", 
            "date_created": "2013-12-18 16:19:32.010362+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "I think i see at least one pattern.\n\n1) Looking at log [1]\n\n   Found this line:\n   Details: Timed out waiting for thing 2da86cc2-0c74-4712-8e7c-36280d231279                       to become ACTIVE\n\n2) Looking at the n-cpu-log, find the request id [2]\n\n   2013-12-18 19:29:21.909 AUDIT nova.compute.manager [req-3333ca13-880e-4df7-84bf-35b68dd4490a demo demo] [instance: 2da86cc2-0c74-4712-8e7c-36280d231279] Starting instance...\n\n3) Find the last instance of the request id\n2013-12-18 19:29:22.792 DEBUG nova.openstack.common.processutils [req-3333ca13-880e-4df7-84bf-35b68dd4490a demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf mount /dev/nbd6 /tmp/openstack-vfs-localfs_CB_RP execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147\n\nCould mount be hung? given that /dev/nbd6 is being reused (see [3])?\n\n[1] http://logs.openstack.org/30/62530/2/gate/gate-tempest-dsvm-neutron-pg/2e085b3/console.html\n[2] http://logs.openstack.org/30/62530/2/gate/gate-tempest-dsvm-neutron-pg/2e085b3/logs/screen-n-cpu.txt.gz\n[3] http://logs.openstack.org/30/62530/2/gate/gate-tempest-dsvm-neutron-pg/2e085b3/logs/screen-n-cpu.txt.gz#_2013-12-18_19_28_32_684\n\nOther logs that has the same issue:\nhttp://logs.openstack.org/43/59743/12/check/check-tempest-dsvm-neutron-pg/689a38c/logs/screen-n-cpu.txt.gz#_2013-12-18_15_11_50_960\nhttp://logs.openstack.org/85/61085/2/check/check-tempest-dsvm-neutron/bb95e64/logs/screen-n-cpu.txt.gz#_2013-12-18_12_14_07_163", 
            "date_created": "2013-12-18 23:46:03.611878+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "I can confirm DIMS findings.\nThe instance being spawned got stuck on mounting the fs", 
            "date_created": "2013-12-19 19:47:53.182097+00:00", 
            "author": "https://api.launchpad.net/1.0/~enikanorov"
        }, 
        {
            "content": "looks like users are seeing it too\nhttps://ask.openstack.org/en/question/6808/new-vm-instance-stuck-in-spawning-state/", 
            "date_created": "2013-12-20 02:29:17.933902+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "I think i'll reopen this bug for the stale nbd connection problem and leave this bug as catch all for other related \"Timed out\" issues.\n\nhttps://bugs.launchpad.net/nova/+bug/973413\n\n", 
            "date_created": "2013-12-20 17:22:24.073941+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "I'm putting it back to neutron. Most of the failures with this bug were seen with neutron-pg or neutron-(pg)-isolated jobs.\nStill it has something to do with neutron, may be in indirect way.", 
            "date_created": "2013-12-21 11:29:43.976430+00:00", 
            "author": "https://api.launchpad.net/1.0/~enikanorov"
        }, 
        {
            "content": "I have been investigating neutron's connection as well, since I had an exceptionally high failure rate with neutron jobs.\nIn neutron logs there is nothing which might point to a failure leading to an instance not being spawn. More precisely, I have been looking for neutron API calls not returning or returning with error codes which might have caused the thread processing network info asynchronously to never end. However, my research was inconclusive.\n\nFailure vs build names [1] shows a neutron job joint 1st in the highest ranked failing jobs. I am not sure however whether these numbers can confirm a skew in the failure rate towards neutron.\n\n\n\n[1] http://logstash.openstack.org/#eyJmaWVsZHMiOltdLCJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogVGltZWQgb3V0IHdhaXRpbmcgZm9yIHRoaW5nXCIgQU5EIG1lc3NhZ2U6XCJ0byBiZWNvbWVcIiBBTkQgIChtZXNzYWdlOlwiQUNUSVZFXCIgT1IgbWVzc2FnZTpcImluLXVzZVwiIE9SIG1lc3NhZ2U6XCJhdmFpbGFibGVcIilcbiIsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50Iiwib2Zmc2V0IjowLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJtb2RlIjoic2NvcmUiLCJhbmFseXplX2ZpZWxkIjoiYnVpbGRfbmFtZSJ9", 
            "date_created": "2013-12-22 14:09:05.131703+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Almost each and every failure with 'waiting for thing' that I've seen is related to postgress jobs. I think that fact is worth digging into. \n\nBy the way, I have also checked sys logs for the nbd devices, unsuccessfull unmounting of which is the direct cause of the issue.\nEvery unmount operation seem to result in the same logs being produced, but for one of them unmounting fails with EBUSY in user mode.\n", 
            "date_created": "2013-12-22 21:56:44.426475+00:00", 
            "author": "https://api.launchpad.net/1.0/~enikanorov"
        }, 
        {
            "content": "Eugene,\n\nThat umount failure seems to go away with this patch - https://review.openstack.org/#/c/63304/ - please do take a look.", 
            "date_created": "2013-12-22 22:23:42.451598+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "\"check experimental\" now runs additional jobs (https://review.openstack.org/#/c/63810/). The jobs show 63304 is not good (i.e. \"umount -l\" and \"qemu-nbd -n\" do not help). We need to find another way.", 
            "date_created": "2013-12-24 16:17:50.866704+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "I'm thinking, can we purely simulate the issue by mounting/unmounting fs with qemu-nbd? I mean without nova or devstack at all?\n\nIt could be helpfull to know what state this ndb came into once unmounting has failed. And what needs to be done to reuse it.\n\nIt could also make sense perform some sanity check before trying to mount it. Say, we mount it in a separate thread and wait for few seconds. If mounting did not succeed, kill the thread and find another device.\nWhat do you think?", 
            "date_created": "2013-12-24 20:59:53.142247+00:00", 
            "author": "https://api.launchpad.net/1.0/~enikanorov"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/64383\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=dd3f96e91581f465a52d10a212eb51b147dc38b5\nSubmitter: Jenkins\nBranch:    master\n\ncommit dd3f96e91581f465a52d10a212eb51b147dc38b5\nAuthor: Davanum Srinivas <email address hidden>\nDate:   Sun Dec 29 19:18:04 2013 -0500\n\n    Fix for qemu-nbd hang\n    \n    NBD device once used seem to run into intermittent trouble when\n    used with mount repeatedly. Adding code to explicitly flush the\n    device buffers using 'blockdev --flushbufs'.\n    \n    Closes-Bug: #973413\n    Partial-Bug: #1254890\n    Change-Id: I2b7053b9a069d6e82f6f6baf9ad480efa4388d91\n", 
            "date_created": "2013-12-31 01:04:44.292261+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I think the status should be reverted from Triaged to Confirmed, since we no longer seem to have a solid suggested fix or root cause for this.", 
            "date_created": "2014-01-09 05:35:41.908583+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaypipes"
        }, 
        {
            "content": "Jay, but we also don't see it appearing as well. Do you think we need to keep it in opened state?", 
            "date_created": "2014-01-09 19:46:50.178442+00:00", 
            "author": "https://api.launchpad.net/1.0/~enikanorov"
        }, 
        {
            "content": "https://etherpad.openstack.org/p/nova-gate-issue-tracking\n\nI think this issue will effectively be resolved by https://review.openstack.org/#/c/65805/", 
            "date_created": "2014-01-13 20:40:44.395216+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "example sysstat showing that the test node is maxed out - http://logs.openstack.org/81/66081/1/gate/gate-tempest-dsvm-postgres-full/57dd449/logs/screen-sysstat.txt.gz", 
            "date_created": "2014-01-16 21:51:58.568164+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Fix proposed to branch: stable/havana\nReview: https://review.openstack.org/67662", 
            "date_created": "2014-01-18 16:30:55.733185+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/66740\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=a0891ad0af316f5423c106e11ca2af7b17b76dd3\nSubmitter: Jenkins\nBranch:    master\n\ncommit a0891ad0af316f5423c106e11ca2af7b17b76dd3\nAuthor: Davanum Srinivas <email address hidden>\nDate:   Tue Jan 14 19:29:46 2014 -0500\n\n    Additional check for qemu-nbd hang\n    \n    /sys/block/*device*/pid check is not enough. I see that the unix\n    socket used by the device my be stuck as well, so let's add another\n    check for the path to the unix socket for the device as well to\n    figure out if the device is free. Complain loud and clear that the\n    qemu-nbd is leaking resources.\n    \n    Change-Id: I28cedffba7a9915ef6f7888989e40e4d0de475c6\n    Closes-Bug: #973413\n    Partial-Bug: #1254890\n", 
            "date_created": "2014-01-25 16:43:00.642507+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I investigated this problem from http://logs.openstack.org/36/69236/2/check/check-tempest-dsvm-full/8820082/ .\n\nAt 2014-01-26 22:04:03.329, the following traceback is written on screen-n-cpu.txt.gz.\n\nTraceback (most recent call last):\n  File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3844, in _attach_volume\n    encryption=encryption)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1166, in attach_volume\n    disk_info)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1124, in volume_driver_method\n    return method(connection_info, *args, **kwargs)\n  File \"/opt/stack/new/nova/nova/openstack/common/lockutils.py\", line 249, in inner\n    return f(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/volume.py\", line 305, in connect_volume\n    % (host_device))\nNovaException: iSCSI device not found at /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2010-10.org.openstack:volume-6f8b43b1-5c3b-4919-8dc2-7041e0966f7d-lun-1\n\nHowever \"iscsiadm --login\" run without any errors before the above traceback:\n2014-01-26 22:03:47.496 iscsiadm -m node -T iqn.2010-10.org.openstack:volume-6f8b43b1-5c3b-4919-8dc2-7041e0966f7d -p 127.0.0.1:3260 --login\n\nSo I cannot find the cause why /dev/disk/by-path/ link did not appear.\n", 
            "date_created": "2014-01-27 12:16:12.282617+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "I found out on friday that every time an hang is observed causing bug 1254890 there is a kernel crash.\nThis happens at least on neutron jobs and and is related to namespace operations.\n\nThis is the kernel crash dump from the logs: http://paste.openstack.org/show/61869/", 
            "date_created": "2014-01-27 12:51:57.870606+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "I've also investigated this bug. I've noticed that many same kernel messages in the syslog[1].\n\nJan 26 22:07:03 localhost kernel: [ 2800.881458]  connection4:0: detected conn error (1020)\nJan 26 22:07:06 localhost kernel: [ 2803.885615]  connection4:0: detected conn error (1019)\nJan 26 22:07:09 localhost kernel: [ 2806.889353]  connection4:0: detected conn error (1019)\n  :\nover 60 times.(connection4:0 only. other connections appeared only one time)\n\nI think \"4\" is  the volume-6f8b43b1-5c3b-4919-8dc2-7041e0966f7d's sid because of this log:\n \"2014-01-26 22:03:48.840 DEBUG nova.virt.libvirt.volume [req-a142bdbd-df70-4c07-8df9-0b5b27970925 tempest.scenario.manager-tempest-1690504423-user tempest.scenario.manager-tempest-1690504423-tenant] iscsiadm ('--rescan',): stdout=Rescanning session [sid: 4, target: iqn.2010-10.org.openstack:volume-6f8b43b1-5c3b-4919-8dc2-7041e0966f7d, portal: 127.0.0.1,3260]\"[2]\n\nHowever, I don't know why this message appeared many times...\n\n[1] http://logs.openstack.org/36/69236/2/check/check-tempest-dsvm-full/8820082/logs/syslog.txt.gz\n[2] http://logs.openstack.org/36/69236/2/check/check-tempest-dsvm-full/8820082/logs/screen-n-cpu.txt.gz#_2014-01-26_22_03_49_006\n", 
            "date_created": "2014-01-27 13:03:10.401379+00:00", 
            "author": "https://api.launchpad.net/1.0/~igawa"
        }, 
        {
            "content": "Filed a new bug to split out the cases of waiting for a volume to become in-use/active from waiting for an instance to become ACTIVE.\n\nhttps://bugs.launchpad.net/cinder/+bug/1273292", 
            "date_created": "2014-01-27 16:06:18.063770+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Based on the kernel findings, I'm looking into getting the kernel upgraded on our test nodes.\n\nRight now they run Ubuntu Precise, but there are newer kernels available: https://wiki.ubuntu.com/Kernel/LTSEnablementStack\n\nI think we should try getting linux-generic-lts-saucy installed.", 
            "date_created": "2014-01-27 18:35:16.216224+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "See also bug #1277494 - in this case, a thread appears to get stuck in libvirt driver's to_xml() function\n\nTo see if this is the issue with a given failure do:\n\n   grep '\\(Start\\|End\\) to_xml' screen-n-cpu.txt.gz\n\nif the last result from this grep is 'Start to_xml()', it could be the same problem\n\nWould be good to figure out whether this is just a once off underlying cause of a timeout, or more common", 
            "date_created": "2014-02-07 12:56:56.772078+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "Was the error message for this revised from \"ACTIVE\" to \"available\", or are those different states? e.g.\n\n2014-04-09 16:14:00.987 | Traceback (most recent call last):\n2014-04-09 16:14:00.988 |   File \"tempest/test.py\", line 122, in wrapper\n2014-04-09 16:14:00.988 |     return f(self, *func_args, **func_kwargs)\n2014-04-09 16:14:00.988 |   File \"tempest/scenario/test_minimum_basic.py\", line 122, in test_minimum_basic_scenario\n2014-04-09 16:14:00.988 |     self.cinder_create()\n2014-04-09 16:14:00.988 |   File \"tempest/scenario/test_minimum_basic.py\", line 64, in cinder_create\n2014-04-09 16:14:00.988 |     self.volume = self.create_volume()\n2014-04-09 16:14:00.988 |   File \"tempest/scenario/manager.py\", line 345, in create_volume\n2014-04-09 16:14:00.988 |     self.status_timeout(client.volumes, volume.id, 'available')\n2014-04-09 16:14:00.988 |   File \"tempest/scenario/manager.py\", line 184, in status_timeout\n2014-04-09 16:14:00.988 |     not_found_exception=not_found_exception)\n2014-04-09 16:14:00.988 |   File \"tempest/scenario/manager.py\", line 247, in _status_timeout\n2014-04-09 16:14:00.989 |     raise exceptions.TimeoutException(message)\n2014-04-09 16:14:00.989 | TimeoutException: Request timed out\n2014-04-09 16:14:00.989 | Details: Timed out waiting for thing 6b99ebc1-9f80-4a38-86c8-7fe606126bd7 to become available\n\nSource: http://logs.openstack.org/21/86321/9/check/check-grenade-dsvm/deb2e1e/console.html", 
            "date_created": "2014-04-09 16:41:33.221904+00:00", 
            "author": "https://api.launchpad.net/1.0/~dolph"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/67662\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=1681b957bfa236c69dc233bc7661fff9b5113ad0\nSubmitter: Jenkins\nBranch:    stable/havana\n\ncommit 1681b957bfa236c69dc233bc7661fff9b5113ad0\nAuthor: Davanum Srinivas <email address hidden>\nDate:   Sun Dec 29 19:18:04 2013 -0500\n\n    Fix for qemu-nbd hang\n    \n    NBD device once used seem to run into intermittent trouble when\n    used with mount repeatedly. Adding code to explicitly flush the\n    device buffers using 'blockdev --flushbufs'.\n    \n    NOTE(dmllr): Also merged regression fix:\n    \n        Consolidate the blockdev related filters\n    \n        Seeing a \"/usr/local/bin/nova-rootwrap: Unauthorized command\" error\n        in the logs when  \"blockdev --flushbufs\" is executed because of the\n        existing blockdev in compute.filters. We need to merge both into a\n        single RegExpFilter\n    \n        Change-Id: Ic323ff00e5c23786a6e376e67a4ad08f92708aef\n    \n    (cherry-picked from dd3f96e91581f465a52d10a212eb51b147dc38b5)\n    (cherry-picked from 31b2791e6be1768c410de5fa32283fcb637bda57)\n    \n    Closes-Bug: #973413\n    Partial-Bug: #1254890\n    Change-Id: I2b7053b9a069d6e82f6f6baf9ad480efa4388d91\n", 
            "date_created": "2014-07-31 06:39:15.748442+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I believe this has been fixed with upgrades to trusty and newer libvirt. please reopen if needed.", 
            "date_created": "2014-09-06 23:57:34.138261+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "We are still seeing hits for this in elastic-recheck", 
            "date_created": "2014-09-11 21:36:02.171003+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "May now be a duplicate of 1357476", 
            "date_created": "2014-09-11 21:36:19.362696+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "marked as critical because 22 hits in elastic-recheck in 24 hours", 
            "date_created": "2014-09-11 21:49:52.549143+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "No hits in the gate queue in a few days looks like this may  be fixed?", 
            "date_created": "2014-09-18 00:12:43.709470+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Removing from juno-rc1 as I don't think this should block the juno release.", 
            "date_created": "2014-09-18 14:44:59.282499+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }
    ], 
    "closed": "2014-10-16 18:09:27.057646+00:00"
}
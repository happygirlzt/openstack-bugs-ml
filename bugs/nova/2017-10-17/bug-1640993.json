{
    "status": "Fix Released", 
    "last_updated": "2017-03-08 17:06:16.741420+00:00", 
    "description": "We're now running neutron by default in CI jobs for ocata if they aren't explicitly specified otherwise to run nova-network.\n\nThat might be unrelated to this, but I saw the citrix xenserver neutron job fail today with a vif plugging timeout:\n\nhttp://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/47/395747/8/check/dsvm-tempest-neutron-network/e3832fe/logs/screen-n-cpu.txt.gz\n\n2016-11-11 00:55:41.191 3175 WARNING nova.virt.xenapi.vmops [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Timeout waiting for vif plugging callback\n\nnova.conf seems to be configured correctly:\n\nhttp://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/47/395747/8/check/dsvm-tempest-neutron-network/e3832fe/logs/etc/nova/nova.conf.txt.gz\n\nvif_plugging_timeout = 300\nvif_plugging_is_fatal = True\n\nSo I'm wondering if there is a problem in the vif plugging callback logic in the xenapi driver code.\n\nI see this earlier before the timeout:\n\n2016-11-11 00:50:29.526 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-b6c3af4c-98cc-4077-8d4d-7009835c0c5c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n\n2016-11-11 00:50:29.527 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-f5d2ac5a-36bf-4562-b2d0-18c40f640a3c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n\n2016-11-11 00:50:29.528 3175 DEBUG nova.virt.xenapi.vmops [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] wait for instance event:[('network-vif-plugged', u'b6c3af4c-98cc-4077-8d4d-7009835c0c5c'), ('network-vif-plugged', u'f5d2ac5a-36bf-4562-b2d0-18c40f640a3c')] _spawn /opt/stack/new/nova/nova/virt/xenapi/vmops.py:599\n\nThen it starts doing the vif plugging. The odd thing is it logs twice that it's preparing to wait for external events, but only logs 'waiting for instance event' once. So is it waiting for another even that doesn't happen?", 
    "tags": [
        "neutron", 
        "xenserver"
    ], 
    "importance": "High", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1640993", 
    "owner": "https://api.launchpad.net/1.0/~huan-xie", 
    "id": 1640993, 
    "index": 2004, 
    "openned": "2016-11-11 02:32:19.155436+00:00", 
    "created": "2016-11-11 02:32:19.155436+00:00", 
    "title": "xenserver hits vif plugging timeout with neutron CI job", 
    "comments": [
        {
            "content": "We're now running neutron by default in CI jobs for ocata if they aren't explicitly specified otherwise to run nova-network.\n\nThat might be unrelated to this, but I saw the citrix xenserver neutron job fail today with a vif plugging timeout:\n\nhttp://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/47/395747/8/check/dsvm-tempest-neutron-network/e3832fe/logs/screen-n-cpu.txt.gz\n\n2016-11-11 00:55:41.191 3175 WARNING nova.virt.xenapi.vmops [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Timeout waiting for vif plugging callback\n\nnova.conf seems to be configured correctly:\n\nhttp://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/47/395747/8/check/dsvm-tempest-neutron-network/e3832fe/logs/etc/nova/nova.conf.txt.gz\n\nvif_plugging_timeout = 300\nvif_plugging_is_fatal = True\n\nSo I'm wondering if there is a problem in the vif plugging callback logic in the xenapi driver code.\n\nI see this earlier before the timeout:\n\n2016-11-11 00:50:29.526 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-b6c3af4c-98cc-4077-8d4d-7009835c0c5c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n\n2016-11-11 00:50:29.527 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-f5d2ac5a-36bf-4562-b2d0-18c40f640a3c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n\n2016-11-11 00:50:29.528 3175 DEBUG nova.virt.xenapi.vmops [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] wait for instance event:[('network-vif-plugged', u'b6c3af4c-98cc-4077-8d4d-7009835c0c5c'), ('network-vif-plugged', u'f5d2ac5a-36bf-4562-b2d0-18c40f640a3c')] _spawn /opt/stack/new/nova/nova/virt/xenapi/vmops.py:599\n\nThen it starts doing the vif plugging. The odd thing is it logs twice that it's preparing to wait for external events, but only logs 'waiting for instance event' once. So is it waiting for another even that doesn't happen?", 
            "date_created": "2016-11-11 02:32:19.155436+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "From the log, we can see it logs twice for preparing external events, but it's for different event, no the same, see the event ID, see:\n1: network-vif-plugged-b6c3af4c-98cc-4077-8d4d-7009835c0c5c\n2: network-vif-plugged-f5d2ac5a-36bf-4562-b2d0-18c40f640a3c\n\n2016-11-11 00:50:29.526 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-b6c3af4c-98cc-4077-8d4d-7009835c0c5c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n2016-11-11 00:50:29.526 3175 DEBUG oslo_concurrency.lockutils [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] Lock \"79c00d5c-b285-44e5-b8db-9fc9a8e31478-events\" acquired by \"nova.compute.manager._create_or_get_event\" :: waited 0.000s inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:270\n2016-11-11 00:50:29.527 3175 DEBUG oslo_concurrency.lockutils [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] Lock \"79c00d5c-b285-44e5-b8db-9fc9a8e31478-events\" released by \"nova.compute.manager._create_or_get_event\" :: held 0.000s inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:282\n2016-11-11 00:50:29.527 3175 DEBUG nova.compute.manager [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] [instance: 79c00d5c-b285-44e5-b8db-9fc9a8e31478] Preparing to wait for external event network-vif-plugged-f5d2ac5a-36bf-4562-b2d0-18c40f640a3c prepare_for_instance_event /opt/stack/new/nova/nova/compute/manager.py:324\n2016-11-11 00:50:29.527 3175 DEBUG oslo_concurrency.lockutils [req-5f5774e3-92ee-4e49-947d-ce1c879bc1ab tempest-ServersTestManualDisk-1338550150 tempest-ServersTestManualDisk-1338550150] Lock \"79c00d5c-b285-44e5-b8db-9fc9a8e31478-events\" acquired by \"nova.compute.manager._create_or_get_event\" :: waited 0.000s inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:270\n\nSo, when you booting the instance, do you choose to attach two networks? Also, could you help double check together with neutron log with the two events?\n\n", 
            "date_created": "2016-11-11 02:57:41.258285+00:00", 
            "author": "https://api.launchpad.net/1.0/~huan-xie"
        }, 
        {
            "content": "The XenAPI integration with Neutron cannot currently use several key performance enhancements in Neutron which fixed many of these timeouts.\nThe main omission is that the daemon-based rootwrap cannot be used as it does not have integration with XenAPI (which needs to run commands over the host-internal network so they can run in dom0).\n\nFurther, we can't use ovsdb monitoring, so we have to continually poll the OVS, making heavier use of the non-daemon rootwrap.\n\nWe are working to fix the non-daemon rootwrap (see https://review.openstack.org/#/c/390931/) issue which will substantially reduce the Neutron demands and we are hoping will correct this failure.", 
            "date_created": "2016-11-11 10:30:46.833826+00:00", 
            "author": "https://api.launchpad.net/1.0/~bob-ball"
        }, 
        {
            "content": "Can we set vif_plugging_is_fatal=False in the xenserver CI jobs until this is stabilized on the neutron side for the rootwrap daemon? That way we still get the wait logic but don't fail if we timeout. We could still fail, but maybe networking is setup enough that tests will pass at a higher rate.", 
            "date_created": "2016-12-14 17:30:37.614164+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Hi Matt, thanks for your suggestion, I will do and test that ASAP", 
            "date_created": "2016-12-15 11:30:50.519177+00:00", 
            "author": "https://api.launchpad.net/1.0/~huan-xie"
        }, 
        {
            "content": "Sorry for late updating, last week we have changed our external CI to use ovs native mode in neutron-openvswitch-agent for Dom0 which will enhance the performance a lot and it looks good, no VIF plug timeout, will continue monitor the problems!", 
            "date_created": "2016-12-20 01:53:43.591516+00:00", 
            "author": "https://api.launchpad.net/1.0/~huan-xie"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/413469", 
            "date_created": "2016-12-21 07:48:37.202863+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Proposed change that closes this bug was merged, but somehow infra did not catch that. Marking as complete.", 
            "date_created": "2017-03-08 17:06:04.118094+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }
    ], 
    "closed": "2017-03-08 17:06:13.259142+00:00"
}
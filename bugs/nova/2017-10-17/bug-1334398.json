{
    "status": "Confirmed", 
    "last_updated": "2016-08-31 14:52:35.272050+00:00", 
    "description": "Seeing this here:\n\nhttp://logs.openstack.org/70/97670/5/check/check-tempest-dsvm-postgres-full/7d4c7cf/console.html\n\n2014-06-24 23:15:41.714 | tempest.api.compute.images.test_images_oneserver.ImagesOneServerTestJSON.test_create_image_specify_multibyte_character_image_name[gate]\n2014-06-24 23:15:41.714 | ---------------------------------------------------------------------------------------------------------------------------------------\n2014-06-24 23:15:41.714 | \n2014-06-24 23:15:41.714 | Captured traceback-1:\n2014-06-24 23:15:41.714 | ~~~~~~~~~~~~~~~~~~~~~\n2014-06-24 23:15:41.715 |     Traceback (most recent call last):\n2014-06-24 23:15:41.715 |       File \"tempest/services/compute/json/images_client.py\", line 86, in delete_image\n2014-06-24 23:15:41.715 |         resp, body = self.delete(\"images/%s\" % str(image_id))\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 224, in delete\n2014-06-24 23:15:41.715 |         return self.request('DELETE', url, extra_headers, headers, body)\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 430, in request\n2014-06-24 23:15:41.715 |         resp, resp_body)\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 474, in _error_checker\n2014-06-24 23:15:41.715 |         raise exceptions.NotFound(resp_body)\n2014-06-24 23:15:41.715 |     NotFound: Object not found\n2014-06-24 23:15:41.715 |     Details: {\"itemNotFound\": {\"message\": \"Image not found.\", \"code\": 404}}\n2014-06-24 23:15:41.716 |     \n2014-06-24 23:15:41.716 | \n2014-06-24 23:15:41.716 | Captured traceback:\n2014-06-24 23:15:41.716 | ~~~~~~~~~~~~~~~~~~~\n2014-06-24 23:15:41.716 |     Traceback (most recent call last):\n2014-06-24 23:15:41.716 |       File \"tempest/api/compute/images/test_images_oneserver.py\", line 31, in tearDown\n2014-06-24 23:15:41.716 |         self.server_check_teardown()\n2014-06-24 23:15:41.716 |       File \"tempest/api/compute/base.py\", line 161, in server_check_teardown\n2014-06-24 23:15:41.716 |         'ACTIVE')\n2014-06-24 23:15:41.716 |       File \"tempest/services/compute/json/servers_client.py\", line 173, in wait_for_server_status\n2014-06-24 23:15:41.716 |         raise_on_error=raise_on_error)\n2014-06-24 23:15:41.717 |       File \"tempest/common/waiters.py\", line 107, in wait_for_server_status\n2014-06-24 23:15:41.717 |         raise exceptions.TimeoutException(message)\n2014-06-24 23:15:41.717 |     TimeoutException: Request timed out\n2014-06-24 23:15:41.717 |     Details: (ImagesOneServerTestJSON:tearDown) Server 90c79adf-4df1-497c-a786-13bdc5cca98d failed to reach ACTIVE status and task state \"None\" within the required time (196 s). Current status: ACTIVE. Current task state: image_pending_upload.\n\n\nLooks like it's trying to delete image with uuid 518a32d0-f323-413c-95c2-dd8299716c19 which doesn't exist, because it's still uploading?\n\n\nThis is maybe related to bug 1320617 as a general performance issue with glance.\n\nLooking in the glance registry log, the image is created here:\n\n2014-06-24 22:51:23.538 15740 INFO glance.registry.api.v1.images [13c1b477-cd22-44ca-ba0d-bf1b19202df6 d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Successfully created image 518a32d0-f323-413c-95c2-dd8299716c19\n\nThe image is deleted here:\n\n2014-06-24 22:54:53.146 15740 INFO glance.registry.api.v1.images [7c29f253-acef-41a0-b62b-c3087f7617ef d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Successfully deleted image 518a32d0-f323-413c-95c2-dd8299716c19\n\nAnd the 'not found' is here:\n\n2014-06-24 22:54:56.508 15740 INFO glance.registry.api.v1.images [c708cf1f-27a8-4003-9c29-6afca7dd9bb8 d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Image 518a32d0-f323-413c-95c2-dd8299716c19 not found", 
    "tags": [
        "libvirt"
    ], 
    "importance": "Undecided", 
    "heat": 68, 
    "link": "https://bugs.launchpad.net/nova/+bug/1334398", 
    "owner": "None", 
    "id": 1334398, 
    "index": 6217, 
    "openned": "2014-06-25 18:40:54.008957+00:00", 
    "created": "2014-06-25 18:40:54.008957+00:00", 
    "title": "libvirt live_snapshot periodically explodes on libvirt 1.2.2 in the gate", 
    "comments": [
        {
            "content": "Seeing this here:\n\nhttp://logs.openstack.org/70/97670/5/check/check-tempest-dsvm-postgres-full/7d4c7cf/console.html\n\n2014-06-24 23:15:41.714 | tempest.api.compute.images.test_images_oneserver.ImagesOneServerTestJSON.test_create_image_specify_multibyte_character_image_name[gate]\n2014-06-24 23:15:41.714 | ---------------------------------------------------------------------------------------------------------------------------------------\n2014-06-24 23:15:41.714 | \n2014-06-24 23:15:41.714 | Captured traceback-1:\n2014-06-24 23:15:41.714 | ~~~~~~~~~~~~~~~~~~~~~\n2014-06-24 23:15:41.715 |     Traceback (most recent call last):\n2014-06-24 23:15:41.715 |       File \"tempest/services/compute/json/images_client.py\", line 86, in delete_image\n2014-06-24 23:15:41.715 |         resp, body = self.delete(\"images/%s\" % str(image_id))\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 224, in delete\n2014-06-24 23:15:41.715 |         return self.request('DELETE', url, extra_headers, headers, body)\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 430, in request\n2014-06-24 23:15:41.715 |         resp, resp_body)\n2014-06-24 23:15:41.715 |       File \"tempest/common/rest_client.py\", line 474, in _error_checker\n2014-06-24 23:15:41.715 |         raise exceptions.NotFound(resp_body)\n2014-06-24 23:15:41.715 |     NotFound: Object not found\n2014-06-24 23:15:41.715 |     Details: {\"itemNotFound\": {\"message\": \"Image not found.\", \"code\": 404}}\n2014-06-24 23:15:41.716 |     \n2014-06-24 23:15:41.716 | \n2014-06-24 23:15:41.716 | Captured traceback:\n2014-06-24 23:15:41.716 | ~~~~~~~~~~~~~~~~~~~\n2014-06-24 23:15:41.716 |     Traceback (most recent call last):\n2014-06-24 23:15:41.716 |       File \"tempest/api/compute/images/test_images_oneserver.py\", line 31, in tearDown\n2014-06-24 23:15:41.716 |         self.server_check_teardown()\n2014-06-24 23:15:41.716 |       File \"tempest/api/compute/base.py\", line 161, in server_check_teardown\n2014-06-24 23:15:41.716 |         'ACTIVE')\n2014-06-24 23:15:41.716 |       File \"tempest/services/compute/json/servers_client.py\", line 173, in wait_for_server_status\n2014-06-24 23:15:41.716 |         raise_on_error=raise_on_error)\n2014-06-24 23:15:41.717 |       File \"tempest/common/waiters.py\", line 107, in wait_for_server_status\n2014-06-24 23:15:41.717 |         raise exceptions.TimeoutException(message)\n2014-06-24 23:15:41.717 |     TimeoutException: Request timed out\n2014-06-24 23:15:41.717 |     Details: (ImagesOneServerTestJSON:tearDown) Server 90c79adf-4df1-497c-a786-13bdc5cca98d failed to reach ACTIVE status and task state \"None\" within the required time (196 s). Current status: ACTIVE. Current task state: image_pending_upload.\n\n\nLooks like it's trying to delete image with uuid 518a32d0-f323-413c-95c2-dd8299716c19 which doesn't exist, because it's still uploading?\n\n\nThis is maybe related to bug 1320617 as a general performance issue with glance.\n\nLooking in the glance registry log, the image is created here:\n\n2014-06-24 22:51:23.538 15740 INFO glance.registry.api.v1.images [13c1b477-cd22-44ca-ba0d-bf1b19202df6 d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Successfully created image 518a32d0-f323-413c-95c2-dd8299716c19\n\nThe image is deleted here:\n\n2014-06-24 22:54:53.146 15740 INFO glance.registry.api.v1.images [7c29f253-acef-41a0-b62b-c3087f7617ef d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Successfully deleted image 518a32d0-f323-413c-95c2-dd8299716c19\n\nAnd the 'not found' is here:\n\n2014-06-24 22:54:56.508 15740 INFO glance.registry.api.v1.images [c708cf1f-27a8-4003-9c29-6afca7dd9bb8 d01d4977b5cc4e20a99e1d7ca58ce444 207d083a31944716b9cd2ecda0f09ce7 - - -] Image 518a32d0-f323-413c-95c2-dd8299716c19 not found", 
            "date_created": "2014-06-25 18:40:54.008957+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The n-cpu logs have several errors for the libvirt connection being reset:\n\nhttp://logs.openstack.org/70/97670/5/check/check-tempest-dsvm-postgres-full/7d4c7cf/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-06-24_22_54_52_973\n\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] Traceback (most recent call last):\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 352, in decorated_function\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     *args, **kwargs)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2788, in snapshot_instance\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     task_states.IMAGE_SNAPSHOT)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2819, in _snapshot_instance\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     update_task_state)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1532, in snapshot\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     image_format)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1631, in _live_snapshot\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     domain.blockJobAbort(disk_path, 0)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 179, in doit\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 139, in proxy_call\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     rv = execute(f,*args,**kwargs)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 77, in tworker\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     rv = meth(*args,**kwargs)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 646, in blockJobAbort\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     if ret == -1: raise libvirtError ('virDomainBlockJobAbort() failed', dom=self)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] libvirtError: Unable to read from monitor: Connection reset by peer\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] ", 
            "date_created": "2014-06-25 18:46:37.358059+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Here is a logstash query on the tempest failure:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwic2VydmVyX2NoZWNrX3RlYXJkb3duXCIgQU5EIG1lc3NhZ2U6XCJTZXJ2ZXJcIiBBTkQgbWVzc2FnZTpcImZhaWxlZCB0byByZWFjaCBBQ1RJVkUgc3RhdHVzIGFuZCB0YXNrIHN0YXRlIFxcXCJOb25lXFxcIiB3aXRoaW4gdGhlIHJlcXVpcmVkIHRpbWVcIiBBTkQgbWVzc2FnZTpcIkN1cnJlbnQgc3RhdHVzOiBBQ1RJVkUuIEN1cnJlbnQgdGFzayBzdGF0ZTogaW1hZ2VfcGVuZGluZ191cGxvYWRcIiBBTkQgdGFnczpcInRlbXBlc3QudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjE3MjgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDM3MjE4NDQ1MjksIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=\n\nmessage:\"server_check_teardown\" AND message:\"Server\" AND message:\"failed to reach ACTIVE status and task state \\\"None\\\" within the required time\" AND message:\"Current status: ACTIVE. Current task state: image_pending_upload\" AND tags:\"tempest.txt\"\n\n18 hits in 2 days.", 
            "date_created": "2014-06-25 18:48:37.405341+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Nova bug 1255624 is tracking libvirt connection reset errors, in that case it failed during virDomainSuspend, here it fails during virDomainBlockJobAbort.", 
            "date_created": "2014-06-25 18:50:46.390884+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Here is a logstash query on the libvirt connection reset error:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwicmFpc2UgbGlidmlydEVycm9yICgndmlyRG9tYWluQmxvY2tKb2JBYm9ydCgpIGZhaWxlZCdcIiBBTkQgbWVzc2FnZTpcImxpYnZpcnRFcnJvcjogVW5hYmxlIHRvIHJlYWQgZnJvbSBtb25pdG9yOiBDb25uZWN0aW9uIHJlc2V0IGJ5IHBlZXJcIiBBTkQgdGFnczpcInNjcmVlbi1uLWNwdS50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiMTcyODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMzcyMjMxMDczNX0=\n\n159 hits in 2 days, there is a small successful percentage when that shows up.\n\nLooking at the tests when this fails, they are doing image snapshots so this seems like a pretty good query.", 
            "date_created": "2014-06-25 18:54:41.249813+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "e-r patch: https://review.openstack.org/#/c/102608/\n\nLooks like this really spiked on 6/24 and goes down again on 6/25.", 
            "date_created": "2014-06-25 19:06:06.279415+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Going down on 6/25 is a mirage, we're just backed up on ES data.", 
            "date_created": "2014-06-25 20:39:40.560429+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Wondering if bug 1193146 has any interesting historical information.", 
            "date_created": "2014-06-25 21:14:31.991796+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I also faced this problem many times today, and most failure happened on ListImageFiltersTestXML.\nI'm not sure why it does not happen on ListImageFiltersTestJSON..\n", 
            "date_created": "2014-06-26 07:04:39.763561+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "It looks like there is a very particular explode around _live_snapshot. The bug actually seems to only explode when we are in _live_snapshot, and not any other cases. (Modifying the elastic recheck search string for _live_snapshot has the same # of counts).\n\n", 
            "date_created": "2014-06-26 10:12:07.709902+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "It's worth noting that the _live_snapshot code path was never tested by us until the trusty update, as it was hidden behind a version flag that meant we didn't run it before in the gate.", 
            "date_created": "2014-06-26 10:18:13.328342+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "This should get us around the gate failures for now https://review.openstack.org/#/c/102643/", 
            "date_created": "2014-06-26 13:34:03.939007+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "@Ken'ichi, per comment 8, it's a pretty intensive setup:\n\nhttp://git.openstack.org/cgit/openstack/tempest/tree/tempest/api/compute/images/test_list_image_filters.py#n29\n\nThe setup creates 2 servers and then 3 snapshots from those 2 servers, and the JSON and XML test classes are running concurrently, so we could be creating multiple snapshots concurrently which is in theory overloading libvirt/qemu and causing the connection reset with libvirt.", 
            "date_created": "2014-06-26 13:42:02.072054+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/102643\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=c1c159460de376a06b8479f90f42d9f62eace961\nSubmitter: Jenkins\nBranch:    master\n\ncommit c1c159460de376a06b8479f90f42d9f62eace961\nAuthor: Sean Dague <email address hidden>\nDate:   Wed Jun 25 16:56:04 2014 -0400\n\n    effectively disable libvirt live snapshotting\n    \n    As being seen in the gate, libvirt 1.2.2 doesn't appear to actually\n    handle live snapshotting under any appreciable load (possibly\n    related to parallel operations). It isn't a 100% failure, but it's\n    currently being hit in a large number of runs.\n    \n    Effectively turn this off by increasing the\n    MIN_LIBVIRT_LIVESNAPSHOT_VERSION to something that doesn't yet\n    exist. This can get us back to a working state, then we can decide\n    if live snapshotting is something that can be made to actually work\n    under load.\n    \n    DocImpact\n    \n    Related-Bug: #1334398\n    \n    Change-Id: I9908b743df2093c8dd2723af89be51630eafc99f\n", 
            "date_created": "2014-06-26 13:50:54.125189+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Thought I'd add the below. \n\nI just created a simple test[1] which creates an external live snapshot[2] of a libvirt guest (with versions affecting the gate -- libvirt 1.2.2 and QEMU 2.0 on Fedora 20), by executing the below command in a loop of 100 (I also tested for 1000, it ran just fine too). I ran the script for 3 virtual machines in parallel.\n\n$ virsh snapshot-create-as --domain $DOMAIN \\\n    --name snap-$i \\\n    --description snap$i-desc \\\n    --disk-only \\\n    --diskspec hda,snapshot=external,file=$SNAPSHOTS_DIR/$DOMAIN-snap$i.qcow2 \\\n    --atomic\n\nResult of a 100 loop run would be an image with a backing chain of 100 qcow2 images[3].\n\nThe above script just creates a snapshot, nothing more. Matt Riedemann pointed out on #openstack-nova that on Gate there could be others tests running concurrently that could be doing things like suspend/resume/rescue, etc.\n\n \n  [1] https://github.com/kashyapc/ostack-misc/blob/master/libvirt-live-snapshots-stress.sh\n  [2] \"external live snapshot \"meaning: Every time you take a snapshot, the current disk becomes a (read-only) 'backing file' and a new qcow2 overlay is created to track the current 'delta'. \n  [3] http://kashyapc.fedorapeople.org/virt/guest-with-backing-chain-of-100-qcow2-images.txt", 
            "date_created": "2014-06-26 14:00:37.885274+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "The interesting thing in the logs is the stack trace about virDomainBlockJobAbort failing.\n\nNova issues this API call right at the start of the snapshot funtion to validated there's no old stale job left over:\n\n            # Abort is an idempotent operation, so make sure any block\n            # jobs which may have failed are ended. This operation also\n            # confirms the running instance, as opposed to the system as a\n            # whole, has a new enough version of the hypervisor (bug 1193146).\n            try:\n                virt_dom.blockJobAbort(disk_path, 0)\n\nAs the comment says, aborting the job is supposed to be a completely safe thing todo. We don't even expect any existing job to be running, so it should basically end up as a no-op inside QEMU.\n\nNow the error message libvirt reports when virDomaniBlockJobAbort fails is:\n\nlibvirtError: Unable to read from monitor: Connection reset by peer\n\nThis is a generic message you get when QEMU crashes & burns unexpectedly, causing the monitor connection to be dropped. \n\nWe've not even got as far as running the libvirt snapshot API at this point when QEMU crashes & burns. This likely explains why Kashyap can't see the error in this test script which just invokes snapshot.\n\nThis all points the finger towards a flaw in QEMU of some kind, but there's no easy way to figure out what this might be from the libvirtd logs.\n\nWhat we need here is the /var/log/libvirt/qemu/instanceNNNNNN.log files corresponding to the failed test run. If we are lucky there might be some assertion error printed from QEMU before it crashed & burned. Failing that, what we need is a stack trace from QEMU when it crashes & burns.  IIUC, ubuntu has something similar to Fedora's abrt daemon which captures stack traces of any process which SEGVs or core dumps. We really need to try to get a stack trace of QEMU from Ubuntu's crash handler too. \n", 
            "date_created": "2014-06-26 14:09:58.438278+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "This is the service I was talking about\n\n  https://wiki.ubuntu.com/Apport\n\nWe need to re-configure it to collect core dumps as it is disabled by default. Then capture any crashes in\n\n/var/crash/", 
            "date_created": "2014-06-26 14:30:01.735570+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Sorry, I was looking at the wrong blockJobAbort call in the code earlier. The actual one that is failing is in this code\n\n            # NOTE (rmk): Establish a temporary mirror of our root disk and                                           \n            #             issue an abort once we have a complete copy.                                                \n            domain.blockRebase(disk_path, disk_delta, 0,\n                               libvirt.VIR_DOMAIN_BLOCK_REBASE_COPY |\n                               libvirt.VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT |\n                               libvirt.VIR_DOMAIN_BLOCK_REBASE_SHALLOW)\n\n            while self._wait_for_block_job(domain, disk_path):\n                time.sleep(0.5)\n\n            domain.blockJobAbort(disk_path, 0)\n\n\nSo we've done a block rebase, and then when we finish waiting for it to finish, we abort the job and at that point we see the crashed QEMU. The QEMU logs are stuff something useful to get", 
            "date_created": "2014-06-26 15:09:35.373752+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I'm actually beginning to wonder if there is a flaw in the tempest tests rather than in QEMU.  The \" Unable to read from monitor: Connection reset by peer\" error message can actually indicate that a second thread has killed QEMU, while the first thread is talking to it - so this is a potential alternative idea to explore vs my previous QEMU-SEGV bug theory.\n\nI've been examining the screen-n-cpu.log file to see what happens with instance  90c79adf-4df1-497c-a786-13bdc5cca98d which is the one with the virDomainBlockJob error trace\n\nFirst I see the snapshot process starting\n\n2014-06-24 22:51:24.314 INFO nova.virt.libvirt.driver [req-e4651efe-7c84-4a57-bbb1-88b107d4a282 ImagesOneServerTestJSON-967160715 ImagesOneServerTestJSON-32972017] [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] Beginning live snapshot process\n\nThen I see the something killing this very same instance\n\n2014-06-24 22:54:40.255 AUDIT nova.compute.manager [req-218dba14-516c-4805-9908-b55cd73a00e5 ImagesOneServerTestJSON-967160715 ImagesOneServerTestJSON-32972017] [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] Terminating instance\n\nAnd a lifecycle event to show that it was killed\n\n2014-06-24 22:54:51.033 16186 INFO nova.compute.manager [-] Lifecycle event 1 on VM 90c79adf-4df1-497c-a786-13bdc5cca98d\n\nthen we see the snapshot process crash & burn\n\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 646, in blockJobAbort\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d]     if ret == -1: raise libvirtError ('virDomainBlockJobAbort() failed', dom=self)\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] libvirtError: Unable to read from monitor: Connection reset by peer\n2014-06-24 22:54:52.973 16186 TRACE nova.compute.manager [instance: 90c79adf-4df1-497c-a786-13bdc5cca98d] \n\n\n\nSo this looks very much to me like something in the test is killing the instance while the snapshot is still being done\n\n\nNow, as for why this doesn't affect non-live snapshots we were testing before...\n\nFor non-live snapshots, we issue a 'managedSave' call, this terminates the guest. Then we do the snapshot process. Then we start up the guest against from the managed save image.  My guess is that this race-ing 'Terminate instance' call is happening while the guest is already shutdown and hence does not cause a failure of the test suite when doing  non-live snapshot (or at least the window in which the race could hit is dramatically smaller).\n\nSo based on the sequence in the screen-n-cpu.log file my money is currently on a race in the test scripts where something explicitly kills the instance  while snapshot is being taken, and that the non-live snapshot code is not exposed to the race.\n", 
            "date_created": "2014-06-26 17:02:02.526890+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "We do kill the snapshot if it exceeds the timeout, which is currently 196s, because at some point we need to actually move on. When these are successful, they typically succeed in about 10s.", 
            "date_created": "2014-06-26 17:10:11.055671+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Ok so it looks like the problem is that the snaphsot is not completing in a reasonable amount of time. The time stamps look like it took 2.5 minutes before it was killed which aligns with the above. So it looks like the BlockMirror is not completing.", 
            "date_created": "2014-06-26 21:27:23.289538+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "After looking a  little bit more (at' _live_snapshot' function in Nova[*]) , the below seem to be  the precise equivalent sequence of (libvirt) operations of what's happening in Nova's '_live_snapshot' function. Thanks to libvirt developer Eric Blake for reviewing this:\n\n(0) Take the Libvirt guest's XML backup:\n\n    $ virsh dumpxml --inactive vm1 > /var/tmp/vm1.xml\n\n(1) Abort any failed/finished block operations:\n\n    $ virsh blockjob vm1 vda --abort\n\n(2) Undefine a running domain. (Note: Undefining a running domain does not _kill_ the domain, it just converts it from persistent to\n      transient.)\n\n    $ virsh undefine vm1\n\n(3) Invoke 'virsh blockcopy' (This will take time, depending on the size of disk image vm1):\n\n    $ virsh blockcopy \\\n        --domain vm1 vda \\\n        /export/backups/vm1-copy.qcow2 \\\n        --wait \\\n        --verbose\n\n(4) Abort any failed/finished block operations: (as Dan pointed out in comment #17, this the abort operation where QEMU \n    might be    failing):\n\n    $ virsh blockjob vm1 vda --abort\n\n   NOTE: If we use '--finish' command in step 3 it is equivalent to the\n   above command (consequently, step 4 can be skipped).\n\n(5) Define the guest again (to make it persistent):\n\n    $ virsh define  /var/tmp/vm1.xml\n\n(6) From the obtained new copy, convert the QCOW2 with a backing file to a flat (raw) image with no backing file:\n\n    $ qemu-img convert -f qcow2 -O raw vm1.qcow2 conv-vm1.img \n\n\nNotes (from Eric Blake):\n    The _live_snapshot function concludes it all with redefining the\n    domain (umm, that part looks fishy in the code - you undefine it\n    only if it was persistent, but redefine the domain unconditionally;\n    so if you call your function on a domain that is initially\n    transient, you end up with a persistent domain at the end of your\n    function).\n\n\n  [*] https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L1593", 
            "date_created": "2014-06-27 05:17:06.735926+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "FWIW, I have encountered libvirt connection reset errors when my DevStack VM ran low on memory. I first saw these when the more recent versions of DevStack started spawning numerous nova-api and nova-conductor instances, which ate up the relatively small RAM of the VM. When this happened, I was unable to boot any instances, with libvirt connection reset errors reported in the n-cpu log. Not sure that low memory is the what's causing errors here (maybe some other resource starvation) but they look awfully similar.", 
            "date_created": "2014-06-27 14:02:14.253874+00:00", 
            "author": "https://api.launchpad.net/1.0/~daniel-genin"
        }, 
        {
            "content": "Here's some investigation of what happens when _live_snapshot is\ninvoked, at Libvirt level. I performed the live_snapshot test w/ current\ngit (after I modified MIN_LIBVIRT_LIVESNAPSHOT_VERSION=1.0.0 for testing\npurpose) with below log filters in /etc/libvirt/libvirtd.conf (and\nrestared libvirt):\n\n    log_level = 1\n    log_outputs=\"1:file:/var/tmp/libvirtd.log\"\n\n    Find what QMP commands libvirt is sending to QEMU\n    log_filters=\"1:qemu_monitor\"\n\n\nLibvirt call sequence (More \n---------------------\n\n(1) virDomainGetXMLDesc\n(2) virDomainBlockJobAbort\n(3) virDomainUndefine\n(4) virDomainBlockRebase\n     - NOTE (from libvirt documentation): By default, the copy job runs\n       in the background, and consists of two phases: (a) the block\n       operation copies all data from the source, and during this phase,\n       the job can only be canceled to revert back to the source disk,\n       with no guarantees about the destination. (b) After phase (a0\n       completes, both the source and the destination remain mirrored\n       until a call to the block opertation with the --abort.\n\n(5) virDomainBlockJobAbort.\n\n\nTest\n----\n\nBoot a new Nova instance:\n\n    $ nova boot --flavor 1 --key_name oskey1 --image \\\n        cirros-0.3.0-x86_64-disk cvm1\n\nIssue a snapshot (this should trigger the _live_snapshot code path):\n\n    $ nova image-create --poll cvm1 snap1-cvm1\n\nEnsure that \"live snapshot\" _did_ take place by searching the\n'screen-n-cpu.log':\n\n    $ grep -i \"Beginning live snapshot process\" ../data/new/screen-logs/screen-n-cpu.log\n    2014-06-30 03:34:32.237 INFO nova.virt.libvirt.driver [req-b02ac5c5-7694-44ce-9185-129b91eaf6b5 admin admin] [instance: 4b62dab0-0efe-4125-84ec-3e24d3371082] Beginning live snapshot process\n    $\n\n\nLibvirt logs\n------------\n\n(1) Save copy of libvirt XML(virDomainGetXMLDesc): \n----\n2014-06-30 09:08:13.586+0000: 8470: debug : virDomainGetXMLDesc:4344 : dom=0x7f2994000fd0, (VM: name=instance-00000001, uuid=4b62dab0-0efe-4125-84ec-3e24d3371082), flags=0\n----\n\n\n(2)  Issue a BlockJobAbort (virDomainBlockJobAbort), so that any prior active block operation on that disk will be cancelled:\n----\n2014-06-30 09:08:13.632+0000: 8470: debug : virDomainBlockJobAbort:19492 : dom=0x7f2994000af0, (VM: name=instance-00000001, uuid=4b62dab0-0efe-4125-84ec-3e24d3371082), disk=/home/kashyapc/src/openstack/data/nova/instances/4b62dab0-0efe-4125-84ec-3e24d3371082/disk, flags=0\n----\n\n\n(3) Undefining the running libvirt domain (virDomainUndefine), to make it transient[*]:\n----\n2014-06-30 09:08:14.069+0000: 8471: debug : virDomainUndefine:8683 : dom=0x7f29a0000910, (VM: name=instance-00000001, uuid=4b62dab0-0efe-4125-84ec-3e24d3371082)\n2014-06-30 09:08:14.069+0000: 8471: info : qemuDomainUndefineFlags:6334 : Undefining domain 'instance-00000001'\n----\n\nWe'll define the guest again, further below from the saved copy from step(1).\n\n[*] Reasoning for making the domain transient: BlockRebase ('blockcopy')\njobs last forever until canceled, which implies that they should last\nacross domain restarts if the domain were persistent. But, QEMU doesn't\nyet provide a way to restart a copy job on domain restart (while\nmirroring is still intact). So the trick is to temporarily make the\ndomain transient.\n\n\n(4) Invoke libvirt BlockRebase (and mirroring):\n----\n2014-06-30 09:08:14.070+0000: 8470: debug : virDomainBlockRebase:19785 : dom=0x7f2994000e40, (VM: name=instance-00000001, uuid=4b62dab0-0efe-4125-84ec-3e24d3371082), disk=/home/kashyapc/src/openstack/data/nova/instances/4b62dab0-0efe-4125-84ec-3e24d3371082/disk, base=/home/kashyapc/src/openstack/data/nova/instances/snapshots/tmp9_j3S0/5644b17a7de6427993d12cbab6ca3205.delta, bandwidth=0, flags=b\n----\n\n(4.1) QEMU 'drive-mirror' in progress:\n[. . .]\n2014-06-30 09:08:14.071+0000: 8470: debug : qemuMonitorJSONCommandWithFd:286 : Send command '{\"execute\":\"drive-mirror\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"target\":\"/home/kashyapc/src/openstack/data/nova/\ninstances/snapshots/tmp9_j3S0/5644b17a7de6427993d12cbab6ca3205.delta\",\"speed\":0,\"sync\":\"top\",\"mode\":\"existing\",\"format\":\"qcow2\"},\"id\":\"libvirt-10\"}' for write with FD -1\n2014-06-30 09:08:14.071+0000: 8470: debug : virEventPollUpdateHandle:152 : EVENT_POLL_UPDATE_HANDLE: watch=10 events=15\n2014-06-30 09:08:14.071+0000: 8470: debug : virEventPollInterruptLocked:729 : Interrupting\n2014-06-30 09:08:14.071+0000: 8470: debug : qemuMonitorSend:969 : QEMU_MONITOR_SEND_MSG: mon=0x7f299c000bc0 msg={\"execute\":\"drive-mirror\",\"arguments\":{\"device\":\"drive-virtio-disk0\",\"target\":\"/home/kashyapc/src/o\npenstack/data/nova/instances/snapshots/tmp9_j3S0/5644b17a7de6427993d12cbab6ca3205.delta\",\"speed\":0,\"sync\":\"top\",\"mode\":\"existing\",\"format\":\"qcow2\"},\"id\":\"libvirt-10\"}\n fd=-1\n[. . .]\n\n\n(5) Issue Abort, so that block mirroring can be completed gracefully:\n----\n2014-06-30 09:08:14.615+0000: 8472: debug : virDomainBlockJobAbort:19492 : dom=0x7f299c001670, (VM: name=instance-00000001, uuid=4b62dab0-0efe-4125-84ec-3e24d3371082), disk=/home/kashyapc/src/openstack/data/nova/instances/4b62dab0-0efe-4125-84ec-3e24d3371082/disk, flags=0\n[. . .]\n2014-06-30 09:08:14.618+0000: 8466: debug : qemuMonitorIOWrite:507 : QEMU_MONITOR_IO_WRITE: mon=0x7f299c000bc0 buf={\"execute\":\"block-job-cancel\",\"arguments\":{\"device\":\"drive-virtio-disk0\"},\"id\":\"libvirt-13\"}\n len=94 ret=94 errno=11\n----\n\n(6)\n----\n2014-06-30 09:08:14.719+0000: 8470: debug : virDomainDefineXML:8639 : conn=0x7f2994000c50, xml=<domain type='kvm' id='2'>\n  <name>instance-00000001</name>\n[. . .]\n2014-06-30 09:08:14.764+0000: 8470: info : qemuDomainDefineXML:6248 : Creating domain 'instance-00000001'\n----\n", 
            "date_created": "2014-06-30 10:20:55.650070+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "I've added some more debugging to the live snapshot code in this change:\n\n  https://review.openstack.org/#/c/103066/\n\nWhen it failed in this test run:\n\n  http://logs.openstack.org/66/103066/7/check/check-tempest-dsvm-postgres-full/02e97b1\n\nI see\n\n2014-06-30 11:55:55.398+0000: 18078: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f1315ffa390, flags=0\n2014-06-30 11:55:55.415 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=0 end=25165824 \n\n2014-06-30 11:55:56.074+0000: 18071: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f13256ea390, flags=0\n2014-06-30 11:55:56.094 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=25165824 end=25165824 \n\n\nThis shows that as far as virDomainGetBlockJobInfo is concerned, the job has completed copying the data in not very much time at all, which seems reasonable considering it is a cirros image.\n\nWe then go into a virDomainBlockJobAbort call to finish the snapshot operation:\n\n  2014-06-30 11:55:56.127+0000: 18070: debug : virDomainBlockJobAbort:19364 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, flags=0\n\nThis should take a fraction of a second, but after 3 minute it still isn't done. Tempest gets fed up waiting and so issues a call to destroy the guest:\n\n  2014-06-30 11:59:10.341+0000: 18090: debug : virDomainDestroy:2173 : dom=0x7f12f0002910, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b)\n\nShortly thereafter QEMU is dead and the virDomainBlockJobAbort call returns, obviously with an error,\n\n2014-06-30 11:59:21.279 17542 TRACE nova.compute.manager [instance: b1e3c5de-af31-4a4d-94dd-ef382936583b] libvirtError: Unable to read from monitor: Connection reset by peer\n\n\nSo, based on this debug info I think that Nova is doing the right thing, and this is probably a bug in QEMU (or possibly, but unlikely, a bug in libvirt).  My inclination is that QEMU is basically hanging in the block job abort call, due to some fairly infrequently hit race condition.\n", 
            "date_created": "2014-06-30 14:01:04.980908+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I've added some more debugging to the live snapshot code in this change:\n\n  https://review.openstack.org/#/c/103066/\n\nWhen it failed in this test run:\n\n  http://logs.openstack.org/66/103066/7/check/check-tempest-dsvm-postgres-full/02e97b1\n\nI see\n\n2014-06-30 11:55:55.398+0000: 18078: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f1315ffa390, flags=0\n2014-06-30 11:55:55.415 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=0 end=25165824 \n\n2014-06-30 11:55:56.074+0000: 18071: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f13256ea390, flags=0\n2014-06-30 11:55:56.094 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=25165824 end=25165824 \n\n\nThis shows that as far as virDomainGetBlockJobInfo is concerned, the job has completed copying the data in not very much time at all, which seems reasonable considering it is a cirros image.\n\nWe then go into a virDomainBlockJobAbort call to finish the snapshot operation:\n\n  2014-06-30 11:55:56.127+0000: 18070: debug : virDomainBlockJobAbort:19364 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, flags=0\n\nThis should take a fraction of a second, but after 3 minute it still isn't done. Tempest gets fed up waiting and so issues a call to destroy the guest:\n\n  2014-06-30 11:59:10.341+0000: 18090: debug : virDomainDestroy:2173 : dom=0x7f12f0002910, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b)\n\nShortly thereafter QEMU is dead and the virDomainBlockJobAbort call returns, obviously with an error,\n\n2014-06-30 11:59:21.279 17542 TRACE nova.compute.manager [instance: b1e3c5de-af31-4a4d-94dd-ef382936583b] libvirtError: Unable to read from monitor: Connection reset by peer\n\n\nSo, based on this debug info I think that Nova is doing the right thing, and this is probably a bug in QEMU (or possibly, but unlikely, a bug in libvirt).  My inclination is that QEMU is basically hanging in the block job abort call, due to some fairly infrequently hit race condition.\n", 
            "date_created": "2014-06-30 14:01:06.459679+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I've added some more debugging to the live snapshot code in this change:\n\n  https://review.openstack.org/#/c/103066/\n\nWhen it failed in this test run:\n\n  http://logs.openstack.org/66/103066/7/check/check-tempest-dsvm-postgres-full/02e97b1\n\nI see\n\n2014-06-30 11:55:55.398+0000: 18078: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f1315ffa390, flags=0\n2014-06-30 11:55:55.415 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=0 end=25165824 \n\n2014-06-30 11:55:56.074+0000: 18071: debug : virDomainGetBlockJobInfo:19415 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, info=0x7f13256ea390, flags=0\n2014-06-30 11:55:56.094 WARNING nova.virt.libvirt.driver [req-3e682ad2-5af5-47d2-a72d-9bac23e8c2bc ListImageFiltersTestJSON-1350287421 ListImageFiltersTestJSON-1927810029] blockJobInfo snapshot cur=25165824 end=25165824 \n\n\nThis shows that as far as virDomainGetBlockJobInfo is concerned, the job has completed copying the data in not very much time at all, which seems reasonable considering it is a cirros image.\n\nWe then go into a virDomainBlockJobAbort call to finish the snapshot operation:\n\n  2014-06-30 11:55:56.127+0000: 18070: debug : virDomainBlockJobAbort:19364 : dom=0x7f13200018f0, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b), disk=/opt/stack/data/nova/instances/b1e3c5de-af31-4a4d-94dd-ef382936583b/disk, flags=0\n\nThis should take a fraction of a second, but after 3 minute it still isn't done. Tempest gets fed up waiting and so issues a call to destroy the guest:\n\n  2014-06-30 11:59:10.341+0000: 18090: debug : virDomainDestroy:2173 : dom=0x7f12f0002910, (VM: name=instance-00000020, uuid=b1e3c5de-af31-4a4d-94dd-ef382936583b)\n\nShortly thereafter QEMU is dead and the virDomainBlockJobAbort call returns, obviously with an error,\n\n2014-06-30 11:59:21.279 17542 TRACE nova.compute.manager [instance: b1e3c5de-af31-4a4d-94dd-ef382936583b] libvirtError: Unable to read from monitor: Connection reset by peer\n\n\nSo, based on this debug info I think that Nova is doing the right thing, and this is probably a bug in QEMU (or possibly, but unlikely, a bug in libvirt).  My inclination is that QEMU is basically hanging in the block job abort call, due to some fairly infrequently hit race condition.\n", 
            "date_created": "2014-06-30 14:12:33.309512+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I have managed to capture a failure with verbose libvirtd.log  enabled.\n\nhttp://logs.openstack.org/66/103066/11/check/check-tempest-dsvm-postgres-full/d8d3b5b/logs/libvirtd.txt.gz\nhttp://logs.openstack.org/66/103066/11/check/check-tempest-dsvm-postgres-full/d8d3b5b/logs/screen-n-cpu.txt.gz\n\n\n2014-07-09 11:05:50.701+0000: 21774: debug : virDomainBlockJobAbort:19364 : dom=0x7fc73c010700, (VM: name=instance-0000001d, uuid=6cfca6f5-8741-4844-a045-a14a644dd82d), disk=/opt/stack/data/nova/instances/6cfca6f5-8741-4844-a045-a14a644dd82d/disk, flags=0\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuDomainObjBeginJobInternal:1050 : Starting job: modify (async=none vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuDomainObjBeginJobInternal:1092 : Started job: modify (async=none vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuMonitorBlockJob:3314 : mon=0x7fc73c010ad0, device=drive-virtio-disk0, base=<null>, bandwidth=0M, info=(nil), mode=0, modern=1\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"block-job-cancel\",\"arguments\":{\"device\":\"drive-virtio-disk0\"},\"id\":\"libvirt-14\"}' for write with FD -1\n2014-07-09 11:05:50.701+0000: 21774: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7fc73c010ad0 msg={\"execute\":\"block-job-cancel\",\"arguments\":{\"device\":\"drive-virtio-disk0\"},\"id\":\"libvirt-14\"}\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=0 rxObject=0x7fc76096cac0\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuMonitorBlockJob:3314 : mon=0x7fc73c010ad0, device=drive-virtio-disk0, base=<null>, bandwidth=0M, info=0x7fc7572be9a0, mode=1, modern=1\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"query-block-jobs\",\"id\":\"libvirt-15\"}' for write with FD -1\n2014-07-09 11:05:50.705+0000: 21774: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7fc73c010ad0 msg={\"execute\":\"query-block-jobs\",\"id\":\"libvirt-15\"}\n2014-07-09 11:05:50.709+0000: 21774: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=0 rxObject=0x7fc76099cce0\n2014-07-09 11:05:50.709+0000: 21774: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.759+0000: 21774: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.759+0000: 21774: debug : qemuMonitorBlockJob:3314 : mon=0x7fc73c010ad0, device=drive-virtio-disk0, base=<null>, bandwidth=0M, info=0x7fc7572be9a0, mode=1, modern=1\n2014-07-09 11:05:50.759+0000: 21774: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"query-block-jobs\",\"id\":\"libvirt-16\"}' for write with FD -1\n2014-07-09 11:05:50.759+0000: 21774: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7fc73c010ad0 msg={\"execute\":\"query-block-jobs\",\"id\":\"libvirt-16\"}\n2014-07-09 11:05:50.762+0000: 21774: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=0 rxObject=0x7fc760999970\n2014-07-09 11:05:50.762+0000: 21774: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.812+0000: 21774: debug : qemuDomainObjEnterMonitorInternal:1278 : Entering monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:05:50.812+0000: 21774: debug : qemuMonitorBlockJob:3314 : mon=0x7fc73c010ad0, device=drive-virtio-disk0, base=<null>, bandwidth=0M, info=0x7fc7572be9a0, mode=1, modern=1\n2014-07-09 11:05:50.812+0000: 21774: debug : qemuMonitorJSONCommandWithFd:264 : Send command '{\"execute\":\"query-block-jobs\",\"id\":\"libvirt-17\"}' for write with FD -1\n2014-07-09 11:05:50.812+0000: 21774: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7fc73c010ad0 msg={\"execute\":\"query-block-jobs\",\"id\":\"libvirt-17\"}\n2014-07-09 11:09:07.169+0000: 21772: debug : virDomainDestroy:2173 : dom=0x7fc744005930, (VM: name=instance-0000001d, uuid=6cfca6f5-8741-4844-a045-a14a644dd82d)\n2014-07-09 11:09:07.169+0000: 21772: debug : qemuProcessKill:4189 : vm=0x7fc740009ee0 name=instance-0000001d pid=5820 flags=1\n2014-07-09 11:09:07.169+0000: 21772: debug : virProcessKillPainfully:274 : vpid=5820 force=1\n2014-07-09 11:09:17.184+0000: 21774: debug : qemuMonitorSend:971 : Send command resulted in error Unable to read from monitor: Connection reset by peer\n2014-07-09 11:09:17.184+0000: 21774: debug : qemuMonitorJSONCommandWithFd:269 : Receive command reply ret=-1 rxObject=(nil)\n2014-07-09 11:09:17.393+0000: 21773: warning : qemuDomainObjBeginJobInternal:1119 : Cannot start job (query, none) for domain instance-0000001d; current job is (modify, none) owned by (21774, 0)\n2014-07-09 11:09:17.395+0000: 21774: debug : qemuDomainObjExitMonitorInternal:1301 : Exited monitor (mon=0x7fc73c010ad0 vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:09:17.395+0000: 21774: debug : qemuDomainObjEndJob:1206 : Stopping job: modify (async=none vm=0x7fc740009ee0 name=instance-0000001d)\n2014-07-09 11:09:17.395+0000: 21774: debug : virDomainFree:2270 : dom=0x7fc73c010700, (VM: name=instance-0000001d, uuid=6cfca6f5-8741-4844-a045-a14a644dd82d)\n\n\nThis clearly shows that at 11:05:50, libvirtd sends a request to QEMU 'query-block-jobs'. Approx three minutes libvirtd gets a 'virDomainDestroy' request from Nova, at which point we get the virDomainBlockAbort error due to death of QEMU.\n\nWe can see this virDomainDestroy call is triggered by Tempest's timeout\n\n2014-07-09 11:09:04.479 23996 ERROR tempest.api.compute.images.test_list_image_filters [-] setUpClass failed\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters Traceback (most recent call last):\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters   File \"tempest/api/compute/images/test_list_image_filters.py\", line 52, in setUpClass\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters     cls.server2['id'], wait_until='ACTIVE')\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters   File \"tempest/api/compute/base.py\", line 328, in create_image_from_server\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters     kwargs['wait_until'])\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters   File \"tempest/services/compute/json/images_client.py\", line 92, in wait_for_image_status\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters     waiters.wait_for_image_status(self, image_id, status)\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters   File \"tempest/common/waiters.py\", line 143, in wait_for_image_status\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters     raise exceptions.TimeoutException(message)\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters TimeoutException: Request timed out\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters Details: (ListImageFiltersTestJSON:setUpClass) Image 8a7d72cc-0715-42d7-894a-83417051f4b9 failed to reach ACTIVE status within the required time (196 s). Current status: SAVING.\n2014-07-09 11:09:04.479 23996 TRACE tempest.api.compute.images.test_list_image_filters \n\n\nIn the time where libvirtd is waiting for the QEMU response, there are many other libvirt API calls taking place, so it doesn't look like libvirtd itself is misbehaving. It also successfully sees the EOF on the QEMU monitor socket, so we can see the event loop is still listening for the reply from QEMU.\n\nSo I'm still left thinking this is a QEMU bug of some sort.", 
            "date_created": "2014-07-09 13:53:29.283883+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Ping.\n\nI recall DanB noted somewhere in a mailing list thread and the QEMU devs I spoke to also suggested -- one of the possible next steps to investigate this is to have GDB attached to the hanging QEMU in  the CI gate to get stack traces out of it.", 
            "date_created": "2014-09-10 12:33:13.721492+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "We've basically just disabled this feature until someone can dig into why qemu doesn't work with it ", 
            "date_created": "2014-09-12 14:37:29.255385+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "This came up in the ML and in openstack-nova IRC chat today.\n\nI'm going to look into adding some sane libvirt/qemu version checking to the feature as well as wrapping the feature with some type of config option so that its possible to conditionally enable it on different platforms.", 
            "date_created": "2015-01-14 22:27:35.576119+00:00", 
            "author": "https://api.launchpad.net/1.0/~cfb-n"
        }, 
        {
            "content": "This came up again today in openstack-nova and recently on the ML.\n\nI'll look at adding some version guarding around this feature. It defiantly works on precise with libvirtd 1.1.3.5 and qemu 1.5 as we have been running it in production with those version. So I think some basic version guarding should be sufficient.\n\nmriedem has also requested the ability to conditionally enable it on different OS gates for easier testing so I will work on that as well.", 
            "date_created": "2015-01-14 22:31:16.597793+00:00", 
            "author": "https://api.launchpad.net/1.0/~cfb-n"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/147332", 
            "date_created": "2015-01-14 23:31:40.283046+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/150639", 
            "date_created": "2015-01-28 00:38:34.818616+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/150639\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=3f69bca12c484a08d8f80c1db581e12607f13f7a\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3f69bca12c484a08d8f80c1db581e12607f13f7a\nAuthor: Tony Breeds <email address hidden>\nDate:   Tue Jan 27 14:05:02 2015 -0800\n\n    Use a workarounds group option to disable live snaphots.\n    \n    Create a workarounds option to disable live snapshotting rather than\n    hack MIN_LIBVIRT_LIVESNAPSHOT_VERSION.\n    \n    DocImpact\n    Related-Bug: #1334398\n    Change-Id: Iee9afc0afaffa1de509c357fcb9fdb18c650a70b\n", 
            "date_created": "2015-02-06 20:47:14.447480+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "https://review.openstack.org/#/c/171795/ adds the fedora 21 job to nova's experimental queue so we can test changes against newer libvirt/qemu than what's in ubuntu 14.04.", 
            "date_created": "2015-04-08 20:08:02.771115+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Joe Gordon (<email address hidden>) on branch: master\nReview: https://review.openstack.org/147332\nReason: This review is > 4 weeks without comment and currently blocked by a core reviewer with a -2. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and contacting the reviewer with the -2 on this review to ensure you address their concerns.", 
            "date_created": "2015-05-13 17:07:48.340214+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Solving an inconsistency: The bug is 'In Progress' but without an assignee. I set the status back to the last known status before the change to 'In Progress'. \n\nFeel free to assign the bug to yourself. If you do so, please set it to 'In Progress'.", 
            "date_created": "2015-07-14 15:01:35.788587+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "At some point soon in Newton we'll have ubuntu 16.04 in the gate jobs with libvirt 1.3.1, we should try turning this back on and see if it works.", 
            "date_created": "2016-04-29 22:35:18.983646+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:49:45.247963+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "CONFIRMED FOR: NEWTON", 
            "date_created": "2016-08-31 14:52:24.731549+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ]
}
{
    "status": "In Progress", 
    "last_updated": "2017-06-28 12:04:37.767837+00:00", 
    "description": "Description\n===========\nWhen using high speed disks and NFS as storage backend, during high loads the nfs mounts hang indefinitely. \n\nSteps to reproduce\n==================\nA chronological list of steps which will bring off the\nissue you noticed:\n* Spin up a VM with a mounted cinder volume from an NFS backend\n* Generate some read/write load \n* Occationally the loopback NFS mounts will hang. The machine and everything else using that mount will also hang.\n\n\nExpected result\n===============\nThe system should run stably \n\nActual result\n=============\nOccasionally  , usually during higher load the system will hang.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running. See the following\nOpenstack Kilo\n\nopenstack-nova-compute-2015.1.1-1.el7.noarch\nopenstack-nova-cert-2015.1.1-1.el7.noarch\npython-nova-2015.1.1-1.el7.noarch\nopenstack-nova-console-2015.1.1-1.el7.noarch\nopenstack-nova-novncproxy-2015.1.1-1.el7.noarch\nopenstack-nova-common-2015.1.1-1.el7.noarch\npython-novaclient-2.23.0-1.el7.noarch\nopenstack-nova-scheduler-2015.1.1-1.el7.noarch\nopenstack-nova-api-2015.1.1-1.el7.noarch\nopenstack-nova-conductor-2015.1.1-1.el7.noarch\n\n\n2. Which hypervisor did you use?\n  Libvirt + KVM,\n\n2. Which storage type did you use?\n   NFS\n\n3. Which networking type did you use?\n    Neutron with OpenVSwitch\n\nLogs & Configs\n==============\n\nNova.conf:\n[DEFAULT]\nnotification_driver=ceilometer.compute.nova_notifier\nnotification_driver=nova.openstack.common.notifier.rpc_notifier\nnotification_driver =\nnotification_topics=notifications\nrpc_backend=rabbit\ninternal_service_availability_zone=internal\ndefault_availability_zone=nova\nnotify_api_faults=False\nstate_path=/openstack/nova\nreport_interval=10\nenabled_apis=ec2,osapi_compute,metadata\nec2_listen=0.0.0.0\nec2_workers=2\nosapi_compute_listen=0.0.0.0\nosapi_compute_workers=2\nmetadata_listen=0.0.0.0\nmetadata_workers=2\ncompute_manager=nova.compute.manager.ComputeManager\nservice_down_time=60\nrootwrap_config=/etc/nova/rootwrap.conf\nauth_strategy=keystone\nuse_forwarded_for=False\nnovncproxy_host=192.168.0.1\nnovncproxy_port=6080\nallow_resize_to_same_host=true\nblock_device_allocate_retries=1560\nheal_instance_info_cache_interval=60\nreserved_host_memory_mb=512\nnetwork_api_class=nova.network.neutronv2.api.API\ndefault_floating_pool=public\nforce_snat_range=0.0.0.0/0\nmetadata_host=192.168.0.1\ndhcp_domain=novalocal\nsecurity_group_api=neutron\ndebug=True\nverbose=True\nlog_dir=/var/log/nova\nuse_syslog=False\ncpu_allocation_ratio=16.0\nram_allocation_ratio=1.5\nscheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,CoreFilter\nscheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler\ncompute_driver=libvirt.LibvirtDriver\nvif_plugging_is_fatal=True\nvif_plugging_timeout=300\nfirewall_driver=nova.virt.firewall.NoopFirewallDriver\nremove_unused_base_images=true\nforce_raw_images=True\nnovncproxy_base_url=http://0.0.0.0:6080/vnc_auto.html\nvncserver_listen=192.168.0.1\nvncserver_proxyclient_address=127.0.0.1\nvnc_enabled=True\nvnc_keymap=en-us\nvolume_api_class=nova.volume.cinder.API\namqp_durable_queues=False\nsql_connection=mysql:XXXXXXXXXXX\nlock_path=/openstack/nova/tmp\nosapi_volume_listen=0.0.0.0\n[api_database]\n[barbican]\n[cells]\n[cinder]\n[conductor]\nworkers=2\n[database]\n[ephemeral_storage_encryption]\n[glance]\napi_servers=192.168.0.1:9292\n[guestfs]\n[hyperv]\n[image_file_url]\n[ironic]\n[keymgr]\n[keystone_authtoken]\nauth_uri=http://192.168.0.1:5000/v2.0\nidentity_uri=http://192.168.0.1:35357\nadmin_user=nova\nadmin_password=XXXXXXx\n[libvirt]\nvirt_type=kvm\ninject_password=False\ninject_key=False\ninject_partition=-1\nlive_migration_uri=qemu+tcp://nova@%s/system\ncpu_mode=host-model\ndisk_cachemodes=file=writethrough,block=writethrough\nnfs_mount_options=rw,hard,intr,nolock,vers=4.1,timeo=10\nvif_driver=nova.virt.libvirt.vif.LibvirtGenericVIFDriver\n[metrics]\n[neutron]\n......\n\n\nCinder.conf:\n\n[nfs_ssd]\nnfs_used_ratio=0.95\nnfs_oversub_ratio=10.0\nvolume_driver=cinder.volume.drivers.nfs.NfsDriver\nnfs_shares_config=/etc/cinder/nfs_shares_ssd.conf\nvolume_backend_name=nfs_ssd\nquota_volumes = -1\nnfs_mount_options=rw,hard,intr,nolock\n\n\n- No notable output in nova log\n\n\n\n- System log /dmesg after a hang:\n\nNov 24 04:10:41 openstack1.itgix.com kernel: INFO: task qemu-kvm:11726 blocked for more than 120 seconds.\nNov 24 04:10:41 openstack1.itgix.com kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nNov 24 04:10:41 openstack1.itgix.com kernel: qemu-kvm        D ffff88118b1b1f60     0 11726      1 0x00000080\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff880da4c77c40 0000000000000082 ffff881184b86780 ffff880da4c77fd8\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff880da4c77fd8 ffff880da4c77fd8 ffff881184b86780 ffff88118b1b1f58\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff88118b1b1f5c ffff881184b86780 00000000ffffffff ffff88118b1b1f60\nNov 24 04:10:41 openstack1.itgix.com kernel: Call Trace:\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163bf29>] schedule_preempt_disabled+0x29/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81639c25>] __mutex_lock_slowpath+0xc5/0x1c0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff812fbff4>] ? timerqueue_del+0x24/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163908f>] mutex_lock+0x1f/0x2f\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8116b60a>] generic_file_aio_write+0x4a/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffffa06dc03b>] nfs_file_write+0xbb/0x1d0 [nfs]\nNov 24 04:10:41 openstack1.itgix.com kernel: Call Trace:\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163bf29>] schedule_preempt_disabled+0x29/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81639c25>] __mutex_lock_slowpath+0xc5/0x1c0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163908f>] mutex_lock+0x1f/0x2f\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8116b60a>] generic_file_aio_write+0x4a/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffffa06dc03b>] nfs_file_write+0xbb/0x1d0 [nfs]\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811dde5d>] do_sync_write+0x8d/0xd0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811de67d>] vfs_write+0xbd/0x1e0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811df2d2>] SyS_pwrite64+0x92/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b", 
    "tags": [
        "nfs", 
        "openstack-version.kilo", 
        "volumes"
    ], 
    "importance": "Low", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1646896", 
    "owner": "None", 
    "id": 1646896, 
    "index": 2124, 
    "openned": "2016-12-02 16:47:05.808082+00:00", 
    "created": "2016-12-02 16:47:05.808082+00:00", 
    "title": "System hangs when using NFS storage backend with loopback mounts", 
    "comments": [
        {
            "content": "Description\n===========\nWhen using high speed disks and NFS as storage backend, during high loads the nfs mounts hang indefinitely. \n\nSteps to reproduce\n==================\nA chronological list of steps which will bring off the\nissue you noticed:\n* Spin up a VM with a mounted cinder volume from an NFS backend\n* Generate some read/write load \n* Occationally the loopback NFS mounts will hang. The machine and everything else using that mount will also hang.\n\n\nExpected result\n===============\nThe system should run stably \n\nActual result\n=============\nOccasionally  , usually during higher load the system will hang.\n\nEnvironment\n===========\n1. Exact version of OpenStack you are running. See the following\nOpenstack Kilo\n\nopenstack-nova-compute-2015.1.1-1.el7.noarch\nopenstack-nova-cert-2015.1.1-1.el7.noarch\npython-nova-2015.1.1-1.el7.noarch\nopenstack-nova-console-2015.1.1-1.el7.noarch\nopenstack-nova-novncproxy-2015.1.1-1.el7.noarch\nopenstack-nova-common-2015.1.1-1.el7.noarch\npython-novaclient-2.23.0-1.el7.noarch\nopenstack-nova-scheduler-2015.1.1-1.el7.noarch\nopenstack-nova-api-2015.1.1-1.el7.noarch\nopenstack-nova-conductor-2015.1.1-1.el7.noarch\n\n\n2. Which hypervisor did you use?\n  Libvirt + KVM,\n\n2. Which storage type did you use?\n   NFS\n\n3. Which networking type did you use?\n    Neutron with OpenVSwitch\n\nLogs & Configs\n==============\n\nNova.conf:\n[DEFAULT]\nnotification_driver=ceilometer.compute.nova_notifier\nnotification_driver=nova.openstack.common.notifier.rpc_notifier\nnotification_driver =\nnotification_topics=notifications\nrpc_backend=rabbit\ninternal_service_availability_zone=internal\ndefault_availability_zone=nova\nnotify_api_faults=False\nstate_path=/openstack/nova\nreport_interval=10\nenabled_apis=ec2,osapi_compute,metadata\nec2_listen=0.0.0.0\nec2_workers=2\nosapi_compute_listen=0.0.0.0\nosapi_compute_workers=2\nmetadata_listen=0.0.0.0\nmetadata_workers=2\ncompute_manager=nova.compute.manager.ComputeManager\nservice_down_time=60\nrootwrap_config=/etc/nova/rootwrap.conf\nauth_strategy=keystone\nuse_forwarded_for=False\nnovncproxy_host=192.168.0.1\nnovncproxy_port=6080\nallow_resize_to_same_host=true\nblock_device_allocate_retries=1560\nheal_instance_info_cache_interval=60\nreserved_host_memory_mb=512\nnetwork_api_class=nova.network.neutronv2.api.API\ndefault_floating_pool=public\nforce_snat_range=0.0.0.0/0\nmetadata_host=192.168.0.1\ndhcp_domain=novalocal\nsecurity_group_api=neutron\ndebug=True\nverbose=True\nlog_dir=/var/log/nova\nuse_syslog=False\ncpu_allocation_ratio=16.0\nram_allocation_ratio=1.5\nscheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,CoreFilter\nscheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler\ncompute_driver=libvirt.LibvirtDriver\nvif_plugging_is_fatal=True\nvif_plugging_timeout=300\nfirewall_driver=nova.virt.firewall.NoopFirewallDriver\nremove_unused_base_images=true\nforce_raw_images=True\nnovncproxy_base_url=http://0.0.0.0:6080/vnc_auto.html\nvncserver_listen=192.168.0.1\nvncserver_proxyclient_address=127.0.0.1\nvnc_enabled=True\nvnc_keymap=en-us\nvolume_api_class=nova.volume.cinder.API\namqp_durable_queues=False\nsql_connection=mysql:XXXXXXXXXXX\nlock_path=/openstack/nova/tmp\nosapi_volume_listen=0.0.0.0\n[api_database]\n[barbican]\n[cells]\n[cinder]\n[conductor]\nworkers=2\n[database]\n[ephemeral_storage_encryption]\n[glance]\napi_servers=192.168.0.1:9292\n[guestfs]\n[hyperv]\n[image_file_url]\n[ironic]\n[keymgr]\n[keystone_authtoken]\nauth_uri=http://192.168.0.1:5000/v2.0\nidentity_uri=http://192.168.0.1:35357\nadmin_user=nova\nadmin_password=XXXXXXx\n[libvirt]\nvirt_type=kvm\ninject_password=False\ninject_key=False\ninject_partition=-1\nlive_migration_uri=qemu+tcp://nova@%s/system\ncpu_mode=host-model\ndisk_cachemodes=file=writethrough,block=writethrough\nnfs_mount_options=rw,hard,intr,nolock,vers=4.1,timeo=10\nvif_driver=nova.virt.libvirt.vif.LibvirtGenericVIFDriver\n[metrics]\n[neutron]\n......\n\n\nCinder.conf:\n\n[nfs_ssd]\nnfs_used_ratio=0.95\nnfs_oversub_ratio=10.0\nvolume_driver=cinder.volume.drivers.nfs.NfsDriver\nnfs_shares_config=/etc/cinder/nfs_shares_ssd.conf\nvolume_backend_name=nfs_ssd\nquota_volumes = -1\nnfs_mount_options=rw,hard,intr,nolock\n\n\n- No notable output in nova log\n\n\n\n- System log /dmesg after a hang:\n\nNov 24 04:10:41 openstack1.itgix.com kernel: INFO: task qemu-kvm:11726 blocked for more than 120 seconds.\nNov 24 04:10:41 openstack1.itgix.com kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nNov 24 04:10:41 openstack1.itgix.com kernel: qemu-kvm        D ffff88118b1b1f60     0 11726      1 0x00000080\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff880da4c77c40 0000000000000082 ffff881184b86780 ffff880da4c77fd8\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff880da4c77fd8 ffff880da4c77fd8 ffff881184b86780 ffff88118b1b1f58\nNov 24 04:10:41 openstack1.itgix.com kernel:  ffff88118b1b1f5c ffff881184b86780 00000000ffffffff ffff88118b1b1f60\nNov 24 04:10:41 openstack1.itgix.com kernel: Call Trace:\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163bf29>] schedule_preempt_disabled+0x29/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81639c25>] __mutex_lock_slowpath+0xc5/0x1c0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff812fbff4>] ? timerqueue_del+0x24/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163908f>] mutex_lock+0x1f/0x2f\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8116b60a>] generic_file_aio_write+0x4a/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffffa06dc03b>] nfs_file_write+0xbb/0x1d0 [nfs]\nNov 24 04:10:41 openstack1.itgix.com kernel: Call Trace:\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163bf29>] schedule_preempt_disabled+0x29/0x70\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81639c25>] __mutex_lock_slowpath+0xc5/0x1c0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8163908f>] mutex_lock+0x1f/0x2f\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff8116b60a>] generic_file_aio_write+0x4a/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffffa06dc03b>] nfs_file_write+0xbb/0x1d0 [nfs]\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811dde5d>] do_sync_write+0x8d/0xd0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811de67d>] vfs_write+0xbd/0x1e0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff811df2d2>] SyS_pwrite64+0x92/0xc0\nNov 24 04:10:41 openstack1.itgix.com kernel:  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b", 
            "date_created": "2016-12-02 16:47:05.808082+00:00", 
            "author": "https://api.launchpad.net/1.0/~mihail.itgix"
        }, 
        {
            "content": "I will post how we mitigate the problem when I formulate it.", 
            "date_created": "2016-12-02 16:50:17.323080+00:00", 
            "author": "https://api.launchpad.net/1.0/~mihail.itgix"
        }, 
        {
            "content": "I'm not sure what nova can be doing with this. Might also be a cinder issue. I'm not sure if there are qemu or nfs configs to adjust for this, or image meta / flavor / volume type extra specs to use to tune this.", 
            "date_created": "2016-12-03 18:37:36.503081+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "We have a reason to beleve that the root cause is the nfs client and server hitting a deadlock, because they are running on the same machine using the same memory resources. This is due to the loopback NFS mounts, as the way nova operates is to mount NFS even for local volumes.\n\nMore information can be found in this article: https://lwn.net/Articles/595652/\n\nWhile this is a purely NFS issue, there are ways to mitigate the issue in nova as resolution in NFS package is nowhere to be seen for years.\n\n\nOur proposal is the following. Add a check in nova to determine if the volume is from a local cinder storage node. If storage is local do a \"bind\" mount instead of a normal NFS client mount. This way we work around the NFS loopback deadlock problem.\nThe imperfection of the change is that a bind mount cannot be discovered by the is_mounted method and will be mounted again on every request. Also it will currently not pick up a local mount if the nfs_shares address is a hostname instead of an IP address. \n\nHere is the diff of the proposed change:\n\n49,55d48\n< \"\"\"Misho : Needed for nfs client to recognize local shares \"\"\"\n< import socket\n< import fcntl\n< import struct\n< import netifaces\n< import pprint\n< \n937d929\n<         LOG.debug(\"Checking if the mount path is mounted: \" + mount_path)\n945,964c937\n<         address = []                                                                                                                                                      \n<                                                                                                                                                                           \n<         for iface in netifaces.interfaces():                                                                                                                              \n<             try:                                                                                                                                                          \n<                 alladdr = netifaces.ifaddresses(iface)[netifaces.AF_INET]                                                                                                 \n<                 ## [{'peer': '127.0.0.1', 'netmask': '255.0.0.0', 'addr': '127.0.0.1'} ...                                                                                \n<                 for theaddr in alladdr:                                                                                                                                   \n<                     address.append( theaddr['addr'] )  # '192.168.0.110'                                                                                                  \n<             except:                                                                                                                                                       \n<                 LOG.debug(\"Couldn't get address for :\"+iface)                                                                                                             \n<                                                                                                                                                                           \n<                                                                                                                                                                           \n<         islocal = False                                                                                                                                                   \n<         for addr in address:                                                                                                                                              \n<             if addr in nfs_share :                                                                                                                                        \n<                 nfs_cmd = ['mount', '-o', 'bind']                                                                                                                         \n<                 nfsserver, nfs_path = nfs_share.split(':')\n<                 islocal = True\n<                 LOG.warn(\"address \"+str(addr)+ \" found in mount string.\")\n<                 nfs_cmd.extend([nfs_path, mount_path])\n---\n> \n966,975c939,944\n<         if islocal:\n<             LOG.warn(\"Address of share is local, do bind mount\")\n<             \n<         else:\n<             nfs_cmd = ['mount', '-t', 'nfs']\n<             if CONF.libvirt.nfs_mount_options is not None:\n<                 nfs_cmd.extend(['-o', CONF.libvirt.nfs_mount_options])\n<             if options:\n<                 nfs_cmd.extend(options.split(' '))\n<             nfs_cmd.extend([nfs_share, mount_path])\n---\n>         nfs_cmd = ['mount', '-t', 'nfs']\n>         if CONF.libvirt.nfs_mount_options is not None:\n>             nfs_cmd.extend(['-o', CONF.libvirt.nfs_mount_options])\n>         if options:\n>             nfs_cmd.extend(options.split(' '))\n>         nfs_cmd.extend([nfs_share, mount_path])", 
            "date_created": "2016-12-05 09:59:05.852885+00:00", 
            "author": "https://api.launchpad.net/1.0/~mihail.itgix"
        }, 
        {
            "content": "Please propose the fix via gerrit, and we can figure out from there if it's invasive enough to need a spec.", 
            "date_created": "2016-12-07 20:18:06.854878+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Submit under: https://review.openstack.org/422041 proposed fix for bug 1646896 - System hangs when using NFS storage backend ...", 
            "date_created": "2017-01-18 15:48:56.651679+00:00", 
            "author": "https://api.launchpad.net/1.0/~mihail.itgix"
        }, 
        {
            "content": "Added mitigation of the issue with duplicate bind mounts.\nPlease review latest patch set :  https://review.openstack.org/422041", 
            "date_created": "2017-06-07 12:17:34.069036+00:00", 
            "author": "https://api.launchpad.net/1.0/~mihail.itgix"
        }, 
        {
            "content": "Automatically discovered version kilo in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:59:41.127190+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ]
}
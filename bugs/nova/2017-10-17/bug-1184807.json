{
    "status": "Fix Released", 
    "last_updated": "2014-03-30 23:05:11.980776+00:00", 
    "description": "I am unable to get snapshots working in my dev setup with the VCDriver.  \n\nsnapshot API call claims to succeed, but we get an internal exception (below) and the snapshot stays  in 'queued' status in Horizon.  \n\nThe relevant code is here: https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L515\n\n It seems like the underlying snapshot succeeds, but the attempt to copy the disk afterward fails.  Browsing the datastore, I see that a vmware-tmp directory was created, but I do not see any files in it.  \n \n\n\nspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:04.615 DEBUG nova.virt.vmwareapi.driver [-] Task [CreateSnapshot_Task] (returnval){\n   value = \"task-123\"\n   _type = \"Task\"\n } status: success from (pid=4595) _poll_task /extraspace/stack/nova/nova/virt/vmwareapi/driver.py:576\n2013-05-27 17:15:04.615 DEBUG nova.virt.vmwareapi.vmops [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] [instance: 0d044e1b-074b-47de-9002-de5d87230aa5] Created Snapshot of the VM instance from (pid=4595) _create_vm_snapshot /extraspace/stack/nova/nova/virt/vmwareapi/vmops.py:477\n2013-05-27 17:15:04.616 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:04.616 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 43832e8692c64fdeabba8b34c531b682 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:04.617 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is ff2a695783ba459e8aef5a0097eefb95. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:04.981 DEBUG nova.openstack.common.lockutils [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Got semaphore \"compute_resources\" for method \"update_usage\"... from (pid=4595) inner /extraspace/stack/nova/nova/openstack/common/lockutils.py:186\n2013-05-27 17:15:04.982 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:04.982 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 8de1b35ad1624e4cbbd25dca3b7ff41e from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:04.983 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is 96b0cd1f45e84bc486863e9bab4b1b8f. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.174 DEBUG nova.virt.vmwareapi.vmops [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] [instance: 0d044e1b-074b-47de-9002-de5d87230aa5] Copying disk data before snapshot of the VM from (pid=4595) _copy_vmdk_content /extraspace/stack/nova/nova/virt/vmwareapi/vmops.py:522\n2013-05-27 17:15:05.222 WARNING nova.virt.vmwareapi.driver [-] Task [CopyVirtualDisk_Task] (returnval){\n   value = \"task-124\"\n   _type = \"Task\"\n } status: error The requested operation is not implemented by the server.\n2013-05-27 17:15:05.224 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.224 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 838e0c86a1b04e46856ed43797442f6f from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.225 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is bcb86dcabbb040bf81bfa0a0676e4b14. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 41c6cb58361f4686b717f3e3f3074178 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is c732adbadd424eb1ac56d2e259041600. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.624 DEBUG nova.openstack.common.lockutils [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Got semaphore \"compute_resources\" for method \"update_usage\"... from (pid=4595) inner /extraspace/stack/nova/nova/openstack/common/lockutils.py:186\n2013-05-27 17:15:05.625 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.626 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 206d42da6b5947caaaa3473d80744544 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.626 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is 2ab81258b1c8413e83eb278d4d3539a2. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.672 ERROR nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Exception during message handling\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/openstack/common/rpc/amqp.py\", line 433, in _process_data\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     **args)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/openstack/common/rpc/dispatcher.py\", line 148, in dispatch\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/exception.py\", line 98, in wrapped\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     temp_level, payload)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/exception.py\", line 75, in wrapped\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 214, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     pass\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 200, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 242, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 229, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 1887, in snapshot_instance\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.driver.snapshot(context, instance, image_id, update_task_state)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/driver.py\", line 180, in snapshot\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self._vmops.snapshot(context, instance, name, update_task_state)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/vmops.py\", line 537, in snapshot\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     _copy_vmdk_content()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/vmops.py\", line 533, in _copy_vmdk_content\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self._session._wait_for_task(instance['uuid'], copy_disk_task)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/driver.py\", line 559, in _wait_for_task\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     ret_val = done.wait()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 116, in wait\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return hubs.get_hub().switch()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 187, in switch\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return self.greenlet.switch()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp NovaException: The requested operation is not implemented by the server.\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp", 
    "tags": [
        "vmware"
    ], 
    "importance": "High", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1184807", 
    "owner": "https://api.launchpad.net/1.0/~vui", 
    "id": 1184807, 
    "index": 1110, 
    "openned": "2013-05-28 00:27:07.045321+00:00", 
    "created": "2013-05-28 00:27:07.045321+00:00", 
    "title": "Snapshot failure with VMwareVCDriver", 
    "comments": [
        {
            "content": "I am unable to get snapshots working in my dev setup with the VCDriver.  \n\nsnapshot API call claims to succeed, but we get an internal exception (below) and the snapshot stays  in 'queued' status in Horizon.  \n\nThe relevant code is here: https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L515\n\n It seems like the underlying snapshot succeeds, but the attempt to copy the disk afterward fails.  Browsing the datastore, I see that a vmware-tmp directory was created, but I do not see any files in it.  \n \n\n\nspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:04.615 DEBUG nova.virt.vmwareapi.driver [-] Task [CreateSnapshot_Task] (returnval){\n   value = \"task-123\"\n   _type = \"Task\"\n } status: success from (pid=4595) _poll_task /extraspace/stack/nova/nova/virt/vmwareapi/driver.py:576\n2013-05-27 17:15:04.615 DEBUG nova.virt.vmwareapi.vmops [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] [instance: 0d044e1b-074b-47de-9002-de5d87230aa5] Created Snapshot of the VM instance from (pid=4595) _create_vm_snapshot /extraspace/stack/nova/nova/virt/vmwareapi/vmops.py:477\n2013-05-27 17:15:04.616 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:04.616 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 43832e8692c64fdeabba8b34c531b682 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:04.617 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is ff2a695783ba459e8aef5a0097eefb95. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:04.981 DEBUG nova.openstack.common.lockutils [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Got semaphore \"compute_resources\" for method \"update_usage\"... from (pid=4595) inner /extraspace/stack/nova/nova/openstack/common/lockutils.py:186\n2013-05-27 17:15:04.982 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:04.982 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 8de1b35ad1624e4cbbd25dca3b7ff41e from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:04.983 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is 96b0cd1f45e84bc486863e9bab4b1b8f. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.174 DEBUG nova.virt.vmwareapi.vmops [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] [instance: 0d044e1b-074b-47de-9002-de5d87230aa5] Copying disk data before snapshot of the VM from (pid=4595) _copy_vmdk_content /extraspace/stack/nova/nova/virt/vmwareapi/vmops.py:522\n2013-05-27 17:15:05.222 WARNING nova.virt.vmwareapi.driver [-] Task [CopyVirtualDisk_Task] (returnval){\n   value = \"task-124\"\n   _type = \"Task\"\n } status: error The requested operation is not implemented by the server.\n2013-05-27 17:15:05.224 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.224 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 838e0c86a1b04e46856ed43797442f6f from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.225 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is bcb86dcabbb040bf81bfa0a0676e4b14. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 41c6cb58361f4686b717f3e3f3074178 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.244 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is c732adbadd424eb1ac56d2e259041600. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.624 DEBUG nova.openstack.common.lockutils [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Got semaphore \"compute_resources\" for method \"update_usage\"... from (pid=4595) inner /extraspace/stack/nova/nova/openstack/common/lockutils.py:186\n2013-05-27 17:15:05.625 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Making synchronous call on conductor ... from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:586\n2013-05-27 17:15:05.626 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] MSG_ID is 206d42da6b5947caaaa3473d80744544 from (pid=4595) multicall /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:589\n2013-05-27 17:15:05.626 DEBUG nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] UNIQUE_ID is 2ab81258b1c8413e83eb278d4d3539a2. from (pid=4595) _add_unique_id /extraspace/stack/nova/nova/openstack/common/rpc/amqp.py:337\n2013-05-27 17:15:05.672 ERROR nova.openstack.common.rpc.amqp [req-5a19f94a-6e87-4196-8bbd-4ea396e2f04f demo demo] Exception during message handling\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/openstack/common/rpc/amqp.py\", line 433, in _process_data\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     **args)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/openstack/common/rpc/dispatcher.py\", line 148, in dispatch\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/exception.py\", line 98, in wrapped\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     temp_level, payload)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/exception.py\", line 75, in wrapped\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 214, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     pass\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 200, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 242, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.gen.next()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 229, in decorated_function\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/compute/manager.py\", line 1887, in snapshot_instance\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self.driver.snapshot(context, instance, image_id, update_task_state)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/driver.py\", line 180, in snapshot\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self._vmops.snapshot(context, instance, name, update_task_state)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/vmops.py\", line 537, in snapshot\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     _copy_vmdk_content()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/vmops.py\", line 533, in _copy_vmdk_content\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     self._session._wait_for_task(instance['uuid'], copy_disk_task)\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/extraspace/stack/nova/nova/virt/vmwareapi/driver.py\", line 559, in _wait_for_task\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     ret_val = done.wait()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 116, in wait\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return hubs.get_hub().switch()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp   File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 187, in switch\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp     return self.greenlet.switch()\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp NovaException: The requested operation is not implemented by the server.\n2013-05-27 17:15:05.672 TRACE nova.openstack.common.rpc.amqp", 
            "date_created": "2013-05-28 00:27:07.045321+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "It seems like CopyVirtualDisk_Task is still valid for vCenter 5.0: http://pubs.vmware.com/vsphere-50/index.jsp?topic=%2Fcom.vmware.wssdk.apiref.doc_50%2Fvim.VirtualDiskManager.html\n\nAlso, not sure if it matters, but I have a single node cluster with a VMFS datastore.  ", 
            "date_created": "2013-05-28 00:58:13.437800+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "Also, on the vCenter, I do not see any info in the Tasks pane about this task, though I do see the proceeding snapshoth calls listed as having been successful. ", 
            "date_created": "2013-05-28 00:59:33.777286+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "After looking at:\nhttps://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L511\nand\nhttps://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L529\n\nI believe the problem may be related to:\n\nhttp://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=900\n\nSpecifically: \"When using snapshots, a virtual machine's virtual disks can be comprised of multiple VMDK files which are part of an interdependent chain.\"\n\nThe code at L511 appears to assume a single VMDK file which is not the case when working with snapshots (in VMware vSphere)", 
            "date_created": "2013-05-28 18:51:51.845502+00:00", 
            "author": "https://api.launchpad.net/1.0/~hartsock"
        }, 
        {
            "content": "No work around. Snapshot system fails. Importance: critical: affects all users. ", 
            "date_created": "2013-06-18 14:09:10.090757+00:00", 
            "author": "https://api.launchpad.net/1.0/~hartsock"
        }, 
        {
            "content": "It looks to my like this behavior has changed since this bug was written.  When i snapshot both from horizon and from the command line via nova image-create it the call doesn't go into nova snapshot at all - in face it appears it's just making a copy from glance - which is really not the desired outcome.  I'll try to see what changed in this area.\n\nDan - you were selecting the instance from horizon and pushing the create snapshot button right?  Another difference is that there is no state - it just appears in the images list with a 0 length file\u2026.", 
            "date_created": "2013-06-20 22:21:25.704245+00:00", 
            "author": "https://api.launchpad.net/1.0/~tjones-i"
        }, 
        {
            "content": "Tracy, I believe I did the following: \n\n- access the /project/instances/ page of horizon showing all instances.\n- click \"create snapshot\".\n\nMy guess is that at least on the command line you're doing something different than me by invoking nova image-create, which seeks to create a glance image by snapshoting a VM.  ", 
            "date_created": "2013-07-29 02:36:41.858347+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "I believe tempest also throws a snapshot error when running with the vSphere driver.  Maybe check what tempest invokes from the CLI for comparison. ", 
            "date_created": "2013-07-29 02:37:23.016603+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/40298", 
            "date_created": "2013-08-05 21:25:11.486781+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/40298\nCommitted: http://github.com/openstack/nova/commit/61bfac8881dd6a71a572a54b2ea1680248fc4bc4\nSubmitter: Jenkins\nBranch:    master\n\ncommit 61bfac8881dd6a71a572a54b2ea1680248fc4bc4\nAuthor: Vui Lam <email address hidden>\nDate:   Mon Aug 5 00:37:47 2013 -0700\n\n    Fix snapshot failure with VMwareVCDriver\n    \n    The snapshot operation was failing because it calls\n    VirtualDiskManager.CopyVirtualDisk with a destination disk spec, which\n    is not supported when called through VC. The fix is to not supply a spec\n    when calling through VC, in which case the disk is consolidated and\n    copied without type transformation.\n    \n    While the fix is to use spec-less CopyVirtualDisk in VC, doing so in ESX\n    too will result in unintended transformation (because it ESX will\n    default to busLogic/preallocated), so the use of the spec was retained\n    in ESX-mode instead.\n    \n    Another issue found and fixed is that the name of the snapshot image is\n    incorrectly set when uploading to glance.\n    \n    The following tempest tests are now unbroken against the VC and ESX\n    nodes:\n      tempest.api.compute.images.\n         test_images_oneserver.ImagesOneServerTest{JSON,XML}.\n            test_create_delete_image\n            test_create_second_image_when_first_image_is_being_saved\n            test_delete_image_of_another_tenant\n         test_list_image_filters\n      tempest.api.compute.test_authorization.AuthorizationTestJSON\n      tempest.api.compute.test_authorization.AuthorizationTestXML\n    \n    Fixes bug 1184807\n    \n    Change-Id: I7ec49859fb2842cc02cdc87a6434aa58612a2964\n", 
            "date_created": "2013-09-20 18:53:56.910204+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "\nAfter git clone the newest repository on sep23 2013  and reinstall devstack, when i did a snapshot but it still failed.\n\n\nThis time, from the vCenter task history i could see both the  \"Create virtual machine snapshot\" and the \"Copy virtual disk\" operation are completed successfully. There was a \"Copy virtual disk\" operation failure before and now in the new code in the devstack repository, it is fixed i think. https://bugs.launchpad.net/nova/+bug/1184807, https://review.openstack.org/#/c/40298/18\n\nFrom the horizon, the new created snapshot image status was firstly \"queued\" and then \"deleted\". And the snapshot image is not in the list when i do a nova image-list, glance image-list or look at the horizon image&snapshot list.\n\nFrom the screen \"g-reg\" log, I found it seemed like the snapshot image was created and then deleted before glance tried to upload the image. The output log is shown as followed:\n\n2013-09-23 12:49:46.634 13386 INFO glance.registry.api.v1.images [61f1aefc-e56b-481e-ac5d-23e5c8a1ab6c 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Successfully retrieved image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:50:09.126 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:50:09.126 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:50:09.126 13386 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token a06afb4e1371592a52ee6cb53b0e2bae _cache_get /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:982\n2013-09-23 12:50:09.127 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:50:09.127 13386 DEBUG routes.middleware [-] Matched GET /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:50:09.127 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:50:09.127 13386 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:50:09.138 13386 INFO glance.registry.api.v1.images [fa8c8425-0637-45cc-a80f-c188a787ad41 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Successfully retrieved image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:50:34.130 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:50:34.131 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:50:34.131 13386 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token a06afb4e1371592a52ee6cb53b0e2bae _cache_get /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:982\n2013-09-23 12:50:34.131 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:50:34.132 13386 DEBUG routes.middleware [-] Matched GET /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:50:34.132 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:50:34.132 13386 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:50:34.139 13386 INFO glance.registry.api.v1.images [5d0b9eec-b064-40b2-8db7-aeeb1f88c97e 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Successfully retrieved image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:50:54.527 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:50:54.527 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:50:54.528 13386 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token a06afb4e1371592a52ee6cb53b0e2bae _cache_get /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:982\n2013-09-23 12:50:54.528 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:50:54.528 13386 DEBUG routes.middleware [-] Matched GET /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:50:54.529 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:50:54.529 13386 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:50:54.535 13386 INFO glance.registry.api.v1.images [3681cb8b-f9b2-480e-b0b1-a05a08649701 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Successfully retrieved image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:50:54.538 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:50:54.538 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:50:54.538 13386 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token a06afb4e1371592a52ee6cb53b0e2bae _cache_get /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:982\n2013-09-23 12:50:54.539 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:50:54.539 13386 DEBUG routes.middleware [-] Matched PUT /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:50:54.539 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'update', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:50:54.540 13386 DEBUG routes.middleware [-] Match dict: {'action': u'update', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:50:54.540 13386 DEBUG glance.registry.api.v1.images [a5473e1c-63d4-447e-9ebf-363f4176bebc 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Updating image 66e47135-8576-49af-a474-47a11de0c46d with metadata: {u'status': u'deleted'} update /opt/stack/glance/glance/registry/api/v1/images.py:436\n2013-09-23 12:50:54.555 13386 INFO glance.registry.api.v1.images [a5473e1c-63d4-447e-9ebf-363f4176bebc 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Updating metadata for image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:50:54.558 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:50:54.558 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:50:54.558 13386 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token a06afb4e1371592a52ee6cb53b0e2bae _cache_get /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:982\n2013-09-23 12:50:54.559 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:50:54.559 13386 DEBUG routes.middleware [-] Matched DELETE /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:50:54.559 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'delete', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:50:54.559 13386 DEBUG routes.middleware [-] Match dict: {'action': u'delete', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:50:54.591 13386 INFO glance.registry.api.v1.images [681e3d40-ecaa-42be-beb8-fc972a2997a5 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Successfully deleted image 66e47135-8576-49af-a474-47a11de0c46d\n2013-09-23 12:51:01.650 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:51:01.650 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:51:01.651 13386 INFO requests.packages.urllib3.connectionpool [-] Starting new HTTP connection (1): 172.20.239.92\n2013-09-23 12:51:01.664 13386 DEBUG requests.packages.urllib3.connectionpool [-] \"GET /v2.0/tokens/a06afb4e1371592a52ee6cb53b0e2bae HTTP/1.1\" 200 5371 _make_request /usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py:296\n2013-09-23 12:51:01.664 13386 DEBUG keystoneclient.middleware.auth_token [-] Storing a06afb4e1371592a52ee6cb53b0e2bae token in memcache _cache_put /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:1042\n2013-09-23 12:51:01.665 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:51:01.665 13386 DEBUG routes.middleware [-] Matched GET /images/66e47135-8576-49af-a474-47a11de0c46d __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:51:01.665 13386 DEBUG routes.middleware [-] Route path: '/images/{id}', defaults: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:51:01.665 13386 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>, 'id': u'66e47135-8576-49af-a474-47a11de0c46d'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:51:01.671 13386 DEBUG glance.db.sqlalchemy.api [94099a83-19e1-40c1-b592-dd61013caf7a 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] No image found with ID 66e47135-8576-49af-a474-47a11de0c46d _image_get /opt/stack/glance/glance/db/sqlalchemy/api.py:334\n2013-09-23 12:51:01.671 13386 INFO glance.registry.api.v1.images [94099a83-19e1-40c1-b592-dd61013caf7a 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Image 66e47135-8576-49af-a474-47a11de0c46d not found\n2013-09-23 12:57:51.769 13386 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:532\n2013-09-23 12:57:51.769 13386 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:591\n2013-09-23 12:57:51.770 13386 INFO requests.packages.urllib3.connectionpool [-] Starting new HTTP connection (1): 172.20.239.92\n2013-09-23 12:57:51.797 13386 DEBUG requests.packages.urllib3.connectionpool [-] \"GET /v2.0/tokens/revoked HTTP/1.1\" 200 794 _make_request /usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py:296\n2013-09-23 12:57:51.809 13386 DEBUG keystoneclient.middleware.auth_token [-] Storing e8ba2019300908551e0c6ccb766af4e6 token in memcache _cache_put /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:1042\n2013-09-23 12:57:51.811 13386 DEBUG glance.api.policy [-] Loaded policy rules: {u'context_is_admin': 'role:admin', u'default': '@', u'manage_image_cache': 'role:admin'} load_rules /opt/stack/glance/glance/api/policy.py:75\n2013-09-23 12:57:51.811 13386 DEBUG routes.middleware [-] Matched GET /images/detail __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100\n2013-09-23 12:57:51.811 13386 DEBUG routes.middleware [-] Route path: '/images/detail', defaults: {'action': u'detail', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102\n2013-09-23 12:57:51.811 13386 DEBUG routes.middleware [-] Match dict: {'action': u'detail', 'controller': <glance.common.wsgi.Resource object at 0x279ba50>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103\n2013-09-23 12:57:51.833 13386 INFO glance.registry.api.v1.images [8fe70cc1-bee3-4629-91fa-0f6aa0acbcc4 1e1e314becc94d2ebe8246f0a36ca99a 09ee20f776914ad7983bb2ace867623a] Returning detailed image list\n\n\n\nAny idea about what caused the problem?   Does it mean that the bug has not been totally fixed, anything wrong with my devstack setting or openstack service?\n\nThanks\n\n", 
            "date_created": "2013-09-23 20:40:04.754279+00:00", 
            "author": "https://api.launchpad.net/1.0/~lhx1031"
        }, 
        {
            "content": "Hi pocketlion,\n\nI am looking into the reported issue. Can you also upload the nova compute log as well? Thanks.", 
            "date_created": "2013-09-24 04:28:52.165632+00:00", 
            "author": "https://api.launchpad.net/1.0/~vui"
        }, 
        {
            "content": "Just to add: \n\nhttps://review.openstack.org/40298 addressed the error encountered during while make a consolidated copy of instance's disk. The copied disk is then uploaded to glance and subsequently deleted. Should an unhandled exception be thrown during the driver's snapshot operation, the just-created snapshot entry along with any uploaded data will be removed from the glance server during cleanup. Given what you reported, it appears that disk copy succeeded. Is it possible that your devstack environment which likely house the glance image repository actually ran out of storage space while receiving a large snapshot image upload?\n\n ", 
            "date_created": "2013-09-24 06:58:10.016668+00:00", 
            "author": "https://api.launchpad.net/1.0/~vui"
        }, 
        {
            "content": "Fix proposed to branch: stable/grizzly\nReview: https://review.openstack.org/49371", 
            "date_created": "2013-10-02 13:46:19.683463+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/49371\nCommitted: http://github.com/openstack/nova/commit/bc9495d2930936618e49e569af03915b3add5bf1\nSubmitter: Jenkins\nBranch:    stable/grizzly\n\ncommit bc9495d2930936618e49e569af03915b3add5bf1\nAuthor: Gary Kotton <email address hidden>\nDate:   Wed Oct 2 06:44:27 2013 -0700\n\n    Fix snapshot failure with VMwareVCDriver\n    \n    The snapshot operation was failing because it calls\n    VirtualDiskManager.CopyVirtualDisk with a destination disk spec, which\n    is not supported when called through VC. The fix is to not supply a spec\n    when calling through VC, in which case the disk is consolidated and\n    copied without type transformation.\n    \n    While the fix is to use spec-less CopyVirtualDisk in VC, doing so in ESX\n    too will result in unintended transformation (because it ESX will\n    default to busLogic/preallocated), so the use of the spec was retained\n    in ESX-mode instead.\n    \n    Another issue found and fixed is that the name of the snapshot image is\n    incorrectly set when uploading to glance.\n    \n    The following tempest tests are now unbroken against the VC and ESX\n    nodes:\n      tempest.api.compute.images.\n         test_images_oneserver.ImagesOneServerTest{JSON,XML}.\n            test_create_delete_image\n            test_create_second_image_when_first_image_is_being_saved\n            test_delete_image_of_another_tenant\n         test_list_image_filters\n      tempest.api.compute.test_authorization.AuthorizationTestJSON\n      tempest.api.compute.test_authorization.AuthorizationTestXML\n    \n    Fixes bug 1184807\n    \n    (cherry picked from commit 61bfac8881dd6a71a572a54b2ea1680248fc4bc4)\n    \n    Conflicts:\n    \n            nova/tests/virt/vmwareapi/test_vmwareapi.py\n            nova/virt/vmwareapi/driver.py\n            nova/virt/vmwareapi/vmops.py\n    \n    Change-Id: I08717a06f23dd1abfd5ccae7606a7ecb1453dc8c\n", 
            "date_created": "2013-10-08 06:33:14.785175+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2013-10-03 07:42:20.302689+00:00"
}
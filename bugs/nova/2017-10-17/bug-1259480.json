{
    "status": "Invalid", 
    "last_updated": "2015-04-01 12:54:20.845704+00:00", 
    "description": " /var/log/nova/nova-compute.log <==\n2013-12-10 10:44:37.345 4112 ERROR nova.compute.manager [req-b513d5bd-c165-4139-b346-bd5564f2529b ff18572f70ed447683714d1287c8525f d0df6fee11754175b5675e2dc2c80aac] [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] Error: Invalid volume: status must be 'available'\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] Traceback (most recent call last):\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1028, in _build_instance\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     context, instance, bdms)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1393, in _prep_block_device\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     instance=instance)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1376, in _prep_block_device\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     self._await_block_device_map_created))\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 283, in attach_block_devices\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     block_device_mapping)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 246, in attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     db_api)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 153, in attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     volume_api.check_attach(context, volume, instance=instance)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/volume/cinder.py\", line 231, in check_attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     raise exception.InvalidVolume(reason=msg)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] InvalidVolume: Invalid volume: status must be 'available'\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1259480", 
    "owner": "None", 
    "id": 1259480, 
    "index": 5644, 
    "openned": "2013-12-10 09:49:09.294087+00:00", 
    "created": "2013-12-10 09:49:09.294087+00:00", 
    "title": "create instance, boot from image and create a new volume", 
    "comments": [
        {
            "content": " /var/log/nova/nova-compute.log <==\n2013-12-10 10:44:37.345 4112 ERROR nova.compute.manager [req-b513d5bd-c165-4139-b346-bd5564f2529b ff18572f70ed447683714d1287c8525f d0df6fee11754175b5675e2dc2c80aac] [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] Error: Invalid volume: status must be 'available'\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] Traceback (most recent call last):\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1028, in _build_instance\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     context, instance, bdms)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1393, in _prep_block_device\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     instance=instance)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1376, in _prep_block_device\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     self._await_block_device_map_created))\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 283, in attach_block_devices\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     block_device_mapping)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 246, in attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     db_api)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 153, in attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     volume_api.check_attach(context, volume, instance=instance)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]   File \"/usr/lib/python2.7/dist-packages/nova/volume/cinder.py\", line 231, in check_attach\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]     raise exception.InvalidVolume(reason=msg)\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088] InvalidVolume: Invalid volume: status must be 'available'\n2013-12-10 10:44:37.345 4112 TRACE nova.compute.manager [instance: 0a4dbc13-f93d-4993-b500-177f4bde6088]", 
            "date_created": "2013-12-10 09:49:09.294087+00:00", 
            "author": "https://api.launchpad.net/1.0/~tindaro-tornabene"
        }, 
        {
            "content": "Can you describe your setup, in particular what back-end are you using with cinder?", 
            "date_created": "2013-12-11 11:47:08.678566+00:00", 
            "author": "https://api.launchpad.net/1.0/~maithem"
        }, 
        {
            "content": "I am seeing the same issue.\nI am using ceph as a backend for the cinder.\nTell me what other information do you need.\n I can be found on irc #openstack-cinder as gstaicu", 
            "date_created": "2014-02-12 18:05:37.894999+00:00", 
            "author": "https://api.launchpad.net/1.0/~gabriel-staicu"
        }, 
        {
            "content": "I am too.\nAnd it seems i found why.\n\nOn fresh installed compute node (using CEPH as cinder backend) after spawn new instance i got error:\n\n> ERROR nova.compute.manager  [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Instance failed to spawn\n> (nova.compute.manager): TRACE: libvirtError: internal error: could not get value of the secret for username cinder\n> AUDIT nova.compute.manager [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Terminating instance\n> INFO nova.virt.libvirt.driver [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Instance destroyed successfully.\n> INFO nova.virt.libvirt.driver [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Deleting instance files /var/lib/nova/instances/b380d63b-ad08-42cf-8723-d2e0025950b3\n> INFO nova.virt.libvirt.driver [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Deletion of /var/lib/nova/instances/b380d63b-ad08-42cf-8723-d2e0025950b3 complete\n> ERROR nova.compute.manager [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Error: internal error: could not get value of the secret for username cinder\n\nAnd immediately after that on second compute node i got:\n\n> AUDIT nova.compute.manager [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Starting instance...\n> AUDIT nova.compute.claims [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Claim successful\n> ERROR nova.compute.manager [req-b4...] [instance: b380d63b-ad08-42cf-8723-d2e0025950b3]Instance failed block device setup\n(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 1387, in _prep_block_device\n(nova.compute.manager): TRACE:     self._await_block_device_map_created) +\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 283, in attach_block_devices\n(nova.compute.manager): TRACE:     block_device_mapping)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 224, in attach\n(nova.compute.manager): TRACE:     db_api)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/virt/block_device.py\", line 153, in attach\n(nova.compute.manager): TRACE:     volume_api.check_attach(context, volume, instance=instance)\n(nova.compute.manager): TRACE:   File \"/usr/lib/python2.7/dist-packages/nova/volume/cinder.py\", line 231, in check_attach\n(nova.compute.manager): TRACE:     raise exception.InvalidVolume(reason=msg)\n(nova.compute.manager): TRACE: InvalidVolume: Invalid volume: status must be 'available'\n(nova.compute.manager): TRACE: \n\n\nSo. After instance failed to start on first compute node (without cinder credentials), instance starts on second compute node, but failed because cinder volume status is already 'attached'.", 
            "date_created": "2014-04-16 07:38:59.309339+00:00", 
            "author": "https://api.launchpad.net/1.0/~askort"
        }, 
        {
            "content": "looks like invalid configuration \"could not get value of the secret for username cinder\"", 
            "date_created": "2015-04-01 12:54:20.143345+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ], 
    "closed": "2015-04-01 12:53:56.278782+00:00"
}
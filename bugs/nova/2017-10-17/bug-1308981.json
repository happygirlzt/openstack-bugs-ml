{
    "status": "Invalid", 
    "last_updated": "2014-09-17 13:45:19.674350+00:00", 
    "description": "I have two controllers which form a RabbitMQ cluster and then a compute node. The problem occurs when I have all the nodes first up and then I shut down one of the controllers. Then in nova.log in compute node the below exception is logged.\n\n<182>Apr 17 14:36:37 compute-01 nova-nova.compute.resource_tracker INFO: Compute_service record updated for compute-01:compute-01.trelab.tieto.com\n<179>Apr 17 14:37:42 compute-01 nova-nova.servicegroup.drivers.db ERROR: model server went away\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py\", line 96, in _report_state\n    service.service_ref, state_catalog)\n  File \"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\", line 269, in service_update\n    return self._manager.service_update(context, service, values)\n  File \"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\", line 397, in service_update\n    service=service_p, values=values)\n  File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 85, in call\n    return self._invoke(self.proxy.call, ctxt, method, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 63, in _invoke\n    return cast_or_call(ctxt, msg, **self.kwargs)\n  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 130, in call\n    exc.info, real_topic, msg.get('method'))\nTimeout: Timeout while waiting on RPC response - topic: \"conductor\", RPC method: \"service_update\" info: \"<unknown>\"\n<180>Apr 17 14:38:08 compute-01 nova-nova.compute.resource_tracker AUDIT: Auditing locally available compute resources\n<182>Apr 17 14:40:09 compute-01 nova-nova.compute.manager INFO: Updating bandwidth usage cache\n<180>Apr 17 14:44:39 compute-01 nova-nova.compute.resource_tracker AUDIT: Auditing locally available compute resources\n\nThe compute node goes to \"down\" state in nova service-list and it does not recover. Only when I start the other controller again the compute node recovers. Sometimes it is needed to restart nova-compute to recover.\n\nI have a havana level system. In the system I have upgraded RabbitMQ to 3.2.4 version and created a policy so that mirrored queues are used in RabbitMQ.\n\n$ rabbitmqctl set_policy HA '^(?!amq\\.).*' '{\"ha-mode\": \"all\"}'\n\nrabbitmqctl cluster_status is showing both controllers as running nodes.", 
    "tags": [
        "compute"
    ], 
    "importance": "Undecided", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1308981", 
    "owner": "None", 
    "id": 1308981, 
    "index": 6038, 
    "openned": "2014-04-17 11:43:05.609093+00:00", 
    "created": "2014-04-17 11:43:05.609093+00:00", 
    "title": "Nova-compute does not recover controller switch over", 
    "comments": [
        {
            "content": "I have two controllers which form a RabbitMQ cluster and then a compute node. The problem occurs when I have all the nodes first up and then I shut down one of the controllers. Then in nova.log in compute node the below exception is logged.\n\n<182>Apr 17 14:36:37 compute-01 nova-nova.compute.resource_tracker INFO: Compute_service record updated for compute-01:compute-01.trelab.tieto.com\n<179>Apr 17 14:37:42 compute-01 nova-nova.servicegroup.drivers.db ERROR: model server went away\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py\", line 96, in _report_state\n    service.service_ref, state_catalog)\n  File \"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\", line 269, in service_update\n    return self._manager.service_update(context, service, values)\n  File \"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\", line 397, in service_update\n    service=service_p, values=values)\n  File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 85, in call\n    return self._invoke(self.proxy.call, ctxt, method, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 63, in _invoke\n    return cast_or_call(ctxt, msg, **self.kwargs)\n  File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 130, in call\n    exc.info, real_topic, msg.get('method'))\nTimeout: Timeout while waiting on RPC response - topic: \"conductor\", RPC method: \"service_update\" info: \"<unknown>\"\n<180>Apr 17 14:38:08 compute-01 nova-nova.compute.resource_tracker AUDIT: Auditing locally available compute resources\n<182>Apr 17 14:40:09 compute-01 nova-nova.compute.manager INFO: Updating bandwidth usage cache\n<180>Apr 17 14:44:39 compute-01 nova-nova.compute.resource_tracker AUDIT: Auditing locally available compute resources\n\nThe compute node goes to \"down\" state in nova service-list and it does not recover. Only when I start the other controller again the compute node recovers. Sometimes it is needed to restart nova-compute to recover.\n\nI have a havana level system. In the system I have upgraded RabbitMQ to 3.2.4 version and created a policy so that mirrored queues are used in RabbitMQ.\n\n$ rabbitmqctl set_policy HA '^(?!amq\\.).*' '{\"ha-mode\": \"all\"}'\n\nrabbitmqctl cluster_status is showing both controllers as running nodes.", 
            "date_created": "2014-04-17 11:43:05.609093+00:00", 
            "author": "https://api.launchpad.net/1.0/~tsierkkis"
        }, 
        {
            "content": "By adding keepalive options to rabbitmq.config I was able to get compute node back online in 15 minutes. Still I think that is too slow recovery.", 
            "date_created": "2014-04-25 09:20:12.933985+00:00", 
            "author": "https://api.launchpad.net/1.0/~tsierkkis"
        }, 
        {
            "content": "This sounds like a HA configuration issue and not a nova bug. Have you configured your api services (use HAProxy etc) as shown in the manual? http://docs.openstack.org/high-availability-guide/content/_api_services.html", 
            "date_created": "2014-06-04 19:58:05.430550+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }
    ], 
    "closed": "2014-09-17 13:45:17.112228+00:00"
}
{
    "status": "Invalid", 
    "last_updated": "2016-10-13 20:23:54.022662+00:00", 
    "description": "When I use Horizon to add the Watchdog Action (hw_watchdog_action) metadata to any flavor and I try to use that flavor to create an instance then the boot process fails. However, if I add the same metadata to an image then everything works flawlessly.\n\nI used devstack to try to find out some details about this issue. (I was able to reproduce this issue on stable/mitaka and on master as well.) I found the following:\n\n\nUSE CASE #1 :: flavor + underscore\n\n$ nova flavor-show m1.nano\n+----------------------------+---------------------------------+\n| Property                   | Value                           |\n+----------------------------+---------------------------------+\n| OS-FLV-DISABLED:disabled   | False                           |\n| OS-FLV-EXT-DATA:ephemeral  | 0                               |\n| disk                       | 0                               |\n| extra_specs                | {\"hw_watchdog_action\": \"reset\"} |\n| id                         | 42                              |\n| name                       | m1.nano                         |\n| os-flavor-access:is_public | True                            |\n| ram                        | 64                              |\n| rxtx_factor                | 1.0                             |\n| swap                       |                                 |\n| vcpus                      | 1                               |\n+----------------------------+---------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm0\n\nResult: Fault\nMessage\nNo valid host was found. There are not enough hosts available.\nCode\n500\nDetails\nFile \"/opt/stack/nova/nova/conductor/manager.py\", line 392, in build_instances context, request_spec, filter_properties) File \"/opt/stack/nova/nova/conductor/manager.py\", line 436, in _schedule_instances hosts = self.scheduler_client.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/utils.py\", line 372, in wrapped return func(*args, **kwargs) File \"/opt/stack/nova/nova/scheduler/client/__init__.py\", line 51, in select_destinations return self.queryclient.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/client/__init__.py\", line 37, in __run_method return getattr(self.instance, __name)(*args, **kwargs) File \"/opt/stack/nova/nova/scheduler/client/query.py\", line 32, in select_destinations return self.scheduler_rpcapi.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/rpcapi.py\", line 121, in select_destinations return cctxt.call(ctxt, 'select_destinations', **msg_args) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call retry=self.retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send timeout=timeout, retry=retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 470, in send retry=retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 461, in _send raise result\n\nn-sch.log shows that nova.scheduler.filters.compute_capabilities_filter removes the only host available during filtering.\n\n\nUse case #2 :: flavor + colon\n\n$ nova flavor-show m1.nano\n+----------------------------+---------------------------------+\n| Property                   | Value                           |\n+----------------------------+---------------------------------+\n| OS-FLV-DISABLED:disabled   | False                           |\n| OS-FLV-EXT-DATA:ephemeral  | 0                               |\n| disk                       | 0                               |\n| extra_specs                | {\"hw:watchdog_action\": \"reset\"} |\n| id                         | 42                              |\n| name                       | m1.nano                         |\n| os-flavor-access:is_public | True                            |\n| ram                        | 64                              |\n| rxtx_factor                | 1.0                             |\n| swap                       |                                 |\n| vcpus                      | 1                               |\n+----------------------------+---------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm1\n$ virsh dumpxml instance-00000131 | grep \"<watchdog\" -A 3\n    <watchdog model='i6300esb' action='reset'>\n      <alias name='watchdog0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>\n    </watchdog>\n\nResult: The instance boots perfectly and the /dev/watchdog device is present.\n\n\nUSE CASE #3 :: image + underscore\n\n$ nova flavor-show m1.nano\n+----------------------------+---------+\n| Property                   | Value   |\n+----------------------------+---------+\n| OS-FLV-DISABLED:disabled   | False   |\n| OS-FLV-EXT-DATA:ephemeral  | 0       |\n| disk                       | 0       |\n| extra_specs                | {}      |\n| id                         | 42      |\n| name                       | m1.nano |\n| os-flavor-access:is_public | True    |\n| ram                        | 64      |\n| rxtx_factor                | 1.0     |\n| swap                       |         |\n| vcpus                      | 1       |\n+----------------------------+---------+\n$ nova image-show cirros-0.3.4-x86_64-uec-watchdog\n+-----------------------------+--------------------------------------+\n| Property                    | Value                                |\n+-----------------------------+--------------------------------------+\n| OS-EXT-IMG-SIZE:size        | 13375488                             |\n| created                     | 2016-05-17T08:49:21Z                 |\n| id                          | 863c2d04-cdd3-42c2-be78-c831c48929b3 |\n| metadata hw_watchdog_action | reset                                |\n| minDisk                     | 0                                    |\n| minRam                      | 0                                    |\n| name                        | cirros-0.3.4-x86_64-uec-watchdog     |\n| progress                    | 100                                  |\n| status                      | ACTIVE                               |\n| updated                     | 2016-05-17T09:10:59Z                 |\n+-----------------------------+--------------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm2\n$ virsh dumpxml instance-00000132 | grep \"<watchdog\" -A 3\n    <watchdog model='i6300esb' action='reset'>\n      <alias name='watchdog0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>\n    </watchdog>\n\nResult: The instance boots perfectly and the /dev/watchdog device is present.\n\n\nUSE CASE #4 :: image + colon\n\n$ nova image-show cirros-0.3.4-x86_64-uec-watchdog\n+----------------------+--------------------------------------+\n| Property             | Value                                |\n+----------------------+--------------------------------------+\n| OS-EXT-IMG-SIZE:size | 13375488                             |\n| created              | 2016-05-17T08:49:21Z                 |\n| id                   | 863c2d04-cdd3-42c2-be78-c831c48929b3 |\n| metadata hw          | watchdog_action: reset               |\n| minDisk              | 0                                    |\n| minRam               | 0                                    |\n| name                 | cirros-0.3.4-x86_64-uec-watchdog     |\n| progress             | 100                                  |\n| status               | ACTIVE                               |\n| updated              | 2016-05-17T09:16:42Z                 |\n+----------------------+--------------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm2\n$ virsh dumpxml instance-00000133 | grep \"<watchdog\" -A 3\n\nResult: Seemingly there are no errors during the boot process, but the watchdog device is not present.", 
    "tags": [
        "scheduler"
    ], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1582693", 
    "owner": "https://api.launchpad.net/1.0/~richil-bhalerao", 
    "id": 1582693, 
    "index": 7486, 
    "openned": "2016-05-17 12:28:22.065114+00:00", 
    "created": "2016-05-17 12:28:22.065114+00:00", 
    "title": "Image and flavor metadata for libvirt watchdog is handled erroneously", 
    "comments": [
        {
            "content": "When I use Horizon to add the Watchdog Action (hw_watchdog_action) metadata to any flavor and I try to use that flavor to create an instance then the boot process fails. However, if I add the same metadata to an image then everything works flawlessly.\n\nI used devstack to try to find out some details about this issue. (I was able to reproduce this issue on stable/mitaka and on master as well.) I found the following:\n\n\nUSE CASE #1 :: flavor + underscore\n\n$ nova flavor-show m1.nano\n+----------------------------+---------------------------------+\n| Property                   | Value                           |\n+----------------------------+---------------------------------+\n| OS-FLV-DISABLED:disabled   | False                           |\n| OS-FLV-EXT-DATA:ephemeral  | 0                               |\n| disk                       | 0                               |\n| extra_specs                | {\"hw_watchdog_action\": \"reset\"} |\n| id                         | 42                              |\n| name                       | m1.nano                         |\n| os-flavor-access:is_public | True                            |\n| ram                        | 64                              |\n| rxtx_factor                | 1.0                             |\n| swap                       |                                 |\n| vcpus                      | 1                               |\n+----------------------------+---------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm0\n\nResult: Fault\nMessage\nNo valid host was found. There are not enough hosts available.\nCode\n500\nDetails\nFile \"/opt/stack/nova/nova/conductor/manager.py\", line 392, in build_instances context, request_spec, filter_properties) File \"/opt/stack/nova/nova/conductor/manager.py\", line 436, in _schedule_instances hosts = self.scheduler_client.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/utils.py\", line 372, in wrapped return func(*args, **kwargs) File \"/opt/stack/nova/nova/scheduler/client/__init__.py\", line 51, in select_destinations return self.queryclient.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/client/__init__.py\", line 37, in __run_method return getattr(self.instance, __name)(*args, **kwargs) File \"/opt/stack/nova/nova/scheduler/client/query.py\", line 32, in select_destinations return self.scheduler_rpcapi.select_destinations(context, spec_obj) File \"/opt/stack/nova/nova/scheduler/rpcapi.py\", line 121, in select_destinations return cctxt.call(ctxt, 'select_destinations', **msg_args) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call retry=self.retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send timeout=timeout, retry=retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 470, in send retry=retry) File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 461, in _send raise result\n\nn-sch.log shows that nova.scheduler.filters.compute_capabilities_filter removes the only host available during filtering.\n\n\nUse case #2 :: flavor + colon\n\n$ nova flavor-show m1.nano\n+----------------------------+---------------------------------+\n| Property                   | Value                           |\n+----------------------------+---------------------------------+\n| OS-FLV-DISABLED:disabled   | False                           |\n| OS-FLV-EXT-DATA:ephemeral  | 0                               |\n| disk                       | 0                               |\n| extra_specs                | {\"hw:watchdog_action\": \"reset\"} |\n| id                         | 42                              |\n| name                       | m1.nano                         |\n| os-flavor-access:is_public | True                            |\n| ram                        | 64                              |\n| rxtx_factor                | 1.0                             |\n| swap                       |                                 |\n| vcpus                      | 1                               |\n+----------------------------+---------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm1\n$ virsh dumpxml instance-00000131 | grep \"<watchdog\" -A 3\n    <watchdog model='i6300esb' action='reset'>\n      <alias name='watchdog0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>\n    </watchdog>\n\nResult: The instance boots perfectly and the /dev/watchdog device is present.\n\n\nUSE CASE #3 :: image + underscore\n\n$ nova flavor-show m1.nano\n+----------------------------+---------+\n| Property                   | Value   |\n+----------------------------+---------+\n| OS-FLV-DISABLED:disabled   | False   |\n| OS-FLV-EXT-DATA:ephemeral  | 0       |\n| disk                       | 0       |\n| extra_specs                | {}      |\n| id                         | 42      |\n| name                       | m1.nano |\n| os-flavor-access:is_public | True    |\n| ram                        | 64      |\n| rxtx_factor                | 1.0     |\n| swap                       |         |\n| vcpus                      | 1       |\n+----------------------------+---------+\n$ nova image-show cirros-0.3.4-x86_64-uec-watchdog\n+-----------------------------+--------------------------------------+\n| Property                    | Value                                |\n+-----------------------------+--------------------------------------+\n| OS-EXT-IMG-SIZE:size        | 13375488                             |\n| created                     | 2016-05-17T08:49:21Z                 |\n| id                          | 863c2d04-cdd3-42c2-be78-c831c48929b3 |\n| metadata hw_watchdog_action | reset                                |\n| minDisk                     | 0                                    |\n| minRam                      | 0                                    |\n| name                        | cirros-0.3.4-x86_64-uec-watchdog     |\n| progress                    | 100                                  |\n| status                      | ACTIVE                               |\n| updated                     | 2016-05-17T09:10:59Z                 |\n+-----------------------------+--------------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm2\n$ virsh dumpxml instance-00000132 | grep \"<watchdog\" -A 3\n    <watchdog model='i6300esb' action='reset'>\n      <alias name='watchdog0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>\n    </watchdog>\n\nResult: The instance boots perfectly and the /dev/watchdog device is present.\n\n\nUSE CASE #4 :: image + colon\n\n$ nova image-show cirros-0.3.4-x86_64-uec-watchdog\n+----------------------+--------------------------------------+\n| Property             | Value                                |\n+----------------------+--------------------------------------+\n| OS-EXT-IMG-SIZE:size | 13375488                             |\n| created              | 2016-05-17T08:49:21Z                 |\n| id                   | 863c2d04-cdd3-42c2-be78-c831c48929b3 |\n| metadata hw          | watchdog_action: reset               |\n| minDisk              | 0                                    |\n| minRam               | 0                                    |\n| name                 | cirros-0.3.4-x86_64-uec-watchdog     |\n| progress             | 100                                  |\n| status               | ACTIVE                               |\n| updated              | 2016-05-17T09:16:42Z                 |\n+----------------------+--------------------------------------+\n$ nova boot --flavor m1.nano --image cirros-0.3.4-x86_64-uec-watchdog --poll vm2\n$ virsh dumpxml instance-00000133 | grep \"<watchdog\" -A 3\n\nResult: Seemingly there are no errors during the boot process, but the watchdog device is not present.", 
            "date_created": "2016-05-17 12:28:22.065114+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "", 
            "date_created": "2016-05-17 12:28:22.065114+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "Please provide the scheduler logs at debug level", 
            "date_created": "2016-06-02 16:35:57.512288+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "I attach the scheduler log at debug level.", 
            "date_created": "2016-07-05 07:59:04.177730+00:00", 
            "author": "https://api.launchpad.net/1.0/~xavvior"
        }, 
        {
            "content": "This issue is occurring because scheduler logic (while determining whether host satisfies those extra specs) assumes that extra specs will be in the format \"capability:something_something\". However, format for watchdog extra specs in Horizon is \"hw_watchdog_action\" and not \"hw:watchdog_action\". Should nova support extra specs with format that use '_' instead of ':'?", 
            "date_created": "2016-09-28 18:41:22.941978+00:00", 
            "author": "https://api.launchpad.net/1.0/~richil-bhalerao"
        }, 
        {
            "content": "Looks like for flavor extra spec 'hw_watchdog_action' format is deprecated in current release and 'hw:watchdog_action' is recommended to be used instead. Is it safe to say that fix for this bug is really in Horizon and not in Nova?", 
            "date_created": "2016-09-28 19:18:35.027208+00:00", 
            "author": "https://api.launchpad.net/1.0/~richil-bhalerao"
        }, 
        {
            "content": "I think there was a time when some modules were changed to use the newer ':' format for extra specs, so I guess nova shouldn't support extra specs with the old '_' format.\n\nHowever, if that is all, then why aren't there any errors in use case #3, i.e. when the '_' format is used in the image metadata?", 
            "date_created": "2016-09-28 19:22:23.599167+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "At first, I'd have said that this is an easy fix in Horizon, I were (and still is) concerned because of use case #3.", 
            "date_created": "2016-09-28 19:43:23.914264+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "@ference I think Format with '_' is only deprecated in case of flavor metadata and not image metadata which is why Use case #3 passes", 
            "date_created": "2016-09-28 21:56:43.985909+00:00", 
            "author": "https://api.launchpad.net/1.0/~richil-bhalerao"
        }, 
        {
            "content": "Where image metadata is handled? In Nova, as well, or in Glance?\n\nEither way, I think this mixed format situation is very unfortunate, therefore I'd say that it should be consolidated, i.e. all the related projects should use the new ':' format.", 
            "date_created": "2016-09-28 23:13:25.779733+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "@ference Within Nova, image metadata supports format with \"_\" but flavor metadata does not. This is based on the deprecation warning that I see in *.po files and also based on what I see in tests here:\n\nhttps://github.com/openstack/nova/blob/master/nova/tests/unit/virt/libvirt/test_driver.py#L4123\nhttps://github.com/openstack/nova/blob/master/nova/tests/unit/virt/libvirt/test_driver.py#L4311\n\n ", 
            "date_created": "2016-09-28 23:41:11.240759+00:00", 
            "author": "https://api.launchpad.net/1.0/~richil-bhalerao"
        }, 
        {
            "content": "I still think that this is not right and metadata support should be unified, thus all projects/modules should use the ':' format.", 
            "date_created": "2016-09-29 06:37:24.639108+00:00", 
            "author": "https://api.launchpad.net/1.0/~hferenc"
        }, 
        {
            "content": "My understanding has always been the formats to use are:\n\n1. image metadata: <capability>_<thing>\n\n2. flavor extra specs: <capability>:<thing>\n\nThat's how it was done recently for the hyper-v uefi secure boot support:\n\nhttps://review.openstack.org/#/c/209581/49/releasenotes/notes/hyperv-uefi-secure-boot-a2a617ac2c313afd.yaml\n\nAnd that's how I've always seen it I guess, as long as I've been thinking about it at least, but I agree it's definitely confusing, and if openstack is going to require a certain format, it should enforce it, i.e. nova shouldn't accept adding an extra spec to a flavor without a '<capability>:' prefix. I think it's a bit harder for glance to define a specific format, besides requiring at least a single underscore, e.g. image meta of foo_bar means foo is the capability, and foo_bar_baz means foo is still the capability and 'bar_baz' is the spec, but image meta like 'foo', '_foo' or 'foo_' is an error.", 
            "date_created": "2016-10-06 15:13:07.148352+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "FYI, the docs also use the <capability>: format for flavor extra specs:\n\nhttp://docs.openstack.org/admin-guide/compute-flavors.html#extra-specs\n\nSo that's what I'd expect users of the compute API to provide, including Horizon.", 
            "date_created": "2016-10-06 15:15:11.112345+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Also, as far as i can tell, AggregateInstanceExtraSpecsFilter and ComputeCapabilitiesFilter both require the extra spec to have a <cap>: prefix.\n\nHowever, the AggregateImagePropertiesIsolation scheduler filter allows configuration of the image metadata namespace and separator, which means we can't really validate image metadata keys since they are totally configurable in the scheduler.", 
            "date_created": "2016-10-06 15:30:12.726335+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This may actually be a problem with the glance metadata catalog, which it looks like that's what horizon uses to build the flavor extra specs dialog. For example, hw:boot_menu and hw:cpu_policy have the expected namespace: prefix, but hw_watchdog_action does not. There are other more exotic things too like capabilities:cpu_info:features and CIM_PASD_InstructionSet, the former is probably OK, the latter is probably not.", 
            "date_created": "2016-10-06 17:33:20.261089+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "You can see the glance metadef prefixes defined here:\n\nhttps://github.com/openstack/glance/blob/master/etc/metadefs/compute-libvirt.json#L7\n\n    \"resource_type_associations\": [\n        {\n            \"name\": \"OS::Glance::Image\",\n            \"prefix\": \"hw_\"\n        },\n        {\n            \"name\": \"OS::Nova::Flavor\",\n            \"prefix\": \"hw:\"\n        }\n    ],\n\nSo that's why for an image the prefix is hw_ and we get hw_boot_menu, but for a flavor extra spec we get hw:boot_menu. But if you look at the watchdog action, it's not the same:\n\nhttps://github.com/openstack/glance/blob/master/etc/metadefs/compute-watchdog.json#L7\n\nThere is no prefix defined in resource_type_associations so the prefix is built into the property name:\n\n    \"properties\": {\n        \"hw_watchdog_action\": {\n            \"title\": \"Watchdog Action\",\n            \"description\": \"For the libvirt driver, you can enable and set the behavior of a virtual hardware watchdog device for each flavor. Watchdog devices keep an eye on the guest server, and carry out the configured action, if the server hangs. The watchdog uses the i6300esb device (emulating a PCI Intel 6300ESB). If hw_watchdog_action is not specified, the watchdog is disabled. Watchdog behavior set using a specific image's properties will override behavior set using flavors.\",\n            \"type\": \"string\",\n            \"enum\": [\n                \"disabled\",\n                \"reset\",\n                \"poweroff\",\n                \"pause\",\n                \"none\"\n            ]\n        }\n    }\n\nWhich is why we don't get hw:watchdog_action.", 
            "date_created": "2016-10-06 17:39:11.381572+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "FYI, https://review.openstack.org/#/c/386145/ drops the support for hw_watchdog_action as a flavor extra spec in nova.", 
            "date_created": "2016-10-13 20:23:52.967799+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }
    ], 
    "closed": "2016-10-06 17:33:32.091968+00:00"
}
{
    "status": "Fix Released", 
    "last_updated": "2015-04-30 09:21:43.816186+00:00", 
    "description": "I'm trying to make use of huge pages as described in \"http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-large-pages.html\".  I'm running nova kilo as of Jan 27th.  The other openstack services are juno.  Libvirt is 1.2.8.\n\nI've allocated 10000 2MB pages on a compute node.  \"virsh capabilities\" on that node contains:\n\n\u00a0\u00a0\u00a0\u00a0<topology>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<cells num='2'>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<cell id='0'>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<memory unit='KiB'>67028244</memory>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='4'>16032069</pages>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='2048'>5000</pages>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='1048576'>1</pages>\n...\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<cell id='1'>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<memory unit='KiB'>67108864</memory>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='4'>16052224</pages>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='2048'>5000</pages>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<pages unit='KiB' size='1048576'>1</pages>\n\nI then restarted nova-compute, I set \"hw:mem_page_size=large\" on a flavor, and then tried to boot up an instance with that flavor.  I got the error logs below in nova-scheduler.  Is this a bug?\n\nFeb  2 16:23:10 controller-0 nova-scheduler Exception during message handling: Cannot load 'mempages' in the base class\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 134, in _dispatch_and_reply\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 177, in _dispatch\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 123, in _do_dispatch\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/server.py\", line 139, in inner\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/manager.py\", line 86, in select_destinations\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filter_scheduler.py\", line 67, in select_destinations\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filter_scheduler.py\", line 138, in _schedule\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties, index=num)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/host_manager.py\", line 391, in get_filtered_hosts\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     hosts, filter_properties, index)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/filters.py\", line 77, in get_filtered_objects\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     list_objs = list(objs)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/filters.py\", line 43, in filter_all\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     if self._filter_one(obj, filter_properties):\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filters/__init__.py\", line 27, in _filter_one\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return self.host_passes(obj, filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filters/numa_topology_filter.py\", line 45, in host_passes\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     limits_topology=limits))\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 1161, in numa_fit_instance_to_host\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     host_cell, instance_cell, limit_cell)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 851, in _numa_fit_instance_cell\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     host_cell, instance_cell)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 692, in _numa_cell_supports_pagesize_request\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     avail_pagesize = [page.size_kb for page in host_cell.mempages]\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/objects/base.py\", line 72, in getter\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     self.obj_load_attr(name)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/objects/base.py\", line 507, in obj_load_attr\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     _(\"Cannot load '%s' in the base class\") % attrname)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher NotImplementedError: Cannot load 'mempages' in the base class\n\nAs far as nova-compute, at the end of nova.virt.libvirt.driver.LibvirtDriver.get_available_resource() I've confirmed that data['numa_topology'] looks like this:\n\n'{\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cells\"], \"nova_object.name\": \"NUMATopology\", \"nova_object.data\": {\"cells\": [{\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cpu_usage\", \"memory_usage\", \"cpuset\", \"pinned_cpus\", \"siblings\", \"memory\", \"mempages\", \"id\"], \"nova_object.name\": \"NUMACell\", \"nova_object.data\": {\"cpu_usage\": 0, \"memory_usage\": 0, \"cpuset\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"pinned_cpus\": [], \"siblings\": [], \"memory\": 65457, \"mempages\": [{\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 16032069, \"used\": 0, \"size_kb\": 4}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 5000, \"used\": 0, \"size_kb\": 2048}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 1, \"used\": 0, \"size_kb\": 1048576}, \"nova_object.namespace\": \"nova\"}], \"id\": 0}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cpu_usage\", \"memory_usage\", \"cpuset\", \"pinned_cpus\", \"siblings\", \"memory\", \"mempages\", \"id\"], \"nova_object.name\": \"NUMACell\", \"nova_object.data\": {\"cpu_usage\": 0, \"memory_usage\": 0, \"cpuset\": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], \"pinned_cpus\": [], \"siblings\": [], \"memory\": 65536, \"mempages\": [{\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 16052224, \"used\": 0, \"size_kb\": 4}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 5000, \"used\": 0, \"size_kb\": 2048}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 1, \"used\": 0, \"size_kb\": 1048576}, \"nova_object.namespace\": \"nova\"}], \"id\": 1}, \"nova_object.namespace\": \"nova\"}]}, \"nova_object.namespace\": \"nova\"}'\n\nI printed out str(host_topology) in NUMATopologyFilter.host_passes() and it gave:\n\nFeb  2 17:07:43 controller-0 nova-scheduler host_topology: NUMATopology(cells=[NUMACell(UNKNOWN),NUMACell(1)])", 
    "tags": [], 
    "importance": "High", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1417201", 
    "owner": "https://api.launchpad.net/1.0/~sahid-ferdjaoui", 
    "id": 1417201, 
    "index": 1681, 
    "openned": "2015-02-02 17:41:55.668717+00:00", 
    "created": "2015-02-02 17:41:55.668717+00:00", 
    "title": "nova-scheduler exception when trying to use hugepages", 
    "comments": [
        {
            "content": "I'm trying to make use of huge pages as described in \"http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-large-pages.html\".  I'm running nova kilo as of Jan 27th.  The other openstack services are juno.\n\nI've allocated 10000 2MB pages on a compute node.  \"virsh capabilities\" on that node contains:\n\n    <topology>\n      <cells num='2'>\n        <cell id='0'>\n          <memory unit='KiB'>67028244</memory>\n          <pages unit='KiB' size='4'>16032069</pages>\n          <pages unit='KiB' size='2048'>5000</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n...\n        <cell id='1'>\n          <memory unit='KiB'>67108864</memory>\n          <pages unit='KiB' size='4'>16052224</pages>\n          <pages unit='KiB' size='2048'>5000</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n\n\nI then restarted nova-compute, I set \"hw:mem_page_size=large\" on a flavor, and then tried to boot up an instance with that flavor.  I got the error logs below in nova-scheduler.  Is this a bug?\n\n\nFeb  2 16:23:10 controller-0 nova-scheduler Exception during message handling: Cannot load 'mempages' in the base class\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 134, in _dispatch_and_reply\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 177, in _dispatch\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py\", line 123, in _do_dispatch\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/oslo/messaging/rpc/server.py\", line 139, in inner\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/manager.py\", line 86, in select_destinations\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filter_scheduler.py\", line 67, in select_destinations\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filter_scheduler.py\", line 138, in _schedule\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     filter_properties, index=num)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/host_manager.py\", line 391, in get_filtered_hosts\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     hosts, filter_properties, index)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/filters.py\", line 77, in get_filtered_objects\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     list_objs = list(objs)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/filters.py\", line 43, in filter_all\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     if self._filter_one(obj, filter_properties):\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filters/__init__.py\", line 27, in _filter_one\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     return self.host_passes(obj, filter_properties)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/scheduler/filters/numa_topology_filter.py\", line 45, in host_passes\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     limits_topology=limits))\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 1161, in numa_fit_instance_to_host\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     host_cell, instance_cell, limit_cell)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 851, in _numa_fit_instance_cell\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     host_cell, instance_cell)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/virt/hardware.py\", line 692, in _numa_cell_supports_pagesize_request\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     avail_pagesize = [page.size_kb for page in host_cell.mempages]\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/objects/base.py\", line 72, in getter\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     self.obj_load_attr(name)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/lib64/python2.7/site-packages/nova/objects/base.py\", line 507, in obj_load_attr\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher     _(\"Cannot load '%s' in the base class\") % attrname)\n2015-02-02 16:23:10.746 37521 TRACE oslo.messaging.rpc.dispatcher NotImplementedError: Cannot load 'mempages' in the base class\n\n\nAs far as nova-compute, at the end of nova.virt.libvirt.driver.LibvirtDriver.get_available_resource() I've confirmed that data['numa_topology'] looks like this:\n\n'{\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cells\"], \"nova_object.name\": \"NUMATopology\", \"nova_object.data\": {\"cells\": [{\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cpu_usage\", \"memory_usage\", \"cpuset\", \"pinned_cpus\", \"siblings\", \"memory\", \"mempages\", \"id\"], \"nova_object.name\": \"NUMACell\", \"nova_object.data\": {\"cpu_usage\": 0, \"memory_usage\": 0, \"cpuset\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"pinned_cpus\": [], \"siblings\": [], \"memory\": 65457, \"mempages\": [{\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 16032069, \"used\": 0, \"size_kb\": 4}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 5000, \"used\": 0, \"size_kb\": 2048}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 1, \"used\": 0, \"size_kb\": 1048576}, \"nova_object.namespace\": \"nova\"}], \"id\": 0}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.2\", \"nova_object.changes\": [\"cpu_usage\", \"memory_usage\", \"cpuset\", \"pinned_cpus\", \"siblings\", \"memory\", \"mempages\", \"id\"], \"nova_object.name\": \"NUMACell\", \"nova_object.data\": {\"cpu_usage\": 0, \"memory_usage\": 0, \"cpuset\": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], \"pinned_cpus\": [], \"siblings\": [], \"memory\": 65536, \"mempages\": [{\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 16052224, \"used\": 0, \"size_kb\": 4}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 5000, \"used\": 0, \"size_kb\": 2048}, \"nova_object.namespace\": \"nova\"}, {\"nova_object.version\": \"1.0\", \"nova_object.changes\": [\"total\", \"size_kb\", \"used\"], \"nova_object.name\": \"NUMAPagesTopology\", \"nova_object.data\": {\"total\": 1, \"used\": 0, \"size_kb\": 1048576}, \"nova_object.namespace\": \"nova\"}], \"id\": 1}, \"nova_object.namespace\": \"nova\"}]}, \"nova_object.namespace\": \"nova\"}'\n\nI printed out str(host_topology) in NUMATopologyFilter.host_passes() and it gave:\n\nFeb  2 17:07:43 controller-0 nova-scheduler host_topology: NUMATopology(cells=[NUMACell(UNKNOWN),NUMACell(1)])", 
            "date_created": "2015-02-02 17:41:55.668717+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "I was not able to reproduce the problem with trunk. Please reopen with more information of your environment if still present. ", 
            "date_created": "2015-02-03 13:54:46.668994+00:00", 
            "author": "https://api.launchpad.net/1.0/~sahid-ferdjaoui"
        }, 
        {
            "content": "I was able to reproduce the problem with current devstack, using the local.conf file below.  The nova-compute log contained the following, notice in particular the \"Cannot load 'mempages' in the base class\" logs.\n\n\n2015-02-04 07:40:35.395 DEBUG oslo_concurrency.lockutils [-] Lock \"compute_resources\" acquired by \"instance_claim\" :: wai\nted 0.000s from (pid=25343) inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:430\n2015-02-04 07:40:35.396 DEBUG nova.compute.resource_tracker [-] Memory overhead for 4096 MB instance; 0 MB fro\nm (pid=25343) instance_claim /opt/stack/nova/nova/compute/resource_tracker.py:130\n2015-02-04 07:40:35.398 AUDIT nova.compute.claims [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] Attempting claim: \nmemory 4096 MB, disk 40 GB\n2015-02-04 07:40:35.399 AUDIT nova.compute.claims [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] Total memory: 1569\n1 MB, used: 512.00 MB\n2015-02-04 07:40:35.399 AUDIT nova.compute.claims [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] memory limit: 23536.50 MB, free: 23024.50 MB\n2015-02-04 07:40:35.399 AUDIT nova.compute.claims [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] Total disk: 82 GB, used: 0.00 GB\n2015-02-04 07:40:35.400 AUDIT nova.compute.claims [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] disk limit not specified, defaulting to unlimited\n2015-02-04 07:40:35.401 DEBUG oslo_concurrency.lockutils [-] Lock \"compute_resources\" released by \"instance_claim\" :: hel\nd 0.005s from (pid=25343) inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:442\n2015-02-04 07:40:35.402 DEBUG nova.compute.utils [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] Cannot load 'mempages' in the base class from (pid=25343) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:324\n2015-02-04 07:40:35.402 DEBUG nova.compute.manager [-] [instance: 2de1b982-13e7-44e9-96ce-ab40ad7b975d] Build of instance 2de1b982-13e7-44e9-96ce-ab40ad7b975d was re-scheduled: Cannot load 'mempages' in the base class from (pid=25343) _do_build_and_run_instance /opt/stack/nova/nova/compute/manager.py:2080\n\n\nThe local.conf file for devstack looked like this:\n\n[[local|localrc]]\nHOST_IP=192.168.100.249\nFLOATING_RANGE=192.168.100.33/27\nFIXED_RANGE=10.11.12.0/24\nFIXED_NETWORK_SIZE=256\nFLAT_INTERFACE=eth0\nNETWORK_GATEWAY=10.11.12.1\nPUBLIC_NETWORK_GATEWAY=192.168.100.33\n\nADMIN_PASSWORD=admin\nDATABASE_PASSWORD=$ADMIN_PASSWORD\nRABBIT_PASSWORD=$ADMIN_PASSWORD\nSERVICE_PASSWORD=$ADMIN_PASSWORD\nSERVICE_TOKEN=a682f596-76f3-11e3-b3b2-e716f9080d50\nSCREEN_LOGDIR=$DEST/logs/screen\n\nNOVA_BRANCH=master\nCEILOMETER_BRANCH=origin/stable/juno\nCINDER_BRANCH=origin/stable/juno\nGLANCE_BRANCH=origin/stable/juno\nHEAT_BRANCH=origin/stable/juno\nHORIZON_BRANCH=origin/stable/juno\nIRONIC_BRANCH=origin/stable/juno\nKEYSTONE_BRANCH=origin/stable/juno\nNEUTRON_BRANCH=origin/stable/juno\nNEUTRON_FWAAS_BRANCH=origin/stable/juno\nNEUTRON_LBAAS_BRANCH=origin/stable/juno\nNEUTRON_VPNAAS_BRANCH=origin/stable/juno\nSAHARA_BRANCH=origin/stable/juno\nSWIFT_BRANCH=origin/stable/juno\nTROVE_BRANCH=origin/stable/juno\n\n\n\"virsh capabilities\" contains:\n    <pages unit='KiB' size='4'/>\n    <pages unit='KiB' size='2048'/>\n\n\"virsh freepages --all\" gives:\nNode 0:\n4KiB: 99004\n2048KiB: 5000\n\nThe flavor in question looks like:\n+-----+-----------+-----------+------+-----------+------+-------+-------------+-----------+---------------------------------+\n| ID  | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | extra_specs                     |\n+-----+-----------+-----------+------+-----------+------+-------+-------------+-----------+---------------------------------+\n3   | m1.medium | 4096      | 40   | 0         |      | 2     | 1.0         | True      | {u'hw:mem_page_size': u'large'} \n\n", 
            "date_created": "2015-02-04 07:59:39.315864+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/152930", 
            "date_created": "2015-02-04 14:43:00.570680+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "With the series of fixes from\n\nhttps://review.openstack.org/152929\nhttps://review.openstack.org/152930\nhttps://review.openstack.org/152931\n\nI'm no longer seeing the error.", 
            "date_created": "2015-02-04 22:27:29.604879+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Thanks for the report and confirm Chris... They are now waiting for review.", 
            "date_created": "2015-02-05 07:36:34.767300+00:00", 
            "author": "https://api.launchpad.net/1.0/~sahid-ferdjaoui"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/152930\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=67e11d45c43facb1b0aab718b48aa8e2f7d3f161\nSubmitter: Jenkins\nBranch:    master\n\ncommit 67e11d45c43facb1b0aab718b48aa8e2f7d3f161\nAuthor: Sahid Orentino Ferdjaoui <email address hidden>\nDate:   Wed Feb 4 05:39:20 2015 -0500\n\n    objects: fix numa obj relationships\n    \n    The attributes used in obj_relationships for mempages\n    and cells are not correctly set. This commit fix the\n    errors.\n    \n    Related-Bug: #1417201\n    Change-Id: I0ae2db6d1cc14787f2d6b4b41047e51a5de61ed8\n", 
            "date_created": "2015-02-13 15:15:06.871340+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/152931\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=6b32c16329dbae332d4f52a9c14e41a24ac07ea1\nSubmitter: Jenkins\nBranch:    master\n\ncommit 6b32c16329dbae332d4f52a9c14e41a24ac07ea1\nAuthor: Sahid Orentino Ferdjaoui <email address hidden>\nDate:   Wed Feb 4 09:31:18 2015 -0500\n\n    hardware: fix reported host mempages in numa cell\n    \n    In commit b11dbfa4902cdd74bad3745db177d80b1c8b07c6 we lost\n    the mempages information when no guests are using huge pages\n    \n    Closes-Bug: #1417201\n    Change-Id: Id0871bf08e8ba43b386c1565e73bf2cb3f6a3a9d\n", 
            "date_created": "2015-02-13 18:12:34.452865+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2015-03-20 07:37:29.780009+00:00"
}
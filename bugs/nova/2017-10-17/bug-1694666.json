{
    "status": "Fix Released", 
    "last_updated": "2017-08-01 09:52:20.653885+00:00", 
    "description": "Description\n===========\n\nLaunching an instance causes a lot of error messages in nova-api logs.\nThe instance is not able to retrieve metadata.\n\nHow to reproduce\n================\n\nDeploy nova master branch.\nSpawn an instance.\nWait the instance to be active.\nIn nova-api logs will see error messages.\n\nExpected results\n================\n\nInstance retrieve metadata information\n\nActual results\n==============\n\nInstance is not able to retrieve metadata\n\nEnvironment configuration\n=========================\n\nOpenStack deployed with kolla\nOnly source images from master fail, binary(rdo or ubuntu packages) works for now\nAffected CentOS, Ubuntu and OracleLinux distributions\nDatabase and memcached works as expected, other services consuming them are not affected.\n\nLogs\n====\n\nAll logs can be found at kolla gates:\n\nNova: http://logs.openstack.org/73/469373/1/check/gate-kolla-ansible-dsvm-deploy-centos-source-centos-7-nv/8cecb36/logs/kolla/nova/\n\nNeutron: http://logs.openstack.org/73/469373/1/check/gate-kolla-ansible-dsvm-deploy-centos-source-centos-7-nv/8cecb36/logs/kolla/neutron/\n\nRelated errors:\n\nNova API:\n\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler [req-3daa8e91-93e5-4676-b77a-048ad3dd53d2 - - - - -] Failed to get metadata for instance id: 8cbd067f-8cd6-4365-b299-3ffc146d0790\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler Traceback (most recent call last):\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/api/metadata/handler.py\", line 285, in _get_meta_by_instance_id\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     remote_address)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/api/metadata/handler.py\", line 87, in get_metadata_by_instance_id\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     self._cache.set(cache_key, data)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/cache_utils.py\", line 116, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return self.region.set(key, value)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/dogpile/cache/region.py\", line 973, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     self.backend.set(key, self._value(value))\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/dogpile/cache/backends/memcached.py\", line 178, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     **self.set_arguments\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/oslo_cache/backends/memcache_pool.py\", line 32, in _run_method\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return getattr(client, __name)(*args, **kwargs)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 740, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return self._set(\"set\", key, val, time, min_compress_len, noreply)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 1060, in _set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return _unsafe_set()\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 1034, in _unsafe_set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     store_info = self._val_to_store_info(val, min_compress_len)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 998, in _val_to_store_info\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     pickler.dump(val)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler PicklingError: Can't pickle <class 'sqlalchemy.orm.session.Session'>: it's not the same object as sqlalchemy.orm.session.Session\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler\n\nNeutron metadata:\n\n2017-05-31 09:09:24.985 23 DEBUG neutron.agent.metadata.agent [-] Request: GET /2009-04-04/meta-data/instance-id HTTP/1.0\nAccept: */*\nConnection: close\nContent-Type: text/plain\nHost: 169.254.169.254\nUser-Agent: curl/7.24.0 (x86_64-pc-linux-gnu) libcurl/7.24.0 OpenSSL/1.0.0j zlib/1.2.6\nX-Forwarded-For: 10.0.0.5\nX-Neutron-Network-Id: 50395d7c-fcc6-4aef-afdc-931c4573b0d1 __call__ /var/lib/kolla/venv/lib/python2.7/site-packages/neutron/agent/metadata/agent.py:86\n2017-05-31 09:09:25.269 23 WARNING neutron.agent.metadata.agent [-] Remote metadata server experienced an internal server error.\n2017-05-31 09:09:25.269 23 INFO eventlet.wsgi.server [-] 10.0.0.5,<local> \"GET /2009-04-04/meta-data/instance-id HTTP/1.1\" status: 500  len: 361 time: 0.2839720\n2017-05-31 09:09:27.460 23 DEBUG eventlet.wsgi.server [-] (23) accepted '' server /var/lib/kolla/venv/lib/python2.7/site-packages/eventlet/wsgi.py:883\n\nLet me know if need more information.\n\nRegards, Eduardo", 
    "tags": [
        "metadata"
    ], 
    "importance": "Medium", 
    "heat": 30, 
    "link": "https://bugs.launchpad.net/nova/+bug/1694666", 
    "owner": "https://api.launchpad.net/1.0/~danms", 
    "id": 1694666, 
    "index": 4840, 
    "openned": "2017-05-31 10:12:41.969236+00:00", 
    "created": "2017-05-31 10:12:41.969236+00:00", 
    "title": "metadata service PicklingError: Can't pickle <class 'sqlalchemy.orm.session.Session'> when using memcached", 
    "comments": [
        {
            "content": "Description\n===========\n\nLaunching an instance causes a lot of error messages in nova-api logs.\nThe instance is not able to retrieve metadata.\n\nHow to reproduce\n================\n\nDeploy nova master branch.\nSpawn an instance.\nWait the instance to be active.\nIn nova-api logs will see error messages.\n\nExpected results\n================\n\nInstance retrieve metadata information\n\nActual results\n==============\n\nInstance is not able to retrieve metadata\n\nEnvironment configuration\n=========================\n\nOpenStack deployed with kolla\nOnly source images from master fail, binary(rdo or ubuntu packages) works for now\nAffected CentOS, Ubuntu and OracleLinux distributions\n\nLogs\n====\n\nAll logs can be found at kolla gates:\n\nNova: http://logs.openstack.org/73/469373/1/check/gate-kolla-ansible-dsvm-deploy-centos-source-centos-7-nv/8cecb36/logs/kolla/nova/\n\nNeutron: http://logs.openstack.org/73/469373/1/check/gate-kolla-ansible-dsvm-deploy-centos-source-centos-7-nv/8cecb36/logs/kolla/neutron/\n\n\nRelated errors:\n\nNova API:\n\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler [req-3daa8e91-93e5-4676-b77a-048ad3dd53d2 - - - - -] Failed to get metadata for instance id: 8cbd067f-8cd6-4365-b299-3ffc146d0790\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler Traceback (most recent call last):\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/api/metadata/handler.py\", line 285, in _get_meta_by_instance_id\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     remote_address)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/api/metadata/handler.py\", line 87, in get_metadata_by_instance_id\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     self._cache.set(cache_key, data)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/nova/cache_utils.py\", line 116, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return self.region.set(key, value)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/dogpile/cache/region.py\", line 973, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     self.backend.set(key, self._value(value))\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/dogpile/cache/backends/memcached.py\", line 178, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     **self.set_arguments\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/oslo_cache/backends/memcache_pool.py\", line 32, in _run_method\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return getattr(client, __name)(*args, **kwargs)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 740, in set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return self._set(\"set\", key, val, time, min_compress_len, noreply)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 1060, in _set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     return _unsafe_set()\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 1034, in _unsafe_set\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     store_info = self._val_to_store_info(val, min_compress_len)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler   File \"/var/lib/kolla/venv/lib/python2.7/site-packages/memcache.py\", line 998, in _val_to_store_info\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler     pickler.dump(val)\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler PicklingError: Can't pickle <class 'sqlalchemy.orm.session.Session'>: it's not the same object as sqlalchemy.orm.session.Session\n2017-05-31 09:09:34.703 31 ERROR nova.api.metadata.handler \n\nNeutron metadata:\n\n2017-05-31 09:09:24.985 23 DEBUG neutron.agent.metadata.agent [-] Request: GET /2009-04-04/meta-data/instance-id HTTP/1.0\nAccept: */*\nConnection: close\nContent-Type: text/plain\nHost: 169.254.169.254\nUser-Agent: curl/7.24.0 (x86_64-pc-linux-gnu) libcurl/7.24.0 OpenSSL/1.0.0j zlib/1.2.6\nX-Forwarded-For: 10.0.0.5\nX-Neutron-Network-Id: 50395d7c-fcc6-4aef-afdc-931c4573b0d1 __call__ /var/lib/kolla/venv/lib/python2.7/site-packages/neutron/agent/metadata/agent.py:86\n2017-05-31 09:09:25.269 23 WARNING neutron.agent.metadata.agent [-] Remote metadata server experienced an internal server error.\n2017-05-31 09:09:25.269 23 INFO eventlet.wsgi.server [-] 10.0.0.5,<local> \"GET /2009-04-04/meta-data/instance-id HTTP/1.1\" status: 500  len: 361 time: 0.2839720\n2017-05-31 09:09:27.460 23 DEBUG eventlet.wsgi.server [-] (23) accepted '' server /var/lib/kolla/venv/lib/python2.7/site-packages/eventlet/wsgi.py:883\n\n\nLet me know if need more information.\n\nRegards, Eduardo", 
            "date_created": "2017-05-31 10:12:41.969236+00:00", 
            "author": "https://api.launchpad.net/1.0/~egonzalez90"
        }, 
        {
            "content": "http://logs.openstack.org/73/469373/1/check/gate-kolla-ansible-dsvm-deploy-centos-source-centos-7-nv/8cecb36/logs/kolla/nova/nova-api.log", 
            "date_created": "2017-06-01 14:49:58.274234+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Does this also happen in Ocata? I'm wondering if this is a regression in Pike (current master branch).\n\nWhen you say \"Only source images from master fail, binary(rdo or ubuntu packages) works for now\" - what is the difference? Are the binary packages from Ocata (nova 15.0.0)?", 
            "date_created": "2017-06-01 14:52:49.141546+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "No, in Ocata is not happening:\nLatest build and deploy from kolla gates (centos source):\n\nhttp://logs.openstack.org/55/468655/1/check/gate-kolla-dsvm-deploy-centos-source-centos-7-nv/8fb1bea/logs/kolla/nova/nova-api.txt.gz", 
            "date_created": "2017-06-01 15:06:55.581876+00:00", 
            "author": "https://api.launchpad.net/1.0/~egonzalez90"
        }, 
        {
            "content": "So it looks like oslo.cache configured for dogpile/memcache will use python-memcached:\n\nhttps://github.com/openstack/oslo.cache/blob/master/setup.cfg#L37\n\nWhich uses pickle:\n\nhttps://github.com/linsomniac/python-memcached/blob/master/memcache.py#L63\n\nReading http://www.benfrederickson.com/dont-pickle-your-data/ it sounds like that's a bad idea, not only in terms of performance but also security.\n\nIt sounds like this has come up before though, and we shouldn't be trying to serialize the entire InstanceMetadata object which contains things inside it like Nova Instance objects, which could contain oslo.db Sessions that have weird c-code or something inside them.\n\nWhat Nova should probably be doing is creating a smaller primitive of the InstanceMetadata object for serializing. Someone though artom might be working on this, or maybe knows something that is. We could ask him.", 
            "date_created": "2017-06-01 15:58:33.986135+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "An idea we agreed on in IRC was to to_primitive() and from_primitive() before/after we hand/get stuff to/from cache. If anyone wants to have a whack at that they're welcome :) Otherwise I'll try to make it happen.", 
            "date_created": "2017-06-01 16:09:29.993665+00:00", 
            "author": "https://api.launchpad.net/1.0/~notartom"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/469978", 
            "date_created": "2017-06-01 19:04:07.278057+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/478991", 
            "date_created": "2017-06-29 16:18:05.544749+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Found this happening in RDO Master (Pike) and we are seeing this in rhos 12.0\n\nTripleo sets backend=oslo_cache.memcache_pool\n\nhttp://git.openstack.org/cgit/openstack/puppet-tripleo/tree/manifests/profile/base/nova.pp#n139\n\nThis job logs are showing same results\n\nhttp://logs.openstack.org/periodic/periodic-tripleo-ci-centos-7-ovb-nonha-tempest-oooq-master/34a88fa/logs/oooq/overcloud-controller-0/var/log/nova/nova-api.log.txt.gz", 
            "date_created": "2017-06-29 17:20:43.077400+00:00", 
            "author": "https://api.launchpad.net/1.0/~jon-schlueter"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/478991\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=2fee972bde4a04d398d32aa6c8b6d27819db697b\nSubmitter: Jenkins\nBranch:    master\n\ncommit 2fee972bde4a04d398d32aa6c8b6d27819db697b\nAuthor: Dan Smith <email address hidden>\nDate:   Thu Jun 29 09:42:20 2017 -0700\n\n    Sanitize instance in InstanceMetadata to avoid un-pickleable context\n    \n    This is a more strategic fix for the issue of us trying to pickle\n    an instance with a context that has complex data structures inside\n    (i.e. SQLAlchemy connections) into the oslo cache. The right solution\n    is for us to stop storing random python objects (InstanceMetadata)\n    in the cache with pickle. However, that's a larger change and more\n    complex for deployers to roll over. This attempts to sanitize the\n    instance before we pickle it to get things working again.\n    \n    Change-Id: Ie7d97ce5c62c8fb9da5822590a64210521f8ae7a\n    Closes-Bug: #1694666\n", 
            "date_created": "2017-06-30 08:03:10.328249+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/479325", 
            "date_created": "2017-06-30 14:17:44.879897+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Following topologies tested OK via TripleO QuickStart Master branch overcloud deployments\n\n1. 1xController + 2xComputes\n2. 1xController + 1xCompute + 2xCephNodes\n\nIt's possible to deploy HA topology, but it seems to be affected by different bug :-\nAttaching interface (private subnet) to HA Neutron Router been setup on PCS Controller's\ncluster comes every time to Down port status.\nNeutron Server log reports :-\n\n2017-07-03 10:04:53.779 68873 WARNING stevedore.named [req-a81938fc-bb4e-4dd3-97ce-a67503e1799c - - - - -] Could not load networking_l2gw.\nservices.l2gateway.service_drivers.rpc_l2gw.L2gwRpcDriver\n2017-07-03 10:04:53.779 68873 WARNING stevedore.named [req-a81938fc-bb4e-4dd3-97ce-a67503e1799c - - - - -]Could not loadnetworking_bgpvpn.\nneutron.services.service_drivers.driver_api.BGPVPNDriver\n2017-07-03 10:04:53.802 68873 INFO neutron.services.service_base [req-a81938fc-bb4e-4dd3-97ce-a67503e1799c - - - - -]\nDefault provider is not specified for service type L3_ROUTER_NAT: DefaultServiceProviderNotFound:\nService type L3_ROUTER_NAT does not have a default service provide\n\nI am not sure are this messages related to the issue or not.", 
            "date_created": "2017-07-03 15:02:46.046723+00:00", 
            "author": "https://api.launchpad.net/1.0/~bderzhavets"
        }, 
        {
            "content": ">It's possible to deploy HA topology, but it seems to be affected by different bug :-\n>Attaching interface (private subnet) to HA Neutron Router been setup on PCS Controller's\n\nBeen built by TripleO QS HA Neutron Router works as expected (normally) no mater that\nstandard tools detect internal inteface port \"DOWN\". It is actually \"UP\"\nSo,TripleO QS HA overcloud deployment is OK with patches been suggested.", 
            "date_created": "2017-07-04 21:15:14.731463+00:00", 
            "author": "https://api.launchpad.net/1.0/~bderzhavets"
        }, 
        {
            "content": "Change abandoned by Dan Smith (<email address hidden>) on branch: master\nReview: https://review.openstack.org/479325", 
            "date_created": "2017-07-06 21:20:29.060799+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 16.0.0.0b3 development milestone.", 
            "date_created": "2017-07-28 07:41:19.138606+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/469978\nReason: This review is > 4 weeks without comment, and is not mergable in it's current state. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and leaving a 'recheck' comment to get fresh test results.", 
            "date_created": "2017-08-01 09:52:19.984863+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2017-06-30 08:03:08.162078+00:00"
}
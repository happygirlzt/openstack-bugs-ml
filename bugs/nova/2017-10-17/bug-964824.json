{
    "status": "Fix Released", 
    "last_updated": "2012-09-27 15:20:04.334392+00:00", 
    "description": "I posted this on the ML, but I think it should also be a bug because we have some potentially serious issues.  See below for details, but here are the issues (let me know if I should split into separate bugs etc):\n\n\"Critical\"\n\nTable scan of fixed_ips on the network service (row per IP address?)\nUse of MyISAM tables, particularly for s3_images and block_device_mapping\nTable scan of virtual_interfaces (row per instance?)\nVerify that MySQL isn't doing a table scan on http://paste.openstack.org/show/12110/ when # of instances is large\n\n\"Naughty\"\n\n(Mostly because the tables are small)\n\nTable scan of s3_images\nTable scan of services\nTable scan of networks\n\nLow importance\n\n(Re-fetches aren't a big deal if the queries are fast)\n\nRow re-fetches & re-re-fetches\n\n\n---\n\nThe performance of the metadata query with cloud-init has been causing some people problems (it's so slow cloud-init times out!), and has led to the suggestion that we need lots of caching.  (My hypothesis is that we don't...)\n\nBy turning on SQL debugging in SQL Alchemy (for which I've proposed a patch for Essex: https://review.openstack.org/#change,5783), I was able to capture the SQL statements.\n\nI'm focusing on the SQL statements for the metadata call.\n\nThe code does this:\n\n1) Checks the cache to see if it has the data\n2) Makes a message-bus call to the network service to get the fixed_ip info from the address\n3) Looks up all sort of metadata in the database\n4) Formats the reply\n\n#1 means that the first call is slower than the others, so we need to focus on the first call.\n#2 could be problematic, if the message queue is overloaded or if the network service is slow to response\n#3 could be problematic if the DB isn't working properly\n#4 is hopefully not the problem.\n\nThe relevant SQL log from the API server: http://paste.openstack.org/show/12109/\n\nAnd from the network server: http://paste.openstack.org/show/12116/\n\nI've analyzed each of the SQL statements:\n\nAPI\n\nhttp://paste.openstack.org/show/12110/ (Need to check that there isn't a table scan when instance_info_caches is large)\nhttp://paste.openstack.org/show/12111/ Table scan on services table, but this is presumably smallish\nhttp://paste.openstack.org/show/12112/ No index.  Table scan on s3_images table.  Also this table is MyISAM.  Also seems to insert rows on the first call (not shown).  Evil.\nhttp://paste.openstack.org/show/12113/ \nhttp://paste.openstack.org/show/12114/ block_device_mapping is MyISAM.\n\n\nNetwork\n\nhttp://paste.openstack.org/show/12117/\nhttp://paste.openstack.org/show/12118/ (Fetch virtual_interface by instance_id)\nhttp://paste.openstack.org/show/12119/ (Fetch network by id)\nhttp://paste.openstack.org/show/12120/ Missing index => table scan on networks.  Unnecessary row re-fetch.\nhttp://paste.openstack.org/show/12121/ Missing index => table scan on virtual_interfaces.  Unnecessary row-refetch.\nhttp://paste.openstack.org/show/12122/ (Fetch fixed_ips on virtual interface)\nhttp://paste.openstack.org/show/12123/ Missing index => table scan on networks.  Unnecessary row re-fetch.  (Double re-fetch.  What does it mean?)\nhttp://paste.openstack.org/show/12124/ Missing index => table scan on virtual_interfaces.  Another re-re-fetch.\nhttp://paste.openstack.org/show/12125/ Missing index => table scan on fixed_ips (Uh-oh - I hope you didn't allocate a /8!!).  We do have this row from the virtual interface lookup; perhaps we could remove this query?\nhttp://paste.openstack.org/show/12126/\nhttp://paste.openstack.org/show/12127/\n\n\nWe still have a bunch of MyISAM tables (at least with a devstack install):\nhttp://paste.openstack.org/show/12115/  \n\n\n\nAs I see it, these are the issues (in sort of priority order):\n\nCritical\n\nTable scan of fixed_ips on the network service (row per IP address?)\nUse of MyISAM tables, particularly for s3_images and block_device_mapping\nTable scan of virtual_interfaces (row per instance?)\nVerify that MySQL isn't doing a table scan on http://paste.openstack.org/show/12110/ when # of instances is large\n\nNaughty\n\n(Mostly because the tables are small)\n\nTable scan of s3_images\nTable scan of services\nTable scan of networks\n\nLow importance\n\n(Re-fetches aren't a big deal if the queries are fast)\n\nRow re-fetches & re-re-fetches\n\n\n\nMy install in nowhere near big enough for any of these to actually cause a real problem, so I'd love to get timings / a log from someone that is having a problem.  Even the table scan of fixed_ips should be OK if you have enough RAM.", 
    "tags": [
        "db"
    ], 
    "importance": "Medium", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/964824", 
    "owner": "https://api.launchpad.net/1.0/~sdague", 
    "id": 964824, 
    "index": 2828, 
    "openned": "2012-03-25 22:00:58.008450+00:00", 
    "created": "2012-03-25 22:00:58.008450+00:00", 
    "title": "Database tuning required", 
    "comments": [
        {
            "content": "I posted this on the ML, but I think it should also be a bug because we have some potentially serious issues.  See below for details, but here are the issues (let me know if I should split into separate bugs etc):\n\n\"Critical\"\n\nTable scan of fixed_ips on the network service (row per IP address?)\nUse of MyISAM tables, particularly for s3_images and block_device_mapping\nTable scan of virtual_interfaces (row per instance?)\nVerify that MySQL isn't doing a table scan on http://paste.openstack.org/show/12110/ when # of instances is large\n\n\"Naughty\"\n\n(Mostly because the tables are small)\n\nTable scan of s3_images\nTable scan of services\nTable scan of networks\n\nLow importance\n\n(Re-fetches aren't a big deal if the queries are fast)\n\nRow re-fetches & re-re-fetches\n\n\n---\n\nThe performance of the metadata query with cloud-init has been causing some people problems (it's so slow cloud-init times out!), and has led to the suggestion that we need lots of caching.  (My hypothesis is that we don't...)\n\nBy turning on SQL debugging in SQL Alchemy (for which I've proposed a patch for Essex: https://review.openstack.org/#change,5783), I was able to capture the SQL statements.\n\nI'm focusing on the SQL statements for the metadata call.\n\nThe code does this:\n\n1) Checks the cache to see if it has the data\n2) Makes a message-bus call to the network service to get the fixed_ip info from the address\n3) Looks up all sort of metadata in the database\n4) Formats the reply\n\n#1 means that the first call is slower than the others, so we need to focus on the first call.\n#2 could be problematic, if the message queue is overloaded or if the network service is slow to response\n#3 could be problematic if the DB isn't working properly\n#4 is hopefully not the problem.\n\nThe relevant SQL log from the API server: http://paste.openstack.org/show/12109/\n\nAnd from the network server: http://paste.openstack.org/show/12116/\n\nI've analyzed each of the SQL statements:\n\nAPI\n\nhttp://paste.openstack.org/show/12110/ (Need to check that there isn't a table scan when instance_info_caches is large)\nhttp://paste.openstack.org/show/12111/ Table scan on services table, but this is presumably smallish\nhttp://paste.openstack.org/show/12112/ No index.  Table scan on s3_images table.  Also this table is MyISAM.  Also seems to insert rows on the first call (not shown).  Evil.\nhttp://paste.openstack.org/show/12113/ \nhttp://paste.openstack.org/show/12114/ block_device_mapping is MyISAM.\n\n\nNetwork\n\nhttp://paste.openstack.org/show/12117/\nhttp://paste.openstack.org/show/12118/ (Fetch virtual_interface by instance_id)\nhttp://paste.openstack.org/show/12119/ (Fetch network by id)\nhttp://paste.openstack.org/show/12120/ Missing index => table scan on networks.  Unnecessary row re-fetch.\nhttp://paste.openstack.org/show/12121/ Missing index => table scan on virtual_interfaces.  Unnecessary row-refetch.\nhttp://paste.openstack.org/show/12122/ (Fetch fixed_ips on virtual interface)\nhttp://paste.openstack.org/show/12123/ Missing index => table scan on networks.  Unnecessary row re-fetch.  (Double re-fetch.  What does it mean?)\nhttp://paste.openstack.org/show/12124/ Missing index => table scan on virtual_interfaces.  Another re-re-fetch.\nhttp://paste.openstack.org/show/12125/ Missing index => table scan on fixed_ips (Uh-oh - I hope you didn't allocate a /8!!).  We do have this row from the virtual interface lookup; perhaps we could remove this query?\nhttp://paste.openstack.org/show/12126/\nhttp://paste.openstack.org/show/12127/\n\n\nWe still have a bunch of MyISAM tables (at least with a devstack install):\nhttp://paste.openstack.org/show/12115/  \n\n\n\nAs I see it, these are the issues (in sort of priority order):\n\nCritical\n\nTable scan of fixed_ips on the network service (row per IP address?)\nUse of MyISAM tables, particularly for s3_images and block_device_mapping\nTable scan of virtual_interfaces (row per instance?)\nVerify that MySQL isn't doing a table scan on http://paste.openstack.org/show/12110/ when # of instances is large\n\nNaughty\n\n(Mostly because the tables are small)\n\nTable scan of s3_images\nTable scan of services\nTable scan of networks\n\nLow importance\n\n(Re-fetches aren't a big deal if the queries are fast)\n\nRow re-fetches & re-re-fetches\n\n\n\nMy install in nowhere near big enough for any of these to actually cause a real problem, so I'd love to get timings / a log from someone that is having a problem.  Even the table scan of fixed_ips should be OK if you have enough RAM.", 
            "date_created": "2012-03-25 22:00:58.008450+00:00", 
            "author": "https://api.launchpad.net/1.0/~justin-fathomdb"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/5970", 
            "date_created": "2012-03-29 20:52:05.998191+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "The fix I added is only for a small piece of this, we should probably explode the overall bug into a bunch of smaller ones and work them through individually.", 
            "date_created": "2012-03-29 20:54:47.152702+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/5970\nCommitted: http://github.com/openstack/nova/commit/27ea547fbd7c936bd017b64b31ecf09ed991c6c0\nSubmitter: Jenkins\nBranch:    master\n\ncommit 27ea547fbd7c936bd017b64b31ecf09ed991c6c0\nAuthor: Sean Dague <email address hidden>\nDate:   Thu Mar 29 16:44:56 2012 -0400\n\n    Add index to fixed_ips.address\n    \n    Addresses one of the Critical items in bug #964824\n    \n    Add migration to add an index to fixed_ips.address to remove a table\n    scan from active code.\n    \n    Change-Id: Ic5a56bbddd0d434f8a37ad049753e6d25c7ca760\n", 
            "date_created": "2012-04-09 23:21:52.505103+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I'm closing this out as most of the Critical pieces are now addressed in master. I think it's time for a new analysis and individual bugs to be opened up for individual tunings after this.", 
            "date_created": "2012-05-02 19:46:07.236902+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ], 
    "closed": "2012-05-23 12:59:38.241991+00:00"
}
{
    "status": "Fix Released", 
    "last_updated": "2014-01-28 10:12:54.300151+00:00", 
    "description": "When the volume creation process fails during scheduling (i.e. there is no appropriate host) the status in DB (and in nova volume-list output as a result) hangs with a \"creating...\" value.\n\nIn such case to figure out that volume creation failed one should go and see /var/log/nova/nova-scheduler.log (which is not an obvious action to do). Moreover, volume stuck with \"creating...\" status cannot be deleted with nova volume-delete. To delete it one have to change it's status to error in DB.\n\n\nSimple scheduler is being used (nova.conf):\n\n--scheduler_driver=nova.scheduler.simple.SimpleScheduler\n\n\nHere is a sample output from DB:\n\n*************************** 3. row ***************************\n         created_at: 2012-09-21 09:55:42\n         updated_at: NULL\n         deleted_at: NULL\n            deleted: 0\n                 id: 15\n             ec2_id: NULL\n            user_id: b0aadfc80b094d94b78d68dcdc7e8757\n         project_id: 3b892f660ea2458aa9aa9c9a21352632\n               host: NULL\n               size: 1\n  availability_zone: nova\n        instance_id: NULL\n         mountpoint: NULL\n        attach_time: NULL\n             status: creating\n      attach_status: detached\n       scheduled_at: NULL\n        launched_at: NULL\n      terminated_at: NULL\n       display_name: NULL\ndisplay_description: NULL\n  provider_location: NULL\n      provider_auth: NULL\n        snapshot_id: NULL\n     volume_type_id: NULL\n\n\nHere is a part of interest in nova-scheduler.log:\n\n    pic': u'volume', u'filter_properties': {u'scheduler_hints': {}}, u'snapshot_id': None, u'volume_id': 16}, u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': True, u'_context_project_id': u'3b    892f660ea2458aa9aa9c9a21352632', u'_context_timestamp': u'2012-09-21T10:15:47.091307', u'_context_user_id': u'b0aadfc80b094d94b78d68dcdc7e8757', u'method': u'create_volume', u'_context_remote_address':     u'172.18.67.146'} from (pid=11609) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:160\n 15 2012-09-21 10:15:47 DEBUG nova.rpc.amqp [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] unpacked context: {'user_id': u'b0aadfc80b094d94b78d    68dcdc7e8757', 'roles': [u'admin'], 'timestamp': '2012-09-21T10:15:47.091307', 'auth_token': '<SANITIZED>', 'remote_address': u'172.18.67.146', 'is_admin': True, 'request_id': u'req-01f7dd30-0421-4ef3-    a675-16b0cf1362eb', 'project_id': u'3b892f660ea2458aa9aa9c9a21352632', 'read_deleted': u'no'} from (pid=11609) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:160\n 14 2012-09-21 10:15:47 WARNING nova.scheduler.manager [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] Failed to schedule_create_volume: No vali    d host was found. Is the appropriate service running?\n 13 2012-09-21 10:15:47 ERROR nova.rpc.amqp [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] Exception during message handling\n 12 2012-09-21 10:15:47 TRACE nova.rpc.amqp Traceback (most recent call last):\n 11 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/rpc/amqp.py\", line 253, in _process_data\n 10 2012-09-21 10:15:47 TRACE nova.rpc.amqp     rval = node_func(context=ctxt, **node_args)\n  9 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 97, in _schedule\n  8 2012-09-21 10:15:47 TRACE nova.rpc.amqp     context, ex, *args, **kwargs)\n  7 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n  6 2012-09-21 10:15:47 TRACE nova.rpc.amqp     self.gen.next()\n  5 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 92, in _schedule\n  4 2012-09-21 10:15:47 TRACE nova.rpc.amqp     return driver_method(*args, **kwargs)\n  3 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/simple.py\", line 227, in schedule_create_volume\n  2 2012-09-21 10:15:47 TRACE nova.rpc.amqp     raise exception.NoValidHost(reason=msg)\n  1 2012-09-21 10:15:47 TRACE nova.rpc.amqp NoValidHost: No valid host was found. Is the appropriate service running?\n  0 2012-09-21 10:15:47 TRACE nova.rpc.amqp", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1053931", 
    "owner": "https://api.launchpad.net/1.0/~russellb", 
    "id": 1053931, 
    "index": 3074, 
    "openned": "2012-09-21 10:17:39.180035+00:00", 
    "created": "2012-09-21 10:17:39.180035+00:00", 
    "title": "Volume hangs in 'creating' status even though scheduler raises 'No valid host' exception", 
    "comments": [
        {
            "content": "When the volume creation process fails during scheduling (i.e. there is no appropriate host) the status in DB (and in nova volume-list output as a result) hangs with a \"creating...\" value.\n\nIn such case to figure out that volume creation failed one should go and see /var/log/nova/nova-scheduler.log (which is not an obvious action to do). Moreover, volume stuck with \"creating...\" status cannot be deleted with nova volume-delete. To delete it one have to change it's status to error in DB.\n\n\nSimple scheduler is being used (nova.conf):\n\n--scheduler_driver=nova.scheduler.simple.SimpleScheduler\n\n\nHere is a sample output from DB:\n\n*************************** 3. row ***************************\n         created_at: 2012-09-21 09:55:42\n         updated_at: NULL\n         deleted_at: NULL\n            deleted: 0\n                 id: 15\n             ec2_id: NULL\n            user_id: b0aadfc80b094d94b78d68dcdc7e8757\n         project_id: 3b892f660ea2458aa9aa9c9a21352632\n               host: NULL\n               size: 1\n  availability_zone: nova\n        instance_id: NULL\n         mountpoint: NULL\n        attach_time: NULL\n             status: creating\n      attach_status: detached\n       scheduled_at: NULL\n        launched_at: NULL\n      terminated_at: NULL\n       display_name: NULL\ndisplay_description: NULL\n  provider_location: NULL\n      provider_auth: NULL\n        snapshot_id: NULL\n     volume_type_id: NULL\n\n\nHere is a part of interest in nova-scheduler.log:\n\n    pic': u'volume', u'filter_properties': {u'scheduler_hints': {}}, u'snapshot_id': None, u'volume_id': 16}, u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': True, u'_context_project_id': u'3b    892f660ea2458aa9aa9c9a21352632', u'_context_timestamp': u'2012-09-21T10:15:47.091307', u'_context_user_id': u'b0aadfc80b094d94b78d68dcdc7e8757', u'method': u'create_volume', u'_context_remote_address':     u'172.18.67.146'} from (pid=11609) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:160\n 15 2012-09-21 10:15:47 DEBUG nova.rpc.amqp [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] unpacked context: {'user_id': u'b0aadfc80b094d94b78d    68dcdc7e8757', 'roles': [u'admin'], 'timestamp': '2012-09-21T10:15:47.091307', 'auth_token': '<SANITIZED>', 'remote_address': u'172.18.67.146', 'is_admin': True, 'request_id': u'req-01f7dd30-0421-4ef3-    a675-16b0cf1362eb', 'project_id': u'3b892f660ea2458aa9aa9c9a21352632', 'read_deleted': u'no'} from (pid=11609) _safe_log /usr/lib/python2.7/dist-packages/nova/rpc/common.py:160\n 14 2012-09-21 10:15:47 WARNING nova.scheduler.manager [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] Failed to schedule_create_volume: No vali    d host was found. Is the appropriate service running?\n 13 2012-09-21 10:15:47 ERROR nova.rpc.amqp [req-01f7dd30-0421-4ef3-a675-16b0cf1362eb b0aadfc80b094d94b78d68dcdc7e8757 3b892f660ea2458aa9aa9c9a21352632] Exception during message handling\n 12 2012-09-21 10:15:47 TRACE nova.rpc.amqp Traceback (most recent call last):\n 11 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/rpc/amqp.py\", line 253, in _process_data\n 10 2012-09-21 10:15:47 TRACE nova.rpc.amqp     rval = node_func(context=ctxt, **node_args)\n  9 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 97, in _schedule\n  8 2012-09-21 10:15:47 TRACE nova.rpc.amqp     context, ex, *args, **kwargs)\n  7 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n  6 2012-09-21 10:15:47 TRACE nova.rpc.amqp     self.gen.next()\n  5 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/manager.py\", line 92, in _schedule\n  4 2012-09-21 10:15:47 TRACE nova.rpc.amqp     return driver_method(*args, **kwargs)\n  3 2012-09-21 10:15:47 TRACE nova.rpc.amqp   File \"/usr/lib/python2.7/dist-packages/nova/scheduler/simple.py\", line 227, in schedule_create_volume\n  2 2012-09-21 10:15:47 TRACE nova.rpc.amqp     raise exception.NoValidHost(reason=msg)\n  1 2012-09-21 10:15:47 TRACE nova.rpc.amqp NoValidHost: No valid host was found. Is the appropriate service running?\n  0 2012-09-21 10:15:47 TRACE nova.rpc.amqp", 
            "date_created": "2012-09-21 10:17:39.180035+00:00", 
            "author": "https://api.launchpad.net/1.0/~galkindmitrii"
        }, 
        {
            "content": "What version of nova?", 
            "date_created": "2012-09-21 16:01:42.879138+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Read through current nova master and appears to still be a problem.  It looks like it applies to cinder, as well.", 
            "date_created": "2012-09-21 17:36:11.156190+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Actually, this does not appear to affect Cinder as of this commit:\n\n\ncommit f758bde47439be52a743b2b4181d4900f2c1bc8a\nAuthor: ZhuRongze <email address hidden>\nDate:   Sat Jul 28 08:14:03 2012 +0000\n\n    Scheduler-clean-up\n    \n    Some instance related functions and opts were still present in Cinder\n    Scheduler.\n    \n    Remove the functions and opts and modify some computer related comments.\n    \n    Change-Id: Ia01099e66bcbb129493870dfbf212d5e6669ebe1\n", 
            "date_created": "2012-09-21 17:43:13.615202+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/13479", 
            "date_created": "2012-09-21 18:06:58.360149+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: milestone-proposed\nReview: https://review.openstack.org/13496", 
            "date_created": "2012-09-21 20:26:59.699490+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/13479\nCommitted: http://github.com/openstack/nova/commit/75fa03557fd6f1e7c62079e9e89556f1af139202\nSubmitter: Jenkins\nBranch:    master\n\ncommit 75fa03557fd6f1e7c62079e9e89556f1af139202\nAuthor: Russell Bryant <email address hidden>\nDate:   Fri Sep 21 13:51:15 2012 -0400\n\n    Set volume status to error if scheduling fails.\n    \n    Fix bug 1053931.\n    \n    When scheduling volume creation fails, the volume was left with a status\n    of 'creating'.  This patch changes the scheduler manager to set the\n    status to 'error' if scheduling fails.  This matches the behavior of the\n    cinder scheduler manager in this case.\n    \n    This particular issue was addressed in Cinder as a part of commit\n    f758bde47439be52a743b2b4181d4900f2c1bc8a.\n    \n    Change-Id: Ieb453ab05b3b84de53f72323c536a9567555df1e\n", 
            "date_created": "2012-09-21 21:48:18.542486+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/13496\nCommitted: http://github.com/openstack/nova/commit/fb1f3502da1839d7bb700af16cc3a107da5d8624\nSubmitter: Jenkins\nBranch:    milestone-proposed\n\ncommit fb1f3502da1839d7bb700af16cc3a107da5d8624\nAuthor: Russell Bryant <email address hidden>\nDate:   Fri Sep 21 13:51:15 2012 -0400\n\n    Set volume status to error if scheduling fails.\n    \n    Fix bug 1053931.\n    \n    When scheduling volume creation fails, the volume was left with a status\n    of 'creating'.  This patch changes the scheduler manager to set the\n    status to 'error' if scheduling fails.  This matches the behavior of the\n    cinder scheduler manager in this case.\n    \n    This particular issue was addressed in Cinder as a part of commit\n    f758bde47439be52a743b2b4181d4900f2c1bc8a.\n    \n    Change-Id: Ieb453ab05b3b84de53f72323c536a9567555df1e\n    (cherry picked from commit 75fa03557fd6f1e7c62079e9e89556f1af139202)\n", 
            "date_created": "2012-09-21 21:55:54.888812+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I faced it with a stable/essex nova. (without any milestones, however I was unable to find such a bug in milestones 2012.1.1 or 2012.1.2).\n\nThanks a lot for the fix!", 
            "date_created": "2012-09-23 05:32:17.937622+00:00", 
            "author": "https://api.launchpad.net/1.0/~galkindmitrii"
        }, 
        {
            "content": "This bug is happening again in Havana release. \nwe had a power outage and I sent a create command while the storage was unavailable and also had some other volume related commands running.\n\n2013-10-07 18:41:06.136 8288 WARNING cinder.scheduler.host_manager [req-462656bc-4bb2-478a-8fa4-90ac89e1c39e c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:41:06.152 8288 ERROR cinder.volume.flows.create_volume [req-462656bc-4bb2-478a-8fa4-90ac89e1c39e c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:44:31.280 8288 WARNING cinder.scheduler.host_manager [req-65c2f4e1-71da-4340-b9f0-afdd05ccdaa9 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:44:31.281 8288 ERROR cinder.volume.flows.create_volume [req-65c2f4e1-71da-4340-b9f0-afdd05ccdaa9 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:44:50.730 8288 WARNING cinder.scheduler.host_manager [req-1c132eb5-ca74-4ab5-91dc-73c25b305165 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:44:50.731 8288 ERROR cinder.volume.flows.create_volume [req-1c132eb5-ca74-4ab5-91dc-73c25b305165 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:47:01.577 8288 WARNING cinder.scheduler.host_manager [req-538ad552-0e19-4307-bea8-10e0a35a8a36 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:47:01.578 8288 ERROR cinder.volume.flows.create_volume [req-538ad552-0e19-4307-bea8-10e0a35a8a36 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:47:18.421 8288 WARNING cinder.scheduler.host_manager [req-3a788eb8-56f5-45f6-b4fd-ade01a05cf9d c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:47:18.422 8288 ERROR cinder.volume.flows.create_volume [req-3a788eb8-56f5-45f6-b4fd-ade01a05cf9d c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:48:27.732 8288 WARNING cinder.scheduler.host_manager [req-1ef1b47a-27b8-4667-9823-2f91dcc0f29e c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:48:27.733 8288 ERROR cinder.volume.flows.create_volume [req-1ef1b47a-27b8-4667-9823-2f91dcc0f29e c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:48:51.125 8288 WARNING cinder.scheduler.host_manager [req-7a45f5ed-c6b2-4b9a-9e5f-c90d60b1bba8 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:48:51.126 8288 ERROR cinder.volume.flows.create_volume [req-7a45f5ed-c6b2-4b9a-9e5f-c90d60b1bba8 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 18:49:54.705 8288 WARNING cinder.scheduler.host_manager [req-5fadfa9b-6d82-4ea4-ac36-15de15aa236a c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 18:49:54.706 8288 ERROR cinder.volume.flows.create_volume [req-5fadfa9b-6d82-4ea4-ac36-15de15aa236a c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 19:31:10.716 8288 CRITICAL cinder [-] need more than 0 values to unpack\n2013-10-07 19:37:27.334 2542 WARNING cinder.scheduler.host_manager [req-53603c25-424e-4c05-9eee-de5ae15fb300 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] volume service is down or disabled. (host: cougar06.scl.lab.tlv.redhat.com)\n2013-10-07 19:37:27.350 2542 ERROR cinder.volume.flows.create_volume [req-53603c25-424e-4c05-9eee-de5ae15fb300 c02995f25ba44cfab1a3cbd419f045a1 c77235c29fd0431a8e6628ef6d18e07f] Failed to schedule_create_volume: No valid host was found. \n2013-10-07 20:02:18.403 2542 CRITICAL cinder [-] need more than 0 values to unpack\n\n[root@cougar06 ~(keystone_admin)]# cinder list \n+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+\n|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |\n+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+\n| 1560fa00-752b-4d7b-a747-3ef9bf483692 | available |     new      |  1   |     None    |   True   |             |\n| 22c3e84c-1d9b-4a45-9244-06b3ab6c401a |  creating |     bla      |  10  |     None    |  False   |             |\n| aadc9c04-17ab-42c4-8bce-c2f63cd287fa | available |  image_new   |  1   |     None    |   True   |             |\n+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+\n", 
            "date_created": "2013-10-07 17:48:03.693800+00:00", 
            "author": "https://api.launchpad.net/1.0/~dron-3"
        }, 
        {
            "content": "Dafna,\nCould you confirm that the volume was stuck in \"creating\" or did it go to \"error\".  The bug was actually that the scheduler failed but never marked the volume as such and left it sitting in creating state.  Let me know if this is what you're still seeing, I believe this is still fixed based on the code and some quick tests.", 
            "date_created": "2014-01-05 23:44:43.468059+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Hi,\n\nI can confirm that in havana happens again. I tried to spawn machine from a volume snapshot with the option create new volume. First interesting thing is that cinder-scheduler chose to create the corresponding volume on the agent that was down although there were other cinder-volume agents up. So the status of the volume is stuck in creating:\n cinder list\n+--------------------------------------+----------------+------------------+------+-------------+----------+--------------------------------------+\n|                  ID                  |     Status     |   Display Name   | Size | Volume Type | Bootable |             Attached to              |\n+--------------------------------------+----------------+------------------+------+-------------+----------+--------------------------------------+\n| 0c7332a2-4ddd-413c-828b-3ba93b2f98da |     in-use     |  bla_ubuntu  |  10  |     None    |  false   | 55840e8b-7cc8-43ea-b8c0-bfcdf82c769d |\n| 20e0debb-55b8-4bde-9395-2e4179559536 |   available    |     debian73     |  10  |     None    |  false   |                                      |\n| 46b9846c-c8ad-4847-a6b9-e8bc086b41b0 |   available    |    ubuntu1204    |  10  |     None    |  false   |                                      |\n| 62b58aff-ce6e-4b7f-883e-589103ecaa87 |     in-use     |    bla_00    | 100  |     None    |  false   | 55840e8b-7cc8-43ea-b8c0-bfcdf82c769d |\n| 9a68af42-e73f-4fb7-8f87-621b2b4375f4 | error_deleting |       ssdf       |  1   |     None    |  false   |                                      |\n| a7728dbf-3752-438a-9395-f14efad98ccf |   available    | ubuntu1204_unity |  10  |     None    |  false   |                                      |\n| e9247a44-cb65-4164-8fa2-9c2433949278 |    creating    |                  |  10  |     None    |  false   |                                      |\n+--------------------------------------+----------------+------------------+------+-------------+----------+--------------------------------------+\n", 
            "date_created": "2014-01-28 09:36:06.404645+00:00", 
            "author": "https://api.launchpad.net/1.0/~gabriel-staicu"
        }, 
        {
            "content": "Gabriel, creating a volume from a snapshot doesn't go through cinder-scheduler at all.  In your case, if the back-end serving the snapshot is down, and the RPC message that Cinder API service sent to Cinder Volume is left in message queue.  That's why volume status stuck in 'creating'.  It's different.", 
            "date_created": "2014-01-28 10:04:09.419278+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhiteng-huang"
        }, 
        {
            "content": "Dafna,\n\nI think this bug has been fixed.  \n\nLet me explain a little bit more on the workflow of creating a volume: 1) User sends request to Cinder API service; 2) API creates a DB entry for the volume and marks its status to 'creating' (https://github.com/openstack/cinder/blob/stable/havana/cinder/volume/flows/create_volume/__init__.py#L545) and sends a RPC message to scheduler; 3) scheduler picks up the message and makes placement decision and if a back-end is available, it sends the request via RPC to volume service; 4) volume service picks up the message to perform the real job creating a volume for user.\n\nThere are multiple cases in which a volume's status can be stuck in 'creating':\n\na) something wrong happened during RPC message being processed by scheduler (e.g. scheduler service is down - related to this change & bug: https://review.openstack.org/#/c/64014/, message is lost, scheduler service goes down while scheduler processing the message);\n\nb) something went wrong AFTER backend is chosen, which means scheduler successfully sends out the message to target back-end, but somehow the message isn't picked up by target volume service or there is unhandled exception while volume service handling the request. \n\nIf somehow this bug happened again, can you describe the steps how to reproduce it?", 
            "date_created": "2014-01-28 10:12:33.024250+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhiteng-huang"
        }
    ], 
    "closed": "2012-09-21 21:55:53.031475+00:00"
}
{
    "status": "Fix Released", 
    "last_updated": "2017-10-03 19:17:33.364603+00:00", 
    "description": "\nMy simple openstack setup:   total three hosts, one controller(also served as cinder volume server), and two compute node called compute1, compute2, all three hosts are running CentOS 7.1.\nthe cinder service is just using lvm/iscsi.  openstack versions are listed at the end of this bug report.\n\nI created a volume with source cirros image, then I launched  an instance to boot from this volume, the command is below:\n\nnova boot --flavor m1.mytiny --block-device-mapping vda=9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4:::0  --nic net-id=ea864ed4-5628-4392-bd79-addf36e7354b  --security-group default --key-name mykey myvm\n\nand I checked the instance is running in compute1,  then I cut all the network connections of compute1, and wait for the \"nova service-list\", \"nova hypervisor-list' shows the status of compute1 is down.\n\nThen I use the command \"nova evacuate  myvm compute2\" to try to evacuate the vm to compute2, however I got the following error message in nova-compute.log on compute2:\n\n\n2015-12-02 21:33:22.465 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Rebuilding instance\n2015-12-02 21:33:22.492 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] disk not on shared storage, rebuilding from: ''\n2015-12-02 21:33:22.645 1041 WARNING oslo_config.cfg [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Option \"username\" from group \"neutron\" is deprecated. Use option \"user-name\" from group \"neutron\".\n2015-12-02 21:33:23.145 1041 WARNING nova.objects.instance [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Trying to apply a migration context that does not seem to be set for this instance\n2015-12-02 21:33:23.182 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Detach volume 25014c7c-a115-4a70-9636-8c69e46fd1fb from mountpoint /dev/vda\n2015-12-02 21:33:23.185 1041 WARNING nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Detaching volume from unknown instance\n2015-12-02 21:33:23.190 1041 WARNING nova.virt.libvirt.driver [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] During detach_volume, instance disappeared.\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Setting instance vm_state to ERROR\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Traceback (most recent call last):\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 6334, in _error_out_instance_on_exception\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     yield\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2683, in rebuild_instance\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     bdms, recreate, on_shared_storage, preserve_ephemeral)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2722, in _do_rebuild_instance_with_claim\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self._do_rebuild_instance(*args, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2830, in _do_rebuild_instance\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self._rebuild_default_impl(**kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2583, in _rebuild_default_impl\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     detach_block_devices(context, bdms)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2807, in detach_block_devices\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     destroy_bdm=False)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 4717, in _detach_volume\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self.volume_api.terminate_connection(context, volume_id, connector)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 223, in wrapper\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     six.reraise(exc_value, None, exc_trace)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 212, in wrapper\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     res = method(self, ctx, volume_id, *args, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 346, in terminate_connection\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     connector)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/v2/volumes.py\", line 480, in terminate_connection\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     {'connector': connector})\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/v2/volumes.py\", line 402, in _action\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self.api.client.post(url, body=body)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 104, in post\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self._cs_request(url, 'POST', **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 98, in _cs_request\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self.request(url, method, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 91, in request\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     raise exceptions.from_response(resp, body)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112] ClientException: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-5031f353-e235-4386-9292-90b516083f9f)\n\nAlso on cinder volume server(which is also the controller in this demo setup), the volume.log file shows following error:\n\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio [req-eaabe123-23f0-4bfb-94ce-bfceb63e922f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Failed to delete initiator iqn iqn.1994-05.com.redhat:f68acb3f9b3 to target.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Traceback (most recent call last):\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/volume/targets/lio.py\", line 194, in terminate_connection\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     run_as_root=True)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py\", line 254, in inner\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return f(*args, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/volume/targets/lio.py\", line 64, in _execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return utils.execute(*args, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/utils.py\", line 155, in execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return processutils.execute(*cmd, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py\", line 275, in execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     cmd=sanitized_cmd)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio ProcessExecutionError: Unexpected error while running command.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf cinder-rtstool delete-initiator iqn.2010-10.org.openstack:volume-9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4 iqn.1994-05.com.redhat:f68acb3f9b3   #my note:  iqn.1994-05.com.redhat:f68acb3f9b3 is for compute2 not compute1, so it will fail.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Exit code: 1\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Stdout: u''\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Stderr: u'Traceback (most recent call last):\\n  File \"/bin/cinder-rtstool\", line 10, in <module>\\n    sys.exit(main())\\n  File \"/usr/lib/python2.7/site-packages/cinder/cmd/rtstool.py\", line 291, in main\\n    delete_initiator(target_iqn, initiator_iqn)\\n  File \"/usr/lib/python2.7/site-packages/cinder/cmd/rtstool.py\", line 149, in delete_initiator\\n    % {\\'target\\': target_iqn, \\'acl\\': initiator_iqn})\\ncinder.cmd.rtstool.RtstoolError\\n'\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio\n2015-12-03 15:24:22.882 12000 ERROR cinder.volume.manager [req-eaabe123-23f0-4bfb-94ce-bfceb63e922f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Terminate volume connection failed: Failed to detach iSCSI target for volume 9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4.\n\nI found out a fixed bug related to the above cinder volume error message,  https://bugs.launchpad.net/cinder/+bug/1506496, that is, don't raise the error if the target_iqn and initiator_iqn combination isn't found in the delete_initiator() function call in rtstool.py file.     I applied the same patch manually, and the nova evacuate command now works.\n\nHowever I think this could be a new bug of nova.   The rational of the above bug fix is that sometimes cinder db is out of sync with back end, so the error should be ignored.   However in this case, I think it's a new bug of nova or cinder, that  the rebuild process will terminate current volume connection, however, from the above log,  it uses the initiator_iqn of  the destination compute node( is compute2 in this case), however, the current connection is actually from compute1 (the current dead node), so the self.volume_api.terminate_connection(context, volume_id, connector) call in nova/compute/manager.py will always throw exception until rtstool.py is patched in the above mentioned bug fix.    Anyway, with the above cinder bug fix, from below volume.log output, the iscsi_target is first removed and then re-created, so the original connection from compute1 is actually destroyed. After I restored the network connection for compute1, the log shows compute1 still tries to connect to this target, however it failed as expected.\n\n2015-12-03 16:24:21.772 12000 INFO cinder.volume.manager [req-bd3b4ee9-6ee5-4cbe-9e55-223f8eecf134 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Terminate volume connection completed successfully. #my note:   actually silently fails.\n2015-12-03 16:24:22.707 12000 INFO cinder.volume.targets.lio [req-a6fc4152-fc1e-44ad-aa4b-089c5ebeb39f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Removing iscsi_target: 9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4   #my node: maybe even there is volume connection, the iscsi target can be removed.\n2015-12-03 16:24:23.410 12000 INFO cinder.volume.manager [req-a6fc4152-fc1e-44ad-aa4b-089c5ebeb39f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Detach volume completed successfully.\n2015-12-03 16:24:24.458 12000 INFO cinder.volume.targets.lio [req-7965e50d-6946-4095-ac04-0f9163d08a47 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Creating iscsi_target for volume: volume-9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4\n2015-12-03 16:24:25.728 12000 INFO cinder.volume.manager [req-7965e50d-6946-4095-ac04-0f9163d08a47 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Initialize volume connection completed successfully.\n2015-12-03 16:24:26.862 12000 INFO cinder.volume.manager [req-5f78baaa-6621-4d03-892e-9c94b5b6d849 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Attach volume completed successfully.\n\nI think when exec  \"nova evacuate\", nova should know who is using the volume, and use that initiator_iqn instead of always using the initiator_iqn of the currently running node(the destination compute), or for single attach volume, just terminate the connection with one input(the target_iqn), without specifying initiator_iqn.\n\nI'm a newbie in openstack, maybe the above suggestion should goes to cinder not nova, or I'm totally wrong, the issue I experienced is due to other set up errors. \n\nOpenstack versions:\n1. versions:\non controller/storage node:\n#rpm -qa |grep nova\nopenstack-nova-conductor-12.0.0-1.el7.noarch\nopenstack-nova-api-12.0.0-1.el7.noarch\npython-novaclient-2.30.1-1.el7.noarch\npython-nova-12.0.0-1.el7.noarch\nopenstack-nova-cert-12.0.0-1.el7.noarch\nopenstack-nova-novncproxy-12.0.0-1.el7.noarch\nopenstack-nova-console-12.0.0-1.el7.noarch\nopenstack-nova-common-12.0.0-1.el7.noarch\nopenstack-nova-scheduler-12.0.0-1.el7.noarch\n\n#rpm -qa |grep cinder\npython-cinder-7.0.0-1.el7.noarch\npython-cinderclient-1.4.0-1.el7.noarch\nopenstack-cinder-7.0.0-1.el7.noarch\n\n# yum list installed |grep nova\nopenstack-nova-api.noarch       1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-cert.noarch      1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-common.noarch    1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-conductor.noarch 1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-console.noarch   1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-novncproxy.noarch\nopenstack-nova-scheduler.noarch 1:12.0.0-1.el7         @centos-openstack-liberty\npython-nova.noarch              1:12.0.0-1.el7         @centos-openstack-liberty\npython-novaclient.noarch        1:2.30.1-1.el7         @centos-openstack-liberty\n\nyum list installed |grep cinder\nopenstack-cinder.noarch         1:7.0.0-1.el7          @centos-openstack-liberty\npython-cinder.noarch            1:7.0.0-1.el7          @centos-openstack-liberty\npython-cinderclient.noarch      1.4.0-1.el7            @centos-openstack-liberty\n\non compute node:\n#rpm -qa |grep nova\nopenstack-nova-common-12.0.0-1.el7.noarch\npython-novaclient-2.30.1-1.el7.noarch\npython-nova-12.0.0-1.el7.noarch\nopenstack-nova-compute-12.0.0-1.el7.noarch\n\n#yum list installed |grep nova\nopenstack-nova-common.noarch    1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-compute.noarch   1:12.0.0-1.el7         @centos-openstack-liberty\npython-nova.noarch              1:12.0.0-1.el7         @centos-openstack-liberty\npython-novaclient.noarch        1:2.30.1-1.el7         @centos-openstack-liberty\n\n# rpm -qa |grep cinder\npython-cinderclient-1.4.0-1.el7.noarch\n\n# yum list installed |grep cinder\npython-cinderclient.noarch      1.4.0-1.el7            @centos-openstack-liberty", 
    "tags": [
        "compute", 
        "evacuate", 
        "in-stable-liberty", 
        "liberty-backport-potential", 
        "openstack-version.liberty", 
        "volumes"
    ], 
    "importance": "High", 
    "heat": 58, 
    "link": "https://bugs.launchpad.net/nova/+bug/1522496", 
    "owner": "None", 
    "id": 1522496, 
    "index": 1848, 
    "openned": "2015-12-03 16:59:37.589602+00:00", 
    "created": "2015-12-03 16:59:37.589602+00:00", 
    "title": "nova evacuate fails for instance booted from volume", 
    "comments": [
        {
            "content": "\nMy simple openstack setup:   total three hosts, one controller(also served as cinder volume server), and two compute node called compute1, compute2, all three hosts are running CentOS 7.1.\nthe cinder service is just using lvm/iscsi.  openstack versions are listed at the end of this bug report.\n\nI created a volume with source cirros image, then I launched  an instance to boot from this volume, the command is below:\n\nnova boot --flavor m1.mytiny --block-device-mapping vda=9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4:::0  --nic net-id=ea864ed4-5628-4392-bd79-addf36e7354b  --security-group default --key-name mykey myvm\n\nand I checked the instance is running in compute1,  then I cut all the network connections of compute1, and wait for the \"nova service-list\", \"nova hypervisor-list' shows the status of compute1 is down.\n\nThen I use the command \"nova evacuate  myvm compute2\" to try to evacuate the vm to compute2, however I got the following error message in nova-compute.log on compute2:\n\n\n2015-12-02 21:33:22.465 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Rebuilding instance\n2015-12-02 21:33:22.492 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] disk not on shared storage, rebuilding from: ''\n2015-12-02 21:33:22.645 1041 WARNING oslo_config.cfg [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Option \"username\" from group \"neutron\" is deprecated. Use option \"user-name\" from group \"neutron\".\n2015-12-02 21:33:23.145 1041 WARNING nova.objects.instance [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Trying to apply a migration context that does not seem to be set for this instance\n2015-12-02 21:33:23.182 1041 INFO nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Detach volume 25014c7c-a115-4a70-9636-8c69e46fd1fb from mountpoint /dev/vda\n2015-12-02 21:33:23.185 1041 WARNING nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Detaching volume from unknown instance\n2015-12-02 21:33:23.190 1041 WARNING nova.virt.libvirt.driver [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] During detach_volume, instance disappeared.\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [req-7ddecb3e-153d-471e-985e-275f18587f87 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Setting instance vm_state to ERROR\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112] Traceback (most recent call last):\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 6334, in _error_out_instance_on_exception\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     yield\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2683, in rebuild_instance\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     bdms, recreate, on_shared_storage, preserve_ephemeral)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2722, in _do_rebuild_instance_with_claim\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self._do_rebuild_instance(*args, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2830, in _do_rebuild_instance\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self._rebuild_default_impl(**kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2583, in _rebuild_default_impl\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     detach_block_devices(context, bdms)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2807, in detach_block_devices\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     destroy_bdm=False)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 4717, in _detach_volume\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     self.volume_api.terminate_connection(context, volume_id, connector)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 223, in wrapper\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     six.reraise(exc_value, None, exc_trace)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 212, in wrapper\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     res = method(self, ctx, volume_id, *args, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/nova/volume/cinder.py\", line 346, in terminate_connection\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     connector)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/v2/volumes.py\", line 480, in terminate_connection\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     {'connector': connector})\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/v2/volumes.py\", line 402, in _action\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self.api.client.post(url, body=body)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 104, in post\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self._cs_request(url, 'POST', **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 98, in _cs_request\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     return self.request(url, method, **kwargs)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]   File \"/usr/lib/python2.7/site-packages/cinderclient/client.py\", line 91, in request\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112]     raise exceptions.from_response(resp, body)\n2015-12-02 21:33:24.507 1041 ERROR nova.compute.manager [instance: 7e22504f-45e4-4e23-ad92-378899aef112] ClientException: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-5031f353-e235-4386-9292-90b516083f9f)\n\nAlso on cinder volume server(which is also the controller in this demo setup), the volume.log file shows following error:\n\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio [req-eaabe123-23f0-4bfb-94ce-bfceb63e922f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Failed to delete initiator iqn iqn.1994-05.com.redhat:f68acb3f9b3 to target.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Traceback (most recent call last):\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/volume/targets/lio.py\", line 194, in terminate_connection\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     run_as_root=True)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py\", line 254, in inner\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return f(*args, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/volume/targets/lio.py\", line 64, in _execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return utils.execute(*args, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/cinder/utils.py\", line 155, in execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     return processutils.execute(*cmd, **kwargs)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio   File \"/usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py\", line 275, in execute\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio     cmd=sanitized_cmd)\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio ProcessExecutionError: Unexpected error while running command.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf cinder-rtstool delete-initiator iqn.2010-10.org.openstack:volume-9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4 iqn.1994-05.com.redhat:f68acb3f9b3   #my note:  iqn.1994-05.com.redhat:f68acb3f9b3 is for compute2 not compute1, so it will fail.\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Exit code: 1\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Stdout: u''\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio Stderr: u'Traceback (most recent call last):\\n  File \"/bin/cinder-rtstool\", line 10, in <module>\\n    sys.exit(main())\\n  File \"/usr/lib/python2.7/site-packages/cinder/cmd/rtstool.py\", line 291, in main\\n    delete_initiator(target_iqn, initiator_iqn)\\n  File \"/usr/lib/python2.7/site-packages/cinder/cmd/rtstool.py\", line 149, in delete_initiator\\n    % {\\'target\\': target_iqn, \\'acl\\': initiator_iqn})\\ncinder.cmd.rtstool.RtstoolError\\n'\n2015-12-03 15:24:22.876 12000 ERROR cinder.volume.targets.lio\n2015-12-03 15:24:22.882 12000 ERROR cinder.volume.manager [req-eaabe123-23f0-4bfb-94ce-bfceb63e922f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Terminate volume connection failed: Failed to detach iSCSI target for volume 9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4.\n\nI found out a fixed bug related to the above cinder volume error message,  https://bugs.launchpad.net/cinder/+bug/1506496, that is, don't raise the error if the target_iqn and initiator_iqn combination isn't found in the delete_initiator() function call in rtstool.py file.     I applied the same patch manually, and the nova evacuate command now works.\n\nHowever I think this could be a new bug of nova.   The rational of the above bug fix is that sometimes cinder db is out of sync with back end, so the error should be ignored.   However in this case, I think it's a new bug of nova or cinder, that  the rebuild process will terminate current volume connection, however, from the above log,  it uses the initiator_iqn of  the destination compute node( is compute2 in this case), however, the current connection is actually from compute1 (the current dead node), so the self.volume_api.terminate_connection(context, volume_id, connector) call in nova/compute/manager.py will always throw exception until rtstool.py is patched in the above mentioned bug fix.    Anyway, with the above cinder bug fix, from below volume.log output, the iscsi_target is first removed and then re-created, so the original connection from compute1 is actually destroyed. After I restored the network connection for compute1, the log shows compute1 still tries to connect to this target, however it failed as expected.\n\n2015-12-03 16:24:21.772 12000 INFO cinder.volume.manager [req-bd3b4ee9-6ee5-4cbe-9e55-223f8eecf134 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Terminate volume connection completed successfully. #my note:   actually silently fails.\n2015-12-03 16:24:22.707 12000 INFO cinder.volume.targets.lio [req-a6fc4152-fc1e-44ad-aa4b-089c5ebeb39f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Removing iscsi_target: 9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4   #my node: maybe even there is volume connection, the iscsi target can be removed.\n2015-12-03 16:24:23.410 12000 INFO cinder.volume.manager [req-a6fc4152-fc1e-44ad-aa4b-089c5ebeb39f cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Detach volume completed successfully.\n2015-12-03 16:24:24.458 12000 INFO cinder.volume.targets.lio [req-7965e50d-6946-4095-ac04-0f9163d08a47 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Creating iscsi_target for volume: volume-9c077c7d-2c94-4fc8-b5f3-22fb70bf86b4\n2015-12-03 16:24:25.728 12000 INFO cinder.volume.manager [req-7965e50d-6946-4095-ac04-0f9163d08a47 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Initialize volume connection completed successfully.\n2015-12-03 16:24:26.862 12000 INFO cinder.volume.manager [req-5f78baaa-6621-4d03-892e-9c94b5b6d849 cb2fd48bf37a42e29672077a45c18681 59e12095d44f430abe14aed87e01fa92 - - -] Attach volume completed successfully.\n\nI think when exec  \"nova evacuate\", nova should know who is using the volume, and use that initiator_iqn instead of always using the initiator_iqn of the currently running node(the destination compute), or for single attach volume, just terminate the connection with one input(the target_iqn), without specifying initiator_iqn.\n\nI'm a newbie in openstack, maybe the above suggestion should goes to cinder not nova, or I'm totally wrong, the issue I experienced is due to other set up errors. \n\nOpenstack versions:\n1. versions:\non controller/storage node:\n#rpm -qa |grep nova\nopenstack-nova-conductor-12.0.0-1.el7.noarch\nopenstack-nova-api-12.0.0-1.el7.noarch\npython-novaclient-2.30.1-1.el7.noarch\npython-nova-12.0.0-1.el7.noarch\nopenstack-nova-cert-12.0.0-1.el7.noarch\nopenstack-nova-novncproxy-12.0.0-1.el7.noarch\nopenstack-nova-console-12.0.0-1.el7.noarch\nopenstack-nova-common-12.0.0-1.el7.noarch\nopenstack-nova-scheduler-12.0.0-1.el7.noarch\n\n#rpm -qa |grep cinder\npython-cinder-7.0.0-1.el7.noarch\npython-cinderclient-1.4.0-1.el7.noarch\nopenstack-cinder-7.0.0-1.el7.noarch\n\n# yum list installed |grep nova\nopenstack-nova-api.noarch       1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-cert.noarch      1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-common.noarch    1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-conductor.noarch 1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-console.noarch   1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-novncproxy.noarch\nopenstack-nova-scheduler.noarch 1:12.0.0-1.el7         @centos-openstack-liberty\npython-nova.noarch              1:12.0.0-1.el7         @centos-openstack-liberty\npython-novaclient.noarch        1:2.30.1-1.el7         @centos-openstack-liberty\n\nyum list installed |grep cinder\nopenstack-cinder.noarch         1:7.0.0-1.el7          @centos-openstack-liberty\npython-cinder.noarch            1:7.0.0-1.el7          @centos-openstack-liberty\npython-cinderclient.noarch      1.4.0-1.el7            @centos-openstack-liberty\n\non compute node:\n#rpm -qa |grep nova\nopenstack-nova-common-12.0.0-1.el7.noarch\npython-novaclient-2.30.1-1.el7.noarch\npython-nova-12.0.0-1.el7.noarch\nopenstack-nova-compute-12.0.0-1.el7.noarch\n\n#yum list installed |grep nova\nopenstack-nova-common.noarch    1:12.0.0-1.el7         @centos-openstack-liberty\nopenstack-nova-compute.noarch   1:12.0.0-1.el7         @centos-openstack-liberty\npython-nova.noarch              1:12.0.0-1.el7         @centos-openstack-liberty\npython-novaclient.noarch        1:2.30.1-1.el7         @centos-openstack-liberty\n\n# rpm -qa |grep cinder\npython-cinderclient-1.4.0-1.el7.noarch\n\n# yum list installed |grep cinder\npython-cinderclient.noarch      1.4.0-1.el7            @centos-openstack-liberty", 
            "date_created": "2015-12-03 16:59:37.589602+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhaoxiangz"
        }, 
        {
            "content": "added volumes tag", 
            "date_created": "2016-01-07 22:23:04.666949+00:00", 
            "author": "https://api.launchpad.net/1.0/~auggy"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/266095", 
            "date_created": "2016-01-12 01:46:52.093366+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/274318", 
            "date_created": "2016-01-30 05:15:31.559240+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/liberty\nReview: https://review.openstack.org/277587", 
            "date_created": "2016-02-08 22:10:02.702741+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Mark Sturdevant (<email address hidden>) on branch: stable/liberty\nReview: https://review.openstack.org/277587\nReason: Waiting for master patch to land.\nNote (see inline comment) for testing this.", 
            "date_created": "2016-02-10 17:31:19.821116+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/266095\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=a686185fc02ec421fd27270a343c19f668b95da6\nSubmitter: Jenkins\nBranch:    master\n\ncommit a686185fc02ec421fd27270a343c19f668b95da6\nAuthor: mark.sturdevant <email address hidden>\nDate:   Mon Jan 11 17:40:09 2016 -0800\n\n    Provide correct connector for evacuate terminate\n    \n    During evacuation a local connector is built. This is the\n    wrong connector to use for cinder terminate_connection.\n    In order to fix this, store the initial connector with\n    the BDM connection_info. Use the stored connector when\n    we detect that we have this wrong host situation.\n    \n    This fix does not work for existing attachments\n    (made prior to this patch) because existing attachments\n    don't have the connector stashed in the bdm.connection_info.\n    In cases where the original connector was not saved, leave\n    the behavior as-is.\n    \n    Change-Id: I793f2996fc0af1c321a240ad9348dc9bce816030\n    Partial-Bug: #1522496\n", 
            "date_created": "2016-03-07 12:25:16.445628+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "The stable/liberty review has been reopened here - https://review.openstack.org/#/c/277587/", 
            "date_created": "2016-03-17 10:32:48.452610+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/277587\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4026dc12fc40d6db850df324c07ab625659bb7b9\nSubmitter: Jenkins\nBranch:    stable/liberty\n\ncommit 4026dc12fc40d6db850df324c07ab625659bb7b9\nAuthor: mark.sturdevant <email address hidden>\nDate:   Mon Jan 11 17:40:09 2016 -0800\n\n    Provide correct connector for evacuate terminate\n    \n    During evacuation a local connector is built. This is the\n    wrong connector to use for cinder terminate_connection.\n    In order to fix this, store the initial connector with\n    the BDM connection_info. Use the stored connector when\n    we detect that we have this wrong host situation.\n    \n    This fix does not work for existing attachments\n    (made prior to this patch) because existing attachments\n    don't have the connector stashed in the bdm.connection_info.\n    In cases where the original connector was not saved, leave\n    the behavior as-is.\n    \n    Conflicts:\n       nova/tests/unit/compute/test_compute_mgr.py\n    \n    Change-Id: I793f2996fc0af1c321a240ad9348dc9bce816030\n    Partial-Bug: #1522496\n    (cherry picked from commit a686185fc02ec421fd27270a343c19f668b95da6)\n", 
            "date_created": "2016-03-18 21:26:38.946042+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Mark Sturdevant (<email address hidden>) on branch: master\nReview: https://review.openstack.org/274318\nReason: Cinder folks don't like this workaround either. No point polluting nova if there isn't significant cinder driver support.", 
            "date_created": "2016-05-10 22:45:05.793491+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I use ceph and have the same problem.", 
            "date_created": "2016-05-19 09:16:41.391575+00:00", 
            "author": "https://api.launchpad.net/1.0/~ljjjustin"
        }, 
        {
            "content": "Shouldn't this bug be marked as Fix Released since the merged patch fixes the problem going forward? I understand that existing attachments aren't fixed by the patch, but is that reason to leave the bug open? It makes it appear evacuate for instance booted from volume is still broken.", 
            "date_created": "2016-05-19 22:01:22.583262+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "\"Fix Released\" sounds good to me.  Not sure how (or if) the Nova bug managers want to keep the problem with pre-existing attachments documented.  The hack to fix pre-existing attachments is abandoned.  It would require all the cinder drivers to also provide their side of that fix.  This works for downstream patches, but it probably not going to happen for a majority of the drivers upstream.", 
            "date_created": "2016-05-23 17:56:04.999440+00:00", 
            "author": "https://api.launchpad.net/1.0/~mark-sturdevant"
        }, 
        {
            "content": "Automatically discovered version liberty in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:51:58.057174+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ], 
    "closed": "2017-10-03 19:17:30.080683+00:00"
}
{
    "status": "Fix Released", 
    "last_updated": "2016-05-09 07:00:45.849516+00:00", 
    "description": "Reproducing method as following:\n1: create a vm instance named test_vm1\n2:create a volume named test_volume1\n3:run nova command : nova volume-attach   test_vm1   test_volume1 , then confirmed the volume attached the instance ok\n[root@opencos_cjl ~(keystone_admin)]# nova list\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                      |\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n| b917e46b-539f-4024-bced-73c6b7c00ea2 | TestZteOneVolumeAttatchTo2Servers-instance-1863856957 | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.107                |\n| d0e5f1a4-9da1-4c39-a17d-12e43d20cd10 | TestZteOneVolumeAttatchTo2Servers-instance-8729469    | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.108                |\n| 9a6c6aff-d77c-4699-a41f-abb9d8e4b09e | test2                                                 | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.101, 10.43.210.232 |\n| 4a338d56-0daf-48d8-bcb5-d46de74b3887 | test_vm1                                              | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.109                |\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n[root@opencos_cjl ~(keystone_admin)]# \n[root@opencos_cjl ~(keystone_admin)]# nova volume-list\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n| ID                                   | Status    | Display Name | Size | Volume Type | Attached to                          |\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n| 22ad798d-77d2-4031-8b82-a5512e9f9284 | in-use    | test_volume1 | 1    | None        | 4a338d56-0daf-48d8-bcb5-d46de74b3887 |\n| 76f708fe-0e47-4f1a-a43b-08001f6a65d9 | available | test         | 1    | None        |                                      |\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n\n4: run nova command again : nova volume-attach   test_vm1   test_volume1 , then will raise exception as following:\n   \n[root@opencos_cjl ~(keystone_admin)]# nova volume-attach 4a338d56-0daf-48d8-bcb5-d46de74b3887  22ad798d-77d2-4031-8b82-a5512e9f9284\nERROR: Invalid volume: Volume has been attached to the instance (HTTP 400) (Request-ID: req-24f8b244-9809-41e4-b8e8-8a5ca7157c1c)\nthe exception is correct , but the issuse is: after the step 4, the  test_vm1 cannot normally use the  test_volume1. If you login the  test_vm1 os, you will find the volume attached as /dev/vdb don't work ok.", 
    "tags": [], 
    "importance": "Low", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1425382", 
    "owner": "None", 
    "id": 1425382, 
    "index": 1552, 
    "openned": "2015-02-25 04:41:55.654074+00:00", 
    "created": "2015-02-25 04:41:55.654074+00:00", 
    "title": "A volume was attached to the vm instance twice, the vm instance will cannot normally use this volume", 
    "comments": [
        {
            "content": "Reproducing method as following:\n1: create a vm instance named test_vm1\n2:create a volume named test_volume1\n3:run nova command : nova volume-attach   test_vm1   test_volume1 , then confirmed the volume attached the instance ok\n[root@opencos_cjl ~(keystone_admin)]# nova list\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                      |\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n| b917e46b-539f-4024-bced-73c6b7c00ea2 | TestZteOneVolumeAttatchTo2Servers-instance-1863856957 | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.107                |\n| d0e5f1a4-9da1-4c39-a17d-12e43d20cd10 | TestZteOneVolumeAttatchTo2Servers-instance-8729469    | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.108                |\n| 9a6c6aff-d77c-4699-a41f-abb9d8e4b09e | test2                                                 | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.101, 10.43.210.232 |\n| 4a338d56-0daf-48d8-bcb5-d46de74b3887 | test_vm1                                              | ACTIVE | -          | Running     | zfl_internal_net=192.168.0.109                |\n+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------------------+\n[root@opencos_cjl ~(keystone_admin)]# \n[root@opencos_cjl ~(keystone_admin)]# nova volume-list\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n| ID                                   | Status    | Display Name | Size | Volume Type | Attached to                          |\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n| 22ad798d-77d2-4031-8b82-a5512e9f9284 | in-use    | test_volume1 | 1    | None        | 4a338d56-0daf-48d8-bcb5-d46de74b3887 |\n| 76f708fe-0e47-4f1a-a43b-08001f6a65d9 | available | test         | 1    | None        |                                      |\n+--------------------------------------+-----------+--------------+------+-------------+--------------------------------------+\n\n4: run nova command again : nova volume-attach   test_vm1   test_volume1 , then will raise exception as following:\n   \n[root@opencos_cjl ~(keystone_admin)]# nova volume-attach 4a338d56-0daf-48d8-bcb5-d46de74b3887  22ad798d-77d2-4031-8b82-a5512e9f9284\nERROR: Invalid volume: Volume has been attached to the instance (HTTP 400) (Request-ID: req-24f8b244-9809-41e4-b8e8-8a5ca7157c1c)\nthe exception is correct , but the issuse is: after the step 4, the  test_vm1 cannot normally use the  test_volume1. If you login the  test_vm1 os, you will find the volume attached as /dev/vdb don't work ok.", 
            "date_created": "2015-02-25 04:41:55.654074+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "the issuse fault is:  after run attached the  volume to the vm again,  as attached failed , then rollback , but when  rollback , delete  the volume's  bdm table's record(the record is added at the first attached), the second attached record will be residual, but it is not complete. so the  volume cannot normally be used.", 
            "date_created": "2015-02-25 06:39:47.061665+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "I'm use I  version, I review code for J and K version, this issuse is exist.\nthis issuse reason is : at /nova/compute/api.py   the function  _attach_volume(self, context, instance, volume_id, device,\n                       disk_bus, device_type):\nfirst run self.compute_rpcapi.reserve_block_device_name( context, instance, device, volume_id, disk_bus=disk_bus,\n                                                                                                              device_type=device_type)\nthis will add one record in bdm tables, \nthen  run  self.volume_api.check_attach(context, volume, instance=instance), here will raise exception. but the record of  bdm tables has been added. \nso ,we should be run self.volume_api.check_attach(context, volume, instance=instance) advance, before run  self.compute_rpcapi.reserve_block_device_name( context, instance, device, volume_id, disk_bus=disk_bus,\n                                                                                                              device_type=device_type),  then this issue will be solved.\n", 
            "date_created": "2015-02-25 07:02:46.862464+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "Hi YaoZheng,\n\nCould you please share which release versions of kilo and juno are you using to test this scenario.\n\nI've tested this issue and it is not reproducible with latest master and stable/juno branch. I've followed below steps to test this scenario\n\n1.  Created an instance named test_vm_1.\n2.  Created a volume named test_volume_1.\n3.  Verified that instance is in active state and volume is in available state.\n4.  Attach volume using command:\n      $ nova volume-attach <instance_id> <volume_id>.\n5.  Confirmed that volume is in 'in-use' status using command:\n     $ cinder list.\n6.  Execute volume-attach command again with same instance_id and volume_id.\n      $ nova volume-attach <instance_id> <volume_id>.\n\nAfter running the step 6 it gives \"Invalid volume\" error with the reason of failure in error message. But when I login to the instance 'test_vm_1', I can still use the volume attached to it successfully.", 
            "date_created": "2015-02-27 13:26:22.752351+00:00", 
            "author": "https://api.launchpad.net/1.0/~ankitagrawal"
        }, 
        {
            "content": "Hi Ankit Agrawal,\n   I'm sorry, the last step was lost. \n  The last step:  Execute command : nova  reboot  --hard  <instance_id> , \n  then  login to the instance 'test_vm_1' ,you will find the volume attached as /dev/vdb don't work ok", 
            "date_created": "2015-03-04 11:14:21.603305+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "Hi YaoZheng,\n\nIMO, this is not an issue with attach volume.\n\nEven if you attach a volume only once and your attached volume(/dev/vdb) is mounted at somewhere at '/mnt/volume' in your instance, you will not be able to access that volume after hard reboot.\n\nIf you want to use that volume after hard reboot, either you need to mount it again or you have to add following lines in \"/etc/fstab\" file under your instance, in order to mount it automatically after hard reboot:\n\n/dev/vdb /mnt/volume ext4 defaults 0 0", 
            "date_created": "2015-03-04 12:26:21.144025+00:00", 
            "author": "https://api.launchpad.net/1.0/~ankitagrawal"
        }, 
        {
            "content": "Hi Ankit Agrawal ,\n   Thank you for your reply. I don't express clearly, when after hard reboot vm, you login the vm os , you will find /dev/vdb lost, not the mount path lost. \nThe essential reason: the volume attached as /dev/vdb, but  in nova bdm table \"connection_info\" information  lost.\nthe volume's bdm table information is not complete. you can observe the nova bdm table about the volume.\n\nthe bdm table is not complete reason: after run the second attached the volume to the vm , as attached failed , then rollback , but when rollback , delete the volume's bdm table's record(the record is added at the first attached), the second attached record will be residual, but it is not complete. so the volume cannot normally be used.", 
            "date_created": "2015-03-05 01:12:39.242512+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "Hi YaoZheng,\n\nIn my case when attach volume fails on second run, it deletes the bdm created in second call and bdm record added at first attached remains unchanged.  Because we are calling volume_bdm.destroy() on newly created bdm on exception.\nPlease refer: https://github.com/openstack/nova/blob/master/nova/compute/api.py#L3019\n\nCould you please check if you are facing this issue on master and stable/juno, or you can mention which release version of Openstack you are using for testing.", 
            "date_created": "2015-03-05 07:46:51.057575+00:00", 
            "author": "https://api.launchpad.net/1.0/~ankitagrawal"
        }, 
        {
            "content": "Hi Ankit Agrawal ,\n   I use icehouse 2014.1.3\uff0c I review your offered code  seriously , they are different in this point. \n my code  :\n def _attach_volume(self, context, instance, volume_id, device,\n                       disk_bus, device_type):\n        \"\"\"Attach an existing volume to an existing instance.\n\n        This method is separated to make it possible for cells version\n        to override it.\n        \"\"\"\n        # NOTE(vish): This is done on the compute host because we want\n        #             to avoid a race where two devices are requested at\n        #             the same time. When db access is removed from\n        #             compute, the bdm will be created here and we will\n        #             have to make sure that they are assigned atomically.\n        \n        \n        device = self.compute_rpcapi.reserve_block_device_name(\n            context, device=device, instance=instance, volume_id=volume_id,\n            disk_bus=disk_bus, device_type=device_type)\n        volume_bdm = block_device_obj.BlockDeviceMapping.get_by_instance_and_volume(\n            context, volume_id, instance['uuid'])\n        try:\n            volume = self.volume_api.get(context, volume_id)\n            self.volume_api.check_attach(context, volume, instance=instance)\n            self.volume_api.reserve_volume(context, volume_id)\n            self.compute_rpcapi.attach_volume(context, instance=instance,\n                    volume_id=volume_id, mountpoint=device, bdm=volume_bdm)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                volume_bdm.destroy(context)\n\n        return device\n\n\n\n", 
            "date_created": "2015-03-06 04:36:44.051330+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }
    ], 
    "closed": "2016-05-09 07:00:43.510082+00:00"
}
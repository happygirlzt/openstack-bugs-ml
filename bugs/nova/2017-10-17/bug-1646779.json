{
    "status": "Incomplete", 
    "last_updated": "2017-05-02 17:37:13.688309+00:00", 
    "description": "A VM fails to spawn with no host available. The nova-cpu logs reveals a problem connecting to libvirt. 84 hits since Nov 23rd:\n\nmessage: \"libvirtError: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\"\n\nRecent failure: http://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/screen-n-cpu.txt.gz?level=ERROR\n\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host [req-12fbb338-7df0-4654-b686-257245421442 tempest-ImagesOneServerNegativeTestJSON-1400886372 tempest-ImagesOneServerNegativeTestJSON-1400886372] Connection to libvirt failed: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host Traceback (most recent call last):\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 453, in get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     conn = self._get_connection()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 436, in _get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     {'msg': ex})\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     self.force_reraise()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     six.reraise(self.type_, self.value, self.tb)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 425, in _get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     self._wrapped_conn = self._get_new_connection()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 370, in _get_new_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     wrapped_conn = self._connect(self._uri, self._read_only)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 226, in _connect\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     libvirt.openAuth, uri, auth, flags)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     rv = execute(f, *args, **kwargs)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     six.reraise(c, e, tb)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     rv = meth(*args, **kwargs)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 105, in openAuth\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     if ret is None:raise libvirtError('virConnectOpenAuth() failed')\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host libvirtError: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host \n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [req-12fbb338-7df0-4654-b686-257245421442 tempest-ImagesOneServerNegativeTestJSON-1400886372 tempest-ImagesOneServerNegativeTestJSON-1400886372] [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] Instance failed to spawn\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] Traceback (most recent call last):\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2117, in _build_resources\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     yield resources\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 1924, in _build_and_run_instance\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     block_device_info=block_device_info)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2647, in spawn\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     block_device_info=block_device_info)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4741, in _get_guest_xml\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     context)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4515, in _get_guest_config\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     instance.numa_topology, flavor, allowed_cpus, image_meta)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 3872, in _get_guest_numa_config\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     if (not self._has_numa_support() and\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5212, in _has_numa_support\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     if self._host.has_version(ver):\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 519, in has_version\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     lv_ver=lv_ver, hv_ver=hv_ver, hv_type=hv_type, op=operator.ne)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 490, in _version_check\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     conn = self.get_connection()\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 462, in get_connection\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     raise exception.HypervisorUnavailable(host=CONF.host)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] HypervisorUnavailable: Connection to the hypervisor is broken on host: ubuntu-xenial-rax-ord-5924789\n\n\nThe issue happens with different tests, so I doubt it is actually a Tempest issue, but I'm filing the issue here for initial triage.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 26, 
    "link": "https://bugs.launchpad.net/nova/+bug/1646779", 
    "owner": "None", 
    "id": 1646779, 
    "index": 7832, 
    "openned": "2016-12-07 16:57:35.528526+00:00", 
    "created": "2016-12-02 10:21:19.464155+00:00", 
    "title": "libvirt killed by kernel on general protection or stack segment traps", 
    "comments": [
        {
            "content": "A VM fails to spawn with no host available. The nova-cpu logs reveals a problem connecting to libvirt. 84 hits since Nov 23rd:\n\nmessage: \"libvirtError: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\"\n\nRecent failure: http://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/screen-n-cpu.txt.gz?level=ERROR\n\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host [req-12fbb338-7df0-4654-b686-257245421442 tempest-ImagesOneServerNegativeTestJSON-1400886372 tempest-ImagesOneServerNegativeTestJSON-1400886372] Connection to libvirt failed: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host Traceback (most recent call last):\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 453, in get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     conn = self._get_connection()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 436, in _get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     {'msg': ex})\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     self.force_reraise()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     six.reraise(self.type_, self.value, self.tb)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 425, in _get_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     self._wrapped_conn = self._get_new_connection()\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 370, in _get_new_connection\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     wrapped_conn = self._connect(self._uri, self._read_only)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 226, in _connect\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     libvirt.openAuth, uri, auth, flags)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 144, in proxy_call\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     rv = execute(f, *args, **kwargs)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 125, in execute\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     six.reraise(c, e, tb)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 83, in tworker\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     rv = meth(*args, **kwargs)\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 105, in openAuth\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host     if ret is None:raise libvirtError('virConnectOpenAuth() failed')\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host libvirtError: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused\n2016-12-01 18:16:05.117 6160 ERROR nova.virt.libvirt.host \n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [req-12fbb338-7df0-4654-b686-257245421442 tempest-ImagesOneServerNegativeTestJSON-1400886372 tempest-ImagesOneServerNegativeTestJSON-1400886372] [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] Instance failed to spawn\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] Traceback (most recent call last):\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2117, in _build_resources\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     yield resources\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 1924, in _build_and_run_instance\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     block_device_info=block_device_info)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2647, in spawn\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     block_device_info=block_device_info)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4741, in _get_guest_xml\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     context)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 4515, in _get_guest_config\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     instance.numa_topology, flavor, allowed_cpus, image_meta)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 3872, in _get_guest_numa_config\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     if (not self._has_numa_support() and\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5212, in _has_numa_support\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     if self._host.has_version(ver):\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 519, in has_version\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     lv_ver=lv_ver, hv_ver=hv_ver, hv_type=hv_type, op=operator.ne)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 490, in _version_check\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     conn = self.get_connection()\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]   File \"/opt/stack/new/nova/nova/virt/libvirt/host.py\", line 462, in get_connection\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc]     raise exception.HypervisorUnavailable(host=CONF.host)\n2016-12-01 18:16:05.123 6160 ERROR nova.compute.manager [instance: 6fa73b04-c6a7-47a8-908b-6738f36f6ffc] HypervisorUnavailable: Connection to the hypervisor is broken on host: ubuntu-xenial-rax-ord-5924789\n\n\nThe issue happens with different tests, so I doubt it is actually a Tempest issue, but I'm filing the issue here for initial triage.", 
            "date_created": "2016-12-02 10:21:19.464155+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "message: \"libvirtError: End of file while reading data: Input/output error\"\n\nThis also started on the 23rd.", 
            "date_created": "2016-12-02 10:42:39.179878+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "The error in the libvirtd logs is here:\n\nhttp://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/libvirt/libvirtd.txt.gz#_2016-12-01_18_16_04_907\n\n2016-12-01 18:16:04.907+0000: 17078: error : qemuMonitorIORead:580 : Unable to read from monitor: Connection reset by peer\n\nSeeing quite a few virtlogd i/o errors in syslog:\n\nhttp://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/syslog.txt.gz\n\nBut virtlogd support was merged on 12/6, not 11/23:\n\nhttps://github.com/openstack/nova/commit/85c781a7229e324155be3db029b7541a27f909db\n\nPlus that requires libvirt>=1.3.3 and we run with 1.3.1 in the xenial nodes, so we're not using that virtlogd code in libvirt yet in the gate.", 
            "date_created": "2016-12-07 19:15:12.782581+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The date of 23/11 may be artificial, perhaps it's simply due to the retention policy in the logstash cluster. ", 
            "date_created": "2016-12-08 11:09:29.176154+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "Looking at the timestamp from libvirt log, I found the following instance log which shows a qemu crash: http://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/libvirt/qemu/instance-0000000c.txt.gz#_2016-12-01_18_14_13_257\n\nwarning: TCG doesn't support requested feature: CPUID.01H:ECX.vmx [bit 5]\nmain-loop: WARNING: I/O thread spun for 1000 iterations\n2016-12-01T18:14:13.226270Z qemu-system-x86_64: terminating on signal 15 from pid 17078\n2016-12-01 18:14:13.257+0000: shutting down", 
            "date_created": "2016-12-08 11:50:07.938000+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "The CPUID.01H:ECX.vmx bit is on every single VM log.\nThe main-loop warning, after some googling, seems to be something that is associated to a lot of I/O in the guest, which causes slow downs.\nThe qemu process terminating is the normal shut down of the VM.\nDigging further.", 
            "date_created": "2016-12-08 12:05:01.594452+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "IN http://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/libvirt/qemu/instance-0000001a.txt.gz#_2016-12-01_18_16_08_032 I see:\n\n2016-12-01T18:16:04.707300Z qemu-system-x86_64: terminating on signal 15 from pid 17078\n\nWhich matches closely (200ms gap) with http://logs.openstack.org/66/401366/4/gate/gate-tempest-dsvm-neutron-full-ubuntu-xenial/3deacc5/logs/libvirt/libvirtd.txt.gz#_2016-12-01_18_16_04_907\n\n2016-12-01 18:16:04.907+0000: 17078: error : qemuMonitorIORead:580 : Unable to read from monitor: Connection reset by peer\n\nSo I think the qemuMonitorIORead errors are possibly related just to VMs being normally shutdown.", 
            "date_created": "2016-12-08 12:09:18.062810+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "This is somewhat related to https://bugs.launchpad.net/nova/+bug/1643911", 
            "date_created": "2016-12-13 17:29:29.913209+00:00", 
            "author": "https://api.launchpad.net/1.0/~jordan-pittier"
        }, 
        {
            "content": "e-r query: https://review.openstack.org/#/c/406278", 
            "date_created": "2016-12-15 11:40:06.841214+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "It looks like sometimes when the following log line happens:\n\nvirtlogd[...] End of file while reading data: Input/output error\n\nlibvirt is killed by the kernel, because of either a general protection or a stack segment IP.\n", 
            "date_created": "2016-12-19 12:34:33.428139+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "I did a few checks as Jordan did on the signature of this bug, and I see that test_delete_saving_image (http://git.openstack.org/cgit/openstack/tempest/tree/tempest/api/compute/images/test_images.py#n43) is always running when the issue occurs.\n\nThe test deletes a snapshot while it's being created from a server, which temporarily pauses a server to then resume it and finally destroy it.\n\nThe instance log is for instance: http://logs.openstack.org/82/409682/3/check/gate-tempest-dsvm-neutron-dvr-ubuntu-xenial/0b05dcf/logs/libvirt/qemu/instance-00000025.txt.gz\n\nI tried to reproduce this locally running something like a few times:\n\ntox -eall -- '(test_delete_saving_image|test_resize_server_from_manual_to_auto|test_add_remove_fixed_ip|test_create_image_from_stopped_server|test_verify_multiple_nics_order|test_max_image_meta_exceed_limit)'\n\nbut I failed to reproduce until now. I did get a kernel panic in my VM after a few runs, but that's probably unrelated.", 
            "date_created": "2016-12-19 16:40:30.066405+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "I marked this as incomplete from a Tempest POV - I couldn't find anything wrong with the tests in Tempest that seem to trigger this, apart from triggering sometimes what looks like a libvirt issue.", 
            "date_created": "2016-12-20 09:19:12.457898+00:00", 
            "author": "https://api.launchpad.net/1.0/~andrea-frittoli"
        }, 
        {
            "content": "Hi (cross post I know),\nthere is a bit of a \"somewhat dup\" context around the list of the following bugs:\n- bug 1646779\n- bug 1643911\n- bug 1638982\n- bug 1673483\n\nUnfortunately, I\u2019ve never hit these bugs in any of my libvirt/qemu runs and these are literally thousands every day due to all the automated tests. I also checked with our Ubuntu OpenStack Team and they didn't see them so far either. That makes it hard to debug them in depth as you will all understand.\n\nBut while the \u201csignature\u201d on each bug is different, they share a lot of things still (lets call it the bug \"meta signature\"):\n- there is no way to recreate yet other than watching gate test crash statistics\n- they seem to haunt the openstack gate testing more than anything else\n- most are directly or indirectly related to memory corruption\n\nAs I\u2019m unable to reproduce any of these bugs myself, I\u2019d like to get some help from anyone that can help to recreate. Therefore I ask all people affected (mostly the same on all these bugs anyway) to test the PPAs I created for bug 1673483. That is the one bug where thanks to the great help of Kashyp, Matt and Dan (and more) at least a potential fix was identified.\n\nThat means two ppa's for you to try:\n- Backport of the minimal recommended fix: https://launchpad.net/~ci-train-ppa-service/+archive/ubuntu/2620\n- Backport of the full series of related fixes: https://launchpad.net/~ci-train-ppa-service/+archive/ubuntu/2619\n\nEspecially since the potential error of these fixes refers to almost anything from memleak to deadlock there is a chance that they all might be caused by the same root cause.\n", 
            "date_created": "2017-04-04 15:43:32.240163+00:00", 
            "author": "https://api.launchpad.net/1.0/~paelzer"
        }, 
        {
            "content": "Attempting to reproduce this outside of the OpenStack gate is proving problematic; I left a minimal Xenial/Mitaka cloud with concurrent test execution (x 16) of the tests described in #10 running for the last 12 hours on a single compute node deployment; 4500 instances later I've still not seen any of the issues this or other bugs describe.", 
            "date_created": "2017-04-05 08:09:21.515528+00:00", 
            "author": "https://api.launchpad.net/1.0/~james-page"
        }, 
        {
            "content": "This is not Tempest bug clearly.\n", 
            "date_created": "2017-05-02 17:37:12.346815+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }
    ]
}
{
    "status": "Invalid", 
    "last_updated": "2013-08-12 09:03:58.820826+00:00", 
    "description": "It may be interesting to add an arping during the live-migration process because the floating ip is being transferred from one compute node the the new one - that means we lose the connectivity the time for the network equipments to flush their arp cache.\nOn the new compute node, I simply run\narping -U floating-ip\nby doing so, the instance is directly reachable - if I don't, i need to wait at least a couple of hours\n\nregards,\nRazique", 
    "tags": [
        "low-hanging-fruit"
    ], 
    "importance": "Medium", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1182373", 
    "owner": "None", 
    "id": 1182373, 
    "index": 3403, 
    "openned": "2013-05-21 09:15:24.629766+00:00", 
    "created": "2013-05-21 09:15:24.629766+00:00", 
    "title": "Add an arping on multi_host for live migration", 
    "comments": [
        {
            "content": "It may be interesting to add an arping during the live-migration process because the floating ip is being transferred from one compute node the the new one - that means we lose the connectivity the time for the network equipments to flush their arp cache.\nOn the new compute node, I simply run\narping -U floating-ip\nby doing so, the instance is directly reachable - if I don't, i need to wait at least a couple of hours\n\nregards,\nRazique", 
            "date_created": "2013-05-21 09:15:24.629766+00:00", 
            "author": "https://api.launchpad.net/1.0/~razique"
        }, 
        {
            "content": "This isn't data loss, but not being able to use the floating IP is kind of a big deal.", 
            "date_created": "2013-05-22 04:17:36.623041+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Not able to reproduce the bug for FLATDHCPManager. I am able to perform live migration successfully.\nDuring live migration path, I observed whether arping command is run on the nova-network on the destination compute node and I could see it in the nova-network.log.\nAfter live migration, I could ping to the vm using the floating ip address. ", 
            "date_created": "2013-06-26 11:28:15.293046+00:00", 
            "author": "https://api.launchpad.net/1.0/~rohitk"
        }, 
        {
            "content": "2013-06-20 16:44:27.656 INFO nova.network.floating_ips [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Finishing migration network for instance 5033a9ac-6309-4c38-a66c-cb6ef1f051bb\n2013-06-20 16:44:27.767 DEBUG nova.openstack.common.lockutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Got semaphore \"iptables\" for method \"_apply\"... from (pid=9698) inner /opt/stack/nova/nova/openstack/common/lockutils.py:190\n2013-06-20 16:44:27.768 DEBUG nova.openstack.common.lockutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Attempting to grab file lock \"iptables\" for method \"_apply\"... from (pid=9698) inner /opt/stack/nova/nova/openstack/common/lockutils.py:201\n2013-06-20 16:44:27.768 DEBUG nova.openstack.common.lockutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Got file lock \"iptables\" at /opt/stack/data/nova/nova-iptables for method \"_apply\"... from (pid=9698) inner /opt/stack/nova/nova/openstack/common/lockutils.py:231\n2013-06-20 16:44:27.769 DEBUG nova.openstack.common.processutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c from (pid=9698) execute /opt/stack/nova/nova/openstack/common/processutils.py:142\n2013-06-20 16:44:27.898 DEBUG nova.openstack.common.processutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c from (pid=9698) execute /opt/stack/nova/nova/openstack/common/processutils.py:142\n2013-06-20 16:44:28.029 DEBUG nova.network.linux_net [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] IPTablesManager.apply completed with success from (pid=9698) _apply /opt/stack/nova/nova/network/linux_net.py:424\n2013-06-20 16:44:28.030 DEBUG nova.openstack.common.lockutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Released file lock \"iptables\" at /opt/stack/data/nova/nova-iptables for method \"_apply\"... from (pid=9698) inner /opt/stack/nova/nova/openstack/common/lockutils.py:238\n2013-06-20 16:44:28.030 DEBUG nova.openstack.common.processutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip addr add 10.2.3.81/32 dev br100 from (pid=9698) execute /opt/stack/nova/nova/openstack/common/processutils.py:142\n2013-06-20 16:44:28.158 DEBUG nova.openstack.common.processutils [req-6b8e6b24-8e52-4d70-8156-647b45bfdf27 admin admin] Running cmd (subprocess): sudo nova-rootwrap /e\ntc/nova/rootwrap.conf arping -U 10.2.3.81 -A -I br100 -c 3 from (pid=9698) execute /opt/stack/nova/nova/openstack/common/processutils.py:142\n2013-06-20 16:44:29.728 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_context_roles': [], u'_context_request_id': u'req-1b30f4b6-a660-4283-a27a-642366055287', u'_context_quota_class': None, u'_context_user_name': None, u'_context_project_name': None, u'_context_service_catalog': [], u'_context_tenant': None, u'_context_auth_token': '<SANITIZED>', u'args': {u'address': u'10.0.0.2'}, u'namespace': None, u'_context_instance_lock_checked': False, u'_context_timestamp': u'2013-06-20T23:44:29.609910', u'_context_is_admin': True, u'version': u'1.0', u'_context_project_id': None, u'_context_user': None, u'_unique_id': u'e56b9a2d85b3407db1198fabae91b1aa', u'_context_read_deleted': u'no', u'_context_user_id': None, u'method': u'release_fixed_ip', u'_context_remote_address': None} from (pid=9698) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:295", 
            "date_created": "2013-06-26 11:36:09.487691+00:00", 
            "author": "https://api.launchpad.net/1.0/~rohitk"
        }, 
        {
            "content": "Add 'send_arp_for_ha = True' and 'send_arp_for_ha_count=3' to nova.conf", 
            "date_created": "2013-08-12 09:03:16.310665+00:00", 
            "author": "https://api.launchpad.net/1.0/~wenjianhn"
        }
    ], 
    "closed": "2013-08-12 09:03:57.183117+00:00"
}
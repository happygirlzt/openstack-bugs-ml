{
    "status": "Fix Released", 
    "last_updated": "2016-01-21 23:18:21.149041+00:00", 
    "description": "Steps to reproduce:\n1) Create a new instance,waiting until it\u2019s status goes to ACTIVE state\n2) Call resize API\n3) Delete the instance immediately after the task_state is \u201cresize_migrated\u201d or vm_state is \u201cresized\u201d\n4) Repeat 1 through 3 in a loop\n\nI have kept attached program running for 4 hours, all instances created are deleted (nova list returns empty list) but I noticed  instances directories with the name \u201c<instance_uuid>_resize> are not deleted from the instance path of the compute nodes (mainly from the source compute nodes where the instance was running before resize). If I keep this program running for couple of more hours (depending on the number of compute nodes), then it completely uses the entire disk of the compute nodes (based on the disk_allocation_ratio parameter value). Later, nova scheduler doesn\u2019t select these compute nodes for launching new vms and starts reporting error \"No valid hosts found\".\n\nNote: Even the periodic tasks doesn't cleanup these orphan instance directories from the instance path.", 
    "tags": [], 
    "importance": "High", 
    "heat": 280, 
    "link": "https://bugs.launchpad.net/nova/+bug/1392527", 
    "owner": "https://api.launchpad.net/1.0/~tristan-cacqueray", 
    "id": 1392527, 
    "index": 1649, 
    "openned": "2014-11-13 23:44:35.998986+00:00", 
    "created": "2014-11-13 23:44:35.998986+00:00", 
    "title": "[OSSA 2015-017] Deleting instance while resize instance is running leads to unuseable compute nodes (CVE-2015-3280)", 
    "comments": [
        {
            "content": "Steps to reproduce:\n1) Create a new instance,waiting until it\u2019s status goes to ACTIVE state\n2) Call resize API\n3) Delete the instance immediately after the task_state is \u201cresize_migrated\u201d or vm_state is \u201cresized\u201d\n4) Repeat 1 through 3 in a loop\n\nI have kept attached program running for 4 hours, all instances created are deleted (nova list returns empty list) but I noticed  instances directories with the name \u201c<instance_uuid>_resize> are not deleted from the instance path of the compute nodes (mainly from the source compute nodes where the instance was running before resize). If I keep this program running for couple of more hours (depending on the number of compute nodes), then it completely uses the entire disk of the compute nodes (based on the disk_allocation_ratio parameter value). Later, nova scheduler doesn\u2019t select these compute nodes for launching new vms and starts reporting error \"No valid hosts found\".\n\nNote: Even the periodic tasks doesn't cleanup these orphan instance directories from the instance path.", 
            "date_created": "2014-11-13 23:44:35.998986+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "", 
            "date_created": "2014-11-13 23:44:35.998986+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "This sounds like a potential denial of service vector, so I've added an incomplete security advisory task and subscribed Nova's core security reviewers to comment.", 
            "date_created": "2014-11-14 00:24:15.872538+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Looks pretty valid to me. I bet Nova coresec will confirm :)", 
            "date_created": "2014-11-17 15:21:04.007977+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Independently reported by george-shuklin as bug 1387543", 
            "date_created": "2014-11-17 15:35:16.083998+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "This is a valid issue.  But it should be limited to issuing a delete before the instance.vm_state is 'resized' since there is code to cleanup a resized instance if vm_state is 'resized'. \n\nIt's going to take a little bit of thought to get the coordination between delete and resize correct for this.", 
            "date_created": "2014-11-19 20:41:00.154572+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "@Tushar Do you want to acknowledge a company along with your name or is this reported independently ? \n\nHere is impact description draft #1:\n\nTitle: Nova instance delete in resize state may leak\nReporter: Tushar Patil \nProducts: Nova\nVersions: up to 2014.1.3 and 2014.2\n\nDescription:\nTushar Patil reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.", 
            "date_created": "2014-12-01 18:25:38.211850+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "I was first to report it... https://bugs.launchpad.net/nova/+bug/1387543 (2014-10-30)", 
            "date_created": "2014-12-01 18:41:11.120544+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "Tristan: If George has reported this issue first, let's give him the credit. I don't have permissions to view bug ID 1387543 so I cannot comment on it.\n\nMy company name: NTT DATA, Inc.", 
            "date_created": "2014-12-01 19:01:49.584635+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "@George, oups I messed up the time line! Indeed you are the original reporter... So should I also credit \"Webzilla LTD\" along with your name ?\n", 
            "date_created": "2014-12-01 19:12:47.694739+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "I think it is independent discovery, we both should be noted. ", 
            "date_created": "2014-12-01 19:40:44.382748+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "@George @Tushar Alright, apologizes for the noise, I put both of you as reporter in this impact description draft #2:\n\nTitle: Nova instance delete in resize state may leak\nReporter: George Shuklin (Webzilla LTD) and Tushar Patil (NTT DATA, Inc)\nProducts: Nova\nVersions: up to 2014.1.3 and 2014.2\n\nDescription:\nGeorge Shuklin from Webzilla LTD and Tushar Patil from NTT DATA, Inc independently reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.", 
            "date_created": "2014-12-01 21:40:23.146532+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Thanks, it's nice, but where is the fix? And I need to backport it to havana... (sigh).", 
            "date_created": "2014-12-01 22:47:40.009228+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "George: our vulnerability managers and software developers work in parallel documenting and fixing security bugs. Hopefully a backportable (at least as far as stable/icehouse which is our oldest supported branch) fix will be along soon from one of the Nova developers since Andrew has triaged it at High importance.\n\nTristan: your latest impact description in comment #11 looks accurate--thanks!", 
            "date_created": "2014-12-02 02:28:06.233149+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Impact desc: 2014.2 up to 2014.2.1 (which was out last week) otherwise looks good.", 
            "date_created": "2014-12-08 15:07:20.683880+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@alaski: any progress on a patch for this one ?", 
            "date_created": "2014-12-12 09:14:53.824602+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Sorry for the long delay.  Still looking for someone to tackle this, or for time to do it myself.", 
            "date_created": "2015-01-12 21:09:17.031784+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "@nova-coresec: this have been reported almost 2 month ago... Any progress on the fix ?\n\nUpdating affected version line:\n\nTitle: Nova instance delete in resize state may leak\nReporter: George Shuklin (Webzilla LTD) and Tushar Patil (NTT DATA, Inc)\nProducts: Nova\nVersions: up to 2014.1.3 and 2014.2 versions up to 2014.2.1\n\nDescription:\nGeorge Shuklin from Webzilla LTD and Tushar Patil from NTT DATA, Inc independently reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.\n\n", 
            "date_created": "2015-02-05 14:08:41.888315+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Hi,\n\nThe attached fix addresses all the issues mentioned in the attached excel sheet. Please have a look at it.\nThe migration db interface code is already submitted by me in patch [1] but since it is not merged yet I have included it here.\n[1] : https://review.openstack.org/122347\n\nAs of now lot of unit tests are failing because of proposed changes. I will fix broken unit tests once my patch is approved by the security team.\nPlease review the code.", 
            "date_created": "2015-02-18 10:43:06.045567+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "", 
            "date_created": "2015-02-18 10:44:42.962870+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "I'm not sure what the problem is but I keep getting 'Processing Failed' when I try to view the patch.", 
            "date_created": "2015-02-19 02:17:29.909073+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Hi Tony,\n\nI had also tried the same and I was able to view the patch twice but after that getting 'Processing Failed' error continuously.\nI tried to view the patch attached on some other bug and there also I am getting same error.\n", 
            "date_created": "2015-02-19 05:39:34.609021+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "@Abhishek  Thanks for confirming it's not just me.  I'll check again tomorrow.", 
            "date_created": "2015-02-19 05:44:08.985474+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Impact desc +1", 
            "date_created": "2015-02-19 13:57:13.809713+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I had to remove the \"patch\" flag from the attachment to be able to view it.", 
            "date_created": "2015-02-20 02:20:16.665331+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Hi,\n\nThe patch attached in comment #18 is giving \"Processing Failed\" when you try to view it.\nAttaching the same patch again without setting \"patch\" flag.", 
            "date_created": "2015-02-20 05:10:53.093374+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "Firstly I want to admit upfront:\n1) I'm new to reviewing code in LP.  I'm sorry if there is an established protocol I'm breaking.  Just tell me the right thing to do.\n2) I could be off base here.  I'll clearly continue to dig into this.\n\nWhile there is no doubt that the get_latest_by_instance() API changes are neat.  They represent an (IIUC) RPC bump, which would be great to avoid for backports to stable/*.  Can't we get the same result using MigrationList.get_by_filters() and then open code the lookup for latest?\n\nBased on the scenarios you logged it looks like catching the InstanceNotFound exception should be enough but I have a vauge nagging fear that we're missing something.  No doubt what you've done is a long way to a solution.", 
            "date_created": "2015-02-20 05:19:09.026754+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Thanks for putting up a patch!\n\nReviewing the patch has caused me to dig into this issue deeply once again so I want to record some thoughts here.\n\nThis issue is exacerbated by not having locks during the resize process. The terminate process would wait for the completion of the resize at which point the delete would be handled fine.  Adding locking is not trivial because the resize executes on both source and dest hosts.\n\nThere is no good way to interrupt a resize.  If a delete comes in during a resize operation with no way to interrupt the resize or grab a lock then it's always possible that the resize disk could be created after any cleanup that's done.  The traditional way to interrupt a task is to delete the instance db record which will cause an exception to be raised on the next instance.save(), but the delete process doesn't know when the resize code will hit that.\n\nAs far as cleaning up the _resize disks, the poll_unconfirmed _resizes task should eventually handle it but it typically has a long window before it will clean up an uncomfirmed resize and the task is turned off by default.\n\nLooking at the patch specifically it doesn't appear to fully address the issue I raised above.  Since the resize is not interrupted it's possible that the cleanup code will run before the _resize disk is created.  It will reduce the timewindow for the race condition but not eliminate it.  It's also libvirt specific and this vulnerability could exist for any driver that creates a separate disk for resize.  The RPC bump is also an issue for backporting the change.  And finally the use of utils.execute to ssh and rm -rf a directory is concerning.\n\nThinking simply, just ensuring that virt driver destroy() methods look for and clean up resize disks would do a lot to alleviate this.", 
            "date_created": "2015-02-23 21:15:21.314538+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "I'm not the developer, just operator.\n\nFrom my point of view (and original report) the problem is with scp/rsync, which keeps copying files after instance is deleted. It keeps file opened even if you remove them, and eats network and cpu.\n\nMy naiive approach (like if I would be developer) would be:\n\n1. Save pid of scp/rsync process in migration record.\n2. If instance is terminated during migration, mark is as 'aborting' in migration record.\n3. On source host for each migration in aborting state (related to 'own' host):\na) Kill scp/rsync (by pid)\nb) Delete local files\n4. On destination:\na) Delete local files\n\nTo keep track of the files on all systems, they all should be saved in migration record.\n\nAgain, I'm not the developer, I don't know all aspects of RPC around nova-api/compute, but as operator I really don't want to see stray scp/rsync processes on compute, operating on deleted disks.", 
            "date_created": "2015-02-24 03:27:26.685360+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "George:  that seems to be to be a slightly different issue that what's reported here, unless I've missed the subtlety of what's reported here.  I was under the impression that the _resize disks were being left around indefinitely, vs your concern that they're not cleaned up immediately.  \n\nThere has been talk for quite a while about getting to a point where tasks are cancel-able within Nova, but at this point there's very little to work with to make that happen.  The solution you've proposed would involve a db migration and multiple RPC changes while being primarily libvirt specific.  Making that work generally across hypervisors will take a bit more abstraction and effort, and doing that in a backwards compatible way may end up being a challenge as well.", 
            "date_created": "2015-02-24 14:27:56.509593+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "For the issue reported here I would propose starting with a simple solution that cleans up any resize disks during the virt driver destroy() call.  The xenapi driver already does a similiar thing for rescue disks.\n\nFor the issue George is discussion I would look at adding a small store of instance->task mappings to each driver that it can use to clean up tasks during the destroy() process.", 
            "date_created": "2015-02-24 14:43:08.416365+00:00", 
            "author": "https://api.launchpad.net/1.0/~alaski"
        }, 
        {
            "content": "Yes, seems fine. One important notice: there is few methods of copy disk (rsync via rsyncd, rsync via ssh, scp) - and all them should be terminated properly.\n\nBtw: If you have problems with reproducing setup, I can give access to realservers laboratory installation with few compute hosts and scp as migration method (juno).", 
            "date_created": "2015-02-24 14:59:46.538287+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "@Andrew: do you plan to propose a simpler patch ?", 
            "date_created": "2015-03-02 15:07:32.672640+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@ttx Yes we're working on one.  I was blocked but a frustrating set of unrelated bugs.", 
            "date_created": "2015-03-03 22:50:15.313330+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Hi,\n\nPlease review the attached patch.\n\nIn this patch I have added periodic tasks to cleanup instance files left out either on the source or destination compute node when the instance is deleted during resizing.\n\nThis is a much cleaner approach than the previous attached patch as it doesn't involve adding new interface to the virt driver and compute rpcapi.\n\nThe attached patch and the patch [1] which is currently under review, will resolve all the issues mentioned in 'resize_scenarios.xlsx ' (refer comment #19).\n\n[1] https://review.openstack.org/#/c/154761/", 
            "date_created": "2015-03-12 09:25:14.415150+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "Hello.\n\nThanks for work. Is this fix cover the killing of rsync/scp processes in the case of 'cold migration'?", 
            "date_created": "2015-03-12 15:52:55.855123+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "Hi George:\n\nThis patch doesn't address killing orphan rsync/scp processes left out in the migrate_disk_and_power_off method. I think this fix is outside the scope of the problem we are trying to solve.\n\nCan you please report this issue as a new bug or else I can do it for you?", 
            "date_created": "2015-03-12 17:30:11.808363+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "I reported this problem (lost 'scp' processes), and it was linked as 'duplicate' here. \n\nSee https://bugs.launchpad.net/nova/+bug/1387543\n\nIf they are differ, duplication link should be removed and both should be treated as separate vulnerabilities. \n\n@ttx, please clarify.\n ", 
            "date_created": "2015-03-13 13:20:15.473710+00:00", 
            "author": "https://api.launchpad.net/1.0/~george-shuklin"
        }, 
        {
            "content": "OK, unduplicated", 
            "date_created": "2015-03-16 15:18:35.628383+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "@nova-coresec, please review proposed patch", 
            "date_created": "2015-03-16 15:18:54.114986+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "This is not a full review but after some review and testing the patch is looking good to me.\n\nIt doesn't eliminate the bug but it does reduce the severity and allow operators to mitigate.\n\nI'll do a more in depth review tomorrow and continue to test.", 
            "date_created": "2015-03-17 08:17:17.977368+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "I'm really wary of just adding a new state to the migration process. That code is already extremely complicated and confusing, and I'm not sure that we can reasonably approve such a change to that logic in a short period of time.\n\nIt seems to me that we should be able to have the periodic task just scan the instances directory for files that don't relate to any instance running on self.host, and that aren't also related to any ongoing migration and delete those. Why won't that work?", 
            "date_created": "2015-03-31 14:43:37.660184+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "Dan,\n\nThere is a one case where the host of the instance is set to the destination compute node during the resize operation. Now, when the instance is deleted during the resize operation, it will cleanup the instance files from the destination compute node leaving the instance files on the source compute node untouched. In this scenario, periodic task running on the source compute node won't be able to find the instance and it won't cleanup the instance files.\n\nThis situation is well handled in the above attached patch in comment #34.", 
            "date_created": "2015-03-31 16:48:54.388399+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "But a periodic task that looks at the local instance file residue (running on the source compute) should be able to tell that the instance is deleted and remove the residue, right?\n\nI'm not going to approve a change to introduce a new state to the migration structure at this point in the release. Others might.", 
            "date_created": "2015-03-31 19:24:20.566960+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "Dan,\n\nNow I have understood what you are proposing. I liked your idea.\n\nI have interpreted your solution as below:\n\n1. Get the list of instances folder based on the instances_path (in case of shared storage, it could return large set and there would performance issues here)\n- Add a new interface in driver.py and implement it in all hypervisors.\n2. get the list of UUIDS of all the instances known to the virtualization layer (list_instance_uuids)\n3. Prepare a list of instances folder not present in the list of UUIDS. Iterate through this list and check whether the instance is deleted or not. If deleted, call self.driver.delete_instance_files(instance) to cleanup the instance files.\n\nThe only issue I see here is in case of shared storage, it would return large set of instances folder in step #1 and iterating through this list would have serious performance impact.\n", 
            "date_created": "2015-03-31 20:20:27.355980+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "Since this will end up being used in stable branch updates, I agree that we should target the smallest fix possible. A whole new state is definitely not the best solution. I like the periodic cleanup. Patch anyone ?", 
            "date_created": "2015-04-02 13:10:57.351018+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Hi Thierry,\n\nFor periodic cleanup approach suggested by Dan Smith, there will be issue in case if instance path is configured on shared storage.\nIn case of shared storage it would return large set of instances folder and iterating through this list will have a performance impact.\n\nPlease let us know your opinion about this.", 
            "date_created": "2015-04-03 06:32:25.991957+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "Would it be possible to filter out shared storage in the request ? I don't think such a load for cleanup would be acceptable...", 
            "date_created": "2015-04-06 14:08:08.875116+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Hi All,\n\nPlease find the attached patch 0001-Delete-orphaned-instance-files-from-compute-nodes.patch, which is\nmodified version of patch attached in comment #34.\n\nIn new patch I have addressed Dan's comment (refer #41) about not adding new migration status.\nIn this patch, no new migration status is added and the periodic task '_cleanup_incomplete_migrations' \ntakes care of deleting instance files from source/destination compute node. I have also added new db\ninterface to mark migration record as deleted so that it doesn't appear again in the next periodic task run.", 
            "date_created": "2015-04-22 06:58:28.821917+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "Hi All,\n\nPlease find the attached patch 0001-Delete-orphaned-instance-files-from-compute-nodes.patch, which is modified version of patch attached in comment #34.\n\nIn new patch I have addressed Dan's comment (refer #41) about not adding new migration status.\nIn this patch, no new migration status is added and the periodic task '_cleanup_incomplete_migrations' takes care of deleting instance files from source/destination compute node. I have also added new db interface to mark migration record as deleted so that it doesn't appear again in the next periodic task run.\n\nWith new patch, I have successfully tested all the scenarios mentioned in  resize_scenarios.xlsx attached in comment #19.", 
            "date_created": "2015-04-22 07:11:46.701352+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "@nova-coresec, please review proposed patch\n", 
            "date_created": "2015-05-04 14:07:26.349059+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Updating affected version to include recent stable release.\n\nTitle: Nova instance delete in resize state may leak\nReporter: George Shuklin (Webzilla LTD) and Tushar Patil (NTT DATA, Inc)\nProducts: Nova\nAffects: versions through 2014.1.4, and 2014.2 versions through 2014.2.3, and version 2015.1.0\n\nDescription:\nGeorge Shuklin from Webzilla LTD and Tushar Patil from NTT DATA, Inc independently reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.", 
            "date_created": "2015-05-04 15:54:37.821799+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Patch looks reasonable to me, but I'd like to see Dan Smith review it as well. I shall ping him.", 
            "date_created": "2015-05-11 20:53:07.586853+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "We don't currently ever delete migrations, AFAICT, which is supported by the fact that you have to add that in the patch. IMHO, that's too much to put in this fix, and could have some unintended effects elsewhere.\n\nInstead, how about just setting the migration.status = 'failed' or something to prevent it from showing back up in your query? That will drop quite a bit of change from your patch. If you do that, then I'm happy with the rest.", 
            "date_created": "2015-05-11 21:02:00.444337+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "@danms isn't the whole point of the patch to delete the failed migration files from disk?  As I understand it the issue is that by running lots of resize/deletes from an account a (poorly behaved) user can DoS a hypervisor.  Or did I misunderstand something?\n\nOh wait.  I See now.  \n\nYeah I agree with Dan,  if we change the patch so that it sets the migration entry in the DB to 'failed' but never delete it, but still call  delete_instance_files() from the periodic task we make the minimal change and we avoid the RPC bump so backports are easy.", 
            "date_created": "2015-05-12 04:18:06.838494+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "I think the issue is that we can leave residue of previous instances on disk if the migrations error out, which could leak information. The actual act of deleting the records in the database is unrelated, AFAICT. It's just done so that the next run of the periodic task will not re-find the error'd migrations. Changing the state of those migrations so they don't match is easier, less code, etc.", 
            "date_created": "2015-05-12 14:52:56.537969+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "Dan, \nI agree with your opinion. We have started preparing for these changes and will upload a new patch very soon.", 
            "date_created": "2015-05-12 16:23:35.863541+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "Hi All,\n\nPlease find the attached patch 0001-Delete-orphaned-instance-files-from-compute-nodes.patch, which is modified version of patch attached in comment #49.\n\nIn new patch I have addressed Dan's comment (refer #53) about instead of deleting the migration record, mark the migration as failed.\n\nPlease review the patch and let me know your suggestions.", 
            "date_created": "2015-05-13 11:23:19.613627+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "Hi All,\n\nPlease find the attached patch 0001-Delete-orphaned-instance-files-from-compute-nodes.patch, which is modified version of patch attached in comment #49.\n\nIn new patch I have addressed Dan's comment (refer #53) about instead of deleting the migration record, mark the migration as failed.\n\nPlease review the patch and let me know your suggestions.", 
            "date_created": "2015-05-13 11:24:36.298261+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "I think that I'm okay with the patch as it is now (in #57). It could still use some cleanup, but let's get this merged into the tree and then we can do some follow-up to tidy a bit.", 
            "date_created": "2015-05-28 14:12:20.978506+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "+1 to dan's comments, it seems like a reasonable extension to the existing delete instance clean up logic, for the case of migrations.\n\nI would prefer to loop around the instance_uuid, fetching each instance, rather than getting the full list, but we can debate that after we get it merged.\n\nI do wonder if there are more things that might need cleaning up for partial transfers with other drivers, but I think this works for libvirt at least.", 
            "date_created": "2015-05-29 11:29:56.312170+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Hi all,\n\nThanks for reviewing and accepting the patch.\n\nAs I am not much familiar with security bugs i.e. how it will be merged into repository ?\n\nShould I submit the patch to gerrit or someone from security team will do that on my behalf ?\n\nPlease give your suggestions.", 
            "date_created": "2015-06-01 07:35:04.773808+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "While the information type is still \"private security\" please don't push any related changes to Gerrit. At the time of the coordinated disclosure, when the security advisory is sent, you can push your change to Gerrit if you expect to be awake and around at that time. Otherwise one of the Nova security reviewers or a member of the Vulnerability Management Team may do so on your behalf.\n\nFor now, however, it doesn't look like we've even scheduled a disclosure date (it's still missing backports to all the supported stable branches) so please just continue attaching patches to this private bug.", 
            "date_created": "2015-06-01 12:14:31.463566+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "This patch looks good to me. It would be nice for it to have some tests though. I have patched this into a local repo and can confirm that tox passes as well.\n\n@Rajesh -- is it possible for you to write a unit test for at the least the periodic task?\n\nThanks!", 
            "date_created": "2015-06-03 02:21:51.569357+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Hi Michael,\n\nI am working on writing unit test for periodic task.\n\nOnce it is done, I will attach updated patch.", 
            "date_created": "2015-06-03 06:04:38.622575+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "Hi Michale,\n\nPlease find the attached patch. In this patch I have added unit tests for periodic task.\n\nPlease review and let me know if you have any suggestions.\n\nThank you!", 
            "date_created": "2015-06-05 20:04:12.819394+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "This patch looks good to me, but I wanted to make some tweaks to the unit test:\n\n - we don't use doc comments in unit test methods for reasons too boring to repeat\n - I added an instance without a migration to the unit test as well\n\nI'll ping Dan and Tony to review this once I hit submit.", 
            "date_created": "2015-06-16 03:18:48.693218+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "And the patch", 
            "date_created": "2015-06-16 03:19:47.793306+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "I'm not convinced the test cases are correct but they're close enough that we can fix them in master later.\n\n+1", 
            "date_created": "2015-06-16 10:32:05.437203+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Great, can you post a single patch (using git format-patch) for master ?\n\nCan it be backported easily ?", 
            "date_created": "2015-06-16 12:27:40.433618+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Hi Tristan,\n\nI have created patch (using git format-patch) for master. The change in this patch is same as patch attached in comment #67.\n\nPlease find attached patch.\n", 
            "date_created": "2015-06-17 11:52:08.807639+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "@nova-coresec, please review last proposed patch", 
            "date_created": "2015-06-22 14:08:30.645343+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "This also needs backports to Juno and Kilo.", 
            "date_created": "2015-06-22 14:11:38.091856+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "+2 on the current patch. I didn't examine the tests closely, but if they pass, agree that we can fix them post-merge.", 
            "date_created": "2015-06-22 23:22:34.306214+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "The backport to Kilo is trivial, the Juno one is a little more complex.", 
            "date_created": "2015-06-23 06:59:55.647110+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Juno backport.  This isn't a simple cherry-pick but the changes aren;t complex and are due\n1) tests moving in kilo\n2) in kilo nova/objects/base.py has \"obj_as_admin()\" that's not in Juno\n\nPasses (nova) tox -epy27,pep8\nTempest didn't pass.  It failed differently on 3 runs so I'm confused.\n\nI'll try to build a Juno multi-compute stack early next week to perform additional checks.", 
            "date_created": "2015-06-26 06:56:58.604037+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "KIlo backport.  This is a simple cherry-pick from master\n\nPasses (nova) tox -epy27,pep8\nTempest didn't pass.  It failed differently on 3 runs so I'm confused.\n\nI'll try to build a Kilo multi-compute stack early next week to perform additional checks.", 
            "date_created": "2015-06-26 06:57:59.266372+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "@nova-coresec, please review proposed patch.\n\nHere is the updated impact description (removing Icehouse from affected releases):\n\nTitle: Nova instance delete in resize state may leak\nReporter: George Shuklin (Webzilla LTD) and Tushar Patil (NTT DATA, Inc)\nProducts: Nova\nAffects: 2014.2 versions through 2014.2.3, and version 2015.1.0\n\nDescription:\nGeorge Shuklin from Webzilla LTD and Tushar Patil from NTT DATA, Inc independently reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.", 
            "date_created": "2015-06-29 14:00:34.363152+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "+1 impact description. ", 
            "date_created": "2015-06-29 14:11:49.503101+00:00", 
            "author": "https://api.launchpad.net/1.0/~gmurphy"
        }, 
        {
            "content": "Should now be \"Affects: versions through 2014.2.3, and version 2015.1.0\".  Other than that, the updated impact description looks good to me.", 
            "date_created": "2015-06-29 14:16:42.002183+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Patch proposed in comment #70 (liberty), #76 (kilo) and #75 (juno) all succeed run_tests.sh here...\nCan we please get another nova core review before setting a disclosure date ?", 
            "date_created": "2015-07-02 17:49:40.504726+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "@Tristan: run_tests.sh or run_tempest.sh ?\n@mikal, @dan: Comments on my backports?", 
            "date_created": "2015-07-02 21:20:16.688189+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Tony, I only ran unit test (run_tests.sh)", 
            "date_created": "2015-07-06 13:56:48.167382+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "For the impact description in comment #77, let's try to avoid non-vulnerability-related uses of the overloaded term \"leak\" to reduce confusion. How about switching the title to something like \"Nova may fail to delete images in resize state\".", 
            "date_created": "2015-07-06 14:18:11.479391+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "Could we get a nova core to +2 these patches so we can set the disclosure date etc?", 
            "date_created": "2015-07-09 16:20:53.711041+00:00", 
            "author": "https://api.launchpad.net/1.0/~gmurphy"
        }, 
        {
            "content": "The backports look okay to me, as best I can tell without seeing test runs (which is not very well). Sounds like tony is still seeing tempest failures on the kilo one. Not sure what to say about that.", 
            "date_created": "2015-07-16 13:51:47.568475+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "We will run tempest tests against stable/kilo after applying kilo cherrypicked patch attached to comment#76 and see if we can find out any issues.", 
            "date_created": "2015-07-16 19:50:34.277971+00:00", 
            "author": "https://api.launchpad.net/1.0/~tpatil"
        }, 
        {
            "content": "Hi Tony,\n\nA) I have ran tempest tests on stable/kilo without applying security patch attached in comment #76. \nIn this case, following 7 tempest test were failing.\n\n1) tempest.api.network.test_metering_extensions.MeteringIpV6TestJSON\n2) tempest.api.network.test_metering_extensions.MeteringTestJSON\n3) tempest.thirdparty.boto.test_ec2_instance_run.InstanceRunTest\n4) tempest.thirdparty.boto.test_s3_buckets.S3BucketsTest.test_create_and_get_delete_bucket[id-4678525d-8da0-4518-81c1-f1f67d595b00]\n5) tempest.thirdparty.boto.test_s3_buckets.S3BucketsTest\n6) tempest.thirdparty.boto.test_s3_ec2_images.S3ImagesTest\n7) tempest.thirdparty.boto.test_s3_objects.S3BucketsTest.test_create_get_delete_object[id-4eea567a-b46a-405b-a475-6097e1faebde]\n\nB) When I ran tempest tests after applying security patch (attached in comment # 76) on stable/kilo, in that case also above mentioned tests were failing.\n\nCould you please share which tempest tests were failing in your stable/kilo environment, so that I can analyze whether these tests are failing because of proposed changes or not ?", 
            "date_created": "2015-07-20 11:41:05.770251+00:00", 
            "author": "https://api.launchpad.net/1.0/~rajesh-tailor"
        }, 
        {
            "content": "Hi Tony,\n\nAny updates on the failures. Till then could we get a nova core to +2 these patches?\n\n\n", 
            "date_created": "2015-08-11 09:11:17.985608+00:00", 
            "author": "https://api.launchpad.net/1.0/~abhishek-kekane"
        }, 
        {
            "content": "Hi all,\n I did some testing yesterday on the juno and kilo backports and for both branches I see a similar (though not identical) set of failures\nI get either 7 or 11 failures of these I'm ignoring the tempest.thirdparty.boto.* tests.  In both cases I don't see a change in the failure rates with the appropriate patch applied.\n\nHowever that's all done with an all-in-one devstack setup.  I think It's important to test with a multi-node setup. (which I'm doing now)\n\nOnce I've completed those tests I'll post a rebased set of patches.\n\nTony.", 
            "date_created": "2015-08-14 04:09:33.693721+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Results from testing\n\nmaster:\nbalder:tmp tony8129$ cat master.log-summary\ntempest.api.compute.admin.test_migrations.MigrationsAdminTest.test_list_migrations_in_flavor_resize_situation\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_auto_to_manual\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\ntempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_resize_server_confirm\ntempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_server_sequence_suspend_resume\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern\n\nmaster+patch\nbalder:tmp tony8129$ cat master-patched.log-summary \ntempest.api.compute.admin.test_migrations.MigrationsAdminTest.test_list_migrations_in_flavor_resize_situation\ntempest.api.compute.servers.test_create_server.ServersTestJSON.test_create_server_with_scheduler_hint_group\ntempest.api.compute.servers.test_create_server.ServersTestManualDisk.test_create_server_with_scheduler_hint_group\ntempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_verify_resize_state\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_auto_to_manual\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_shelve_unshelve_server\ntempest.api.compute.servers.test_servers_negative.ServersNegativeTestJSON.test_shelve_shelved_server\ntempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_resize_server_confirm\ntempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern\n\nThe delta looks like:\n--- master.log-summary\t2015-08-17 16:35:21.000000000 +1000\n+++ master-patched.log-summary\t2015-08-17 16:35:21.000000000 +1000\n@@ -1,0 +2,3 @@\n+tempest.api.compute.servers.test_create_server.ServersTestJSON.test_create_server_with_scheduler_hint_group\n+tempest.api.compute.servers.test_create_server.ServersTestManualDisk.test_create_server_with_scheduler_hint_group\n+tempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_verify_resize_state\n@@ -7,0 +11,2 @@\n+tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_shelve_unshelve_server\n+tempest.api.compute.servers.test_servers_negative.ServersNegativeTestJSON.test_shelve_shelved_server\n@@ -9 +14 @@\n-tempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_server_sequence_suspend_resume\n+tempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance", 
            "date_created": "2015-08-17 06:40:26.110348+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Results form testing:\n\nKilo\nbalder:tmp tony8129$ cat master-patched.log-summary \ntempest.api.compute.admin.test_migrations.MigrationsAdminTest.test_list_migrations_in_flavor_resize_situation\ntempest.api.compute.servers.test_create_server.ServersTestJSON.test_create_server_with_scheduler_hint_group\ntempest.api.compute.servers.test_create_server.ServersTestManualDisk.test_create_server_with_scheduler_hint_group\ntempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_verify_resize_state\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_auto_to_manual\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_shelve_unshelve_server\ntempest.api.compute.servers.test_servers_negative.ServersNegativeTestJSON.test_shelve_shelved_server\ntempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_resize_server_confirm\ntempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern\n\nKilo+patch\nbalder:tmp tony8129$ cat kilo-patched.log-summary\ntempest.api.compute.admin.test_migrations.MigrationsAdminTest.test_list_migrations_in_flavor_resize_situation\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\ntempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern\n\n\nDelta:\n--- kilo.log-summary\t2015-08-17 16:35:21.000000000 +1000\n+++ kilo-patched.log-summary\t2015-08-17 16:35:21.000000000 +1000\n@@ -1,5 +1 @@\n-tempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_verify_resize_state\n-tempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\n-tempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\n-tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\n-tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\n+tempest.api.compute.admin.test_migrations.MigrationsAdminTest.test_list_migrations_in_flavor_resize_situation\n@@ -6,0 +3 @@\n+tempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\n@@ -9 +5,0 @@\n-tempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern", 
            "date_created": "2015-08-17 06:42:51.804080+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "\nJuno\nbalder:tmp tony8129$ cat juno.log-summary \ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\ntempest.api.volume.test_volumes_snapshots.VolumesV1SnapshotTestJSON.test_snapshot_create_with_volume_in_use\ntempest.scenario.test_minimum_basic.TestMinimumBasicScenario.test_minimum_basic_scenario\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern\n\nJuno+patch\nbalder:tmp tony8129$ cat juno-patched.log-summary \ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_resize_server_from_manual_to_auto\ntempest.api.compute.servers.test_disk_config.ServerDiskConfigTestJSON.test_update_server_from_auto_to_manual\ntempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\ntempest.scenario.test_minimum_basic.TestMinimumBasicScenario.test_minimum_basic_scenario\ntempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_resize_server_confirm\ntempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\ntempest.scenario.test_snapshot_pattern.TestSnapshotPattern.test_snapshot_pattern\ntempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern\n\nDelta:\n--- juno.log-summary\t2015-08-17 16:35:21.000000000 +1000\n+++ juno-patched.log-summary\t2015-08-17 16:35:21.000000000 +1000\n@@ -3,3 +3 @@\n-tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm\n-tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_confirm_from_stopped\n-tempest.api.volume.test_volumes_snapshots.VolumesV1SnapshotTestJSON.test_snapshot_create_with_volume_in_use\n+tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert\n@@ -6,0 +5,2 @@\n+tempest.scenario.test_server_advanced_ops.TestServerAdvancedOps.test_resize_server_confirm\n+tempest.scenario.test_shelve_instance.TestShelveInstance.test_shelve_instance\n@@ -8 +8 @@\n-tempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern\n+tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern", 
            "date_created": "2015-08-17 06:46:04.196802+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "I haven't looked at the logs in detail yet as collecting the data took longer than expected.", 
            "date_created": "2015-08-17 07:08:33.484834+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "@nova-coresec, please review last proposed patch", 
            "date_created": "2015-08-17 14:05:53.379189+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "All three patches succeed ./run_tests.sh here.", 
            "date_created": "2015-08-17 19:12:50.245464+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "@nova-coresec, we need someone to approve the proposed patch before moving on this bug, please review patch in comments #90, #91 and #92", 
            "date_created": "2015-08-26 17:13:56.604314+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "All three patches look reasonable to me.", 
            "date_created": "2015-08-27 04:42:12.273631+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "Here is the updated impact description (fixing affected version and including comment #83 suggestions):\n\nTitle: Nova may fail to delete images in resize state\nReporter: George Shuklin (Webzilla LTD) and Tushar Patil (NTT DATA, Inc)\nProducts: Nova\nAffects: 2014.2 versions through 2014.2.3, and 2015.1 versions through 2015.1.1\n\nDescription:\nGeorge Shuklin from Webzilla LTD and Tushar Patil from NTT DATA, Inc independently reported a vulnerability in Nova resize state. If an authenticated user deletes an instance while it is in resize state, it will cause the original instance to not be deleted from the compute node it was running on. An attacker can use this to launch a denial of service attack. All Nova setups are affected.", 
            "date_created": "2015-08-27 13:44:16.516433+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Proposed disclosure date:\n2015-09-01, 1500UTC", 
            "date_created": "2015-08-27 14:49:55.791606+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/219299", 
            "date_created": "2015-09-01 15:00:24.820445+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/219300", 
            "date_created": "2015-09-01 15:00:52.533641+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/219301", 
            "date_created": "2015-09-01 15:01:04.152041+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/219299\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=18d6b5cc79973fc553daf7a92f22cce4dc0ca013\nSubmitter: Jenkins\nBranch:    master\n\ncommit 18d6b5cc79973fc553daf7a92f22cce4dc0ca013\nAuthor: Rajesh Tailor <email address hidden>\nDate:   Wed Mar 4 05:05:19 2015 -0800\n\n    Delete orphaned instance files from compute nodes\n    \n    While resizing/revert-resizing instance, if instance gets deleted\n    in between, then instance files remains either on the source or\n    destination compute node.\n    \n    To address this issue, added a new periodic task\n    '_cleanup_incomplete_migrations' which takes care of deleting\n    instance files from source/destination compute nodes and then\n    mark migration record as failed so that it doesn't appear again\n    in the next periodic task run.\n    \n    SecurityImpact\n    \n    Closes-Bug: 1392527\n    Change-Id: I9866d8e32e99b9f907921f4b226edf7b62bd83a7\n", 
            "date_created": "2015-09-02 22:38:16.358270+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/219300\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=5642b17cf4a18543f02a29ddaf4ab1b8d9819b76\nSubmitter: Jenkins\nBranch:    stable/kilo\n\ncommit 5642b17cf4a18543f02a29ddaf4ab1b8d9819b76\nAuthor: Rajesh Tailor <email address hidden>\nDate:   Wed Mar 4 05:05:19 2015 -0800\n\n    Delete orphaned instance files from compute nodes\n    \n    While resizing/revert-resizing instance, if instance gets deleted\n    in between, then instance files remains either on the source or\n    destination compute node.\n    \n    To address this issue, added a new periodic task\n    '_cleanup_incomplete_migrations' which takes care of deleting\n    instance files from source/destination compute nodes and then\n    mark migration record as failed so that it doesn't appear again\n    in the next periodic task run.\n    \n    SecurityImpact\n    \n    Closes-Bug: 1392527\n    Change-Id: I9866d8e32e99b9f907921f4b226edf7b62bd83a7\n    (cherry picked from commit 18d6b5cc79973fc553daf7a92f22cce4dc0ca013)\n", 
            "date_created": "2015-09-14 12:25:34.593344+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "A fix for this issue has been released, but I don't believe it addresses the CVE. The essence of the CVE is that by manipulating a race condition a malicious authenticated user can cause a compute host to run out of disk space. The fix is to periodically clean up the disk space. This fix means that a malicious authenticated user can still waste disk space during the interval of the periodic task. With automation, this is still likely to be a lot.\n\nContrast this with change Ie03acc00a7c904aec13c90ae6a53938d08e5e0c9 . In this instance, we decided that the ability for a malicious authenticated user to waste disk space for any period of time was a security bug, which we closed. The fix for this CVE is not consistent.\n\nI note that although this race condition is somewhat complicated, it is not described anywhere in either the commit message or code comments of the accepted fix. I will describe it here:\n\nRESIZE:\n\nOn src compute:\n1. Copy instance files from src to dest\n2. Set instance.host == dest\n3. Cast finish_resize on dest\n\nOn dest compute:\n4. Set instance.vm_state = RESIZED\n\nDELETE, on api:\n1. If instance.vm_state == RESIZED: confirm_resized()\n2. Call terminate_instance on instance.host\n\nIf delete is called between resize steps 1 and 2, delete will be called on src because instance.host has not yet been updated. As vm_state != RESIZED, confirm_resized will not be called, and files will be left on dest.\n\nIf delete is called between resize steps 2 and 4, delete will be called on dest, because instance.host has been updated. As vm_state != RESIZED, confirm_resized will not be called, and files will be left on src.\n\nIt should be possible to fix this properly by closing the race condition. The periodic task might remain, but would be redundant in normal operation. The outline of the fix is:\n\n* Remove the confirm_resized call from nova api delete.\n* Add a synchronized(instance.uuid) block around RESIZE steps 2 and 3 above (yes, including the cast).\n* Add a synchronized(instance.uuid) block around finish_resize\n* Add check inside the synchronized blocks in finish_resize for deleted. Fail if deleted.\n* Add check inside do_terminate_instance for current migration. If current migration, also cleanup other node following current periodic task.", 
            "date_created": "2015-10-01 15:03:31.004088+00:00", 
            "author": "https://api.launchpad.net/1.0/~mbooth-9"
        }, 
        {
            "content": "Thanks Matthew for digging that out!\n\nYour scenario seems correct in theory, but is it a race someone can win in practice ?\n\nIs there a reason why the Nova bug report is still New ?", 
            "date_created": "2015-10-01 18:21:06.823348+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Are we expecting more code changes in nova to address this CVE?", 
            "date_created": "2015-10-06 15:55:39.225543+00:00", 
            "author": "https://api.launchpad.net/1.0/~annasort"
        }, 
        {
            "content": "It looks like Matthew has changed the status of the bug from fix released to new in an attempt to solicit further input from nova developers, but ultimately this should I think be opened as a separate bug report. If the described scenario can be confirmed exploitable then the VMT would end up requesting a new CVE for an incomplete/partial fix instead.", 
            "date_created": "2015-10-06 16:19:23.841973+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I think this can (and should) be moved back to fix released:\n\nthe related issues are being tracked in https://bugs.launchpad.net/nova/+bug/1501808", 
            "date_created": "2015-10-07 03:12:53.161744+00:00", 
            "author": "https://api.launchpad.net/1.0/~o-tony"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/219301\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=fa72fb8b51d59e04913c871539cee98a3da79058\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit fa72fb8b51d59e04913c871539cee98a3da79058\nAuthor: Rajesh Tailor <email address hidden>\nDate:   Wed Mar 4 05:05:19 2015 -0800\n\n    Delete orphaned instance files from compute nodes\n    \n    While resizing/revert-resizing instance, if instance gets deleted\n    in between, then instance files remains either on the source or\n    destination compute node.\n    \n    To address this issue, added a new periodic task\n    '_cleanup_incomplete_migrations' which takes care of deleting\n    instance files from source/destination compute nodes and then\n    mark migration record as failed so that it doesn't appear again\n    in the next periodic task run.\n    \n    Conflicts:\n    \tnova/compute/manager.py\n    \tnova/tests/unit/compute/test_compute_mgr.py\n    \n    SecurityImpact\n    Closes-Bug: 1392527\n    Change-Id: I9866d8e32e99b9f907921f4b226edf7b62bd83a7\n    (cherry picked from commit 18d6b5cc79973fc553daf7a92f22cce4dc0ca013)\n", 
            "date_created": "2015-10-07 04:14:45.200423+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Hi, I changed the status because I wanted it documented somewhere. I felt that it made more sense to re-open the existing bug than open a new one because the original bug still exists. It was quite a lot of work to first determine what the problem was that the original patch was trying to fix[1], and then convince myself that it was not fixed, and I wanted to ensure that was recorded somewhere. I am not, however, immediately about to work on this myself.\n\n[1] Better commit messages, please!", 
            "date_created": "2015-10-12 12:29:26.074447+00:00", 
            "author": "https://api.launchpad.net/1.0/~mbooth-9"
        }
    ], 
    "closed": "2015-10-15 09:11:30.858048+00:00"
}
{
    "status": "Won't Fix", 
    "last_updated": "2016-12-09 12:56:03.699848+00:00", 
    "description": "Hi, \n\nI've found that under certain circumstances cinder does not create volumes fast enough. \n\nI can launch an image from a new volume from image with 4GB. I use LVM to allocate space. After a while I found that the instance didn't worked. \n\nLooking at logs I can find:\n\n2014-01-23 21:44:15.337 2398 TRACE nova.compute.manager [instance: a0e35767-424e-434d-99b4-35e19422054f]     attempts=attempts)\n2014-01-23 21:44:15.337 2398 TRACE nova.compute.manager [instance: a0e35767-424e-434d-99b4-35e19422054f] VolumeNotCreated: Volume 137bc77b-c9e6-47ba-b2f\n5-c83f440a988b did not finish being created even after we waited 66 seconds or 60 attempts.\n\n\nI was looking around and the cinder was \"downloading\". I think it was taking the image from the image server and building the volume. I don't know why it took so long since installation is gigabit ethernet and even more, the image is in a instance launched on the cinder hardware machine.  So it does not even any networking. All resolves internally. \n\n\nImage is saucy (About 300MB). \n\nThe problem is that after a while volume creation finished and instance failed. So I recereated instance and made it work from volume with no problems. \n\nHow should I track where the processing slows down?\n\nI know that iscsi attachment is slow. One of possible point of faillure is when you have iscsi target that are in a machine that's not reachable. This slows down the rest of processing but I'm not sure if this is a point here. \n\nAnyway. I'm sure hardware is not the best but pretty decent. Raid1 array with WD black label. Good sata controller and Intel gigabit network cards. So disk should not be the problem. I'm thinking about networking/config related problem. \n\nBut I'm lost on this.\n\nAny help.", 
    "tags": [
        "volumes"
    ], 
    "importance": "Undecided", 
    "heat": 40, 
    "link": "https://bugs.launchpad.net/nova/+bug/1272076", 
    "owner": "None", 
    "id": 1272076, 
    "index": 5768, 
    "openned": "2014-01-23 21:14:54.095400+00:00", 
    "created": "2014-01-23 21:14:54.095400+00:00", 
    "title": "VolumeNotCreated - Instance failed, cinder too slow with", 
    "comments": [
        {
            "content": "Hi, \n\nI've found that under certain circumstances cinder does not create volumes fast enough. \n\nI can launch an image from a new volume from image with 4GB. I use LVM to allocate space. After a while I found that the instance didn't worked. \n\nLooking at logs I can find:\n\n2014-01-23 21:44:15.337 2398 TRACE nova.compute.manager [instance: a0e35767-424e-434d-99b4-35e19422054f]     attempts=attempts)\n2014-01-23 21:44:15.337 2398 TRACE nova.compute.manager [instance: a0e35767-424e-434d-99b4-35e19422054f] VolumeNotCreated: Volume 137bc77b-c9e6-47ba-b2f\n5-c83f440a988b did not finish being created even after we waited 66 seconds or 60 attempts.\n\n\nI was looking around and the cinder was \"downloading\". I think it was taking the image from the image server and building the volume. I don't know why it took so long since installation is gigabit ethernet and even more, the image is in a instance launched on the cinder hardware machine.  So it does not even any networking. All resolves internally. \n\n\nImage is saucy (About 300MB). \n\nThe problem is that after a while volume creation finished and instance failed. So I recereated instance and made it work from volume with no problems. \n\nHow should I track where the processing slows down?\n\nI know that iscsi attachment is slow. One of possible point of faillure is when you have iscsi target that are in a machine that's not reachable. This slows down the rest of processing but I'm not sure if this is a point here. \n\nAnyway. I'm sure hardware is not the best but pretty decent. Raid1 array with WD black label. Good sata controller and Intel gigabit network cards. So disk should not be the problem. I'm thinking about networking/config related problem. \n\nBut I'm lost on this.\n\nAny help.", 
            "date_created": "2014-01-23 21:14:54.095400+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "This is the target stats.\n\n iscsiadm -m session -r 4 --stats\nStats for session [sid: 4, target: iqn.2010-10.org.openstack:volume-137bc77b-c9e6-47ba-b2f5-c83f440a988b, portal: 172.16.0.119,3260]\niSCSI SNMP:\n\ttxdata_octets: 1137908\n\trxdata_octets: 26062596\n\tnoptx_pdus: 0\n\tscsicmd_pdus: 2601\n\ttmfcmd_pdus: 0\n\tlogin_pdus: 0\n\ttext_pdus: 0\n\tdataout_pdus: 56\n\tlogout_pdus: 0\n\tsnack_pdus: 0\n\tnoprx_pdus: 0\n\tscsirsp_pdus: 2601\n\ttmfrsp_pdus: 0\n\ttextrsp_pdus: 0\n\tdatain_pdus: 2381\n\tlogoutrsp_pdus: 0\n\tr2t_pdus: 27\n\tasync_pdus: 0\n\trjt_pdus: 0\n\tdigest_err: 0\n\ttimeout_err: 0\niSCSI Extended:\n\ttx_sendpage_failures: 0\n\trx_discontiguous_hdr: 0\n\teh_abort_cnt: 0\n\n\nping 172.16.0.119\nPING 172.16.0.119 (172.16.0.119) 56(84) bytes of data.\n64 bytes from 172.16.0.119: icmp_seq=1 ttl=64 time=0.033 ms\n64 bytes from 172.16.0.119: icmp_seq=2 ttl=64 time=0.033 ms\n", 
            "date_created": "2014-01-23 21:18:15.363329+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "Machine even doesn't finish booting. \n\nThi is volume stats. I removed the rest for brief\n\nCada 2,0s: iostat -N                                                                                                            Thu Jan 23 22:21:32 2014\n\nLinux 3.11.0-12-generic (blue-compute)  23/01/14        _x86_64_        (4 CPU)\n\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n           5,01    0,00    1,46    6,22    0,00   87,31\n\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\ndata_vg-volume--137bc77b--c9e6--47ba--b2f5--c83f440a988b   123,15        30,75       461,86     140304    2107208\n\n", 
            "date_created": "2014-01-23 21:22:28.189437+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "Got it. \n\nI see lots of:\n\nJan 23 22:28:01 blue-compute kernel: [ 4957.380009]  connection2:0: detected conn error (1020)\nJan 23 22:28:01 blue-compute kernel: [ 4957.380062]  connection1:0: detected conn error (1020)\nJan 23 22:28:01 blue-compute iscsid: semop down failed 22\nJan 23 22:28:01 blue-compute iscsid: semop down failed 22\nJan 23 22:28:02 blue-compute tgtd: conn_close(103) connection closed, 0xdb6350 1\nJan 23 22:28:02 blue-compute tgtd: conn_close(103) connection closed, 0xdbd400 1\nJan 23 22:28:04 blue-compute kernel: [ 4960.387587]  connection2:0: detected conn error (1020)\nJan 23 22:28:04 blue-compute kernel: [ 4960.387637]  connection1:0: detected conn error (1020)\nJan 23 22:28:04 blue-compute iscsid: semop down failed 22\nJan 23 22:28:04 blue-compute iscsid: semop down failed 22\nJan 23 22:28:05 blue-compute tgtd: conn_close(103) connection closed, 0xdb6350 1\nJan 23 22:28:05 blue-compute tgtd: conn_close(103) connection closed, 0xdbd400 1\nJan 23 22:28:07 blue-compute kernel: [ 4963.394790]  connection2:0: detected conn error (1020)\nJan 23 22:28:07 blue-compute kernel: [ 4963.394836]  connection1:0: detected conn error (1020)\nJan 23 22:28:07 blue-compute iscsid: semop down failed 22\nJan 23 22:28:07 blue-compute iscsid: semop down failed 22\nJan 23 22:28:08 blue-compute tgtd: conn_close(103) connection closed, 0xdb6350 1\nJan 23 22:28:08 blue-compute tgtd: conn_close(103) connection closed, 0xdbd400 1\n", 
            "date_created": "2014-01-23 21:29:03.204062+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "We need to look at how we do this in Nova.  The 66 seconds isn't going to be sufficient in some cases depending on what backend you're using.  \n\nThe reason for example in your case of LVM:\n1  The Cinder volume is created. \n2. The image is downloaded from Glance to a temp-file on the cinder node\n3. The image is converted to raw and written to the volume\n\nFor the 256 MB image I'm kinda sad it's not fast enough, I'd be curious if you timed just the image download in Cinder to see what you come up with?  Do a \"cinder create --image-id xxxxx 5\". \n\nWith some setups this process will be pretty quick, but depending on the cinder backend in use, and the load on the system this can end up taking a while.  We should probably look at ways to improve how we do this on the Nova side.  Time-outs are always tricky though.  I would have to look to see if this timeout is already configurable in Nova, if it's not that might be a reasonable temporary solution.\n\nOne thing that we're working on in cinder is getting rid of the intermediate temp file to do the convert, but its not simple because you can't do qemu convert in place.", 
            "date_created": "2014-01-24 16:34:41.196061+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Hi John, \n\nI didn't know about the temp file. This also can cause the system to fail because this temp file does not fit into the cinder disk that can be small.  I suppose that if you get rid of the temp file it will perform better. \n\nI propose to sort things out. So when volume is create it should be attached to the image and leave the instance ready and downloading. So if it fails we can retry. Right now if it fails you have to redo everything. Terminate instance and build a new one. But from my point of view, the instance is ready the only problem is that the SO is not downloaded and installed. This should be retryable. \n\nI discovered that I'm suffering a tgtd/iscsi but. So the iscsi system disconnects every 2 seconds. I'm looking at it. So this is possible the responsible for the timeout. \n\nIf you tell me what you want to meassure. I will do it. Maybe we can find bottlenecks. \n\nI must report the iscsi problem and will post here when done. \n\n\n", 
            "date_created": "2014-01-26 10:59:05.087379+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/80619\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=4111e3586de88d5d580e6f11bafb644ff9144400\nSubmitter: Jenkins\nBranch:    master\n\ncommit 4111e3586de88d5d580e6f11bafb644ff9144400\nAuthor: JordanP <email address hidden>\nDate:   Fri Mar 14 16:21:47 2014 +0000\n\n    Increase volume creation max waiting time\n    \n    In some cases waiting 60sec, is not enough for cinder volume to be created.\n    For example, a 20Go LVM volume (hosting a Windows Image) would take 100sec\n    to be copied assuming a 200Mo/s cinder volume backend.\n    \n    Issue has already been discussed in I6bcd395a where a config option was\n    rejected.\n    \n    This patches raises the max timeout at 180sec.\n    \n    Change-Id: I7e739fc20af0ae963269459b5e224c4a8b7bb87d\n    Partial-Bug: #1253612\n    Partial-Bug: #1272076\n", 
            "date_created": "2014-03-18 00:35:29.566844+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I see this same behavior in an openstack deployment where my cinder instance is ceph backed any time I create a \"large\" volume. Anything over about 500G usually fails because cinder can't produce the volume fast enough.\n\nI'm using raw images (about 2.2G) also stored in ceph as my source, so no qemu convert should be needed in this case.\n\nI poked through the code and config, and it seems like this behavior should be controlled by the instance_build_timeout variable, which defaults to 0, so it should wait forever, but that does not seem to be the case. \n\nHere is a Redhat bug that seems to be tracking the same problem. https://bugzilla.redhat.com/show_bug.cgi?id=1019401  Note though that the code seems to have changed since their link into github was created. The line it links to does not seem relevant.", 
            "date_created": "2014-05-19 14:41:11.228936+00:00", 
            "author": "https://api.launchpad.net/1.0/~qhartman"
        }, 
        {
            "content": "Great, this seems to be addressed. I upgraded to icehouse and had no problem so long.", 
            "date_created": "2014-05-19 17:00:32.754060+00:00", 
            "author": "https://api.launchpad.net/1.0/~gad-aguilardelgado"
        }, 
        {
            "content": "For ceph backed glance and cinder , would it not be appropriate to have the copy occur between ceph pools rather than via the tmp folder locations on cinder ?", 
            "date_created": "2014-06-28 05:37:57.475057+00:00", 
            "author": "https://api.launchpad.net/1.0/~tlink"
        }, 
        {
            "content": "Tony,\n\nThis is going to be addressed by this blueprint https://blueprints.launchpad.net/cinder/+spec/enable-cloning-glance-rbd-image", 
            "date_created": "2014-09-16 10:06:21.944673+00:00", 
            "author": "https://api.launchpad.net/1.0/~heut2008"
        }, 
        {
            "content": "I looked at the patch that is discussed in https://git.openstack.org/cgit/openstack/nova/commit/?id=4111e3586de88d5d580e6f11bafb644ff9144400.  180 is not enough for a 60 GByte image.  I changed the value of max_tries to 600 and that seems to work better for me.  I made some measurements and the largest time it actually took was 7-1/2 minutes.\n\n", 
            "date_created": "2014-09-18 00:48:59.225972+00:00", 
            "author": "https://api.launchpad.net/1.0/~jeffsilverm"
        }, 
        {
            "content": "If anyone is seeing this with ceph when they have copy-on-write cloning enabled, please post a cinder log.", 
            "date_created": "2015-03-10 16:45:56.823443+00:00", 
            "author": "https://api.launchpad.net/1.0/~jdurgin"
        }, 
        {
            "content": "Ok, there are a couple of things here:\n\nFirst, if you ask nova to create a cinder volume from an image and boot, it unfortunately doesn't do any checks or make any compensation for the fact that the download/convert process from glance to the Cinder volume can take a bit of time.\n\nIt sounds like Josh and Jon are also looking at an issue specific to Ceph; I wonder if perhaps that should be posted as a separate bug?\n\nThis issue in the general sense I think is a problem with Nova and it's expectations that cinder create should always be speedy as it is with a raw volume.  FWIW, I believe someone was looking at this recently in Liberty.\n\nHonestly, I've always though the combining of the create-from-image and boot call shouldn't be used and it's better to do the steps explicitly outside of each other.  \n\n", 
            "date_created": "2015-09-03 15:59:57.916475+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "I believe this is something that will need to be addressed on the nova side. The reality is some backends take longer than others to create a volume. This isn't something that's going to change, certainly not by the OpenStack community. I think we would need to have some adaptable timeouts for this. Perhaps some kind of async polling to make sure the request is still being worked on.", 
            "date_created": "2016-09-27 22:32:38.431327+00:00", 
            "author": "https://api.launchpad.net/1.0/~sean-mcginnis"
        }, 
        {
            "content": "Changing the status of this bug to \"New\" from \"Incomplete\". We use \"Incomplete\" in Nova when we need further information from the original reporter. In this case, it looks like the issue has been evaluated and on Nova the question is about verifying if there's anything on the Nova side that can be done to address this issue.", 
            "date_created": "2016-10-01 22:02:46.189005+00:00", 
            "author": "https://api.launchpad.net/1.0/~auggy"
        }, 
        {
            "content": "Just making Nova wait longer doesn't seem to be a good strategy, because then we just end up in a different deadlock scenario of waiting with no idea when things are finished.\n\nI think that for long provisioning situations like this we just need to expect people to preprovision the volumes first from cinder. Nova will never be good enough at orchestration to solve this issue generically.\n\nMarking as Won't Fix. This isn't a simple bug, this is going to require at least one spec to change the way flows work. ", 
            "date_created": "2016-12-09 12:56:02.294510+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }
    ], 
    "closed": "2016-12-09 12:53:36.474004+00:00"
}
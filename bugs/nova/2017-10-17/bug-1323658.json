{
    "status": "Invalid", 
    "last_updated": "2015-11-17 07:23:53.486367+00:00", 
    "description": "Was looking at this when investigating bug 1310852, so that might be a duplicate of this, but the \"Public network connectivity check failed\" message doesn't show up in the logs for that bug, so opening a new one.\n\nThis is also maybe related to or regressions of bug 1194026 and/or bug 1253896.\n\nThe error in the console log:\n\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:24,369 Creating ssh connection to '172.24.4.110' as 'cirros' with public key authentication\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:24,491 Failed to establish authenticated ssh connection to cirros@172.24.4.110 ([Errno 111] Connection refused). Number attempts: 1. Retry after 2 seconds.\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:27,162 Failed to establish authenticated ssh connection to cirros@172.24.4.110 ([Errno 111] Connection refused). Number attempts: 2. Retry after 3 seconds.\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,049 starting thread (client mode): 0x9e9cf10L\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,050 EOF in transport thread\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,051 Public network connectivity check failed\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops Traceback (most recent call last):\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/scenario/test_network_advanced_server_ops.py\", line 119, in _check_public_network_connectivity\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     should_connect=should_connect)\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/scenario/manager.py\", line 779, in _check_vm_connectivity\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     linux_client.validate_authentication()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/utils/linux/remote_client.py\", line 53, in validate_authentication\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     self.ssh_client.test_connection_auth()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/ssh.py\", line 150, in test_connection_auth\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     connection = self._get_ssh_connection()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/ssh.py\", line 75, in _get_ssh_connection\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     timeout=self.channel_timeout, pkey=self.pkey)\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"/usr/local/lib/python2.7/dist-packages/paramiko/client.py\", line 242, in connect\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     t.start_client()\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"/usr/local/lib/python2.7/dist-packages/paramiko/transport.py\", line 346, in start_client\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     raise e\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops EOFError\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops \n\n\nLogstash query:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiUHVibGljIG5ldHdvcmsgY29ubmVjdGl2aXR5IGNoZWNrIGZhaWxlZFwiIEFORCB0YWdzOmNvbnNvbGUiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDExOTk0MTI1Mzd9\n\n664 hits in 7 days, check and gate, master and stable/icehouse branches.", 
    "tags": [
        "gate-failure", 
        "testing"
    ], 
    "importance": "Medium", 
    "heat": 88, 
    "link": "https://bugs.launchpad.net/nova/+bug/1323658", 
    "owner": "https://api.launchpad.net/1.0/~smigiel-dariusz", 
    "id": 1323658, 
    "index": 3914, 
    "openned": "2014-06-07 08:10:49.176456+00:00", 
    "created": "2014-05-27 14:06:02.810292+00:00", 
    "title": "Nova resize/restart results in guest ending up in inconsistent state with Neutron", 
    "comments": [
        {
            "content": "Was looking at this when investigating bug 1310852, so that might be a duplicate of this, but the \"Public network connectivity check failed\" message doesn't show up in the logs for that bug, so opening a new one.\n\nThis is also maybe related to or regressions of bug 1194026 and/or bug 1253896.\n\nThe error in the console log:\n\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:24,369 Creating ssh connection to '172.24.4.110' as 'cirros' with public key authentication\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:24,491 Failed to establish authenticated ssh connection to cirros@172.24.4.110 ([Errno 111] Connection refused). Number attempts: 1. Retry after 2 seconds.\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:27,162 Failed to establish authenticated ssh connection to cirros@172.24.4.110 ([Errno 111] Connection refused). Number attempts: 2. Retry after 3 seconds.\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,049 starting thread (client mode): 0x9e9cf10L\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,050 EOF in transport thread\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32,051 Public network connectivity check failed\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops Traceback (most recent call last):\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/scenario/test_network_advanced_server_ops.py\", line 119, in _check_public_network_connectivity\n2014-05-27 13:34:49.707 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     should_connect=should_connect)\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/scenario/manager.py\", line 779, in _check_vm_connectivity\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     linux_client.validate_authentication()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/utils/linux/remote_client.py\", line 53, in validate_authentication\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     self.ssh_client.test_connection_auth()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/ssh.py\", line 150, in test_connection_auth\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     connection = self._get_ssh_connection()\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"tempest/common/ssh.py\", line 75, in _get_ssh_connection\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     timeout=self.channel_timeout, pkey=self.pkey)\n2014-05-27 13:34:49.708 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"/usr/local/lib/python2.7/dist-packages/paramiko/client.py\", line 242, in connect\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     t.start_client()\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops   File \"/usr/local/lib/python2.7/dist-packages/paramiko/transport.py\", line 346, in start_client\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops     raise e\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops EOFError\n2014-05-27 13:34:49.709 |     2014-05-27 13:33:32.051 10354 TRACE tempest.scenario.test_network_advanced_server_ops \n\n\nLogstash query:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiUHVibGljIG5ldHdvcmsgY29ubmVjdGl2aXR5IGNoZWNrIGZhaWxlZFwiIEFORCB0YWdzOmNvbnNvbGUiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDExOTk0MTI1Mzd9\n\n664 hits in 7 days, check and gate, master and stable/icehouse branches.", 
            "date_created": "2014-05-27 14:06:02.810292+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Hmm, \"Public network connectivity check failed\" shows up in both basic and advanced server ops scenario tests, very similar _check_public_network_connectivity methods (that should probably be consolidated).", 
            "date_created": "2014-05-27 14:22:49.753823+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This also shows up all over the tempest logs when this fails:\n\nDEBUG paramiko.transport [-] EOF in transport thread _log /usr/local/lib/python2.7/dist-packages/paramiko/transport.py:1269\n\nThat looks like the same as Ubuntu test harness bug 1158224, so maybe just an underlying paramiko limitation in the version we use in the gate?", 
            "date_created": "2014-05-27 14:45:41.847977+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "mriedem: How can we verify if paramiko problem here?", 
            "date_created": "2014-05-27 20:19:58.793378+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "Extremely doubtful that it's a paramiko issue. the EOF is normal for paramiko. It's just confusing.", 
            "date_created": "2014-05-27 20:28:31.776677+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "This kind of bug has actually the same trace as bug 1253896.\nIt pretty much says: the network is not working, please have a look at what's going on.\n\nAs the failure frequency apparently spiked I'm ok with having another catch-all bug.\nIf I found a better root cause, I'll amend the bug and the e-r query.", 
            "date_created": "2014-06-02 12:44:44.726891+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Salvatore, Armando and myself have been trying to root cause this one all week. It's re-creatable locally, and appears to be a timing issue, though we haven't tracked it down yet 100%. Hoping to do that today.", 
            "date_created": "2014-06-05 13:51:16.494505+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "I don't know if this is are red herrings or hallucinations, but I've noticed a few things.\n\nThe neutron jobs always fail on two tests - those which do stop/start and resize for an instance. \nreboot, rebuild, suspend, and pause never cause failures.\n\nSo a first attempt would the conditionally skip these tests and see if the failures disappear or move to some other test. In the former case we'd need to investigate the server actions, in the latter case we'd have an indirect confirmation of timing issues.\n\nI also noticed that the fingerprint is neutron specific, so I decided to look if there were similar nova-network failures.\nI found a few, albeit they seem to have started more recently than neutron's. the query below only checks for gate failures in the full job with the infamous signature of bug 1253896:\n\nmessage:\"SSHTimeout: Connection to the\" AND message:\"via SSH timed out\" AND build_queue:\"gate\" AND build_name:\"gate-tempest-dsvm-full\"\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiU1NIVGltZW91dDogQ29ubmVjdGlvbiB0byB0aGVcIiBBTkQgbWVzc2FnZTpcInZpYSBTU0ggdGltZWQgb3V0XCIgQU5EIGJ1aWxkX3F1ZXVlOlwiZ2F0ZVwiIEFORCBidWlsZF9uYW1lOlwiZ2F0ZS10ZW1wZXN0LWRzdm0tZnVsbFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDAyMDQ3ODU2NTg3LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9\n\nI have been scavenging failures for instance console logs. Unfortunately none was found. However, extending the search to other nova-net jobs failing in the gate,  I found one. In the console log the instance was hang at startup - which would justify the crazy behaviour we're seeing in neutron failures:\n\nhttp://logs.openstack.org/12/94812/6/gate/gate-grenade-dsvm-partial-ncpu/901f7dd/console.html#_2014-06-04_05_34_54_072\n\nHowever, one data point has no significance at all.\nLooking for similar failures not just in the gate queue (and excluding those occuring on heat jobs as they use fedora images):\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiU1NIVGltZW91dDogQ29ubmVjdGlvbiB0byB0aGVcIiBBTkQgbWVzc2FnZTpcInZpYSBTU0ggdGltZWQgb3V0XCIgQU5EIGJ1aWxkX3F1ZXVlOlwiZ2F0ZVwiIEFORCBidWlsZF9uYW1lOlwiZ2F0ZS10ZW1wZXN0LWRzdm0tZnVsbFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDAyMDQ3ODU2NTg3LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9\n\nI can find a 100% correlation between the timer error in the instance and job failures. Still, there is not enough to assess that this timer failures are actually the root cause we're looking for, but it's definetely a reason for having console logs printed for each timeout failure.\n\nIn the case we decide it worth looking at these failures, what could have started triggering them? Could it be again a kernel update in the machines we use for run tests? (kern.log provides no hints)\n", 
            "date_created": "2014-06-06 10:11:48.754369+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Sean Dague has a patch up in Tempest to dump server console logs on timeout failures, but those are on server state changes (e.g. resize) rather than on this network scenario path.\n\nhttps://review.openstack.org/#/c/97812/", 
            "date_created": "2014-06-06 19:56:06.455017+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "It's worth noting that with the change here [1] (and it's subsequent dependent change), I can reproduce this. That change should dump the console, but I never see it. An example is here [2]. It tries to dump the console, I even see the API request in the nova logs, but I don't actually see a console dump. If someone can help interpret that nova console log dump in the paste it would greatly help me.\n\nAlso, for me, if I run a fresh devstack, I can then recreate this every time. If I then re-run devstack (without unstacking), I can never seem to recreate this. Seems to happen readily on the first run, and then never again. A datapoint at least.\n\nAnd one final note: armax has pushed this change [3], which will hopefully trigger and leave the VM around for us to debug. fungi is on watch to catch this and point armax and I at it once it happens over the weekend. So fingers crossed, we can maybe see what state the guest VM is in when this happens.\n\n[1] https://review.openstack.org/#/c/95831/\n[2] http://paste.openstack.org/show/83183/\n[3] https://review.openstack.org/#/c/98483/", 
            "date_created": "2014-06-06 21:28:41.624448+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "The tempest revert https://review.openstack.org/#/c/97245/ has pretty much nullified impact on gate.\n\nThis is good for the gate, but does not help us nailing the root cause.\n\"Some sort of timing issue\" is the closest thing we have at the moment.\n\nHere's a summary of the analysis so far:\n- the elastic recheck query has a fingerprint that can be matched only by neutron jobs. SSH failures have been observed also in jobs running nova-network, but it's not clear whether there is the same failure mode.\n- Failures occur only on start/stop and resize tests. The other tests in the network advanced server ops scenario seem to pass always.\n- The failure has been observed in upstream CI, ODL CI, and VMware CI - with exactly the same failure mode. This probably rules out any issue in neutron's agents. (VMware CI does not even run the L3 agent)\n- syslog reveals the VM gets an IP even after the reboot, when instead it's not reachable through ssh.\n  L2 logs and L3 logs for the same interval do not report instead changes to secuity groups, nat rules, or router interfaces.\n- the ssh timeout occurs because of \"connection refused\" (111) rather than \"no route to host\" (113). This could be because:\n  - instance booted but ssh service is disabled (waiting for console log output on tests)\n  - ssh traffic being rejected at the host (iptables drop counters suggest this is not the case)\n  - floating ip acting as a responder (nat rules for the floating IP are in place, so this should not be the case)\n- no errors are seen in kernel logs.\n\n", 
            "date_created": "2014-06-06 21:47:42.887245+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Salvatore, from what I can tell, there is no console dump. See my analysis in #11. The patches I was using there should have dumped the console, and I see the API call, but the data returned appears to be empty. This is peculiar to me, I'm wondering if anyone else has any input on this. To me, it appears like the VM is never fully running.", 
            "date_created": "2014-06-07 02:08:09.502757+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "Adding Nova as affected project as well, from what we've observed so far we cannot rule it out as a potential culprit in this whole saga.", 
            "date_created": "2014-06-07 08:13:23.107474+00:00", 
            "author": "https://api.launchpad.net/1.0/~armando-migliaccio"
        }, 
        {
            "content": "I'm now able to reproduce this locally, and in each case (either restart or resize), the guest VM itself appears to not be booted. I've added code to dump the console of the guest during the failure, and in each case the recall from Nova comes back with no data in the response body (see below). This leads me to believe this is a problem with the guest after the restart/resize:\n\n2014-06-09 16:14:32.640 30408 TRACE tempest.scenario.test_network_advanced_server_ops AssertionError: Timed out waiting for 172.24.4.16 to become reachable\n2014-06-09 16:14:32.640 30408 TRACE tempest.scenario.test_network_advanced_server_ops \n2014-06-09 16:14:32.642 30408 DEBUG tempest.scenario.manager [-] Console output for 86e7e749-83ec-4e39-9a39-3042213569b5 _log_console_output tempest/scenario/manager.py:397\n2014-06-09 16:14:32.642 30408 DEBUG novaclient.client [-] \nREQ: curl -i 'http://173.39.225.77:8774/v2/54a724dc112c482e84975fc6c1e3014e/servers/86e7e749-83ec-4e39-9a39-3042213569b5/action' -X POST -H \"X-Auth-Project-Id: TestNetworkAdvancedServerOps-1411494319\" -H \"User-Agent: python-novaclient\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -H \"X-Auth-Token: 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-jaLI6y-Dek2VSzwiKUcL7xDhzj9srnXL7lks12DZ0LW8SKQb2F1jxL6vcJzOaz5NGSt5NyPtK4gtjg+-W30gZEriBHzwYX+lKS3V4J5UrwoatkA4I-BJ+SUfMNGlKxKeUm7akccMSrQ7iD2aXQcNzYQ8SIG3bTS0zhtkRKcDFdyRUgTMV4Kpk5XgG5yodANeMbwXoK7aQgsg==\" -d '{\"os-getConsoleOutput\": {\"length\": null}}'\n http_log_req /opt/stack/python-novaclient/novaclient/client.py:181\n2014-06-09 16:14:32.758 30408 DEBUG novaclient.client [-] RESP: [200] CaseInsensitiveDict({'date': 'Mon, 09 Jun 2014 16:14:32 GMT', 'content-length': '14', 'content-type': 'application/json', 'x-compute-request-id': 'req-f24c4321-5cfa-46f5-93a6-636fe19f7daf'})\nRESP BODY: {\"output\": \"\"}\n", 
            "date_created": "2014-06-09 17:08:52.789175+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "BTW, my test environment has this information:\n\nubuntu@ubuntu-1204-1:~/devstack$ uname -a\nLinux ubuntu-1204-1 3.2.0-63-virtual #95-Ubuntu SMP Thu May 15 23:24:31 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n\nAnd this is my local.conf:\n\n[[local|localrc]] LOGFILE=stack.sh.log\nSCREEN_LOGDIR=/opt/stack/data/log\nLOG_COLOR=False\n#OFFLINE=True\nRECLONE=yes\n\n# DOCKER\n#VIRT_DRIVER=docker\n\ndisable_service n-net\nenable_service q-svc\nenable_service q-dhcp\nenable_service q-l3\nenable_service q-meta\nenable_service q-vpn\nenable_service q-metering\nenable_service q-lbaas\nenable_service q-fwaas\nenable_service neutron\nenable_service tempest\n\nHOST_IP=<redacted>\n#enable_service opendaylight\n\n# ODL WITH ML2\nQ_PLUGIN=ml2\n\nenable_service q-agt\n\nHOST_NAME=$(hostname)\nSERVICE_HOST_NAME=${HOST_NAME}\nSERVICE_HOST=<redacted>\n\nVNCSERVER_PROXYCLIENT_ADDRESS=<redacted>\nVNCSERVER_LISTEN=0.0.0.0\n\nMYSQL_HOST=$SERVICE_HOST\nRABBIT_HOST=$SERVICE_HOST\nGLANCE_HOSTPORT=$SERVICE_HOST:9292\nKEYSTONE_AUTH_HOST=$SERVICE_HOST\nKEYSTONE_SERVICE_HOST=$SERVICE_HOST\n\nMYSQL_PASSWORD=mysql\nRABBIT_PASSWORD=rabbit\nSERVICE_TOKEN=service\nSERVICE_PASSWORD=admin\nADMIN_PASSWORD=admin\n\n# For \"ssh timeout\" debugging\nAPI_RATE_LIMIT=False\nFORCE_CONFIG_DRIVE=always\nTEMPEST_HTTP_IMAGE=http://127.0.0.1/\n#CINDER_PERIODIC_INTERVAL=10\n#CEILOMETER_PIPELINE_INTERVAL=15\nQ_USE_DEBUG_COMMAND=True\n\n[[post-config|/etc/neutron/plugins/ml2/ml2_conf.ini]]\n[agent]\nminimize_polling=True\novsdb_monitor_respawn_interval=2\n\n", 
            "date_created": "2014-06-09 17:40:11.526887+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "Looks there are two boot up failure situation exists:\n1. https://bugs.launchpad.net/cirros/+bug/1312199\nThe console output contains:  \"MP-BIOS bug\"\n\n(an extra no_timer_check kernel parameter can skip the buggy test)\n\n2. empty console output\nWould be nice to have a 'virsh dump' from this situation.\n\n\n", 
            "date_created": "2014-06-09 23:31:18.313820+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "I did nt found any evidence for neutron did something wrong here[1]\nAnd I did not find any evidence the instance is started after os-stop and os-start.\n\n[1] http://logs.openstack.org/78/98178/1/check/check-tempest-dsvm-neutron/fbea859/console.html.gz", 
            "date_created": "2014-06-09 23:34:35.442595+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Thanks for looking at the logs for this Attila. Your analysis is the same as mine: This bug is due to the guest VM not being started or running after a restart or resize operation.", 
            "date_created": "2014-06-10 01:55:23.587870+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "Removed Neutron from affected, changed title of this bug based on analysis.", 
            "date_created": "2014-06-11 12:30:58.726250+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "I wonder if what were seeing here is the filesystem for the Guest sometimes being corrupted because Nova does a power-off for stop/resize without giving the guest a chance to shut down.   (Maybe grasping at straws).\n\nI have a patch up for review that fixes this for stop if you want to grab it and see if it helps:\nhttps://review.openstack.org/#/c/68942/\n\n", 
            "date_created": "2014-06-12 14:44:44.802796+00:00", 
            "author": "https://api.launchpad.net/1.0/~philip-day"
        }, 
        {
            "content": "Hi Phil,\n\nThis is one of the theories I am trying to confirm.\nUnfortunately the failing test suite did not report console log output, which is a shame.\nSome user-driven reproductions of the bug revealed an empty console log - which might be consistent with your hypothesis.\nHowever, using logstash I observed the VM taking an IP after reboot always, meaning that the file system was not corrupted. (This can be observed from syslog.txt.gz in test logs).\n\nOne might then point the finger on the l3 agent, however:\n- l3 agent logs have no event at all between VM shutoff and subsequent destroy after failure\n- the same failure was observed also on VMware minesweeper for NSX, which does not use the l3 agent\n\nI think we should reason on what triggered the bug on the gate.\nThat was a simple patch for verifying a guest was active and reachable before doing further tests on it.\nThat patch, at the end of the day  did two things:\n- ensure the VM went into active state, and the floating ip was wired\n- add a bit of delay before performing the actual test\n\nThe difference is that before the shutoff action was performed most of the time even before the vm reached active status.\nI'm not sure if this might explain anything.", 
            "date_created": "2014-06-15 13:02:56.790089+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "Now that we have console logs thanks to some tempest patches merging, we can see the guest doesn't even boot all the way. See the failed test here [1] for this patch [2].\n\n[1] http://logs.openstack.org/92/96792/18/gate/gate-tempest-dsvm-neutron-pg-2/54420bc/console.html\n[2] https://review.openstack.org/#/c/96792/", 
            "date_created": "2014-06-16 16:42:04.657095+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "looks like this is happening in neutron-full the most", 
            "date_created": "2014-09-02 19:32:07.916902+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "I posted https://review.openstack.org/#/c/121790/ for a part of this problem(os-stop/os-start).\n\nI could not find the reason we are checking the ssh connectivity during server SHUTOFF,\nand the check has been implemented since the first commit of the scenario test.\nThe above patch tries to remove the check.", 
            "date_created": "2014-09-16 09:54:36.143190+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "I investigated this problem based on the latest failure http://logs.openstack.org/59/120159/1/gate/gate-tempest-dsvm-neutron/be422ac/ , and Tempest could not get server console-log[1] again as Kyle at comment #21.\nIn this case, the scheduling a sever started 18:57:10 and the server became Active 18:57:21.\n\nAccording to nova-cpu log, a server creation seems successful with the message \"Instance spawned successfully.\"\nand there is nothing special on both libvirtd.txt.gz and kern_log.txt.gz in the above period.\nI guessed this problem was a server creation failure but I could not find its evidence and the reason why we are\nfacing this problem on Neutron gate.\n\n[1]: https://github.com/openstack/tempest/blob/master/tempest/scenario/manager.py#L705\n", 
            "date_created": "2014-09-18 07:40:46.634077+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "one more point:\nMy previous investigation source was a failure of test_server_connectivity_reboot.\nin addition, the failure happened at the server creation which calling self._setup_network_and_servers() before any actions.", 
            "date_created": "2014-09-18 07:45:55.135518+00:00", 
            "author": "https://api.launchpad.net/1.0/~oomichi"
        }, 
        {
            "content": "Setting target to RC1 for neutron as well - to ensure neutron devs' eyes are on this bug as well", 
            "date_created": "2014-09-18 09:05:21.757175+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "So I'm not sure this is the same bug or not, but we just started running the 'check-tempest-dsvm-ironic-parallel-nv' in the Ironic pipeline yesterday and we are seeing a percentage of failures in test_network_advanced_server_ops @ test_server_connectivity_reboot.  In our case:\n\n* Original server boots\n* First ssh connectivity check passes (via floating ip)\n* Server reboots, become active\n* Second ssh connectivity check fails:\n   - Connection is made to the server, but _get_ssh_connection() times out ~6 min. (!) later\n\nThe symptoms in our case look consistent across all failures and it certainly smells like something wrong in the guest. Unfortunately, Ironic does not support getting console log so I'm in the blind there.   \n\n2014-09-23 19:30:55,769 29820 INFO     [paramiko.transport] Authentication (publickey) successful!\n2014-09-23 19:30:55,771 29820 INFO     [tempest.common.ssh] ssh connection to cirros@172.24.4.39 successfuly created\n2014-09-23 19:30:56,069 29820 INFO     [tempest.common.rest_client] Request (TestNetworkAdvancedServerOps:test_server_connectivity_reboot): 202 POST http://127.0.0.1:8774/v2/403a6c74810c47ea9b109f7f07e14c90/servers/788eaed6-284c-4b95-bef0-b08251eef043/action 0.117s\n2014-09-23 19:30:56,168 29820 INFO     [tempest.common.rest_client] Request (TestNetworkAdvancedServerOps:test_server_connectivity_reboot): 200 GET http://127.0.0.1:8774/v2/403a6c74810c47ea9b109f7f07e14c90/servers/788eaed6-284c-4b95-bef0-b08251eef043 0.099s\n2014-09-23 19:30:57,283 29820 INFO     [tempest.common.rest_client] Request (TestNetworkAdvancedServerOps:test_server_connectivity_reboot): 200 GET http://127.0.0.1:8774/v2/403a6c74810c47ea9b109f7f07e14c90/servers/788eaed6-284c-4b95-bef0-b08251eef043 0.098s\n2014-09-23 19:30:57,297 29820 INFO     [tempest.common.waiters] State transition \"REBOOT/rebooting\" ==> \"REBOOT/reboot_started\" after 1 second wait\n2014-09-23 19:30:58,402 29820 INFO     [tempest.common.rest_client] Request (TestNetworkAdvancedServerOps:test_server_connectivity_reboot): 200 GET http://127.0.0.1:8774/v2/403a6c74810c47ea9b109f7f07e14c90/servers/788eaed6-284c-4b95-bef0-b08251eef043 0.104s\n2014-09-23 19:30:59,516 29820 INFO     [tempest.common.rest_client] Request (TestNetworkAdvancedServerOps:test_server_connectivity_reboot): 200 GET http://127.0.0.1:8774/v2/403a6c74810c47ea9b109f7f07e14c90/servers/788eaed6-284c-4b95-bef0-b08251eef043 0.096s\n2014-09-23 19:30:59,531 29820 INFO     [tempest.common.waiters] State transition \"REBOOT/reboot_started\" ==> \"ACTIVE/None\" after 3 second wait\n2014-09-23 19:30:59,531 29820 INFO     [tempest.scenario.manager] Tenant networks not configured to be reachable.\n2014-09-23 19:30:59,531 29820 DEBUG    [tempest.scenario.manager] checking network connections to IP 172.24.4.39 with user: cirros\n2014-09-23 19:30:59,541 29820 INFO     [tempest.common.ssh] Creating ssh connection to '172.24.4.39' as 'cirros' with public key authentication\n2014-09-23 19:30:59,548 29820 INFO     [paramiko.transport] Connected (version 2.0, client dropbear_2012.55)\n2014-09-23 19:36:39,266 29820 ERROR    [tempest.scenario.manager] Public network connectivity check failed\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager Traceback (most recent call last):\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/scenario/manager.py\", line 670, in _check_public_network_connectivity\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     should_connect=should_connect)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/scenario/manager.py\", line 657, in _check_vm_connectivity\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     self.get_remote_client(ip_address, username, private_key)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/scenario/manager.py\", line 333, in get_remote_client\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     linux_client.validate_authentication()\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/common/utils/linux/remote_client.py\", line 54, in validate_authentication\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     self.ssh_client.test_connection_auth()\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/common/ssh.py\", line 151, in test_connection_auth\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     connection = self._get_ssh_connection()\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"tempest/common/ssh.py\", line 76, in _get_ssh_connection\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     timeout=self.channel_timeout, pkey=self.pkey)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"/usr/local/lib/python2.7/dist-packages/paramiko/client.py\", line 265, in connect\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     t.start_client()\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"/usr/local/lib/python2.7/dist-packages/paramiko/transport.py\", line 402, in start_client\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     event.wait(0.1)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"/usr/lib/python2.7/threading.py\", line 620, in wait\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     self.__cond.wait(timeout)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"/usr/lib/python2.7/threading.py\", line 358, in wait\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     _sleep(delay)\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager   File \"/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py\", line 52, in signal_handler\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager     raise TimeoutException()\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager TimeoutException\n2014-09-23 19:36:39.266 29820 TRACE tempest.scenario.manager ", 
            "date_created": "2014-09-23 22:47:37.530941+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "Is anyone on the nova side working on this bug? If not we might need to drop it from rc1.\n\n(I'm not saying its not important, just that I am not sure we can justify holding rc1 for it if there's no one working on it).", 
            "date_created": "2014-09-24 01:27:48.063219+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "I've heard from Salvatore that Aaron is working on this one, so assigning it to him for now unless I hear otherwise.", 
            "date_created": "2014-09-24 02:29:17.320267+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "In the nova meeting today dansmith said that he worked with arosen on this and arosen things it's something in tempest that's causing this so shouldn't be a problem in the real world, so removing from rc1.", 
            "date_created": "2014-09-25 21:14:20.974065+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The issue started to show up in Icehouse recently (a week or two?) in a very painful way. It's very hard to merge anything into Neutron due to this, since failure rate is very high. Nova also suffers a lot because it runs multiple neutron jobs. Other projects are affected, but not that hard (less neutron jobs in runs => lesser chance to fail in gate).\n\nIt would be beneficial to find out when exactly it started to fail in Icehouse, to check patches around that time in devstack/master and neutron/icehouse.", 
            "date_created": "2014-09-26 09:55:11.656166+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "The following search shows that it started in icehouse gate some time around 2014-09-17:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiU1NIVGltZW91dDogQ29ubmVjdGlvbiB0byB0aGVcIiBBTkQgbWVzc2FnZTpcInZpYSBTU0ggdGltZWQgb3V0XCIgQU5EIGJ1aWxkX2JyYW5jaDpcInN0YWJsZS9pY2Vob3VzZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiJjdXN0b20iLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsiZnJvbSI6IjIwMTQtMDktMDFUMDk6NTg6NDIrMDA6MDAiLCJ0byI6IjIwMTQtMDktMjZUMDk6NTg6NDIrMDA6MDAiLCJ1c2VyX2ludGVydmFsIjoiMCJ9LCJzdGFtcCI6MTQxMTcyNTU1ODIyNSwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==", 
            "date_created": "2014-09-26 10:03:11.293815+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "The following search shows that master also started to fail with the error around 2014-09-17:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiU1NIVGltZW91dDogQ29ubmVjdGlvbiB0byB0aGVcIiBBTkQgbWVzc2FnZTpcInZpYSBTU0ggdGltZWQgb3V0XCIgQU5EIGJ1aWxkX2JyYW5jaDpcIm1hc3RlclwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiJjdXN0b20iLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsiZnJvbSI6IjIwMTQtMDktMDFUMDk6NTg6NDIrMDA6MDAiLCJ0byI6IjIwMTQtMDktMjZUMDk6NTg6NDIrMDA6MDAiLCJ1c2VyX2ludGVydmFsIjoiMCJ9LCJzdGFtcCI6MTQxMTcyODY4NDg3MSwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==\n\nMeaning, something was broken in components that are common to both branches. There are:\n- clients;\n- oslo libraries;\n- third-party libraries not pinned in Icehouse;\n- tempest.", 
            "date_created": "2014-09-26 10:53:15.897483+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "Logstash only goes back 10 days, so 9/17 isn't when it showed up probably, it's just how far back we go. I've made that mistake before. :)", 
            "date_created": "2014-09-26 13:52:59.516362+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Matt, thanks for clearing my ignorance! :)\nThat said, the issue started to show up in Icehouse branches so frequently since recently, so there was probably some change in stable branches that triggered it.", 
            "date_created": "2014-09-27 14:54:45.608792+00:00", 
            "author": "https://api.launchpad.net/1.0/~ihar-hrachyshka"
        }, 
        {
            "content": "Similar to comment #30, removing from RC1 for Neutron.", 
            "date_created": "2014-09-28 19:04:40.161198+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "I don't think that neutron has anything to do with this bug because nova never unplugs the vif (and the connectivity was tested and working before the resize occured). ", 
            "date_created": "2014-09-30 21:20:12.020048+00:00", 
            "author": "https://api.launchpad.net/1.0/~arosen"
        }, 
        {
            "content": "For this failed run: \n\nhttp://logs.openstack.org/06/123106/2/gate/gate-tempest-dsvm-neutron-full/1924e8c/logs/\n\nAfter the resize the instance did come up and make the same metadata quries: \n\n 2014-09-25 16:36:58.607 23696 INFO eventlet.wsgi.server [req-2695eb5f-5327-47f4-a541-29fb0abf8493 None] 10.100.0.2,<local> - - [25/Sep/2014 16:36:58] \"GET /2009-04-04/meta-data/placement/availability-zone HTTP/1.1\" 200 119 0.045631\n\n 2014-09-25 16:37:28.402 23696 INFO eventlet.wsgi.server [req-2695eb5f-5327-47f4-a541-29fb0abf8493 None] 10.100.0.2,<local> - - [25/Sep/2014 16:37:28] \"GET /2009-04-04/meta-data/placement/availability-zone HTTP/1.1\" 200 119 0.035326", 
            "date_created": "2014-09-30 22:02:32.254769+00:00", 
            "author": "https://api.launchpad.net/1.0/~arosen"
        }, 
        {
            "content": "I am currently pretty suspicious that we have 3 chains for the same IP still sitting around by the time we get to this test - http://logs.openstack.org/31/122931/2/check/check-tempest-dsvm-neutron-pg-2/b540280/console.html.gz#_2014-09-25_16_55_37_692\n\nMy current guess is that we are leaking rules because there are races in the rules constructor, and that after the guest comes down and back up something is making inbound rules not actually work.", 
            "date_created": "2014-10-01 10:37:42.167896+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "My attempt at a summary so far:\n\nmestery @ 2014-06-25 comment #6 [1] suggests \"It's re-creatable locally, and appears to be a timing issue\"\n\nmestery@ 2014-06-06:  #9 [2] \"can recreate with a fresh devstack\" (but only on first run). - references the test consolidation patch @ [3] which merged\n\nmestery @ #13 [4] can reproduce it locally - from the trace can see failed tempest.scenario.test_network_advanced_server_ops\n\nihar @ #33 [5] suggests the root cause is something common to branches from the time this started to be seen in the gate.\n\nThe offending tests were skipped at [6] - merged Oct 01st, but I can't find reference to that review here (even though the review does have partial-bug to here). I unskip them but on my devstack box:\n\n./run_tempest.sh tempest.scenario.test_network_advanced_server_ops\n\nruns 6 tests clean. So, does anyone have a reliable repro for this for a devstack box?\n\nthanks, marios\n\n[1] https://bugs.launchpad.net/neutron/+bug/1323658/comments/6\n[2] https://bugs.launchpad.net/neutron/+bug/1323658/comments/9\n[3] https://review.openstack.org/#/c/95831/\n[4] https://bugs.launchpad.net/neutron/+bug/1323658/comments/13\n[5] https://bugs.launchpad.net/neutron/+bug/1323658/comments/33\n[6] https://review.openstack.org/#/c/125150/", 
            "date_created": "2014-10-10 14:24:23.236722+00:00", 
            "author": "https://api.launchpad.net/1.0/~marios-b"
        }, 
        {
            "content": "I guest I have got this issue on a real running cluster.\n\nI have made some investigation, to find a difference between instance that loose their network interfaces after hard reboot, and those that doesn't.\nWhen as say 'loose their network interfaces', I means the network device is no more present into the guest and the libvirt configuration.\n\nOn a bugged instance:\n\nWe can see that networks is empty in 'nova list', but the interface is still attached:\n\n    # nova list --name XXXXXXXXXXXX\n    +--------------------------------------+---------------------+--------+------------+-------------+----------+\n    | ID                                   | Name                | Status | Task State | Power State | Networks |\n    +--------------------------------------+---------------------+--------+------------+-------------+----------+\n    | 940be621-97a9-409a-8c31-7c0c9c8afbfd | XXXXXX | ACTIVE | -          | Running     |          |\n    +--------------------------------------+---------------------+--------+------------+-------------+----------+\n    # nova interface-list 940be621-97a9-409a-8c31-7c0c9c8afbfd\n    +------------+--------------------------------------+--------------------------------------+---------------------------------------+-------------------+\n    | Port State | Port ID                              | Net ID                               | IP addresses                          | MAC Addr          |\n    +------------+--------------------------------------+--------------------------------------+---------------------------------------+-------------------+\n    | ACTIVE     | cdfb0e9f-d0cd-4ca7-9954-0fce25680905 | 07487477-8cbd-4d2a-b549-713a964ddb51 | XXXXXXXXXXXXXXXXX | fa:16:3e:fc:78:a3 |\n    +------------+--------------------------------------+--------------------------------------+---------------------------------------+-------------------+\n\nSo I have looked into the db and it seems that something have emptied the columns \"instance_info_caches.network_info\" of some of my instances:\n\n    mysql> select * from instance_info_caches where id = 174;\n    +---------------------+---------------------+------------+-----+--------------+--------------------------------------+---------+\n    | created_at          | updated_at          | deleted_at | id  | network_info | instance_uuid                        | deleted |\n    +---------------------+---------------------+------------+-----+--------------+--------------------------------------+---------+\n    | 2014-09-22 13:59:11 | 2014-10-20 05:39:48 | NULL       | 174 | []           | 940be621-97a9-409a-8c31-7c0c9c8afbfd |       0 |\n    +---------------------+---------------------+------------+-----+--------------+--------------------------------------+---------+\n    1 row in set (0.00 sec)\n", 
            "date_created": "2014-10-22 06:28:32.722866+00:00", 
            "author": "https://api.launchpad.net/1.0/~sileht"
        }, 
        {
            "content": "when I was in unitedstack, we run into this problem. Here I tentatively commit my patch and let us see if it can be mitigated or even fixed.  Even more, we disabled the heal instance info cache periodic task by setting heal_instance_info_cache_interval to -1.", 
            "date_created": "2014-11-15 02:37:08.469018+00:00", 
            "author": "https://api.launchpad.net/1.0/~gongysh"
        }, 
        {
            "content": "review URL:  https://review.openstack.org/#/c/134689/", 
            "date_created": "2014-11-15 02:38:13.571254+00:00", 
            "author": "https://api.launchpad.net/1.0/~gongysh"
        }, 
        {
            "content": "not seeing this as much in the gate not critical anymore", 
            "date_created": "2014-11-20 21:10:33.246174+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "I am sorry for changing yong sheng gong to nobody.", 
            "date_created": "2014-12-02 08:20:18.935362+00:00", 
            "author": "https://api.launchpad.net/1.0/~wangjunqing"
        }, 
        {
            "content": "Moving down to High and out of Kilo-1 until someone is assigned this bug.", 
            "date_created": "2014-12-02 15:11:51.590312+00:00", 
            "author": "https://api.launchpad.net/1.0/~mestery"
        }, 
        {
            "content": "this has been the top gate bug for a long time.  Lowering the priority because no one wants to work on it is backwards.", 
            "date_created": "2015-02-12 23:41:06.035523+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Trying to read through this again (there are a lot of comments), for awhile it sounded like this was happening more for reboot and we've had a long-standing problem with reboot where we don't test soft reboot in tempest because we can't actually tell when a soft reboot happens or it fallsback to hard reboot, see bug 1014647.  That's really more of a test sanity issue though I think, stepping back it sounds like we are really just hearing about a problem bringing up a guest after stop/start (resize) and/or ssh'ing into that guest.\n\nIf the problem is ssh, and we think it's due to missing network_info in the instance_info_cache, as suggested in comment 41 and comment 42, we could add some diagnostic trace to tempest on failure by getting the network info from the instance and see if there are inconsistencies, e.g. comment 41 where there is no network listed but there are interfaces attached.\n\nThere have been races in the neutronv2 API code in nova around refreshing the info cache, I'm wondering if there is something in the compute manager in the resize/reboot operations where we need to refresh the info_cache from neutron and that's missing today.\n\nWe could also look at refreshing https://review.openstack.org/#/c/134689/ but changing it given Dan Smith's comments about how we don't need to refresh everything, just instance_info_cache.  I can take a look at that.", 
            "date_created": "2015-03-05 15:01:06.460448+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Also, per comment 40, we still have the two tests skipped in test_network_advanced_server_ops.py, we could push a WIP patch to unskip those and see how bad things fail on that to get a recreate, then push a nova change which tries to address the problem, and use the Depends-On tag in the tempest revert to only run with the nova change and see if it helps.", 
            "date_created": "2015-03-05 15:04:04.555353+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "When this fails in the gate today, it's happening on this test:\n\ntempest.scenario.test_network_advanced_server_ops.TestNetworkAdvancedServerOps.test_server_connectivity_reboot\n\ne.g.:\n\nhttp://logs.openstack.org/97/152997/2/gate/gate-grenade-dsvm-ironic-sideways/229ce51/console.html#_2015-03-03_19_14_56_542", 
            "date_created": "2015-03-05 15:29:30.101771+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "One thing I'd be interested in, is on initial server create with the libvirt driver, we have the callback from neutron on VIF plugging that we wait for, I wonder if that same code path is used for resize/migrate/reboot?  I'm wondering if we're just getting lucky when things pass that the ports are still attached/associated with the instance on the neutron side after doing a resize/migration.  In the reboot case, it might be something completely different, like the guest not coming up/being ready even though the ports are still attached after the reboot.", 
            "date_created": "2015-03-05 15:57:12.110912+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/161763", 
            "date_created": "2015-03-05 16:21:59.465405+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/134689\nReason: This has had a -1 since November, so it's abandoned.", 
            "date_created": "2015-03-05 16:52:50.692352+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "If there is something in here related to vif plugging callbacks with neutron and we're not waiting long enough, we could add some logging in resize_instance and _post_live_migration like was added to _delete_instance here:\n\nhttps://github.com/openstack/nova/commit/272aca6926f50b584dab77fb2d1c29fc8bdbdb9e", 
            "date_created": "2015-03-05 20:58:46.966582+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This is also doing unnecessary work with neutron if we're resizing to the same host, since the host hasn't changed in that case so there is really nothing to update in the port:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/network/neutronv2/api.py#n1592", 
            "date_created": "2015-03-05 21:28:49.599653+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This should probably also be INFO instead of WARNING:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py#n681\n\nGiven it's basically expected and happens a lot in normal gate runs: http://goo.gl/R5Uesa", 
            "date_created": "2015-03-05 21:29:36.805497+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Regarding waiting for a neutron event callback after we've migrated (resized), we don't wait for any events back from neutron because vifs_already_plugged is set to True:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py#n6358\n\nThis doesn't seem necessarily correct, the things that call unplug_vifs are _cleanup_reize:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py#n957\n\nwhich is called from confirm_migration:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py#n6415\n\nThat only unplugs vifs if the host has changed though, which for the resize tests in the gate with tempest they are the same host, so vifs wouldn't be unplugged on resize to the same host, but still seems like we have a bug waiting to happen here.\n\nThe other place is post live migration:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py#n5927\n\n\n\nTalking with dansmith in IRC, apparently in finish_migration we pass vifs_already_plugged=True even if the host has changed because apparently neutron will not send the vif plugged event twice (or for a resize/migrate), even if the host changed.  When they were waiting on that when the neutron callback code was previously added it was stalling out in the gate b/c nova was waiting for an event from neutron that would never come.  We should at least document something in the code that explains this and have the neutron cores assert and explain why.", 
            "date_created": "2015-03-05 22:01:15.639411+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/161933", 
            "date_created": "2015-03-05 22:47:07.593069+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/161934", 
            "date_created": "2015-03-05 22:47:16.190553+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/161763\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=b09262414a26e849fff45699169781bb9924bddf\nSubmitter: Jenkins\nBranch:    master\n\ncommit b09262414a26e849fff45699169781bb9924bddf\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Mar 5 08:12:49 2015 -0800\n\n    neutron: add logging during nw info_cache refresh when port is gone\n    \n    When doing a resize/migrate/reboot, we want to make sure that the\n    network info we're getting is current and nothing was lost in the\n    process, so add some logging when what was in the instance_info_cache is\n    not considered associated with the instance from neutron's point of view\n    during a refresh.\n    \n    Related-Bug: #1323658\n    \n    Change-Id: I0a8d8dcabf0b2c8f0e4552178e8cb99113640b11\n", 
            "date_created": "2015-03-06 19:38:55.403197+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/161933\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=cc8b8dc826f460104b30ceee67ebc1f8cc41e198\nSubmitter: Jenkins\nBranch:    master\n\ncommit cc8b8dc826f460104b30ceee67ebc1f8cc41e198\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Mar 5 14:11:18 2015 -0800\n\n    neutron: check for same host in _update_port_binding_for_instance\n    \n    If we're doing a resize to the same host, like all resize tests in the\n    gate, the host hasn't changed so there is no point in updating the port\n    binding host in neutron so add a check for that case.\n    \n    Related-Bug: #1323658\n    \n    Change-Id: Ieb5ade398da8c11b29a3fa83b01ecf14e5e1f5b7\n", 
            "date_created": "2015-03-06 22:29:01.613380+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/161934\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=e4e1c6a2f060a91899549b9643132abde27d033e\nSubmitter: Jenkins\nBranch:    master\n\ncommit e4e1c6a2f060a91899549b9643132abde27d033e\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Mar 5 14:44:00 2015 -0800\n\n    libvirt: add comment for vifs_already_plugged=True in finish_migration\n    \n    Given we unplug VIFs in _cleanup_resize if the host has changed, it's\n    confusing to pass vifs_already_plugged=True in finish_migration without\n    some explanation of why we don't check to see if the host has changed\n    before deciding what the value of vifs_already_plugged should be when\n    creating the new domain and plugging VIFs again.\n    \n    After some discussion with Neutron cores, this is the explanation I've\n    come up with, albeit not a great one, and indicates a gap in some of the\n    nova/neutron integration story.\n    \n    Related-Bug: #1323658\n    \n    Change-Id: I982b6bd0650f29f34c213b6eebbe066f7bd65815\n", 
            "date_created": "2015-03-06 22:29:32.953263+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "https://review.openstack.org/#/c/161768/ has been rechecked 8 times and hasn't hit the failures, so maybe this was already fixed/mitigated by something else since October.", 
            "date_created": "2015-03-11 20:22:36.745863+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Dropping from critical since this isn't failing after several rechecks: https://review.openstack.org/#/c/161768/", 
            "date_created": "2015-03-12 21:26:15.856932+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Bug 1326183 could be related here, we might be hitting a weird sync issue with the _heal_instance_info_cache periodic task.", 
            "date_created": "2015-03-16 14:42:22.006978+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Retested this with latest devstack configuration. Based on description from other bug (duplicate of this: https://bugs.launchpad.net/nova/+bug/1364588) I'm not able to reproduce this problem.\n\nResized about 20 times on running and disabled servers. All the time, everything is OK. There are no problems with \"ERROR\" state or losing connectivity.\nLogstash doesn't show any similar problems with this issue.\n\nClosing as works-for-me, fix probably was already released.\n\nIf anyone has another experiences, please reopen with additional info.", 
            "date_created": "2015-11-14 11:18:15.832645+00:00", 
            "author": "https://api.launchpad.net/1.0/~smigiel-dariusz"
        }, 
        {
            "content": "If this doesn't appear to be a problem anymore and we can't cite a patch or change that fixes it, I think it's more appropriate to mark it as Invalid rather than Fix Released. Please reopen if needed.", 
            "date_created": "2015-11-16 20:55:50.412093+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "@Melanie, OK. Thank you for clarification.\nSo I'll change also Neutron from Fix Released to Invalid.", 
            "date_created": "2015-11-17 07:23:10.006899+00:00", 
            "author": "https://api.launchpad.net/1.0/~smigiel-dariusz"
        }, 
        {
            "content": "Eh.. I'm not able to do this for Neutron, so probably someone else, with greater permissions should do this.", 
            "date_created": "2015-11-17 07:23:52.045368+00:00", 
            "author": "https://api.launchpad.net/1.0/~smigiel-dariusz"
        }
    ], 
    "closed": "2015-11-14 11:18:41.211591+00:00"
}
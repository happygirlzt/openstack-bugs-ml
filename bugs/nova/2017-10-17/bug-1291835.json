{
    "status": "Expired", 
    "last_updated": "2016-07-05 09:47:01.723368+00:00", 
    "description": "If you attach and detach the same volume to same server in loop,  the n-api may report the device name is already use.\n\nI used the following stress test https://review.openstack.org/#/c/77196/.\nWith the blow configuration.\n[{\"action\": \"tempest.stress.actions.volume_attach_verify.VolumeVerifyStress\",\n  \"threads\": 1,\n  \"use_admin\": false,\n  \"use_isolated_tenants\": false,\n  \"kwargs\": {\"vm_extra_args\": {},\n             \"new_volume\": false,\n             \"new_server\": false,\n             \"ssh_test_before_attach\": false,\n             \"enable_ssh_verify\": false}\n}\n]\n\nThe issue happens with all config options, but this is the fastest way.\n\nThe issue can happen even after the device disappearance confirmed via ssh, ie. not listed in /proc/partitions anymore.\n\nI used similar  devstack setup as the gate uses  with multiple nova api/cond workers.\n\nNOTE: libvirt/qemu/linux disregards the device name.\n\n\nFor reproducing the issue\n1.  add tempest to enabled devstack services.\n2.  apply the https://review.openstack.org/#/c/77196 locally\n3.  change the logging options in the tempst.conf [DEFAULT]log_config_append=etc/logging.conf.sample\n4. ./tempest/stress/run_stress.py -t tempest/stress/etc/volume-attach-verify.json -n 128 -S\n\nIf 128 attempt is not enough, you can increase the number of threads (in the json config) or the attempts as cli option.", 
    "tags": [
        "volumes"
    ], 
    "importance": "Undecided", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1291835", 
    "owner": "None", 
    "id": 1291835, 
    "index": 5917, 
    "openned": "2014-03-13 07:42:59.848990+00:00", 
    "created": "2014-03-13 07:42:59.848990+00:00", 
    "title": "Repeated volume attach can cause u'message': u'The supplied device path (/dev/vdc) is in use.'", 
    "comments": [
        {
            "content": "If you attach and detach the same volume to same server in loop,  the n-api may report the device name is already use.\n\nI used the following stress test https://review.openstack.org/#/c/77196/.\nWith the blow configuration.\n[{\"action\": \"tempest.stress.actions.volume_attach_verify.VolumeVerifyStress\",\n  \"threads\": 1,\n  \"use_admin\": false,\n  \"use_isolated_tenants\": false,\n  \"kwargs\": {\"vm_extra_args\": {},\n             \"new_volume\": false,\n             \"new_server\": false,\n             \"ssh_test_before_attach\": false,\n             \"enable_ssh_verify\": false}\n}\n]\n\nThe issue happens with all config options, but this is the fastest way.\n\nThe issue can happen even after the device disappearance confirmed via ssh, ie. not listed in /proc/partitions anymore.\n\nI used similar  devstack setup as the gate uses  with multiple nova api/cond workers.\n\nNOTE: libvirt/qemu/linux disregards the device name.\n\n\nFor reproducing the issue\n1.  add tempest to enabled devstack services.\n2.  apply the https://review.openstack.org/#/c/77196 locally\n3.  change the logging options in the tempst.conf [DEFAULT]log_config_append=etc/logging.conf.sample\n4. ./tempest/stress/run_stress.py -t tempest/stress/etc/volume-attach-verify.json -n 128 -S\n\nIf 128 attempt is not enough, you can increase the number of threads (in the json config) or the attempts as cli option.", 
            "date_created": "2014-03-13 07:42:59.848990+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Yep - there seems to be a race window (not huge to be honest) that can cause this kind of issue with stress testing. The solution is to do two things:\n\n1) In the API part of attach - we should reserve the volume before we call out to the compute service to reserve the device name\n2) I the compute detach path -  We should release the device name  before we release the volume in detach\n\nI will  target this for Juno as it is somewhat risky at this point. It should be an easy backport to Icehouse once we get it in.", 
            "date_created": "2014-03-27 17:30:44.732147+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "We should re-test this against liberty code.  The issue is probably still there though given the async nature of the volume attach/detach.\n\nBug 1374508 might be related here.  As is bug 1492026.", 
            "date_created": "2015-09-18 13:36:48.461574+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:47:01.061194+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ], 
    "closed": "2016-07-05 09:46:58.584629+00:00"
}
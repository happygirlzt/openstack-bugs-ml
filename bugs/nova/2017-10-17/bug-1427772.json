{
    "status": "Confirmed", 
    "last_updated": "2017-08-21 18:54:33.428348+00:00", 
    "description": "More in-depth discussion can be found here:\n\nhttp://lists.openstack.org/pipermail/openstack-dev/2015-February/056695.html\n\nBasically - there is a number of filters that need to be re-run even if we force a host. The reasons are two-fold. Placing some instances on some hosts is an obvious mistake and should be disallowed (instances with specific CPU pinning are an example), though this will be eventually rejected by the host. Second reason is that claims logic on compute hosts depends on limits being set by the filters, and if they are not some of the oversubscription as well as more complex placement logic will not work for the instance (see the following bug report as to how it impacts NUMA placement logic https://bugzilla.redhat.com/show_bug.cgi?id=1189906)\n\nOverall completely bypassing the filters is not ideal.", 
    "tags": [
        "conductor", 
        "placement", 
        "scheduler"
    ], 
    "importance": "Low", 
    "heat": 36, 
    "link": "https://bugs.launchpad.net/nova/+bug/1427772", 
    "owner": "None", 
    "id": 1427772, 
    "index": 1574, 
    "openned": "2015-03-03 17:06:46.102431+00:00", 
    "created": "2015-03-03 17:06:46.102431+00:00", 
    "title": "Instance that uses force-host still needs to run some filters", 
    "comments": [
        {
            "content": "More in-depth discussion can be found here:\n\nhttp://lists.openstack.org/pipermail/openstack-dev/2015-February/056695.html\n\nBasically - there is a number of filters that need to be re-run even if we force a host. The reasons are two-fold. Placing some instances on some hosts is an obvious mistake and should be disallowed (instances with specific CPU pinning are an example), though this will be eventually rejected by the host. Second reason is that claims logic on compute hosts depends on limits being set by the filters, and if they are not some of the oversubscription as well as more complex placement logic will not work for the instance (see the following bug report as to how it impacts NUMA placement logic https://bugzilla.redhat.com/show_bug.cgi?id=1189906)\n\nOverall completely bypassing the filters is not ideal.", 
            "date_created": "2015-03-03 17:06:46.102431+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "I got to  the end of the referenced thread <http://lists.openstack.org/pipermail/openstack-dev/2015-February/057090.html> and was none the wiser on what the chosen outcome was for this. There were several options bounced back and forth but nothing that seemed to win consensus. The options appeared to be:\n\n* do nothing, let the targeted compute node fail when force_host is set if the host can't accept\n    * except that apparently some claims can't be validated on-node\n* pass the hosts listed as forced into the filters without changing the filters\n* pass the hosts listed as forced into the filters but only run those filters which are deemed required when doing a force\n* make a new command that is like force but means \"force with filtering\"\n* but wait, forcing is antithetical to \"cloud\"; end users should not know or care about where stuff ends up as long as it is good enough, let's get rid of the force command\n\nFrom what I can tell the sentence in the description beginning \"Second reason...\" suggests that \"pass the hosts listed as forced into the filters without changing the filters\" is the change that resolves the presenting problems with the smallest surface area of a change to the system.\n\nSo the question becomes: does the second reason matter?\n\nIf not we should mark this as incomplete, or invalid, or not a bug and get it off the radar.", 
            "date_created": "2016-01-12 16:27:43.678192+00:00", 
            "author": "https://api.launchpad.net/1.0/~cdent"
        }, 
        {
            "content": "Chris Dent wrote in #1:\n\n> * pass the hosts listed as forced into the filters but only run \n> those filters which are deemed required when doing a force\n\nI think that's the outcome of the ML post.\n\n> So the question becomes: does the second reason matter?\n\nI don't know enough of the claim logic to answer that. Sylvain and\nNikola seem to know more about that.\n\n> If not we should mark this as incomplete, or invalid, or not a \n> bug and get it off the radar.\n\nI think it's fair to close a bug report when we don't come to a\nconclusion within 1 year, as it indicates that this is an issue \nwhich might not be important enough to spent effort on it. To \nclearly indicate that this bug report is expired, I'm in favor\nof using the status \"won't fix\" with a comment that this is an\nexpired issue. But this expiration policy I have in mind is not yet\nin place [1].\n\n[1] https://review.openstack.org/#/c/266453/", 
            "date_created": "2016-01-13 10:08:32.145301+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Let's close this and let it come back up later. There's been no progress on it for greater than a year and we don't even seem to have consensus on what the right behavior should be.", 
            "date_created": "2016-03-09 13:00:39.227480+00:00", 
            "author": "https://api.launchpad.net/1.0/~cdent"
        }, 
        {
            "content": "I am not sure that's valid grounds for closing. We should really keep it open unless we decide we will not fix this ever. If we'll come back to it - leave it open and maybe tag as long-standing or similar to be able to filter it out. Bugs don't need food to stick around in the bugtracker... :)", 
            "date_created": "2016-03-09 13:15:47.326549+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Bugs that aren't getting attention and don't have  clear replication strategy (after a year) are just noise and noise makes it harder to get things done or to have a sense of purpose or direction. Having to filter a list of bugs between those that are actionable and those that are not is extremely discouraging and makes me not want to look at the bug list. However I'm obliged to do so and every time I do I just think: I do not want to do this again. \n\nWe need to make the process more actionable. So if one way to do that is to remove bugs that nobody seems to really care about then we should do so. The great things about bugs is that if they are _actually_ bugs they'll get reported again. If they aren't reported then they must not be real.", 
            "date_created": "2016-03-09 13:32:35.280417+00:00", 
            "author": "https://api.launchpad.net/1.0/~cdent"
        }, 
        {
            "content": "I don't think that we want to close bugs just because they are not getting immediate attention. Ultimately - that's what the bug tracker is for, and this bug is (I believe) a valid defect so there is no reason to close it unless we decide we really don't want to fix it.\n\nI don't think we need to get discouraged by the number of open bugs, just organize them better to make them actionable. There is definitely value in keeping backlog bugs. OTOH - we could have a different way to track these kind of issues if we want to keep the bug list here as only immediately actionable stuff.\n\nEither way we should keep useful info accessible. I can see people hitting this issue, and trying to find what's the story and getting discouraged by a closed bug.", 
            "date_created": "2016-03-10 09:38:54.429473+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Removing assignee due to lack of visible progress. Feel free to reassign if it is not the case.", 
            "date_created": "2016-08-17 03:34:39.612941+00:00", 
            "author": "https://api.launchpad.net/1.0/~mszankin"
        }, 
        {
            "content": "I was going to open a new bug for this code:\n\nhttps://github.com/openstack/nova/blob/16.0.0.0rc1/nova/conductor/tasks/live_migrate.py#L103\n\nBut this existing bug is probably close enough, especially since in Pike we removed RamFilter and DiskFilter from the default list of enabled filters, so the limits for those aren't going to come from the scheduler to the compute anymore for claims (by default for new installs).\n\nBut the code here specifically:\n\nhttps://github.com/openstack/nova/blob/16.0.0.0rc1/nova/conductor/tasks/live_migrate.py#L103\n\nIs mimicing the RamFilter and uses the ComputeNode information, when it should be using the Placement service now which would have the latest inventory information for that ComputeNode resource provider at this point.\n\nSo as we remove core/ram/disk filters from the scheduler and replace those with using the Placement service, in the hopes of eventually removing that kind of claims logic in the compute service, we need to replace anything that relied on it to also use Placement, like this code in conductor.", 
            "date_created": "2017-08-21 18:54:33.012414+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }
    ]
}
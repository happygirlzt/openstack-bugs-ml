{
    "status": "Fix Released", 
    "last_updated": "2015-10-12 21:15:58.323273+00:00", 
    "description": "2014-09-22 15:09:59.534 ERROR nova.compute.manager [req-74866bd8-5382-4354-89ca-7683a013d99c ServerDiskConfigTestJSON-218625483 ServerDiskConfigTestJSON-1404511620] [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] Setting instance vm_state to ERROR\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] Traceback (most recent call last):\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 6054, in _error_out_instance_on_exception\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     yield\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3740, in resize_instance\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5838, in migrate_disk_and_power_off\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     self.power_off(instance, timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2474, in power_off\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     self._clean_shutdown(instance, timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2434, in _clean_shutdown\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     (state, _max_mem, _mem, _cpus, _t) = dom.info()\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     rv = execute(f, *args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     six.reraise(c, e, tb)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     rv = meth(*args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 1068, in info\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     if ret is None: raise libvirtError ('virDomainGetInfo() failed', dom=self)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] libvirtError: operation failed: cannot read cputime for domain\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] \n\n\nhttp://logs.openstack.org/71/123071/1/gate/gate-tempest-dsvm-postgres-full/7369ae8/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-09-22_15_09_59_534\n\nI am seeing this stacktrace in some nova-compute logs, it looks like this is showing up in passing jobs, but I think that is because tempest doesn't always fail if a 'nova delete' fails.", 
    "tags": [
        "libvirt"
    ], 
    "importance": "High", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1372670", 
    "owner": "https://api.launchpad.net/1.0/~ecosta", 
    "id": 1372670, 
    "index": 1620, 
    "openned": "2014-09-22 22:31:07.565996+00:00", 
    "created": "2014-09-22 22:31:07.565996+00:00", 
    "title": "libvirtError: operation failed: cannot read cputime for domain", 
    "comments": [
        {
            "content": "2014-09-22 15:09:59.534 ERROR nova.compute.manager [req-74866bd8-5382-4354-89ca-7683a013d99c ServerDiskConfigTestJSON-218625483 ServerDiskConfigTestJSON-1404511620] [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] Setting instance vm_state to ERROR\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] Traceback (most recent call last):\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 6054, in _error_out_instance_on_exception\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     yield\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3740, in resize_instance\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5838, in migrate_disk_and_power_off\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     self.power_off(instance, timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2474, in power_off\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     self._clean_shutdown(instance, timeout, retry_interval)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2434, in _clean_shutdown\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     (state, _max_mem, _mem, _cpus, _t) = dom.info()\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     rv = execute(f, *args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     six.reraise(c, e, tb)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     rv = meth(*args, **kwargs)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 1068, in info\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738]     if ret is None: raise libvirtError ('virDomainGetInfo() failed', dom=self)\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] libvirtError: operation failed: cannot read cputime for domain\n2014-09-22 15:09:59.534 25333 TRACE nova.compute.manager [instance: c09099c1-5dde-4ba9-8a8e-94ff75309738] \n\n\nhttp://logs.openstack.org/71/123071/1/gate/gate-tempest-dsvm-postgres-full/7369ae8/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-09-22_15_09_59_534\n\nI am seeing this stacktrace in some nova-compute logs, it looks like this is showing up in passing jobs, but I think that is because tempest doesn't always fail if a 'nova delete' fails.", 
            "date_created": "2014-09-22 22:31:07.565996+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Is there a proposed next step here?", 
            "date_created": "2014-09-22 23:08:58.244175+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "libvirtd.txt says\n\n2014-09-22 15:09:59.532+0000: 17850: warning : qemuGetProcessInfo:1306 : cannot parse process status data\n2014-09-22 15:09:59.532+0000: 17850: error : qemuDomainGetInfo:2509 : operation failed: cannot read cputime for domain\n\nand it also says on the same thread earlier\n\n2014-09-22 15:07:28.627+0000: 17850: warning : AppArmorSetFDLabel:919 : could not find path for descriptor /proc/self/fd/27, skipping\n\nwhich seems to map to a missing /proc/self/fd/27\n\nhttp://code.metager.de/source/xref/lib/virt/src/qemu/qemu_driver.c#1341", 
            "date_created": "2014-09-23 01:51:26.312726+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "The next step is to have someone familiar with libvirt to triage this. IMHO stacktraces should always be treated as bugs.", 
            "date_created": "2014-09-23 19:20:14.491348+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "message:\"operation failed: cannot read cputime for domain\" AND tags:\"libvirt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwib3BlcmF0aW9uIGZhaWxlZDogY2Fubm90IHJlYWQgY3B1dGltZSBmb3IgZG9tYWluXCIgQU5EIHRhZ3M6XCJsaWJ2aXJ0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTE1MDI2NTgyNjEsIm1vZGUiOiJ0cmVuZCIsImFuYWx5emVfZmllbGQiOiJidWlsZF9zdGF0dXMifQ==\n\n20 hits in 7 days, 80% failure runs.\n\nThis is maybe a race on delete, i.e. can't read the cputime b/c the domain xml (instance) is already deleted?  What is it stacktracing on in the code?", 
            "date_created": "2014-09-23 20:07:45.860597+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Oh it's a race on resize, so the old domain xml is probably deleted and it's trying to read that and kaboom.  Should have danpb take a look.  Maybe this is trying to do some kind of recovery on failure and that's why it's not always resulting in a failed tempest run.", 
            "date_created": "2014-09-23 20:08:54.097156+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Yeah, it looks like some sort of race condition.  In qemu_driver.c in the libvirt source we have:\n\n    if (!virDomainObjIsActive(vm)) {\n        info->cpuTime = 0;\n    } else {\n        if (qemudGetProcessInfo(&(info->cpuTime), NULL, NULL, vm->pid, 0) < 0) {\n            virReportError(VIR_ERR_OPERATION_FAILED, \"%s\",\n                           _(\"cannot read cputime for domain\"));\n            goto cleanup;\n        }\n    }\n\nWhich calls qemudGetProcessInfo.  Now here's the interesting part: essentially, qemudGetProcessInfo tries to open /proc/{pid}/stat or /proc/{pid}/task/stat.  If it fails to asprintf the pid into the path, the method fails (although I doubt this is what's happening).   Interestingly enough, if we fail to read from the path, we don't error out, we just return all zeros.  The only other place we can error out is if we fail to fscanf properly.  That implies that we're able to open the path for reading, but not able to parse it.\n\nAre there any libvirt warnings before the stack trace?", 
            "date_created": "2014-09-23 22:05:13.416469+00:00", 
            "author": "https://api.launchpad.net/1.0/~sross-7"
        }, 
        {
            "content": "I didn't see any libvirt warnings in the nova logs prior to the stacktrace. but here are the libvirt logs as well:\n\nhttp://logs.openstack.org/71/123071/1/gate/gate-tempest-dsvm-postgres-full/7369ae8/logs/libvirtd.txt.gz", 
            "date_created": "2014-09-27 21:24:42.597943+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "just saw this hit again: http://logs.openstack.org/00/122100/10/gate/gate-tempest-dsvm-neutron-full/0e71cd2/logs/screen-n-cpu.txt.gz?level=TRACE", 
            "date_created": "2014-10-09 00:32:37.781574+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "2014-10-08 16:23:21.083+0000: 12577: error : qemuMonitorIO:656 : internal error: End of file from monitor\n2014-10-08 16:23:28.465+0000: 12579: warning : qemuGetProcessInfo:1306 : cannot parse process status data\n2014-10-08 16:23:28.465+0000: 12579: error : qemuDomainGetInfo:2509 : operation failed: cannot read cputime for domain\n\nhttp://logs.openstack.org/00/122100/10/gate/gate-tempest-dsvm-neutron-full/0e71cd2/logs/libvirtd.txt.gz", 
            "date_created": "2014-10-30 20:46:40.417759+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "That sequence of log messages show that the QEMU process is exiting while we are reading the CPU time. THat's not neccesarily bug - it all depends on whether we are *expecting* the QEMU process to be exiting or not.  In general any libvirt API call on a domain can fail in this way if the guest shuts down while it it being performed.", 
            "date_created": "2014-10-31 15:58:53.035585+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "message:\"migrate_disk_and_power_off\" AND message:\"libvirtError: operation failed: cannot read cputime for domain\" AND tags:\"screen-n-cpu.txt\" AND module:\"nova.compute.manager\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwibWlncmF0ZV9kaXNrX2FuZF9wb3dlcl9vZmZcIiBBTkQgbWVzc2FnZTpcImxpYnZpcnRFcnJvcjogb3BlcmF0aW9uIGZhaWxlZDogY2Fubm90IHJlYWQgY3B1dGltZSBmb3IgZG9tYWluXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1jcHUudHh0XCIgQU5EIG1vZHVsZTpcIm5vdmEuY29tcHV0ZS5tYW5hZ2VyXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTY5Mzg3MzI3NDJ9\n\n8 hits in 7 days, check queue only but on multiple changes, all failures.", 
            "date_created": "2014-11-25 18:06:17.024276+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Created a bug report for libvirt: https://bugzilla.redhat.com/show_bug.cgi?id=1169055\nAs said before, it seems to be a race condition between fopen() and fscanf() in libvirt. Proposed a patch as well.", 
            "date_created": "2014-11-29 16:36:01.507461+00:00", 
            "author": "https://api.launchpad.net/1.0/~ecosta"
        }, 
        {
            "content": "Patch applied, libvirt bug closed. Now we just need to wait for the next libvirt release...\n\nhttp://libvirt.org/git/?p=libvirt.git;a=commit;h=ff018e686a8a412255bc34d3dc558a1bcf74fac5", 
            "date_created": "2014-12-01 23:36:31.716310+00:00", 
            "author": "https://api.launchpad.net/1.0/~ecosta"
        }, 
        {
            "content": "Is there anything we can do in the nova code to detect this and try to workaround or ignore it?", 
            "date_created": "2015-01-07 20:31:36.448078+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/151953", 
            "date_created": "2015-02-01 22:29:12.936111+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/151953\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=32c10850ae733c524e403592771b02491a96e67d\nSubmitter: Jenkins\nBranch:    master\n\ncommit 32c10850ae733c524e403592771b02491a96e67d\nAuthor: Eduardo Costa <email address hidden>\nDate:   Sun Feb 1 19:49:31 2015 -0200\n\n    Workaround for race condition in libvirt\n    \n    Method \"info\" is affected by a race condition in libvirt.\n    Methods \"vcpus\" and \"memoryStats\" are also affected, but their\n    errors are handled gracefully by present code.\n    \n    This patch detects the race and then performs a retry on error.\n    \n    New code should avoid calling libvirt.virDomain.info directly.\n    Instead, method Host.get_domain_info() should be called.\n    \n    A full fix for this bug would involve upgrading libvirt to\n    version >=1.2.11.\n    \n    Change-Id: I4e3baceabdbc843635d07652abf80d39d58d9e06\n    Partial-Bug: #1372670\n", 
            "date_created": "2015-03-30 13:12:49.620581+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2015-10-12 21:15:55.893514+00:00"
}
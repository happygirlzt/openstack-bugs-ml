{
    "status": "Fix Released", 
    "last_updated": "2017-08-28 09:55:14.729958+00:00", 
    "description": "Description\n===========\n\n[This was initially reported by a Red Hat OSP customer.]\n\nThe I/O latency of a Cinder volume after live migration of an instance\nto which it's attached increases significantly. This stays increased\ntill the VM is stopped and started again. [VM is booted with Cinder\nvolume.\n\nThis is not the case when using a disk from a Nova store backend [\nwithout Cinder volume] -- or at least the difference isn't so\nsignificantly high after a live migration.\n\nThe storage backend is Ceph 2.0.\n\n\nHow reproducible: Consistently\n\n\nSteps to Reproduce\n==================\n\n(0) Both the Nova instances and Cinder volumes are located on Ceph\n\n(1) Create a Nova instance with a Cinder volume attached to it\n\n(2) Live migrate it to a target Compute node\n\n(3) Run `ioping` (`ioping -c 10 .`) on the Cinder volume.\n    Alternatively, run other I/O benchmarks like using `fio` with\n    'direct=1' (which uses non-bufferred I/O) as a good sanity check to\n    get a second opinion regarding latency.\n\n\nActual result\n=============\n\nBefore live migration: `ioping` output on the Cinder volume attached to a Nova\ninstance:\n\n    [guest]$ sudo ioping -c 10 .\n    4 KiB <<< . (xfs /dev/sda1): request=1 time=98.0 us (warmup)\n    4 KiB <<< . (xfs /dev/sda1): request=2 time=135.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=3 time=155.5 us\n    4 KiB <<< . (xfs /dev/sda1): request=4 time=161.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=5 time=148.4 us\n    4 KiB <<< . (xfs /dev/sda1): request=6 time=354.3 us\n    4 KiB <<< . (xfs /dev/sda1): request=7 time=138.0 us (fast)\n    4 KiB <<< . (xfs /dev/sda1): request=8 time=150.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=9 time=149.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=10 time=138.6 us (fast)\n    \n    --- . (xfs /dev/sda1) ioping statistics ---\n    9 requests completed in 1.53 ms, 36 KiB read, 5.87 k iops, 22.9 MiB/s\n    generated 10 requests in 9.00 s, 40 KiB, 1 iops, 4.44 KiB/s\n    min/avg/max/mdev = 135.6 us / 170.3 us / 354.3 us / 65.6 us\n\n\nAfter live migration, `ioping` output on the Cinder \n\n    [guest]$ sudo ioping -c 10 .\n    4 KiB <<< . (xfs /dev/sda1): request=1 time=1.03 ms (warmup)\n    4 KiB <<< . (xfs /dev/sda1): request=2 time=948.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=3 time=955.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=4 time=920.5 us\n    4 KiB <<< . (xfs /dev/sda1): request=5 time=1.03 ms\n    4 KiB <<< . (xfs /dev/sda1): request=6 time=838.2 us\n    4 KiB <<< . (xfs /dev/sda1): request=7 time=1.13 ms (slow)\n    4 KiB <<< . (xfs /dev/sda1): request=8 time=868.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=9 time=985.2 us\n    4 KiB <<< . (xfs /dev/sda1): request=10 time=936.6 us\n    \n    --- . (xfs /dev/sda1) ioping statistics ---\n    9 requests completed in 8.61 ms, 36 KiB read, 1.04 k iops, 4.08 MiB/s\n    generated 10 requests in 9.00 s, 40 KiB, 1 iops, 4.44 KiB/s\n    min/avg/max/mdev = 838.2 us / 956.9 us / 1.13 ms / 81.0 us\n\nThis goes back to an average of 200us again after shutting down and\nstarting up the instance. \n\n\nExpected result\n===============\n\nNo I/O latency experienced on Cinder volumes.", 
    "tags": [
        "libvirt", 
        "live-migration", 
        "volumes"
    ], 
    "importance": "Medium", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1706083", 
    "owner": "https://api.launchpad.net/1.0/~kashyapc", 
    "id": 1706083, 
    "index": 4874, 
    "openned": "2017-07-24 14:04:06.236560+00:00", 
    "created": "2017-07-24 14:04:06.236560+00:00", 
    "title": "Post-migration, Cinder volumes lose disk cache value, resulting in I/O latency", 
    "comments": [
        {
            "content": "Description\n===========\n\n[This was initially reported by a Red Hat OSP customer.]\n\nThe I/O latency of a Cinder volume after live migration of an instance\nto which it's attached increases significantly. This stays increased\ntill the VM is stopped and started again. [VM is booted with Cinder\nvolume.\n\nThis is not the case when using a disk from a Nova store backend [\nwithout Cinder volume] -- or at least the difference isn't so\nsignificantly high after a live migration.\n\nThe storage backend is Ceph 2.0.\n\n\nHow reproducible: Consistently\n\n\nSteps to Reproduce\n==================\n\n(0) Both the Nova instances and Cinder volumes are located on Ceph\n\n(1) Create a Nova instance with a Cinder volume attached to it\n\n(2) Live migrate it to a target Compute node\n\n(3) Run `ioping` (`ioping -c 10 .`) on the Cinder volume.\n    Alternatively, run other I/O benchmarks like using `fio` with\n    'direct=1' (which uses non-bufferred I/O) as a good sanity check to\n    get a second opinion regarding latency.\n\n\nActual result\n=============\n\nBefore live migration: `ioping` output on the Cinder volume attached to a Nova\ninstance:\n\n    [guest]$ sudo ioping -c 10 .\n    4 KiB <<< . (xfs /dev/sda1): request=1 time=98.0 us (warmup)\n    4 KiB <<< . (xfs /dev/sda1): request=2 time=135.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=3 time=155.5 us\n    4 KiB <<< . (xfs /dev/sda1): request=4 time=161.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=5 time=148.4 us\n    4 KiB <<< . (xfs /dev/sda1): request=6 time=354.3 us\n    4 KiB <<< . (xfs /dev/sda1): request=7 time=138.0 us (fast)\n    4 KiB <<< . (xfs /dev/sda1): request=8 time=150.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=9 time=149.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=10 time=138.6 us (fast)\n    \n    --- . (xfs /dev/sda1) ioping statistics ---\n    9 requests completed in 1.53 ms, 36 KiB read, 5.87 k iops, 22.9 MiB/s\n    generated 10 requests in 9.00 s, 40 KiB, 1 iops, 4.44 KiB/s\n    min/avg/max/mdev = 135.6 us / 170.3 us / 354.3 us / 65.6 us\n\n\nAfter live migration, `ioping` output on the Cinder \n\n    [guest]$ sudo ioping -c 10 .\n    4 KiB <<< . (xfs /dev/sda1): request=1 time=1.03 ms (warmup)\n    4 KiB <<< . (xfs /dev/sda1): request=2 time=948.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=3 time=955.7 us\n    4 KiB <<< . (xfs /dev/sda1): request=4 time=920.5 us\n    4 KiB <<< . (xfs /dev/sda1): request=5 time=1.03 ms\n    4 KiB <<< . (xfs /dev/sda1): request=6 time=838.2 us\n    4 KiB <<< . (xfs /dev/sda1): request=7 time=1.13 ms (slow)\n    4 KiB <<< . (xfs /dev/sda1): request=8 time=868.6 us\n    4 KiB <<< . (xfs /dev/sda1): request=9 time=985.2 us\n    4 KiB <<< . (xfs /dev/sda1): request=10 time=936.6 us\n    \n    --- . (xfs /dev/sda1) ioping statistics ---\n    9 requests completed in 8.61 ms, 36 KiB read, 1.04 k iops, 4.08 MiB/s\n    generated 10 requests in 9.00 s, 40 KiB, 1 iops, 4.44 KiB/s\n    min/avg/max/mdev = 838.2 us / 956.9 us / 1.13 ms / 81.0 us\n\nThis goes back to an average of 200us again after shutting down and\nstarting up the instance. \n\n\nExpected result\n===============\n\nNo I/O latency experienced on Cinder volumes.", 
            "date_created": "2017-07-24 14:04:06.236560+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "Here's the change to fix this issue:\n\n    https://review.openstack.org/#/c/485752/ -- libvirt: Post-migration, \n    set cache value for Cinder volume(s) ", 
            "date_created": "2017-07-24 15:30:56.697926+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/485752\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=14c38ac0f253036da79f9d07aedf7dfd5778fde8\nSubmitter: Jenkins\nBranch:    master\n\ncommit 14c38ac0f253036da79f9d07aedf7dfd5778fde8\nAuthor: Kashyap Chamarthy <email address hidden>\nDate:   Thu Jul 20 19:01:23 2017 +0200\n\n    libvirt: Post-migration, set cache value for Cinder volume(s)\n    \n    This was noticed in a downstream bug when a Nova instance with Cinder\n    volume (in this case, both the Nova instance storage _and_ Cinder volume\n    are located on Ceph) is migrated to a target Compute node, the disk\n    cache value for the Cinder volume gets changed.  I.e. the QEMU\n    command-line for the Cinder volume stored on Ceph turns into the\n    following:\n    \n    Pre-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=writeback\n    \n    Post-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=none\n    \n    Furthermore, Jason Dillaman from Ceph confirms RBD cache being enabled\n    pre-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"true\"\n        }\n    \n    And disabled, post-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"false\"\n        }\n    \n    This change in cache value post-migration causes I/O latency on the\n    Cinder volume.\n    \n    From a chat with Daniel Berrang\u00e9 on IRC: Prior to live migration, Nova\n    rewrites all the <disk> elements, and passes this updated guest XML\n    across to target libvirt.  And it is never calling _set_cache_mode()\n    when doing this.  So `nova.conf`'s `writeback` setting is getting lost,\n    leaving the default `cache=none` setting.  And this mistake (of leaving\n    the default cache value to 'none') will of course be correct when you\n    reboot the guest on the target later.\n    \n    So:\n    \n      - Call _set_cache_mode() in _get_volume_config() method -- because it\n        is a callback function to _update_volume_xml() in\n        nova/virt/libvirt/migration.py.\n    \n      - And remove duplicate calls to _set_cache_mode() in\n        _get_guest_storage_config() and attach_volume().\n    \n      - Fix broken unit tests; adjust test_get_volume_config() to reflect\n        the disk cache mode.\n    \n    Thanks: Jason Dillaman of Ceph for observing the change in cache modes\n            in a downstream bug analysis, Daniel Berrang\u00e9 for help in\n            analysis from a Nova libvirt driver POV, and Stefan Hajnoczi\n            from QEMU for help on I/O latency instrumentation with `perf`.\n    \n    Closes-bug: 1706083\n    Change-Id: I4184382b49dd2193d6a21bfe02ea973d02d8b09f\n", 
            "date_created": "2017-07-28 16:15:16.079879+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/newton\nReview: https://review.openstack.org/488959", 
            "date_created": "2017-07-29 15:18:31.874028+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/ocata\nReview: https://review.openstack.org/489198", 
            "date_created": "2017-07-31 12:18:37.273465+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 16.0.0.0rc1 release candidate.", 
            "date_created": "2017-08-11 12:27:19.119572+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/489198\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=306b3f1aac4fc30ee14a7f5f66474d68860768fa\nSubmitter: Jenkins\nBranch:    stable/ocata\n\ncommit 306b3f1aac4fc30ee14a7f5f66474d68860768fa\nAuthor: Kashyap Chamarthy <email address hidden>\nDate:   Thu Jul 20 19:01:23 2017 +0200\n\n    libvirt: Post-migration, set cache value for Cinder volume(s)\n    \n    This was noticed in a downstream bug when a Nova instance with Cinder\n    volume (in this case, both the Nova instance storage _and_ Cinder volume\n    are located on Ceph) is migrated to a target Compute node, the disk\n    cache value for the Cinder volume gets changed.  I.e. the QEMU\n    command-line for the Cinder volume stored on Ceph turns into the\n    following:\n    \n    Pre-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=writeback\n    \n    Post-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=none\n    \n    Furthermore, Jason Dillaman from Ceph confirms RBD cache being enabled\n    pre-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"true\"\n        }\n    \n    And disabled, post-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"false\"\n        }\n    \n    This change in cache value post-migration causes I/O latency on the\n    Cinder volume.\n    \n    From a chat with Daniel Berrang\u00e9 on IRC: Prior to live migration, Nova\n    rewrites all the <disk> elements, and passes this updated guest XML\n    across to target libvirt.  And it is never calling _set_cache_mode()\n    when doing this.  So `nova.conf`'s `writeback` setting is getting lost,\n    leaving the default `cache=none` setting.  And this mistake (of leaving\n    the default cache value to 'none') will of course be correct when you\n    reboot the guest on the target later.\n    \n    So:\n    \n      - Call _set_cache_mode() in _get_volume_config() method -- because it\n        is a callback function to _update_volume_xml() in\n        nova/virt/libvirt/migration.py.\n    \n      - And remove duplicate calls to _set_cache_mode() in\n        _get_guest_storage_config() and attach_volume().\n    \n      - Fix broken unit tests; adjust test_get_volume_config() to reflect\n        the disk cache mode.\n    \n    Thanks: Jason Dillaman of Ceph for observing the change in cache modes\n            in a downstream bug analysis, Daniel Berrang\u00e9 for help in\n            analysis from a Nova libvirt driver POV, and Stefan Hajnoczi\n            from QEMU for help on I/O latency instrumentation with `perf`.\n    \n    Conflicts [stable/ocata]:\n     - libvirt/driver.py: The _get_scsi_controller() method from Git master\n       isn't in Ocata, so adjust the _get_guest_storage_config() method\n       accordingly.\n     - Fix unit test conflicts in the method\n       test_attach_volume_with_vir_domain_affect_live_flag().\n    \n    Closes-bug: 1706083\n    Change-Id: I4184382b49dd2193d6a21bfe02ea973d02d8b09f\n    (cherry picked from commit 14c38ac0f253036da79f9d07aedf7dfd5778fde8)\n", 
            "date_created": "2017-08-15 16:04:34.349869+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/488959\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=1797d73efc0601a0a664d32a127669e93bce3d45\nSubmitter: Jenkins\nBranch:    stable/newton\n\ncommit 1797d73efc0601a0a664d32a127669e93bce3d45\nAuthor: Kashyap Chamarthy <email address hidden>\nDate:   Thu Jul 20 19:01:23 2017 +0200\n\n    libvirt: Post-migration, set cache value for Cinder volume(s)\n    \n    This was noticed in a downstream bug when a Nova instance with Cinder\n    volume (in this case, both the Nova instance storage _and_ Cinder volume\n    are located on Ceph) is migrated to a target Compute node, the disk\n    cache value for the Cinder volume gets changed.  I.e. the QEMU\n    command-line for the Cinder volume stored on Ceph turns into the\n    following:\n    \n    Pre-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=writeback\n    \n    Post-migration, QEMU command-line for the Nova instance:\n    \n        [...] -drive file=rbd:volumes/volume-[...],cache=none\n    \n    Furthermore, Jason Dillaman from Ceph confirms RBD cache being enabled\n    pre-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"true\"\n        }\n    \n    And disabled, post-migration:\n    \n        $ ceph --admin-daemon /var/run/qemu/ceph-client.openstack.[...] \\\n            config get rbd_cache\n        {\n            \"rbd_cache\": \"false\"\n        }\n    \n    This change in cache value post-migration causes I/O latency on the\n    Cinder volume.\n    \n    From a chat with Daniel Berrang\u00e9 on IRC: Prior to live migration, Nova\n    rewrites all the <disk> elements, and passes this updated guest XML\n    across to target libvirt.  And it is never calling _set_cache_mode()\n    when doing this.  So `nova.conf`'s `writeback` setting is getting lost,\n    leaving the default `cache=none` setting.  And this mistake (of leaving\n    the default cache value to 'none') will of course be correct when you\n    reboot the guest on the target later.\n    \n    So:\n    \n      - Call _set_cache_mode() in _get_volume_config() method -- because it\n        is a callback function to _update_volume_xml() in\n        nova/virt/libvirt/migration.py.\n    \n      - And remove duplicate calls to _set_cache_mode() in\n        _get_guest_storage_config() and attach_volume().\n    \n      - Fix broken unit tests; adjust test_get_volume_config() to reflect\n        the disk cache mode.\n    \n    Thanks: Jason Dillaman of Ceph for observing the change in cache modes\n            in a downstream bug analysis, Daniel Berrang\u00e9 for help in\n            analysis from a Nova libvirt driver POV, and Stefan Hajnoczi\n            from QEMU for help on I/O latency instrumentation with `perf`.\n    \n    Conflicts [stable/newton]:\n     - libvirt/driver.py: The _get_scsi_controller() method from Git master\n       isn't in Newton, so adjust the _get_guest_storage_config() method\n       accordingly.\n     - Fix unit test conflicts in the method\n       test_attach_volume_with_vir_domain_affect_live_flag().\n    \n    Closes-bug: 1706083\n    Change-Id: I4184382b49dd2193d6a21bfe02ea973d02d8b09f\n    (cherry picked from commit 14c38ac0f253036da79f9d07aedf7dfd5778fde8)\n", 
            "date_created": "2017-08-21 13:54:23.142832+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 15.0.7 release.", 
            "date_created": "2017-08-22 11:38:50.918607+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 14.0.8 release.", 
            "date_created": "2017-08-28 09:55:12.947436+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2017-07-28 16:15:13.839117+00:00"
}
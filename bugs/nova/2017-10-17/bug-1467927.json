{
    "status": "Fix Released", 
    "last_updated": "2016-01-05 21:45:38.166871+00:00", 
    "description": "Using a CPU policy of dedicated ('hw:cpu_policy=dedicated') results in vCPUs being pinned to pCPUs, per the original blueprint:\n\n    http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-cpu-pinning.html\n\nWhen scheduling instance with this extra spec there appears to be an implicit use of the 'prefer' threads policy, i.e. where possible vCPUs are pinned to thread siblings first. This is \"implicit\" because the threads policy aspect of this spec has not yet been implemented.\n\nHowever, this implicit 'prefer' policy breaks when a VM with an odd number of vCPUs is booted. This has been seen on a Hyper-Threading-enabled host where \"sibling sets\" are two long, but it would presumably happen on any host where the number of siblings (or any number between this value and one) is not an factor of the number of vCPUs (i.e. vCPUs % n != 0, for siblings <= n > 0).\n\nIt is reasonable to assume that a three vCPU VM, for example, should try best effort and use siblings for at the first two vCPUs of the VM (assuming you're on a host system with HyperThreading and sibling sets are of length two). This would give us a true best effort implementation.\n\n---\n\n# Testing Configuration\n\nTesting was conducted on a single-node, Fedora 21-based (3.17.8-300.fc21.x86_64) OpenStack instance (built with devstack). The system is a dual-socket, 10 core, HT-enabled system (2 sockets * 10 cores * 2 threads = 40 \"pCPUs\". 0-9,20-29 = node0, 10-19,30-39 = node1). Two flavors were used:\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 3 demo.odd\n    nova flavor-key demo.odd set hw:cpu_policy=dedicated\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 4 demo.even\n    nova flavor-key demo.even set hw:cpu_policy=dedicated\n\n# Results\n\nCorrect case (\"even\" number of vCPUs)\n=====================================\n\nThe output from 'virsh dumpxml [ID]' for the four vCPU VM is given below. Similar results can be seen for varying \"even\" numbers of vCPUs (2, 4, 10 tested):\n\n    <cputune>\n        <shares>4096</shares>\n        <vcpupin vcpu='0' cpuset='3'/>\n        <vcpupin vcpu='1' cpuset='23'/>\n        <vcpupin vcpu='2' cpuset='26'/>\n        <vcpupin vcpu='3' cpuset='6'/>\n        <emulatorpin cpuset='3,6,23,26'/>\n    </cputune>\n\nIncorrect case (\"odd\" number of vCPUs)\n======================================\n\nThe output from 'virsh dumpxml [ID]' for the three vCPU VM is given below. Similar results can be seen for varying \"odd\" numbers of vCPUs (3, 5 tested):\n\n    <cputune>\n        <shares>3072</shares>\n        <vcpupin vcpu='0' cpuset='1'/>\n        <vcpupin vcpu='1' cpuset='0'/>\n        <vcpupin vcpu='2' cpuset='25'/>\n        <emulatorpin cpuset='0-1,25'/>\n    </cputune>\n\nThis isn't correct. We would expect something closer to this:\n\n    <cputune>\n        <shares>3072</shares>\n        <vcpupin vcpu='0' cpuset='0'/>\n        <vcpupin vcpu='1' cpuset='20'/>\n        <vcpupin vcpu='2' cpuset='1'/>\n        <emulatorpin cpuset='0-1,20'/>\n    </cputune>", 
    "tags": [
        "libvirt"
    ], 
    "importance": "Undecided", 
    "heat": 24, 
    "link": "https://bugs.launchpad.net/nova/+bug/1467927", 
    "owner": "https://api.launchpad.net/1.0/~stephenfinucane", 
    "id": 1467927, 
    "index": 6887, 
    "openned": "2015-06-23 12:27:07.641162+00:00", 
    "created": "2015-06-23 12:27:07.641162+00:00", 
    "title": "Odd number of vCPUs breaks 'prefer' threads policy", 
    "comments": [
        {
            "content": "Using a CPU policy of dedicated ('hw:cpu_policy=dedicated') results in vCPUs being pinned to pCPUs, per the original blueprint:\n\n    http://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/virt-driver-cpu-pinning.html\n\nWhen scheduling instance with this extra spec there appears to be an implicit use of the 'prefer' threads policy, i.e. where possible vCPUs are pinned to thread siblings first. This is \"implicit\" because the threads policy aspect of this spec has not yet been implemented.\n\nHowever, this implicit 'prefer' policy breaks when a VM with an odd number of vCPUs is booted. This has been seen on a Hyper-Threading-enabled host where \"sibling sets\" are two long, but it would presumably happen on any host where the number of siblings (or any number between this value and one) is not an factor of the number of vCPUs (i.e. vCPUs % n != 0, for siblings <= n > 0).\n\nIt is reasonable to assume that a three vCPU VM, for example, should try best effort and use siblings for at the first two vCPUs of the VM (assuming you're on a host system with HyperThreading and sibling sets are of length two). This would give us a true best effort implementation.\n\n---\n\n# Testing Configuration\n\nTesting was conducted on a single-node, Fedora 21-based (3.17.8-300.fc21.x86_64) OpenStack instance (built with devstack). The system is a dual-socket, 10 core, HT-enabled system (2 sockets * 10 cores * 2 threads = 40 \"pCPUs\". 0-9,20-29 = node0, 10-19,30-39 = node1). Two flavors were used:\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 3 demo.odd\n    nova flavor-key demo.odd set hw:cpu_policy=dedicated\n\n    openstack flavor create --ram 4096 --disk 20 --vcpus 4 demo.even\n    nova flavor-key demo.even set hw:cpu_policy=dedicated\n\n# Results\n\nCorrect case (\"even\" number of vCPUs)\n=====================================\n\nThe output from 'virsh dumpxml [ID]' for the four vCPU VM is given below. Similar results can be seen for varying \"even\" numbers of vCPUs (2, 4, 10 tested):\n\n    <cputune>\n        <shares>4096</shares>\n        <vcpupin vcpu='0' cpuset='3'/>\n        <vcpupin vcpu='1' cpuset='23'/>\n        <vcpupin vcpu='2' cpuset='26'/>\n        <vcpupin vcpu='3' cpuset='6'/>\n        <emulatorpin cpuset='3,6,23,26'/>\n    </cputune>\n\nIncorrect case (\"odd\" number of vCPUs)\n======================================\n\nThe output from 'virsh dumpxml [ID]' for the three vCPU VM is given below. Similar results can be seen for varying \"odd\" numbers of vCPUs (3, 5 tested):\n\n    <cputune>\n        <shares>3072</shares>\n        <vcpupin vcpu='0' cpuset='1'/>\n        <vcpupin vcpu='1' cpuset='0'/>\n        <vcpupin vcpu='2' cpuset='25'/>\n        <emulatorpin cpuset='0-1,25'/>\n    </cputune>\n\nThis isn't correct. We would expect something closer to this:\n\n    <cputune>\n        <shares>3072</shares>\n        <vcpupin vcpu='0' cpuset='0'/>\n        <vcpupin vcpu='1' cpuset='20'/>\n        <vcpupin vcpu='2' cpuset='1'/>\n        <emulatorpin cpuset='0-1,20'/>\n    </cputune>", 
            "date_created": "2015-06-23 12:27:07.641162+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "@Stephen Finucane (sfinucan):\n\nSince you are set as assignee, I switch the status to \"In Progress\".", 
            "date_created": "2015-06-24 15:26:34.909020+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Before considering the placement on the host OS, you need to consider the topology exposed to the guest OS\n\nFor a 3 vCPU guests there are 3 possible topologies\n\n - 1 socket, 1 core, 3 threads\n - 1 socket, 3 cores, 1 thread\n - 3 sockets, 1 core, 1 thread\n\nIn the description it does not mention any of the flavour / image settings for vCPU topology, so I'm assuming the testing was done with the defaults, which means   3 sockets, 1 core, 1 thread.  Given that every vCPU would appear as a socket, Nova should not be attempting to prefer  hyperthread siblings at all, because telling the guest that it has separate sockets, when in fact it is running on thread siblings would result in bad guest schedular decisions. The Nova schedular should only prefer pinning to hyperthread siblings in the host, if the guest topology also has been setup with hyperthreads. ", 
            "date_created": "2015-06-29 12:56:22.130364+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I was unsure how important the CPU topology exposed to the guest was, but you're correct in saying that using a best-effort prefer policy would result in bad scheduler decisions. We still have an 'implicit' separate policy for odd numbers of cores and an implicit 'prefer' policy for even numbers, but seeing as we don't really support thread policies yet then this isn't really a bug.\n\nI will close this bug and keep the above in mind when adding support for the thread policies.", 
            "date_created": "2015-06-30 09:28:27.449311+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "I think Daniel has a good point, that it's invalid to prefer thread siblings and then tell the guest it's on separate sockets.   I would suggest that it's also not valid to use host cores and tell the guest they're separate sockets.\n\nIdeally the topology we specify to the guest should match the topology on the host in order to allow the guest scheduler to make the best possible decisions.\n\nSo I think we should fix the fact that odd/even gives different behaviour, but ideally we should also fix https://bugs.launchpad.net/nova/+bug/1417723", 
            "date_created": "2015-11-09 17:55:01.227558+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/229573\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=46e98cfeb8463c68f912052ace036b490774a89f\nSubmitter: Jenkins\nBranch:    master\n\ncommit 46e98cfeb8463c68f912052ace036b490774a89f\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Wed Sep 30 16:24:28 2015 +0100\n\n    Revert \"Store correct VirtCPUTopology\"\n    \n    This reverts commit 8358936a24cd223046580ddfa3bfb37a943abc91.\n    \n    The bug patch being reverted was trying to fix is not a bug bug at all\n    but is actually by design. The NUMA cell CPU\n    topology was meant to carry information about threads that we want to\n    expose to the single cell based on how it was fitted with regards to\n    threading on the host, so that we can expose this information to the\n    guest OS if possible for optimal performance.\n    \n    The final topology exposed to the guest takes this information into\n    account as well as any request for particular topology passed in by the\n    user and decides on the final solution. There is no reason to store this\n    as it will be different for different hosts.\n    \n    We want to revert this as it is really not something that we want to be\n    doing, and will make it easier to cleanly fix 1501358.\n    \n    Change-Id: Iae06c468c076337b9d6e85d0d0dc85a063b827d1\n    Partial-bug: 1501358\n    Partial-bug: 1467927\n", 
            "date_created": "2015-11-12 15:39:10.241997+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/229574\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=cc500b2c9880ceb62686e9f1f5db8e6d0e7613d1\nSubmitter: Jenkins\nBranch:    master\n\ncommit cc500b2c9880ceb62686e9f1f5db8e6d0e7613d1\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Wed Sep 30 17:21:49 2015 +0100\n\n    hardware: stop using instance cell topology in CPU pinning logic\n    \n    Currently we consider the existing topology of the instance NUMA cell when\n    deciding on the proper way to fit instance CPUs onto host. This is\n    actually wrong after https://review.openstack.org/#/c/198312/. We don't\n    need to consider the requested topology in the CPU fitting any more as\n    the code that decides on the final CPU topology takes all of this into\n    account.\n    \n    We still need to expose the topology so that the spawning process can\n    use that to calculate the final CPU topology for the instance, but we\n    should not consider this information relevant on any of the subsequent\n    CPU pinning fit recalculations.\n    \n    Change-Id: I9a947a984bf8ec6413e9b4c45d61e8989bf903a1\n    Co-Authored-By: Stephen Finucane <email address hidden>\n    Related-bug: 1501358\n    Related-bug: 1467927\n", 
            "date_created": "2016-01-05 16:46:37.137817+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/229575\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d5bed8fad9d51dd0d4ba9cf452c6f5c8c543b264\nSubmitter: Jenkins\nBranch:    master\n\ncommit d5bed8fad9d51dd0d4ba9cf452c6f5c8c543b264\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Wed Sep 30 19:30:59 2015 +0100\n\n    Fix CPU pinning for odd number of CPUs w hyperthreading\n    \n    Previous logic was not handling the cases when the instance cell has an\n    odd number of vcpus that is larger than any sibling set on the host. The\n    logic would (accidentally) either reject to pin such an instance even\n    though there is ample free cores, or would (in case there were enough\n    free sibling sets on the host) spread the instance instead of pack it\n    onto siblings as the default policy suggests.\n    \n    This patch fixes some incorrect assumptions in the code, while also\n    simplifying it. As an added bonus - we still attempt to expose\n    (via the Topology, and this time correctly) the largest possible number\n    of threads that we can expose to the instance.\n    \n    Finally - we add some more comments to clear up the intent behind the\n    current packing logic with pointers how it could be tweaked to achieve\n    different results in the future.\n    \n    Change-Id: I2c0b3b250ffb1a7483299df13b317cdb24f8141d\n    Co-Authored-By: Stephen Finucane <email address hidden>\n    Closes-bug: 1501358\n    Closes-bug: 1467927\n", 
            "date_created": "2016-01-05 21:45:37.229659+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2016-01-05 21:45:34.828825+00:00"
}
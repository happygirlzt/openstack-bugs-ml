{
    "status": "In Progress", 
    "last_updated": "2017-06-28 12:11:11.770048+00:00", 
    "description": "Volume based live migration is failing during tempest testing in the check and experimental pipelines\n\nhttp://logstash.openstack.org/#/dashboard/file/logstash.json?query=message:\\%22Live%20Migration%20failure:%20operation%20failed:%20migration%20job:%20unexpectedly%20failed\\%22%20AND%20tags:\\%22screen-n-cpu.txt\\%22\n\nshows 42 failures since 12/8\n\n2015-12-18 15:26:26.411 ERROR nova.virt.libvirt.driver [req-9a4dda34-987c-42a6-a20b-22729cd202e7 tempest-LiveBlockMigrationTestJSON-1501972407 tempest-LiveBlockMigrationTestJSON-1128863260] [instance: b50e1dc8-9b4c-4edb-be0e-8777cae14f01] Live Migration failure: operation failed: migration job: unexpectedly failed\n2015-12-18 15:26:26.412 DEBUG nova.virt.libvirt.driver [req-9a4dda34-987c-42a6-a20b-22729cd202e7 tempest-LiveBlockMigrationTestJSON-1501972407 tempest-LiveBlockMigrationTestJSON-1128863260] [instance: b50e1dc8-9b4c-4edb-be0e-8777cae14f01] Migration operation thread notification thread_finished /opt/stack/new/nova/nova/virt/libvirt/driver.py:6094\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 457, in fire_timers\n    timer()\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py\", line 58, in __call__\n    cb(*args, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 168, in _do_send\n    waiter.switch(result)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n    result = function(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/utils.py\", line 1161, in context_wrapper\n    return func(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5696, in _live_migration_operation\n    instance=instance)\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 204, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5651, in _live_migration_operation\n    CONF.libvirt.live_migration_bandwidth)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n    result = proxy_call(self._autowrap, f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n    rv = execute(f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n    six.reraise(c, e, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n    rv = meth(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1511, in migrateToURI\n    if ret == -1: raise libvirtError ('virDomainMigrateToURI() failed', dom=self)\nlibvirtError: operation failed: migration job: unexpectedly failed", 
    "tags": [
        "libvirt", 
        "live-migration", 
        "volumes"
    ], 
    "importance": "High", 
    "heat": 28, 
    "link": "https://bugs.launchpad.net/nova/+bug/1524898", 
    "owner": "None", 
    "id": 1524898, 
    "index": 1856, 
    "openned": "2015-12-10 16:13:25.394376+00:00", 
    "created": "2015-12-10 16:13:25.394376+00:00", 
    "title": "Volume based live migration aborted unexpectedly", 
    "comments": [
        {
            "content": "Volume based live migration is failing during tempest testing in the check and experimental pipelines\n\nhttp://logstash.openstack.org/#/dashboard/file/logstash.json?query=message:\\%22Live%20Migration%20failure:%20operation%20failed:%20migration%20job:%20unexpectedly%20failed\\%22%20AND%20tags:\\%22screen-n-cpu.txt\\%22%20AND%20build_name:\\%22gate-tempest-dsvm-neutron-dvr-multinode-full\\%22 shows 42 failures since 12/8", 
            "date_created": "2015-12-10 16:13:25.394376+00:00", 
            "author": "https://api.launchpad.net/1.0/~rmoats"
        }, 
        {
            "content": "http://logs.openstack.org/17/255217/5/check/gate-tempest-dsvm-neutron-dvr-multinode-full/15995d2/logs/subnode-2/screen-n-cpu.txt.gz?#_2015-12-10_08_14_34_437\n\n2015-12-10 08:14:34.437 ERROR nova.virt.libvirt.driver [req-e8766399-ceeb-4b6c-82e9-bb5de1d8eed3 tempest-LiveBlockMigrationTestJSON-1334467432 tempest-LiveBlockMigrationTestJSON-138512217] [instance: f21cf8ce-fd9e-4fa3-9cf6-9fb5da8751ca] Live Migration failure: operation failed: migration job: unexpectedly failed\n2015-12-10 08:14:34.438 DEBUG nova.virt.libvirt.driver [req-e8766399-ceeb-4b6c-82e9-bb5de1d8eed3 tempest-LiveBlockMigrationTestJSON-1334467432 tempest-LiveBlockMigrationTestJSON-138512217] [instance: f21cf8ce-fd9e-4fa3-9cf6-9fb5da8751ca] Migration operation thread notification thread_finished /opt/stack/new/nova/nova/virt/libvirt/driver.py:6130\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 457, in fire_timers\n    timer()\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py\", line 58, in __call__\n    cb(*args, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 168, in _do_send\n    waiter.switch(result)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n    result = function(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/utils.py\", line 1178, in context_wrapper\n    return func(*args, **kwargs)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5732, in _live_migration_operation\n    instance=instance)\n  File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 204, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n  File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 5700, in _live_migration_operation\n    CONF.libvirt.live_migration_bandwidth)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n    result = proxy_call(self._autowrap, f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n    rv = execute(f, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n    six.reraise(c, e, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n    rv = meth(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1586, in migrateToURI2\n    if ret == -1: raise libvirtError ('virDomainMigrateToURI2() failed', dom=self)\nlibvirtError: operation failed: migration job: unexpectedly failed\n\nFrom the libvirtd logs:\n\nhttp://logs.openstack.org/17/255217/5/check/gate-tempest-dsvm-neutron-dvr-multinode-full/15995d2/logs/subnode-2/libvirt/libvirtd.txt.gz#_2015-12-10_08_14_33_802\n\n2015-12-10 08:14:33.802+0000: 12436: debug : qemuMonitorIOProcess:396 : QEMU_MONITOR_IO_PROCESS: mon=0x7f70100095e0 buf={\"return\": {\"status\": \"failed\"}, \"id\": \"libvirt-108\"}\n\nQEMU log for the domain:\n\nhttp://logs.openstack.org/17/255217/5/check/gate-tempest-dsvm-neutron-dvr-multinode-full/15995d2/logs/libvirt/qemu/instance-0000000a.txt.gz\n\n\nFrom logstash it looks like this started on 12/8, and is isolated to the OVH and BlueBox nodes, which might correlate with enabling config drive:\n\nhttps://review.openstack.org/#/c/254858/", 
            "date_created": "2015-12-10 16:16:27.521394+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "http://logstash.openstack.org/#/dashboard/file/logstash.json?query=message:%5C%22Live%20Migration%20failure:%20operation%20failed:%20migration%20job:%20unexpectedly%20failed%5C%22%20AND%20tags:%5C%22screen-n-cpu.txt%5C%22%20AND%20build_name:%5C%22gate-tempest-dsvm-neutron-dvr-multinode-full%5C%22", 
            "date_created": "2015-12-10 16:19:34.453371+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The odd thing here is the sequence of events from QEMU\n\nWe see a normal sequence of 'query-migrate' monitor calls showing ongoing progress until we only have 300 kb left:\n \n2015-12-10 08:14:30.282+0000: 12440: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f70100095e0 msg={\"execute\":\"query-migrate\",\"id\":\"libvirt-106\"}\n2015-12-10 08:14:30.284+0000: 12436: debug : qemuMonitorJSONIOProcessLine:177 : QEMU_MONITOR_RECV_REPLY: mon=0x7f70100095e0 reply={\"return\": {\"expected-downtime\": 138, \"status\": \"active\", \"setup-time\": 1, \"total-time\": 4524, \"ram\": {\"total\": 67514368, \"remaining\": 344064, \"mbps\": 102.161541, \"transferred\": 58021706, \"duplicate\": 3962, \"dirty-pages-rate\": 431, \"skipped\": 0, \"normal-bytes\": 58519552, \"normal\": 14287}}, \"id\": \"libvirt-106\"}\n\nThen we see the STOP event, which shows QEMU is about to switch to the target host\n\n\n2015-12-10 08:14:30.301+0000: 12436: debug : qemuMonitorJSONIOProcessLine:172 : QEMU_MONITOR_RECV_EVENT: mon=0x7f70100095e0 event={\"timestamp\": {\"seconds\": 1449735270, \"microseconds\": 301459}, \"event\": \"STOP\"}\n\nWe can see from the status only 200 KB are left at this point\n\n2015-12-10 08:14:30.335+0000: 12440: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f70100095e0 msg={\"execute\":\"query-migrate\",\"id\":\"libvirt-107\"}\n2015-12-10 08:14:33.749+0000: 12436: debug : qemuMonitorJSONIOProcessLine:177 : QEMU_MONITOR_RECV_REPLY: mon=0x7f70100095e0 reply={\"return\": {\"expected-downtime\": 144, \"status\": \"active\", \"setup-time\": 1, \"total-time\": 7988, \"ram\": {\"total\": 67514368, \"remaining\": 204800, \"mbps\": 97.487385, \"transferred\": 58813786, \"duplicate\": 3962, \"dirty-pages-rate\": 431, \"skipped\": 0, \"normal-bytes\": 58662912, \"normal\": 14322}}, \"id\": \"libvirt-107\"}\n\n\nNow the very strange thing - we see a RESUME event, which shows QEMU has started running the source once more - the switchover to the target host has failed:\n\n2015-12-10 08:14:33.761+0000: 12436: debug : qemuMonitorJSONIOProcessLine:172 : QEMU_MONITOR_RECV_EVENT: mon=0x7f70100095e0 event={\"timestamp\": {\"seconds\": 1449735273, \"microseconds\": 760967}, \"event\": \"RESUME\"}\n\n\nThe migrate status confirms the failure\n\n2015-12-10 08:14:33.801+0000: 12440: debug : qemuMonitorSend:959 : QEMU_MONITOR_SEND_MSG: mon=0x7f70100095e0 msg={\"execute\":\"query-migrate\",\"id\":\"libvirt-108\"}\n2015-12-10 08:14:33.802+0000: 12436: debug : qemuMonitorJSONIOProcessLine:177 : QEMU_MONITOR_RECV_REPLY: mon=0x7f70100095e0 reply={\"return\": {\"status\": \"failed\"}, \"id\": \"libvirt-108\"}\n\n\nLooking at the libvirtd logs I see this QEMU version\n\nDebian-2.0.0+dfsg-2ubuntu1.21\n\n\nLooking in the QEMU code for version 2.0.0, there are only two ways this can happen - one is an I/O failure on the migration data stream (which there is no sign of here) and the other is a failure to stop VM CPUs.\n\nThe only reason we can fail to stop VM CPUs is if  QEMU fails to flush outstanding guest I/O operations.\n\nThere have been some issues related to I/O and migration fixed in QEMU over time since 2.0.0 was released. So my money is probably on this being a QEMU bug. AFAIK, Nova and libvirt are both operating correctly.\n", 
            "date_created": "2015-12-10 16:58:10.228095+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "This fails primarily on OVH nodes which are the slowest ones we have in the community infra system, so that indicates there is a speed component to this.", 
            "date_created": "2015-12-18 17:10:07.999312+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looking at todays failures, which is a much more modern qemu we've got a similar case:\n\nhttp://logs.openstack.org/55/328055/12/check/gate-tempest-dsvm-multinode-live-migration/e4d241f/logs/subnode-2/libvirt/libvirtd.txt.gz#_2016-06-28_12_51_58_804\nhttp://logs.openstack.org/55/328055/12/check/gate-tempest-dsvm-multinode-live-migration/e4d241f/logs/libvirt/qemu/instance-00000003.txt.gz\n\na) 12:51:58.804+0000: receives a query-migrate reply saying all is fine\nb) 12:51:58.908+0000: receives a STOP from the source qemu saying it's nearly finished\nc) 12:52:01.358+0000: ! receives a query-migrate reply saying migration has failed\nd) 12:52:01.377760Z qemu-system-x86_64: load of migration failed: Input/output error \ne) 12:52:01.360+0000: \"event\": \"RESUME\"\nf) 12:52:03.136445Z qemu-system-x86_64: terminating on signal 15 from pid 18359\n\n(d) comes from the destination.\nOK, so migration failed - but we can't really tell who blinked first; the destination gives the I/O error probably because the source migration went away from it's point of view (probably!), either due to a network issue or the source dieing.  Given that we see the migration failed slightly earlier on the source it's probably the source dieing by itself - but unfortunately with no error in teh source qemu log before it quits.\n\nWe need some debugging in the version of qemu that's run - unfortunately the ubuntu qemu packages dont seem to have any of qemu's tracing turned on either.", 
            "date_created": "2016-06-28 15:47:59.280974+00:00", 
            "author": "https://api.launchpad.net/1.0/~dgilbert-h"
        }, 
        {
            "content": "This change is attempting to replace the system provided QEMU with a custom build containing some debug hacks to try and gather more data\n\nhttps://review.openstack.org/#/c/335549/", 
            "date_created": "2016-06-29 16:51:08.554483+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "We now have a job which has failed with the custom QEMU present\n\nhttp://logs.openstack.org/03/335903/1/check/gate-tempest-dsvm-multinode-live-migration/5bae103/\n\nSee the source QEMU log:\n\nhttp://logs.openstack.org/03/335903/1/check/gate-tempest-dsvm-multinode-live-migration/5bae103/logs/subnode-2/libvirt/qemu/instance-00000003.txt.gz\n\nmigration_completion: global_state_store ret=0\nmigration_completion: vm_stop_force_state ret=-5\n\nThis message with ret=-5 is reporting the return value of the 'vm_stop' API call, which in turn is reporting do_vm_stop which in turn is reporting the value from bdrv_flush_all().  The latter reports errno values, and -5 corresponds to EIO.\n\nSo, we're failing the migration switch over because the block driver was unable to flush outstanding I/O for some unknown reason.", 
            "date_created": "2016-06-30 14:44:24.280153+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "The failure from QEMU happens shortly before  14:28:15\n\nIf we look in syslog at this same time we see I/O errors:\n\nhttp://logs.openstack.org/03/335903/1/check/gate-tempest-dsvm-multinode-live-migration/5bae103/logs/subnode-2/syslog.txt.gz#_Jun_30_14_28_12\n\nJun 30 14:28:11 ubuntu-xenial-2-node-ovh-gra1-2121639-75684 kernel:  connection1:0: detected conn error (1020)\nJun 30 14:28:12 ubuntu-xenial-2-node-ovh-gra1-2121639-75684 iscsid[537]: Kernel reported iSCSI connection 1:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\nJun 30 14:28:15 ubuntu-xenial-2-node-ovh-gra1-2121639-75684 kernel: blk_update_request: I/O error, dev sda, sector 0\nJun 30 14:28:15 ubuntu-xenial-2-node-ovh-gra1-2121639-75684 iscsid[537]: connection1:0 is operational after recovery (1 attempts)", 
            "date_created": "2016-06-30 14:50:29.582162+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Yes, the same is true on yesterdays:\n\nhttp://logs.openstack.org/55/328055/12/check/gate-tempest-dsvm-multinode-live-migration/e4d241f/logs/syslog.txt.gz\n\nJun 28 12:51:56 ubuntu-xenial-2-node-ovh-bhs1-2053574 iscsid[507]: Kernel reported iSCSI connection 1:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\n\n(There is a libvirt assert shortly after that but it looks like iscsi falls over first:\nJun 28 12:52:01 ubuntu-xenial-2-node-ovh-bhs1-2053574 libvirtd[1769]: libvirtd: malloc.c:3723: _int_malloc: Assertion `(unsigned long) (size) >= (unsigned long) (nb)' failed.", 
            "date_created": "2016-06-30 14:55:48.657838+00:00", 
            "author": "https://api.launchpad.net/1.0/~dgilbert-h"
        }, 
        {
            "content": "The syslog on the host running the target QEMU also sees an iscsi error a few seconds before the source host got one\n\nhttp://logs.openstack.org/03/335903/1/check/gate-tempest-dsvm-multinode-live-migration/5bae103/logs/syslog.txt.gz\n\nJun 30 14:28:08 ubuntu-xenial-2-node-ovh-gra1-2121639 kernel:  connection1:0: detected conn error (1020)\nJun 30 14:28:09 ubuntu-xenial-2-node-ovh-gra1-2121639 iscsid[525]: Kernel reported iSCSI connection 1:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (3)\n\nInterestingly, this host is the one actually running the tgtd iscsi server and cinder", 
            "date_created": "2016-06-30 15:50:57.852974+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Interestingly we also see these iscsi connection errors on CI runs which don't fail. It looks like they're happening at a slightly different time though, so QEMU is lucky to not be doing I/O and thus doesn't fail", 
            "date_created": "2016-07-01 16:05:13.543037+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "Dan: On both sides?  Of course that's the optimistic view; the pessimistic view\nis that it did the I/O and the error didn't get reported.", 
            "date_created": "2016-07-01 16:13:31.428772+00:00", 
            "author": "https://api.launchpad.net/1.0/~dgilbert-h"
        }, 
        {
            "content": "This could be use to both nodes using the same initiator (/etc/iscsi/initiatorname.iscsi). I can see that the src host is using  iqn.1993-08.org.debian:01:d492516748 from the n-cpu.log [1] but I can't seem to find a way of confirming what the dest is using.\n\n[1] http://logs.openstack.org/03/335903/1/check/gate-tempest-dsvm-multinode-live-migration/5bae103/logs/subnode-2/screen-n-cpu.txt.gz?#_2016-06-30_14_27_53_898", 
            "date_created": "2016-07-08 13:00:28.146102+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Lee points out on IRC (#openstack-nova) that the Ubuntu Xenial machines have a pre-populated 'InitiatorName':\n\n-----\n$ wget https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img\n$ virt-cat -a xenial-server-cloudimg-amd64-disk1.img /etc/iscsi/initiatorname.iscsi\n## DO NOT EDIT OR REMOVE THIS FILE!\n## If you remove this file, the iSCSI daemon will not start.\n## If you change the InitiatorName, existing access control lists\n## may reject this initiator.  The InitiatorName must be unique\n## for each iSCSI initiator.  Do NOT duplicate iSCSI InitiatorNames.\nInitiatorName=iqn.1993-08.org.debian:01:56aff7b418d\n-----\n\nAnd that it ought to be auto-generated to get a unique name, by providing: \"GenerateName=yes\" (Source: https://wiki.debian.org/SAN/iSCSI/open-iscsi)", 
            "date_created": "2016-07-08 13:34:59.506017+00:00", 
            "author": "https://api.launchpad.net/1.0/~kashyapc"
        }, 
        {
            "content": "I've added two more DEBUG statements to capture both the connection_info dict used by connect_volume and each hosts connectors/initiators using danpb's topic below :\n\nhttps://review.openstack.org/#/q/topic:mig-debug", 
            "date_created": "2016-07-08 16:58:02.964059+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Here's an example of a failure where both hosts are reporting iqn.1993-08.org.debian:01:7f4319d7c785 as their initiator name [1][2] :\n\nhttps://review.openstack.org/#/c/335906/\n\n[1] http://logs.openstack.org/06/335906/2/check/gate-tempest-dsvm-multinode-live-migration/1e1cddd/logs/screen-n-cpu.txt.gz#_2016-07-08_17_28_25_645 \n[2] http://logs.openstack.org/06/335906/2/check/gate-tempest-dsvm-multinode-live-migration/1e1cddd/logs/subnode-2/screen-n-cpu.txt.gz#_2016-07-08_17_27_40_061", 
            "date_created": "2016-07-08 17:57:38.021422+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Maybe we should try using in-qemu iscsi to see if we still have the same issue, or at least see if it fails with a better error message. The package is available for xenial which the live migration job is using now:\n\nhttp://packages.ubuntu.com/xenial/qemu-block-extra\n\nThe qemu native iscsi support was added to nova in kilo:\n\nhttps://specs.openstack.org/openstack/nova-specs/specs/kilo/implemented/qemu-built-in-iscsi-initiator.html\n\nOtherwise I'd say let's skip the volume-backed live migration test(s) in the live migration job so we can try and stabilize that job (the tests would still run in the non-voting multinode job).", 
            "date_created": "2016-08-02 14:37:03.516495+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/353002\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=2f68cd3e9f47ca5bb864c26c1b0b123918197ef4\nSubmitter: Jenkins\nBranch:    master\n\ncommit 2f68cd3e9f47ca5bb864c26c1b0b123918197ef4\nAuthor: Timofey Durakov <email address hidden>\nDate:   Tue Aug 9 19:35:23 2016 +0300\n\n    Skipping test_volume_backed_live_migration for live_migration job\n    \n    For non-shared storage live-migration job is unstable due to\n    https://bugs.launchpad.net/nova/+bug/1524898. This patch temporary\n    exclude this test from tests to be run.\n    \n    Related-Bug: #1524898\n    \n    Change-Id: I87559b8e90855837ad7b5f291bd9a8fc646421d3\n", 
            "date_created": "2016-08-11 13:19:49.347671+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "We're testing against the ubuntu cloud archive on xenial nodes now so we have libvirt 2.5.0 and qemu 2.8. We should re-enable the test_volume_backed_live_migration test to see if anything has improved.", 
            "date_created": "2017-04-24 13:59:06.479586+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/459321", 
            "date_created": "2017-04-24 14:11:15.894502+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
{
    "status": "Fix Released", 
    "last_updated": "2015-11-19 21:51:09.015739+00:00", 
    "description": "Example of this here:\n\nhttp://logs.openstack.org/33/97233/1/check/check-grenade-dsvm/f7b8a11/logs/old/screen-n-cpu.txt.gz?level=TRACE#_2014-06-02_14_13_51_125\n\n\u00a0\u00a0\u00a0File \"/opt/stack/old/nova/nova/compute/manager.py\", line 4153, in _detach_volume\n\u00a0\u00a0\u00a0\u00a0\u00a0connection_info = jsonutils.loads(bdm.connection_info)\n\u00a0\u00a0\u00a0File \"/opt/stack/old/nova/nova/openstack/common/jsonutils.py\", line 164, in loads\n\u00a0\u00a0\u00a0\u00a0\u00a0return json.loads(s)\n\u00a0\u00a0\u00a0File \"/usr/lib/python2.7/json/__init__.py\", line 326, in loads\n\u00a0\u00a0\u00a0\u00a0\u00a0return _default_decoder.decode(s)\n\u00a0\u00a0\u00a0File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n\u00a0\u00a0\u00a0\u00a0\u00a0obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n\u00a0TypeError: expected string or buffer\n\nand this was in grenade with stable/icehouse nova commit 7431cb9\n\nThere's nothing unusual about the test which triggers this - simply attaches a volume to an instance, waits for it to show up in the instance and then tries to detach it\n\nlogstash query for this:\n\n\u00a0\u00a0message:\"Exception during message handling\" AND message:\"expected string or buffer\" AND message:\"connection_info = jsonutils.loads(bdm.connection_info)\" AND tags:\"screen-n-cpu.txt\"\n\nbut it seems to be very rare", 
    "tags": [
        "in-stable-juno", 
        "juno-backport-potential", 
        "volumes"
    ], 
    "importance": "High", 
    "heat": 76, 
    "link": "https://bugs.launchpad.net/nova/+bug/1327218", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1327218, 
    "index": 1507, 
    "openned": "2014-06-06 13:37:17.117838+00:00", 
    "created": "2014-06-06 13:37:17.117838+00:00", 
    "title": "Volume detach failure because of invalid bdm.connection_info", 
    "comments": [
        {
            "content": "Example of this here:\n\nhttp://logs.openstack.org/33/97233/1/check/check-grenade-dsvm/f7b8a11/logs/old/screen-n-cpu.txt.gz?level=TRACE#_2014-06-02_14_13_51_125\n\n   File \"/opt/stack/old/nova/nova/compute/manager.py\", line 4153, in _detach_volume\n     connection_info = jsonutils.loads(bdm.connection_info)\n   File \"/opt/stack/old/nova/nova/openstack/common/jsonutils.py\", line 164, in loads\n     return json.loads(s)\n   File \"/usr/lib/python2.7/json/__init__.py\", line 326, in loads\n     return _default_decoder.decode(s)\n   File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n TypeError: expected string or buffer\n\nand this was in grenade with stable/icehouse nova commit 7431cb9\n\nThere's nothing unusual about the test which triggers this - simply attaches a volume to an instance, waits for it to show up in the instance and then tries to detach it\n\nlogstash query for this:\n\n  message:\"Exception during message handling\" AND message:\"expected string or buffer\" AND message:\"connection_info = jsonutils.loads(bdm.connection_info)\" AND filename:\"logs/screen-n-cpu.txt\"\n\nbut it seems to be very rare", 
            "date_created": "2014-06-06 13:37:17.117838+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "Looks in the same ballpark as bug #1302774", 
            "date_created": "2014-06-06 13:47:11.364119+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "Here's a case of the DiskNotFound traceback from bug #1302774 and the traceback from this bug in the same log:\n\nhttp://logs.openstack.org/33/96333/2/check/check-grenade-dsvm/105af93/logs/old/screen-n-cpu.txt.gz?level=TRACE", 
            "date_created": "2014-06-06 13:51:22.490938+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "Looks like that this did not get fixed by the cinder fix https://review.openstack.org/#/c/90353/ looking at logstash. The fix landed on June 10th but we have a hit on June 11th.\n\nLet's monitor this a bit more to be sure, but seems there are more races in there.", 
            "date_created": "2014-06-12 16:13:35.537122+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "No  hits for this one, marking as resolved", 
            "date_created": "2014-08-29 23:00:02.701492+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "This is still an issue and from what I can tell a specific change wasn't merged against this bug, so re-opening since I couldn't find it via LP search before (since it was Fix Committed):\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_17_567\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiX2RldGFjaF92b2x1bWVcIiBBTkQgbWVzc2FnZTpcImNhblxcJ3QgYmUgZGVjb2RlZFwiIEFORCB0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDI2MjYyNTc2ODI4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9", 
            "date_created": "2015-03-13 16:04:01.229683+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "e-r query: https://review.openstack.org/#/c/164235/", 
            "date_created": "2015-03-13 16:08:20.035414+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "In one case, we're attaching the encrypted luks volume to the instance here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_09_061\n\nWe initialize the connection and get the connection_info back here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_11_064\n\nI see an os-attach call here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_15_223\n\nWe start detaching the volume here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_16_902\n\nWe're failing to detach the volume here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_38_17_567\n\nAnd six minutes later we're terminating the bdm for that volume here:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_44_54_876\n\n\n\nAfter failing to detach, I'm also seeing the same volume_id showing up in the logs in other test runs:\n\nVolumesV1SnapshotTestJSON: \n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_42_03_507\n\nTestMinimumBasicScenario:\n\nhttp://logs.openstack.org/93/156693/7/check/check-tempest-dsvm-postgres-full/d3b26e8/logs/screen-n-cpu.txt.gz#_2015-03-12_16_44_40_119", 
            "date_created": "2015-03-13 16:35:20.129909+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This should help with some of the confusion when tracing throug the n-cpu debug logs:\n\nhttps://review.openstack.org/#/c/164259/\n\nThe test that fails is for cryptsetup but the volume type had 'luks' in the name which is confusing.", 
            "date_created": "2015-03-13 16:51:56.822327+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/164330", 
            "date_created": "2015-03-13 20:08:20.209748+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This patch https://review.openstack.org/#/c/163937/ also seems to avoid  NULL of connection_info.", 
            "date_created": "2015-03-16 09:34:37.035495+00:00", 
            "author": "https://api.launchpad.net/1.0/~h-tanizawa"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/164330\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=6fb2ef96d6aaf9ca0ad394fd7621ef1e6003f5a1\nSubmitter: Jenkins\nBranch:    master\n\ncommit 6fb2ef96d6aaf9ca0ad394fd7621ef1e6003f5a1\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Mar 18 12:42:42 2015 -0700\n\n    Save bdm.connection_info before calling volume_api.attach_volume\n    \n    There is a race in attach/detach of a volume where the volume status\n    goes to 'in-use' before the bdm.connection_info data is stored in the\n    database. Since attach is a cast, the caller can see the volume go to\n    'in-use' and immediately try to detach the volume and blow up in the\n    compute manager because bdm.connection_info isn't set stored in the\n    database.\n    \n    This fixes the issue by saving the connection_info immediately before\n    calling volume_api.attach_volume (which sets the volume status to\n    'in-use').\n    \n    Closes-Bug: #1327218\n    \n    Change-Id: Ib95c8f7b66aca0c4ac7b92d140cbeb5e85c2717f\n", 
            "date_created": "2015-03-19 20:13:40.504292+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/166017", 
            "date_created": "2015-03-19 21:46:50.960684+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/166017\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=bbf6348997fee02f9dadd556565f44005e2c7f23\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit bbf6348997fee02f9dadd556565f44005e2c7f23\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Mar 18 12:42:42 2015 -0700\n\n    Save bdm.connection_info before calling volume_api.attach_volume\n    \n    There is a race in attach/detach of a volume where the volume status\n    goes to 'in-use' before the bdm.connection_info data is stored in the\n    database. Since attach is a cast, the caller can see the volume go to\n    'in-use' and immediately try to detach the volume and blow up in the\n    compute manager because bdm.connection_info isn't set stored in the\n    database.\n    \n    This fixes the issue by saving the connection_info immediately before\n    calling volume_api.attach_volume (which sets the volume status to\n    'in-use').\n    \n    Closes-Bug: #1327218\n    \n    Conflicts:\n            nova/tests/unit/compute/test_compute.py\n            nova/tests/unit/virt/test_block_device.py\n            nova/virt/block_device.py\n    \n    NOTE(mriedem): The block_device conflicts are due to using dot\n    notation when accessing object fields and in kilo the context is\n    no longer passed to bdm.save(). The test conflicts are due to moving\n    the test modules in kilo and passing the context on save().\n    \n    Change-Id: Ib95c8f7b66aca0c4ac7b92d140cbeb5e85c2717f\n    (cherry picked from commit 6fb2ef96d6aaf9ca0ad394fd7621ef1e6003f5a1)\n", 
            "date_created": "2015-04-16 14:53:57.995998+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I am not sure whether this bug is fixed. I can reproduce it on master branch.\n\nmy test environment is setup with commit 172d4a00ce609da7ea6d8d97f635e6c9afecb373.\n\n\n2015-08-12 23:23:15.793 INFO nova.compute.manager [req-6e6eb653-cb52-47a2-93e5-d520633e4e10 admin admin] [instance: 0030bb4e-10f3-4300-9ab7-2a2bd609678f] Detach volume fe82d173-1500-4bfb-a541-3046b46c8be0 from mountpoint /dev/vdb\n ----BlockDeviceMapping(boot_index=None,connection_info=None,created_at=2015-08-12T13:38:16Z,delete_on_termination=False,deleted=False,deleted_at=None,destination_type='volume',device_name='/dev/vdb',device_type=None,disk_bus=None,guest_format=None,id=2685,image_id=None,instance=<?>,instance_uuid=0030bb4e-10f3-4300-9ab7-2a2bd609678f,no_device=False,snapshot_id=None,source_type='volume',updated_at=None,volume_id='fe82d173-1500-4bfb-a541-3046b46c8be0',volume_size=None) --- (This is printed by me)\n2015-08-12 23:23:15.808 ERROR oslo_messaging.rpc.dispatcher [req-6e6eb653-cb52-47a2-93e5-d520633e4e10 admin admin] Exception during message handling: <type 'NoneType'> can't be decoded\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher Traceback (most recent call last):\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 142, in _dispatch_and_reply\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     executor_callback))\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 186, in _dispatch\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     executor_callback)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 129, in _do_dispatch\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     result = func(ctxt, **new_args)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/exception.py\", line 89, in wrapped\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     payload)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 119, in __exit__\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/exception.py\", line 72, in wrapped\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 364, in decorated_function\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 119, in __exit__\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 352, in decorated_function\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 4594, in detach_volume\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     self._detach_volume(context, volume_id, instance)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 4577, in _detach_volume\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     self._driver_detach_volume(context, instance, bdm)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/nova/nova/compute/manager.py\", line 4512, in _driver_detach_volume\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     connection_info = jsonutils.loads(bdm.connection_info)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_serialization/jsonutils.py\", line 214, in loads\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     return json.loads(encodeutils.safe_decode(s, encoding), **kwargs)\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/encodeutils.py\", line 33, in safe_decode\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher     raise TypeError(\"%s can't be decoded\" % type(text))\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded\n2015-08-12 23:23:15.808 136462 ERROR oslo_messaging.rpc.dispatcher\n\n\n", 
            "date_created": "2015-08-13 04:07:23.491383+00:00", 
            "author": "https://api.launchpad.net/1.0/~xi-yang"
        }, 
        {
            "content": "The fix went into 2015.1.0 and 2015.1.1 is now in the cloud archive.", 
            "date_created": "2015-09-07 15:43:03.231188+00:00", 
            "author": "https://api.launchpad.net/1.0/~gnuoy"
        }, 
        {
            "content": "Status changed to 'Confirmed' because the bug affects multiple users.", 
            "date_created": "2015-09-18 14:38:09.268753+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "Do you know when/if this will be backported to Juno?\n\nThank you.", 
            "date_created": "2015-09-18 14:39:00.886730+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmihaiescu"
        }, 
        {
            "content": "@lmihaiescu Hi, this patch is already in stable/juno and as such will be included in the next point release of Juno (2014.2.4) but is not yet targeted for SRU into Juno.", 
            "date_created": "2015-09-20 17:28:02.166427+00:00", 
            "author": "https://api.launchpad.net/1.0/~hopem"
        }
    ], 
    "closed": "2015-03-20 07:36:53.165913+00:00"
}
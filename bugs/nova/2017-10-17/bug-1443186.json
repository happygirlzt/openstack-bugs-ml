{
    "status": "Fix Released", 
    "last_updated": "2015-11-19 21:49:26.703575+00:00", 
    "description": "This is a continuation of bug 1293480 (which created bug 1433049).  Those were reported against xen domains with the libvirt driver but we have a recreate with CONF.libvirt.virt_type=kvm, see the attached logs and reference the instance with uuid 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78.\n\nIn this case, we're running a stress test of soft rebooting 30 active instances at once.  Because of a delay in the libvirt lifecycle event handling, they are all shutdown after the reboot operation is complete and the instances go from ACTIVE to SHUTDOWN.\n\nThis was reported to me against Icehouse code but the recreate is against Juno code with patch:\n\nhttps://review.openstack.org/#/c/169782/\n\nFor better logging.\n\nSnippets from the log:\n\n2015-04-10 21:02:38.234 11195 AUDIT nova.compute.manager [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Rebooting instance\n\n2015-04-10 21:03:47.703 11195 DEBUG nova.compute.manager [req-8219e6cf-dce8-44e7-a5c1-bf1879e155b2 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Received event network-vif-unplugged-0b2c7633-a5bc-4150-86b2-c8ba58ffa785 external_instance_event /usr/lib/python2.6/site-packages/nova/compute/manager.py:6285\n\n2015-04-10 21:03:49.299 11195 INFO nova.virt.libvirt.driver [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance shutdown successfully.\n\n2015-04-10 21:03:53.251 11195 DEBUG nova.compute.manager [req-521a6bdb-172f-4c0c-9bef-855087d7dff0 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Received event network-vif-plugged-0b2c7633-a5bc-4150-86b2-c8ba58ffa785 external_instance_event /usr/lib/python2.6/site-packages/nova/compute/manager.py:6285\n\n2015-04-10 21:03:53.259 11195 INFO nova.virt.libvirt.driver [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance running successfully.\n\n2015-04-10 21:03:53.261 11195 INFO nova.virt.libvirt.driver [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance soft rebooted successfully.\n\n**\nAt this point we have successfully soft rebooted the instance\n**\n\nnow we get a lifecycle event from libvirt that the instance is stopped, since we're no longer running a task we assume the hypervisor is correct and we call the stop API\n\n2015-04-10 21:04:01.133 11195 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1428699829.13, 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78 => Stopped> emit_event /usr/lib/python2.6/site-packages/nova/virt/driver.py:1298\n2015-04-10 21:04:01.134 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] VM Stopped (Lifecycle Event)\n2015-04-10 21:04:01.245 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Synchronizing instance power state after lifecycle event \"Stopped\"; current vm_state: active, current task_state: None, current DB power_state: 1, VM power_state: 4\n2015-04-10 21:04:01.334 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] During _sync_instance_power_state the DB power_state (1) does not match the vm_power_state from the hypervisor (4). Updating power_state in the DB to match the hypervisor.\n2015-04-10 21:04:01.463 11195 WARNING nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance shutdown by itself. Calling the stop API. Current vm_state: active, current task_state: None, original DB power_state: 1, current VM power_state: 4\n\n**\nnow we get a lifecycle event from libvirt that the instance is started, but since the instance already has a task_state of 'powering-off' because of the previous stop API call from _sync_instance_power_state, we ignore it.\n**\n\n\n2015-04-10 21:04:02.085 11195 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1428699831.45, 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78 => Started> emit_event /usr/lib/python2.6/site-packages/nova/virt/driver.py:1298\n2015-04-10 21:04:02.086 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] VM Started (Lifecycle Event)\n2015-04-10 21:04:02.190 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Synchronizing instance power state after lifecycle event \"Started\"; current vm_state: active, current task_state: powering-off, current DB power_state: 4, VM power_state: 1\n2015-04-10 21:04:02.414 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] During sync_power_state the instance has a pending task (powering-off). Skip.\n2015-04-10 21:04:05.152 11195 DEBUG nova.compute.manager [req-682ce732-8a99-47d7-82c2-49ebdaea5332 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Stopping instance; current vm_state: active, current task_state: powering-off, current DB power_state: 4, current VM power_state: 1 do_stop_instance /usr/lib/python2.6/site-packages/nova/compute/manager.py:2631\n\n**\nfrom here on out, we'd shutdown the instance and that's why it's left in this state.\n**\n\n--\n\nThere was a change made in kilo https://review.openstack.org/#/c/112946/ that delays sending the stopped lifecycle event waiting for a started lifecycle event which will cancel out the stopped event if it falls within a 15 second window.  That was only made for xen domains but as we're seeing this isn't just xen domains, it's also kvm, and therefore is probably an issue for any libvirt domain in nova.\n\nWe made a change to include kvm in the delay logic:\n\n        if CONF.libvirt.virt_type in (\"xen\", \"kvm\"):\n            self._lifecycle_delay = 15\n        else:\n            self._lifecycle_delay = 0\n\nAfter that, only 1 out of 372 instances hit the problem and there was a warning in the logs about a looping call reporting state:\n\n2015-04-12 17:13:31.330 30126 AUDIT nova.compute.manager [req-7d7976d8-6e66-4dd3-958f-f8ef2454dd8c None] [instance: 2189fcd6-8bed-4a1c-916a-1190b40ba568] Rebooting instance\n2015-04-12 17:13:52.281 30126 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources\n2015-04-12 17:14:02.296 30126 AUDIT nova.compute.manager [req-9dfd29ff-9bb0-4e10-a16d-4c8f86744176 None] [instance: 4a659b53-75ad-4410-bd91-d85dafe0175f] Rebooting instance\n2015-04-12 17:14:28.122 30126 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 129026, total allocated virtual ram (MB): 12800\n2015-04-12 17:14:28.124 30126 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 4875\n2015-04-12 17:14:28.125 30126 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 32, total allocated vcpus: 6\n2015-04-12 17:14:28.126 30126 AUDIT nova.compute.resource_tracker [-] PCI stats: []\n2015-04-12 17:14:38.036 30126 AUDIT nova.compute.manager [req-aa25e5f9-5083-46b8-bf29-cc16c2c44c75 None] [instance: 81688c40-3eeb-4d6c-85b3-b2b7dd66eb81] Rebooting instance\n2015-04-12 17:14:59.883 30126 WARNING nova.openstack.common.loopingcall [-] task <bound method DbDriver._report_state of <nova.servicegroup.drivers.db.DbDriver object at 0x3401750>> run outlasted interval by 14.99 sec\n2015-04-12 17:15:02.486 30126 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources\n2015-04-12 17:15:05.019 30126 WARNING nova.compute.manager [req-c17f5069-7540-42be-b7cf-718bf551df66 None] [instance: 81688c40-3eeb-4d6c-85b3-b2b7dd66eb81] Instance shutdown by itself. Calling the stop API. Current vm_state: active, current task_state: None, current DB power_state: 4, current VM power_state: 4\n2015-04-12 17:15:20.602 30126 AUDIT nova.compute.manager [req-34a41e3b-3460-4170-b06e-6e506efc5d69 None] [instance: c9ba059c-e0a2-43e8-a7de-e3710565337f] Rebooting instance\n2015-04-12 17:15:40.152 30126 AUDIT nova.compute.manager [req-2d790be7-cc9b-4256-859d-e0b521459f5d None] [instance: e996f312-4dd3-456a-9747-972ff674e9fe] Rebooting instance\n2015-04-12 17:15:45.047 30126 AUDIT nova.compute.manager [req-b4ab6799-7d15-471b-b76f-cc4e0e35b359 None] [instance: ef46f5c4-daef-4b42-b5c9-1764cfead244] Rebooting instance\n\n\n\nI'll post a change to remove the xen-specific filtering in the lifecycle delay processing and I also already had a change posted to add a config option to disable the lifecycle event handling since it's racy and not tested at scale in the gate so operators should be able to turn it off:\n\nhttps://review.openstack.org/#/c/159275/", 
    "tags": [
        "compute", 
        "in-stable-juno", 
        "in-stable-kilo", 
        "kilo-backport-potential", 
        "libvirt"
    ], 
    "importance": "Medium", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1443186", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1443186, 
    "index": 4217, 
    "openned": "2015-04-12 22:13:14.667468+00:00", 
    "created": "2015-04-12 22:13:14.667468+00:00", 
    "title": "rebooted instances are shutdown by libvirt lifecycle event handling", 
    "comments": [
        {
            "content": "This is a continuation of bug 1293480 (which created bug 1433049).  Those were reported against xen domains with the libvirt driver but we have a recreate with CONF.libvirt.virt_type=kvm, see the attached logs and reference the instance with uuid 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78.\n\nIn this case, we're running a stress test of soft rebooting 30 active instances at once.  Because of a delay in the libvirt lifecycle event handling, they are all shutdown after the reboot operation is complete and the instances go from ACTIVE to SHUTDOWN.\n\nThis was reported to me against Icehouse code but the recreate is against Juno code with patch:\n\nhttps://review.openstack.org/#/c/169782/\n\nFor better logging.\n\nSnippets from the log:\n\n2015-04-10 21:02:38.234 11195 AUDIT nova.compute.manager [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Rebooting instance\n\n2015-04-10 21:03:47.703 11195 DEBUG nova.compute.manager [req-8219e6cf-dce8-44e7-a5c1-bf1879e155b2 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Received event network-vif-unplugged-0b2c7633-a5bc-4150-86b2-c8ba58ffa785 external_instance_event /usr/lib/python2.6/site-packages/nova/compute/manager.py:6285\n\n2015-04-10 21:03:49.299 11195 INFO nova.virt.libvirt.driver [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance shutdown successfully.\n\n2015-04-10 21:03:53.251 11195 DEBUG nova.compute.manager [req-521a6bdb-172f-4c0c-9bef-855087d7dff0 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Received event network-vif-plugged-0b2c7633-a5bc-4150-86b2-c8ba58ffa785 external_instance_event /usr/lib/python2.6/site-packages/nova/compute/manager.py:6285\n\n2015-04-10 21:03:53.259 11195 INFO nova.virt.libvirt.driver [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance running successfully.\n\n2015-04-10 21:03:53.261 11195 INFO nova.virt.libvirt.driver [req-b24d4f8d-4a10-44c8-81d7-f79f27e3a3e7 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance soft rebooted successfully.\n\n**\nAt this point we have successfully soft rebooted the instance\n**\n\nnow we get a lifecycle event from libvirt that the instance is stopped, since we're no longer running a task we assume the hypervisor is correct and we call the stop API\n\n2015-04-10 21:04:01.133 11195 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1428699829.13, 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78 => Stopped> emit_event /usr/lib/python2.6/site-packages/nova/virt/driver.py:1298\n2015-04-10 21:04:01.134 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] VM Stopped (Lifecycle Event)\n2015-04-10 21:04:01.245 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Synchronizing instance power state after lifecycle event \"Stopped\"; current vm_state: active, current task_state: None, current DB power_state: 1, VM power_state: 4\n2015-04-10 21:04:01.334 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] During _sync_instance_power_state the DB power_state (1) does not match the vm_power_state from the hypervisor (4). Updating power_state in the DB to match the hypervisor.\n2015-04-10 21:04:01.463 11195 WARNING nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Instance shutdown by itself. Calling the stop API. Current vm_state: active, current task_state: None, original DB power_state: 1, current VM power_state: 4\n\n**\nnow we get a lifecycle event from libvirt that the instance is started, but since the instance already has a task_state of 'powering-off' because of the previous stop API call from _sync_instance_power_state, we ignore it.\n**\n\n\n2015-04-10 21:04:02.085 11195 DEBUG nova.virt.driver [-] Emitting event <LifecycleEvent: 1428699831.45, 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78 => Started> emit_event /usr/lib/python2.6/site-packages/nova/virt/driver.py:1298\n2015-04-10 21:04:02.086 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] VM Started (Lifecycle Event)\n2015-04-10 21:04:02.190 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Synchronizing instance power state after lifecycle event \"Started\"; current vm_state: active, current task_state: powering-off, current DB power_state: 4, VM power_state: 1\n2015-04-10 21:04:02.414 11195 INFO nova.compute.manager [-] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] During sync_power_state the instance has a pending task (powering-off). Skip.\n2015-04-10 21:04:05.152 11195 DEBUG nova.compute.manager [req-682ce732-8a99-47d7-82c2-49ebdaea5332 None] [instance: 9ad8f6c5-a5dc-4820-9ea5-fa081e74ec78] Stopping instance; current vm_state: active, current task_state: powering-off, current DB power_state: 4, current VM power_state: 1 do_stop_instance /usr/lib/python2.6/site-packages/nova/compute/manager.py:2631\n\n**\nfrom here on out, we'd shutdown the instance and that's why it's left in this state.\n**\n\n--\n\nThere was a change made in kilo https://review.openstack.org/#/c/112946/ that delays sending the stopped lifecycle event waiting for a started lifecycle event which will cancel out the stopped event if it falls within a 15 second window.  That was only made for xen domains but as we're seeing this isn't just xen domains, it's also kvm, and therefore is probably an issue for any libvirt domain in nova.\n\nWe made a change to include kvm in the delay logic:\n\n        if CONF.libvirt.virt_type in (\"xen\", \"kvm\"):\n            self._lifecycle_delay = 15\n        else:\n            self._lifecycle_delay = 0\n\nAfter that, only 1 out of 372 instances hit the problem and there was a warning in the logs about a looping call reporting state:\n\n2015-04-12 17:13:31.330 30126 AUDIT nova.compute.manager [req-7d7976d8-6e66-4dd3-958f-f8ef2454dd8c None] [instance: 2189fcd6-8bed-4a1c-916a-1190b40ba568] Rebooting instance\n2015-04-12 17:13:52.281 30126 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources\n2015-04-12 17:14:02.296 30126 AUDIT nova.compute.manager [req-9dfd29ff-9bb0-4e10-a16d-4c8f86744176 None] [instance: 4a659b53-75ad-4410-bd91-d85dafe0175f] Rebooting instance\n2015-04-12 17:14:28.122 30126 AUDIT nova.compute.resource_tracker [-] Total physical ram (MB): 129026, total allocated virtual ram (MB): 12800\n2015-04-12 17:14:28.124 30126 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 4875\n2015-04-12 17:14:28.125 30126 AUDIT nova.compute.resource_tracker [-] Total usable vcpus: 32, total allocated vcpus: 6\n2015-04-12 17:14:28.126 30126 AUDIT nova.compute.resource_tracker [-] PCI stats: []\n2015-04-12 17:14:38.036 30126 AUDIT nova.compute.manager [req-aa25e5f9-5083-46b8-bf29-cc16c2c44c75 None] [instance: 81688c40-3eeb-4d6c-85b3-b2b7dd66eb81] Rebooting instance\n2015-04-12 17:14:59.883 30126 WARNING nova.openstack.common.loopingcall [-] task <bound method DbDriver._report_state of <nova.servicegroup.drivers.db.DbDriver object at 0x3401750>> run outlasted interval by 14.99 sec\n2015-04-12 17:15:02.486 30126 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources\n2015-04-12 17:15:05.019 30126 WARNING nova.compute.manager [req-c17f5069-7540-42be-b7cf-718bf551df66 None] [instance: 81688c40-3eeb-4d6c-85b3-b2b7dd66eb81] Instance shutdown by itself. Calling the stop API. Current vm_state: active, current task_state: None, current DB power_state: 4, current VM power_state: 4\n2015-04-12 17:15:20.602 30126 AUDIT nova.compute.manager [req-34a41e3b-3460-4170-b06e-6e506efc5d69 None] [instance: c9ba059c-e0a2-43e8-a7de-e3710565337f] Rebooting instance\n2015-04-12 17:15:40.152 30126 AUDIT nova.compute.manager [req-2d790be7-cc9b-4256-859d-e0b521459f5d None] [instance: e996f312-4dd3-456a-9747-972ff674e9fe] Rebooting instance\n2015-04-12 17:15:45.047 30126 AUDIT nova.compute.manager [req-b4ab6799-7d15-471b-b76f-cc4e0e35b359 None] [instance: ef46f5c4-daef-4b42-b5c9-1764cfead244] Rebooting instance\n\n\n\nI'll post a change to remove the xen-specific filtering in the lifecycle delay processing and I also already had a change posted to add a config option to disable the lifecycle event handling since it's racy and not tested at scale in the gate so operators should be able to turn it off:\n\nhttps://review.openstack.org/#/c/159275/", 
            "date_created": "2015-04-12 22:13:14.667468+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "", 
            "date_created": "2015-04-12 22:13:14.667468+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/172775", 
            "date_created": "2015-04-13 01:54:18.203947+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/172775\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=1d4d00ec1c6d59fd6df5be776c781c38a4de8e28\nSubmitter: Jenkins\nBranch:    master\n\ncommit 1d4d00ec1c6d59fd6df5be776c781c38a4de8e28\nAuthor: Matt Riedemann <email address hidden>\nDate:   Sun Apr 12 15:59:52 2015 -0700\n\n    Delay STOPPED lifecycle event for all domains, not just Xen\n    \n    Commit bd8329b34098436d18441a8129f3f20af53c2b91 added code to delay\n    sending a STOPPED lifecycle event for Xen domains to try and ease a race\n    during reboot where the STOPPED event is sent after the instance is\n    running again after the reboot operation completed and the\n    instance.task_state is None. This would trigger handling in the compute\n    manager which would assume the instance shouldn't be running and shuts\n    it down.  Since the instance is being shutdown, it's task_state is\n    'powering-off' when the lifecycle STARTED event comes in shortly after\n    the STOPPED event and it's ignored in _sync_instance_power_state.\n    \n    The change here is to remove the Xen restriction because there is a\n    recreate (on Icehouse and Juno) with virt_type=KVM, so we can assume\n    this is racy for any configuration of libvirt and the reboot operation\n    and should add the delay for all virt types. The detailed logs with the\n    recreate are attached in the bug report.\n    \n    Closes-Bug: #1443186\n    \n    Change-Id: I3667786ae72f782b3664960e914c98ab94767970\n", 
            "date_created": "2015-04-28 18:32:45.714562+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/178381", 
            "date_created": "2015-04-28 20:03:22.791995+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/178446", 
            "date_created": "2015-04-28 22:41:54.829533+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/159275\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d09785b97a282e8538642f6f8bcdd8491197ed74\nSubmitter: Jenkins\nBranch:    master\n\ncommit d09785b97a282e8538642f6f8bcdd8491197ed74\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Feb 25 14:13:45 2015 -0800\n\n    Add config option to disable handling virt lifecycle events\n    \n    Historically the _sync_power_states periodic task has had the potential\n    for race conditions and several changes have been made to try and\n    tighten up this code:\n    \n    cc5388bbe81aba635fb757e202d860aeed98f3e8\n    aa1792eb4c1d10e9a192142ce7e20d37871d916a\n    baabab45e0ae0e9e35872cae77eb04bdb5ee0545\n    bd8329b34098436d18441a8129f3f20af53c2b91\n    \n    The handle_lifecycle_events method which gets power state change events\n    from the compute driver (currently only implemented by the libvirt\n    driver) and calls _sync_instance_power_state - the same method that the\n    _sync_power_states periodic task uses, except the periodic task at least\n    locks when it's running - expands the scope for race problems in the\n    compute manager so cloud providers should be able to turn it off. It is\n    also known to have races with reboot where rebooted instances are\n    automatically shutdown because of delayed lifecycle events that the\n    instance is stopped even though it's running.\n    \n    This is consistent with the view that Nova should manage it's own state\n    and not rely on external events telling it what to do about state\n    changes. For example, in _sync_instance_power_state, if the Nova\n    database thinks an instance is stopped but the hypervisor says it's\n    running, the compute manager issues a force-stop on the instance.\n    \n    Also, although not documented (at least from what I can find), Nova has\n    historically held a stance that it does not support out-of-band\n    discovery and management of instances, so allowing external events to\n    change state somewhat contradicts that stance and should be at least a\n    configurable deployment option.\n    \n    DocImpact: New config option \"handle_virt_lifecycle_events\" in the\n               DEFAULT group of nova.conf. By default the value is True\n               so there is no upgrade impact or change in functionality.\n    \n    Related-Bug: #1293480\n    Partial-Bug: #1443186\n    Partial-Bug: #1444630\n    \n    Change-Id: I26a1bc70939fb40dc38e9c5c43bf58ed1378bcc7\n", 
            "date_created": "2015-04-29 12:39:27.439932+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/178381\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=709f0bb5b1839a9b5c534bb93f25f6998c162190\nSubmitter: Jenkins\nBranch:    stable/kilo\n\ncommit 709f0bb5b1839a9b5c534bb93f25f6998c162190\nAuthor: Matt Riedemann <email address hidden>\nDate:   Sun Apr 12 15:59:52 2015 -0700\n\n    Delay STOPPED lifecycle event for all domains, not just Xen\n    \n    Commit bd8329b34098436d18441a8129f3f20af53c2b91 added code to delay\n    sending a STOPPED lifecycle event for Xen domains to try and ease a race\n    during reboot where the STOPPED event is sent after the instance is\n    running again after the reboot operation completed and the\n    instance.task_state is None. This would trigger handling in the compute\n    manager which would assume the instance shouldn't be running and shuts\n    it down.  Since the instance is being shutdown, it's task_state is\n    'powering-off' when the lifecycle STARTED event comes in shortly after\n    the STOPPED event and it's ignored in _sync_instance_power_state.\n    \n    The change here is to remove the Xen restriction because there is a\n    recreate (on Icehouse and Juno) with virt_type=KVM, so we can assume\n    this is racy for any configuration of libvirt and the reboot operation\n    and should add the delay for all virt types. The detailed logs with the\n    recreate are attached in the bug report.\n    \n    Closes-Bug: #1443186\n    \n    Change-Id: I3667786ae72f782b3664960e914c98ab94767970\n    (cherry picked from commit 1d4d00ec1c6d59fd6df5be776c781c38a4de8e28)\n", 
            "date_created": "2015-05-28 20:41:03.597930+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/178446\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=6208365d061c7abe58678222671a19ff63e24b24\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit 6208365d061c7abe58678222671a19ff63e24b24\nAuthor: Matt Riedemann <email address hidden>\nDate:   Sun Apr 12 15:59:52 2015 -0700\n\n    Delay STOPPED lifecycle event for all domains, not just Xen\n    \n    Commit bd8329b34098436d18441a8129f3f20af53c2b91 added code to delay\n    sending a STOPPED lifecycle event for Xen domains to try and ease a race\n    during reboot where the STOPPED event is sent after the instance is\n    running again after the reboot operation completed and the\n    instance.task_state is None. This would trigger handling in the compute\n    manager which would assume the instance shouldn't be running and shuts\n    it down.  Since the instance is being shutdown, it's task_state is\n    'powering-off' when the lifecycle STARTED event comes in shortly after\n    the STOPPED event and it's ignored in _sync_instance_power_state.\n    \n    The change here is to remove the Xen restriction because there is a\n    recreate (on Icehouse and Juno) with virt_type=KVM, so we can assume\n    this is racy for any configuration of libvirt and the reboot operation\n    and should add the delay for all virt types. The detailed logs with the\n    recreate are attached in the bug report.\n    \n    Conflicts:\n    \tnova/tests/unit/virt/libvirt/test_host.py\n    \tnova/virt/libvirt/host.py\n    \n    NOTE(mriedem): The conflicts are mainly due to a large refactor in kilo\n    which pulled this code into the nova.virt.libvirt.host module. The\n    tests were also moved in kilo.\n    \n    Closes-Bug: #1443186\n    \n    Change-Id: I3667786ae72f782b3664960e914c98ab94767970\n    (cherry picked from commit 1d4d00ec1c6d59fd6df5be776c781c38a4de8e28)\n    (cherry picked from commit c6575c6918e3ef01e4e8b828bf7bda6d80b43dc1)\n", 
            "date_created": "2015-07-24 17:22:06.117279+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Hi Matt, I have applied your fix of this bug to my Juno environment, it works well, thanks! : )", 
            "date_created": "2015-09-25 03:42:35.711901+00:00", 
            "author": "https://api.launchpad.net/1.0/~bjzyang"
        }
    ], 
    "closed": "2015-06-24 12:17:25.739027+00:00"
}
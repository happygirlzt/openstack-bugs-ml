{
    "status": "Opinion", 
    "last_updated": "2016-05-18 15:38:43.406671+00:00", 
    "description": "screen-n-sch.txt:\n2014-02-24 22:43:41.502 WARNING nova.scheduler.filters.compute_filter [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo]  ram:6577 disk:75776 io_ops:9 instances:14 has not been heard from in a while\n2014-02-24 22:43:41.502 INFO nova.filters [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo] Filter ComputeFilter returned 0 hosts\n2014-02-24 22:43:41.503 WARNING nova.scheduler.driver [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo] [instance: b5a607f0-5280-4033-ba8f-087884d41d28] Setting instance to ERROR state.\n\nThe tempest stress runner with the following example https://github.com/openstack/tempest/blob/master/tempest/stress/etc/server-create-destroy-test.json can cause this kind of load.\n\n./tempest/stress/run_stress.py -t tempest/stress/etc/server-create-destroy-test.json -n 1024 -S\n\nThe example config  uses only 8 threads, If you would like to increase the number of thread you may need to increase the demo user's quota,\nor enable the use_tenant_isolation.\n\ntempest.log:\nINFO: Statistics (per process):\nINFO:  Process 0 (ServerCreateDestroyTest): Run 103 actions (0 failed)\nINFO:  Process 1 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 2 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 3 (ServerCreateDestroyTest): Run 100 actions (0 failed)\nINFO:  Process 4 (ServerCreateDestroyTest): Run 102 actions (2 failed)\nINFO:  Process 5 (ServerCreateDestroyTest): Run 102 actions (1 failed)\nINFO:  Process 6 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 7 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO: Summary:\nINFO - 2014-02-24 22:44:22,713.713 INFO: Run 811 actions (3 failed)", 
    "tags": [
        "compute", 
        "quotas"
    ], 
    "importance": "Wishlist", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1284708", 
    "owner": "None", 
    "id": 1284708, 
    "index": 2768, 
    "openned": "2014-02-25 16:21:03.243555+00:00", 
    "created": "2014-02-25 16:21:03.243555+00:00", 
    "title": "n-cpu under load does not uptdates it's status", 
    "comments": [
        {
            "content": "screen-n-sch.txt:\n2014-02-24 22:43:41.502 WARNING nova.scheduler.filters.compute_filter [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo]  ram:6577 disk:75776 io_ops:9 instances:14 has not been heard from in a while\n2014-02-24 22:43:41.502 INFO nova.filters [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo] Filter ComputeFilter returned 0 hosts\n2014-02-24 22:43:41.503 WARNING nova.scheduler.driver [req-ff34935c-c472-47df-ac4a-1286a7944b17 demo demo] [instance: b5a607f0-5280-4033-ba8f-087884d41d28] Setting instance to ERROR state.\n\nThe tempest stress runner with the following example https://github.com/openstack/tempest/blob/master/tempest/stress/etc/server-create-destroy-test.json can cause this kind of load.\n\n./tempest/stress/run_stress.py -t tempest/stress/etc/server-create-destroy-test.json -n 1024 -S\n\nThe example config  uses only 8 threads, If you would like to increase the number of thread you may need to increase the demo user's quota,\nor enable the use_tenant_isolation.\n\ntempest.log:\nINFO: Statistics (per process):\nINFO:  Process 0 (ServerCreateDestroyTest): Run 103 actions (0 failed)\nINFO:  Process 1 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 2 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 3 (ServerCreateDestroyTest): Run 100 actions (0 failed)\nINFO:  Process 4 (ServerCreateDestroyTest): Run 102 actions (2 failed)\nINFO:  Process 5 (ServerCreateDestroyTest): Run 102 actions (1 failed)\nINFO:  Process 6 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO:  Process 7 (ServerCreateDestroyTest): Run 101 actions (0 failed)\nINFO: Summary:\nINFO - 2014-02-24 22:44:22,713.713 INFO: Run 811 actions (3 failed)", 
            "date_created": "2014-02-25 16:21:03.243555+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "what kind of load is n-cpu under? do you have any stats is this in the gate?   Its reasonable to expect that if a system is very overloaded things won't work.", 
            "date_created": "2014-02-26 17:17:49.054145+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "marked as incomplete because unclear how loaded the system is, if the load is 'reasonable' then this could be a valid bug, otherwise I think this is just expected behavior.", 
            "date_created": "2014-02-26 17:18:56.617835+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "A single 4 core VM with 8 GiB memory system used to create-wait-active-destory servers with 8 user (worker thread).\n\nOne  top output when the tests is running:\nhttp://www.fpaste.org/81175/57776013/\nThe load is under 10.\n\nUsually the actually existing qemu processes number are below 2, some cases 0.  So the VM does not really consumes resources.\nMany CPU time spent in short leaving commands and in the nova services.\n\nJust with 8 users (workers) this kind of issue should not happen, the system frequently survives  it, but not always.", 
            "date_created": "2014-02-28 09:11:37.849638+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Based on reporter input.", 
            "date_created": "2014-03-16 02:00:21.279093+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "With a load of 10 this type of thing is expected, that being said if we can fix this it would be great so moving to wishlist.", 
            "date_created": "2014-03-20 18:36:08.752586+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "This wishlist bug has been open a year without any activity. I'm going to move it to \"Opinion / Wishlist\", which is an easily-obtainable queue of older requests that have come on. \n", 
            "date_created": "2016-05-17 15:51:21.826590+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ], 
    "closed": "2016-05-18 15:38:41.365949+00:00"
}
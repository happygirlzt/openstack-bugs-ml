{
    "status": "Expired", 
    "last_updated": "2016-04-24 04:17:33.876018+00:00", 
    "description": "During live migration with attached volume, nova ignores initialize connection errors and does not roll back. \n\nSteps:\n* Create a nova instance\n* Attach a cinder volume\n* Perform \u2018nova live-migration\u2019 to a different backend\n \t-Cause a failure in the \u2018initialize_connection\u2019 call to the new host\n* Wait for nova to call \u2018terminate_connection\u2019 on the connection to the original host\n\nResult:\n* Instance remains on original host with Cinder volume attached according to Cinder but no longer mapped on the backend. This removes connectivity from storage to the host and can cause data loss.\n\n\nTriage:\nWhat seems to be happening is that Nova is not stopping the migration when receiving an error from Cinder and ends up calling terminate_connection for the src host when it should not be.", 
    "tags": [
        "live-migration"
    ], 
    "importance": "Low", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1405294", 
    "owner": "None", 
    "id": 1405294, 
    "index": 1469, 
    "openned": "2014-12-23 22:32:47.871905+00:00", 
    "created": "2014-12-23 22:32:47.871905+00:00", 
    "title": "Live migration with attached volume peforms breaking rollback on failure", 
    "comments": [
        {
            "content": "During live migration with attached volume, nova ignores initialize connection errors and does not roll back. \n\nSteps:\n* Create a nova instance\n* Attach a cinder volume\n* Perform \u2018nova live-migration\u2019 to a different backend\n \t-Cause a failure in the \u2018initialize_connection\u2019 call to the new host\n* Wait for nova to call \u2018terminate_connection\u2019 on the connection to the original host\n\nResult:\n* Instance remains on original host with Cinder volume attached according to Cinder but no longer mapped on the backend. This removes connectivity from storage to the host and can cause data loss.\n\n\nTriage:\nWhat seems to be happening is that Nova is not stopping the migration when receiving an error from Cinder and ends up calling terminate_connection for the src host when it should not be.", 
            "date_created": "2014-12-23 22:32:47.871905+00:00", 
            "author": "https://api.launchpad.net/1.0/~alex-meade"
        }, 
        {
            "content": "Hi, I have tried to reproduce your error, but I didn't see it.. Here's my steps:\ncompute nodes: ly-compute1, ly-compute2\nVM: test11, uuid = 30c4dac1-f3bc-4e6a-8a38-ee49671eee6a, is running on ly-compute1\nvolume: vol1, is attached to test11\n\nHere, I use the command to migrate:\nnova live-migration test11 ly-compute2\n\nThe migration is successful, and the vol1 volume is still attached to test11, the file in it is readable. My openstack is Icehouse, and I didn't see a failure in the \u2018initialize_connection\u2019?\n\n\nbelow are some prompt contents may be helpful:\n\nC:\\Windows\\system32>cinder list\n+--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+\n|                  ID                  | Status | Display Name | Size | Volume Type | Bootable |             Attached to              |\n+--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+\n| fef21ea2-c5bb-4553-ac6d-5745e3ae27fe | in-use |     vol1     |  1   |     None    |  false   | 30c4dac1-f3bc-4e6a-8a38-ee49671eee6a |\n+--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+\n\nC:\\Windows\\system32>nova show test11\n+--------------------------------------+------------------------------------------------------------+\n| Property                             | Value                                                      |\n+--------------------------------------+------------------------------------------------------------+\n| OS-DCF:diskConfig                    | AUTO                                                       |\n| OS-EXT-AZ:availability_zone          | nova                                                       |\n| OS-EXT-SRV-ATTR:host                 | ly-compute1                                                |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | ly-compute1                                                |\n| OS-EXT-SRV-ATTR:instance_name        | instance-00000022                                          |\n| OS-EXT-STS:power_state               | 1                                                          |\n| OS-EXT-STS:task_state                | -                                                          |\n| OS-EXT-STS:vm_state                  | active                                                     |\n| OS-SRV-USG:launched_at               | 2014-12-27T15:28:18.000000                                 |\n| OS-SRV-USG:terminated_at             | -                                                          |\n| accessIPv4                           |                                                            |\n| accessIPv6                           |                                                            |\n| config_drive                         |                                                            |\n| created                              | 2014-12-27T15:28:11Z                                       |\n| demo-net network                     | 192.168.10.38                                              |\n| flavor                               | m1.tiny (1)                                                |\n| hostId                               | 5261838058a81120b45f8d8dbe2a0259924abf28dadf859e3cd24ca7   |\n| id                                   | 30c4dac1-f3bc-4e6a-8a38-ee49671eee6a                       |\n| image                                | cirros-0.3.2-x86_64 (67d638d4-c512-4eef-b3f1-e3bcf00101b2) |\n| key_name                             | -                                                          |\n| metadata                             | {}                                                         |\n| name                                 | test11                                                     |\n| os-extended-volumes:volumes_attached | [{\"id\": \"fef21ea2-c5bb-4553-ac6d-5745e3ae27fe\"}]           |\n| progress                             | 0                                                          |\n| security_groups                      | default                                                    |\n| status                               | ACTIVE                                                     |\n| tenant_id                            | eaca1af2b7b74cdfaf1e61c081d6d255                           |\n| updated                              | 2014-12-29T10:12:25Z                                       |\n| user_id                              | 8481fe632326487db70e308ae070040f                           |\n+--------------------------------------+------------------------------------------------------------+\n\nC:\\Windows\\system32>nova live-migration test11 ly-compute2\n\nC:\\Windows\\system32>nova show test11\n+--------------------------------------+------------------------------------------------------------+\n| Property                             | Value                                                      |\n+--------------------------------------+------------------------------------------------------------+\n| OS-DCF:diskConfig                    | AUTO                                                       |\n| OS-EXT-AZ:availability_zone          | nova                                                       |\n| OS-EXT-SRV-ATTR:host                 | ly-compute1                                                |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | ly-compute1                                                |\n| OS-EXT-SRV-ATTR:instance_name        | instance-00000022                                          |\n| OS-EXT-STS:power_state               | 1                                                          |\n| OS-EXT-STS:task_state                | migrating                                                  |\n| OS-EXT-STS:vm_state                  | active                                                     |\n| OS-SRV-USG:launched_at               | 2014-12-27T15:28:18.000000                                 |\n| OS-SRV-USG:terminated_at             | -                                                          |\n| accessIPv4                           |                                                            |\n| accessIPv6                           |                                                            |\n| config_drive                         |                                                            |\n| created                              | 2014-12-27T15:28:11Z                                       |\n| demo-net network                     | 192.168.10.38                                              |\n| flavor                               | m1.tiny (1)                                                |\n| hostId                               | 5261838058a81120b45f8d8dbe2a0259924abf28dadf859e3cd24ca7   |\n| id                                   | 30c4dac1-f3bc-4e6a-8a38-ee49671eee6a                       |\n| image                                | cirros-0.3.2-x86_64 (67d638d4-c512-4eef-b3f1-e3bcf00101b2) |\n| key_name                             | -                                                          |\n| metadata                             | {}                                                         |\n| name                                 | test11                                                     |\n| os-extended-volumes:volumes_attached | [{\"id\": \"fef21ea2-c5bb-4553-ac6d-5745e3ae27fe\"}]           |\n| security_groups                      | default                                                    |\n| status                               | MIGRATING                                                  |\n| tenant_id                            | eaca1af2b7b74cdfaf1e61c081d6d255                           |\n| updated                              | 2014-12-29T12:16:03Z                                       |\n| user_id                              | 8481fe632326487db70e308ae070040f                           |\n+--------------------------------------+------------------------------------------------------------+\n\nC:\\Windows\\system32>nova show test11\n+--------------------------------------+------------------------------------------------------------+\n| Property                             | Value                                                      |\n+--------------------------------------+------------------------------------------------------------+\n| OS-DCF:diskConfig                    | AUTO                                                       |\n| OS-EXT-AZ:availability_zone          | nova                                                       |\n| OS-EXT-SRV-ATTR:host                 | ly-compute2                                                |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | ly-compute2                                                |\n| OS-EXT-SRV-ATTR:instance_name        | instance-00000022                                          |\n| OS-EXT-STS:power_state               | 1                                                          |\n| OS-EXT-STS:task_state                | -                                                          |\n| OS-EXT-STS:vm_state                  | active                                                     |\n| OS-SRV-USG:launched_at               | 2014-12-27T15:28:18.000000                                 |\n| OS-SRV-USG:terminated_at             | -                                                          |\n| accessIPv4                           |                                                            |\n| accessIPv6                           |                                                            |\n| config_drive                         |                                                            |\n| created                              | 2014-12-27T15:28:11Z                                       |\n| demo-net network                     | 192.168.10.38                                              |\n| flavor                               | m1.tiny (1)                                                |\n| hostId                               | d21997093cecfbed529805a7021b7afea4fe7698e76b4c8404832c65   |\n| id                                   | 30c4dac1-f3bc-4e6a-8a38-ee49671eee6a                       |\n| image                                | cirros-0.3.2-x86_64 (67d638d4-c512-4eef-b3f1-e3bcf00101b2) |\n| key_name                             | -                                                          |\n| metadata                             | {}                                                         |\n| name                                 | test11                                                     |\n| os-extended-volumes:volumes_attached | [{\"id\": \"fef21ea2-c5bb-4553-ac6d-5745e3ae27fe\"}]           |\n| progress                             | 0                                                          |\n| security_groups                      | default                                                    |\n| status                               | ACTIVE                                                     |\n| tenant_id                            | eaca1af2b7b74cdfaf1e61c081d6d255                           |\n| updated                              | 2014-12-29T12:16:12Z                                       |\n| user_id                              | 8481fe632326487db70e308ae070040f                           |\n+--------------------------------------+------------------------------------------------------------+", 
            "date_created": "2014-12-29 12:29:07.256574+00:00", 
            "author": "https://api.launchpad.net/1.0/~hsluoyz"
        }, 
        {
            "content": "The issue is not that initialize_connection always fails, it's that when it does fail we continue the migration logic which is shown by a call to terminate_connection which should not occur.", 
            "date_created": "2014-12-30 15:09:29.068493+00:00", 
            "author": "https://api.launchpad.net/1.0/~alex-meade"
        }, 
        {
            "content": "nova/compute/manager.py\n\nfinally:\n            conn_volume = new_volume_id if failed else old_volume_id # <---------------------- this assignment is weird, maybe cause problem\n            if new_cinfo:\n                self.volume_api.terminate_connection(context,\n                                                     conn_volume,\n                                                     connector)\n            # If Cinder initiated the swap, it will keep\n            # the original ID\n            comp_ret = self.volume_api.migrate_volume_completion(\n                                                      context,\n                                                      old_volume_id,\n                                                      new_volume_id,\n                                                      error=failed)\n", 
            "date_created": "2015-01-01 04:05:49.582069+00:00", 
            "author": "https://api.launchpad.net/1.0/~hsluoyz"
        }, 
        {
            "content": "Looks like a good find Yang. I'll try to have a deeper look at that if I get a chance.", 
            "date_created": "2015-01-09 19:52:13.687618+00:00", 
            "author": "https://api.launchpad.net/1.0/~alex-meade"
        }, 
        {
            "content": "I'm not able to reproduce the issue. Did NFS shared storage setup on Multi node devstack. Raised cinderclient exception in initialize_connection() in cinder.py. Then tried live-migration for instance with attached volume.\n\nExpected: Instance should lose its connectivity to volume.\nActual : Instance is still connected to the volume but the instance moves to 'ERROR' state and task state 'migrating'. Terminate connection was successful.\n\nI'm not sure if this is related to any specific Cinder driver. This is not reproducible with LVM driver.\n\nMarking this as Incomplete. Please mention which cinder driver caused this issue.", 
            "date_created": "2016-02-10 19:54:23.751409+00:00", 
            "author": "https://api.launchpad.net/1.0/~sujitha-neti"
        }, 
        {
            "content": "Sujitha, I see difference between steps provided in description and the way you tried to reproduce it. Description says that exception should happen during live migration, but you forced exception prior to live migration. Therefore marking back as confirmed.", 
            "date_created": "2016-02-17 09:10:07.721516+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-koniszewski"
        }, 
        {
            "content": "The change in code is the reason for the change in the repo steps.\n\nThere was a check added here:\nhttps://github.com/openstack/nova/commit/7735902f6eaab907929efead53aa98428457c42d\nsee:\nhttps://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L5560\n\nNow this bug is probably technically still valid against the older release, but we need more info to be sure.", 
            "date_created": "2016-02-19 10:15:06.562181+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "I should be more specific. We can't reproduce this with the latest code, due to the above check correctly stopping the live-migrate.\n\nLets be more explicit about the version this works with. I think it has to be before that commit, i.e. before 12.x", 
            "date_created": "2016-02-19 10:21:40.802737+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "John, I'm able to reproduce this issue on trunk with provided steps. I'm only getting a bit different result. Your case points to block live migration, but author didn't mention that this affects block live migration only.\n\nWhat I'm getting instead of:\n\n* Instance remains on original host with Cinder volume attached according to Cinder but no longer mapped on the backend. This removes connectivity from storage to the host and can cause data loss.\n\nis:\n\n* Instance remains on original, but in state ERROR/MIGRATING. I'm getting 500 unexpected API error. This happens because cinderexception is thrown during initialize_connection in volume/cinder.py.\n\ncinderexception thrown in initialize_connection goes all the way back to the API and results in error 500. I'm fine with leaving VM in ERROR/MIGRATING state, because there might be some garbage connections left (i'm not sure about that yet), but we really should change this 500 to something more meaningful (e.g. MigrationPreCheckError here https://github.com/openstack/nova/blob/master/nova/conductor/manager.py#L310 ).\n\nI will investigate more to see if nova can really break connection to storage.", 
            "date_created": "2016-02-22 15:54:55.499110+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-koniszewski"
        }, 
        {
            "content": "I tried different scenarios with both types of live migration to see whether nova will break connection between VM and volume but no luck. The only issue I can see there is that we don't handle cinderexception which results in internal error in the API. ", 
            "date_created": "2016-02-23 10:23:26.645585+00:00", 
            "author": "https://api.launchpad.net/1.0/~pawel-koniszewski"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2016-04-24 04:17:30.175746+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ], 
    "closed": "2016-04-24 04:17:31.202291+00:00"
}
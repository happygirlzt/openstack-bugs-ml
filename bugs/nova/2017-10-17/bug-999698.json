{
    "status": "Fix Released", 
    "last_updated": "2012-09-27 15:22:08.739864+00:00", 
    "description": "Following a service restart the exception below was outputted to the compute log file and the service crashes\n\nOnce this occurs, the same exception is raised every time I start the compute service \n\n2012-05-15 11:48:57 AUDIT nova.service [-] Starting compute node (version 2012.1-LOCALBRANCH:LOCALREVISION)\n2012-05-15 11:48:57 WARNING nova.utils [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] /usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.3-py2.6-linux-x86_64.egg/sqlalchemy/pool.py:639: SADeprecationWarning: The 'listeners' argument to Pool (and create_engine()) is deprecated.  Use event.listen().\n  Pool.__init__(self, creator, **kw)\n\n2012-05-15 11:48:57 WARNING nova.utils [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] /usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.3-py2.6-linux-x86_64.egg/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated.  Use event.listen()\n  self.add_listener(l)\n\n2012-05-15 11:48:58 INFO nova.rpc.impl_qpid [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] Connected to AMQP server on localhost:5672\n2012-05-15 11:49:58 ERROR nova.rpc.impl_qpid [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] Timed out waiting for RPC response: None\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid Traceback (most recent call last):\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 359, in ensure\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     return method(*args, **kwargs)\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 408, in _consume\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     nxt_receiver = self.session.next_receiver(timeout=timeout)\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"<string>\", line 6, in next_receiver\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py\", line 651, in next_receiver\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     raise Empty\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid Empty: None\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid\n2012-05-15 11:49:58 CRITICAL nova [-] Timeout while waiting on RPC response.", 
    "tags": [
        "verification-done"
    ], 
    "importance": "High", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/999698", 
    "owner": "https://api.launchpad.net/1.0/~russellb", 
    "id": 999698, 
    "index": 682, 
    "openned": "2012-05-15 13:53:09.881591+00:00", 
    "created": "2012-05-15 13:53:09.881591+00:00", 
    "title": "qpid timeout causing compute service to crash", 
    "comments": [
        {
            "content": "Following a service restart the exception below was outputted to the compute log file and the service crashes\n\nOnce this occurs, the same exception is raised every time I start the compute service \n\n2012-05-15 11:48:57 AUDIT nova.service [-] Starting compute node (version 2012.1-LOCALBRANCH:LOCALREVISION)\n2012-05-15 11:48:57 WARNING nova.utils [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] /usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.3-py2.6-linux-x86_64.egg/sqlalchemy/pool.py:639: SADeprecationWarning: The 'listeners' argument to Pool (and create_engine()) is deprecated.  Use event.listen().\n  Pool.__init__(self, creator, **kw)\n\n2012-05-15 11:48:57 WARNING nova.utils [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] /usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.3-py2.6-linux-x86_64.egg/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated.  Use event.listen()\n  self.add_listener(l)\n\n2012-05-15 11:48:58 INFO nova.rpc.impl_qpid [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] Connected to AMQP server on localhost:5672\n2012-05-15 11:49:58 ERROR nova.rpc.impl_qpid [req-1cdae63a-8885-4711-a39d-2416a1f6477a None None] Timed out waiting for RPC response: None\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid Traceback (most recent call last):\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 359, in ensure\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     return method(*args, **kwargs)\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 408, in _consume\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     nxt_receiver = self.session.next_receiver(timeout=timeout)\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"<string>\", line 6, in next_receiver\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid   File \"/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py\", line 651, in next_receiver\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid     raise Empty\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid Empty: None\n2012-05-15 11:49:58 TRACE nova.rpc.impl_qpid\n2012-05-15 11:49:58 CRITICAL nova [-] Timeout while waiting on RPC response.", 
            "date_created": "2012-05-15 13:53:09.881591+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "\n\nsome version information \n\nqpid-cpp-server-0.14-16.el6.x86_64\nqpid-cpp-client-0.14-16.el6.x86_64\nqpid-qmf-0.14-7.el6_2.x86_64\nqpid-tools-0.14-2.el6_2.noarch\npython-qpid-0.14-8.el6.noarch\npython-qpid-qmf-0.14-7.el6_2.x86_64\n\nRunning on RHEL 6.3 Beta", 
            "date_created": "2012-05-15 13:56:55.012606+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "Restarting qpidd doesn't help the situation", 
            "date_created": "2012-05-15 14:00:51.841791+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "Some more information that may be relevant\n\nI noticed nova-compute was subscribing to a queue with a random hex uuid name e.g. 93c92020b8bc4dab88c166a615002f0b\n\nI was restarting all services together\nfor srv in api cert compute network objectstore scheduler volume ; do sudo service openstack-nova-$srv restart ; done\n\nnova-compute starts to successfully work when I restarted it by itself, and it is now subscriped to \ncompute and compute_fanout_53de51e809cf4c99a390903bbda3d6a0\n", 
            "date_created": "2012-05-15 14:47:37.663263+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "I can now reproduce this by starting starting a instance, \nand restarting all nova services together\n\nstarting nova-compute by itself then fixes the problem\n\nWith verbose enabled \n\nCRITICAL nova [-] Timeout while waiting on RPC response.\nTRACE nova Traceback (most recent call last):\nTRACE nova   File \"/usr/bin/nova-compute\", line 49, in <module>\nTRACE nova     service.wait()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 413, in wait\nTRACE nova     _launcher.wait()    \nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 131, in wait\nTRACE nova     service.wait()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 166, in wait\nTRACE nova     return self._exit_event.wait()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/eventlet/event.py\", line 116, in wait\nTRACE nova     return hubs.get_hub().switch()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py\", line 177, in switch \nTRACE nova     return self.greenlet.switch()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/eventlet/greenthread.py\", line 192, in main\nTRACE nova     result = function(*args, **kwargs)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 101, in run_server\nTRACE nova     server.start()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/service.py\", line 162, in start\nTRACE nova     self.manager.init_host()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 251, in init_host\nTRACE nova     net_info = self._get_instance_nw_info(context, instance)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/compute/manager.py\", line 313, in _get_instance_nw_info\nTRACE nova     instance)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/network/api.py\", line 219, in get_instance_nw_info\nTRACE nova     'args': args})\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/__init__.py\", line 68, in call\nTRACE nova     return _get_impl().call(context, topic, msg, timeout)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 521, in call\nTRACE nova     return rpc_amqp.call(context, topic, msg, timeout, Connection.pool)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/amqp.py\", line 338, in call\nTRACE nova     rv = list(rv)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/amqp.py\", line 299, in __iter__\nTRACE nova     self._iterator.next()\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 417, in iterconsume \nTRACE nova     yield self.ensure(_error_callback, _consume)\nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 363, in ensure \nTRACE nova     error_callback(e)   \nTRACE nova   File \"/usr/lib/python2.6/site-packages/nova/rpc/impl_qpid.py\", line 402, in _error_callback\nTRACE nova     raise rpc_common.Timeout()\nTRACE nova Timeout: Timeout while waiting on RPC response.\nTRACE nova \n", 
            "date_created": "2012-05-15 15:20:53.127678+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "Can you show me the exact command(s) you are using to restart all services at the same time?  It's starting to sound like:\n\n1) When nova-compute gets restarted, it's making a call out to nova-network.\n\n2) nova-network is getting restarted before this call is completed, resulting in the timeout error that occurred.\n\n3) This code path in nova-compute is not properly handling the timeout exception, resulting in the service stopping.\n\n\nSo if it's just a bash for loop restarting all of the services, try restarting network before compute.\n\n", 
            "date_created": "2012-05-15 17:27:31.729069+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "I was using a bash loop to start the services\nfor srv in api cert compute network objectstore scheduler volume ; do sudo service openstack-nova-$srv restart ; done\n\nchanging the order didn't help \nfor srv in api cert network objectstore scheduler volume compute ; do sudo service openstack-nova-$srv restart ; done\n\nit does work if I add a sleep in between each service restart\nfor srv in api cert network objectstore scheduler volume compute ; do sudo service openstack-nova-$srv restart ; sleep 5 ; done", 
            "date_created": "2012-05-16 14:55:30.309091+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "Ok, thanks.  I'm still thinking that what's happening is that nova-network is reading the message off the queue before it restarts, even when the order is changed in your loop.  The service restarts must be happening asyncrhonously.\n\nRegarding the service stopping part ... this error is happening in service startup code, so I think it's pretty much just an expected failure if nova-compute can't talk to nova-network during startup.\n\nThere is a patch on gerrit right now that may fix this scenario.\n\nhttps://review.openstack.org/#/c/6694/\n\nI haven't reviewed the patch yet, but based on the description, it sounds like it should make it so nova-network can't restart wihle it's in the middle of processing the request from nova-compute.  Mind giving that patch a try?", 
            "date_created": "2012-05-16 15:49:36.073191+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "When running in a loop like that, systemd does not guarantee that the process has actually finished starting up.  systemd checks for the pidfile to make a determination when to start dependent processes.  The proper solution would be for the systemd devs to fix the problem, but it is unclear if they intend to, as they think the pid lockfile is consistent with the daemon being in a runnable state.  The proper semantics of the lock file should be \"the daemon has started, no new ones can start now\".  This problem pervades all of Fedora init scripts now for dependent services.", 
            "date_created": "2012-05-21 14:45:41.342965+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdake"
        }, 
        {
            "content": "Related: https://lists.launchpad.net/openstack/msg12443.html\n\nMy previous suggestions on this bug were around the case that nova-network is stopped in the middle of processing a message.  There is a much more likely and common case, as well as a couple of possible solutions, that I describe in the linked post above.", 
            "date_created": "2012-05-29 19:51:32.485709+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Can you give this patch a try and see if it makes the problem go away?  It's a bit of a hack, but it will help confirm that the issue is just nova-network not being up yet, meaning that the queue the request is destined for doesn't exist yet.", 
            "date_created": "2012-05-29 20:10:19.731371+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "I've applied the above patch to Essex and it looks like it is working\n\nwith the patch applied when I restart all service together in the nova-compute log I get a timeout traceback but the service stays running and continues with its startup sequence.\n\n", 
            "date_created": "2012-05-30 11:38:08.484885+00:00", 
            "author": "https://api.launchpad.net/1.0/~derekh"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/7961", 
            "date_created": "2012-05-31 01:26:13.029848+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Derek, I have written a new version of this patch based on discussion on the bug.  Can you verify that this still resolves the problem for you?  I ran it through the unit tests and the functional tests on smokestack, but I'm not sure that any of those are hitting this code (restarting nova-compute with instances running).  I'll do some more targeted testing on it tomorrow.", 
            "date_created": "2012-05-31 22:31:22.047140+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "I was able to test restarting nova-compute with an instance running and it seems to work.  I'll submit the patch.", 
            "date_created": "2012-06-01 15:44:56.381833+00:00", 
            "author": "https://api.launchpad.net/1.0/~russellb"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/7961\nCommitted: http://github.com/openstack/nova/commit/8db54f3bd590e71c6c6e383c928aa82fc28b3379\nSubmitter: Jenkins\nBranch:    master\n\ncommit 8db54f3bd590e71c6c6e383c928aa82fc28b3379\nAuthor: Russell Bryant <email address hidden>\nDate:   Tue May 29 16:04:16 2012 -0400\n\n    Don't query nova-network on startup.\n    \n    Fix bug 999698.\n    \n    nova-compute requested network info for each instance on startup via rpc.\n    If all services get (re)started at the same time, nova-network may not\n    be available to take this request, resulting in a lost request.  To\n    combat this issue, get the network info from the cache in the database\n    on startup.  If by some chance this information is not correct, it will\n    get fixed up by a periodic task.\n    \n    Change-Id: I0bbd475e078ac2a67c99c2be4711e86d617c609a\n", 
            "date_created": "2012-06-07 18:45:59.293382+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Is this appropriate for essex?\nDerek reported it works there.", 
            "date_created": "2012-06-11 10:00:36.855423+00:00", 
            "author": "https://api.launchpad.net/1.0/~p-draigbrady"
        }, 
        {
            "content": "Fix proposed to branch: stable/essex\nReview: https://review.openstack.org/8393", 
            "date_created": "2012-06-11 13:46:21.091933+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/8393\nCommitted: http://github.com/openstack/nova/commit/d89c2f309221afb8b25cb0afe5291d432f033076\nSubmitter: Jenkins\nBranch:    stable/essex\n\ncommit d89c2f309221afb8b25cb0afe5291d432f033076\nAuthor: Russell Bryant <email address hidden>\nDate:   Mon Jun 11 14:07:19 2012 +0100\n\n    Don't query nova-network on startup.\n    \n    Backport from commit 8db54f3.\n    Fix bug 999698.\n    \n    nova-compute requested network info for each instance on startup via rpc.\n    If all services get (re)started at the same time, nova-network may not\n    be available to take this request, resulting in a lost request.  To\n    combat this issue, get the network info from the cache in the database\n    on startup.  If by some chance this information is not correct, it will\n    get fixed up by a periodic task.\n    \n    Change-Id: Ifb0634e87770f565e4ab36a54f6e9e19e5f31632\n", 
            "date_created": "2012-06-15 13:59:51.625412+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Please find the attached test log from the Ubuntu Server Team's CI infrastructure.  As part of the verification process for this bug, Nova has been deployed and configured across multiple nodes using precise-proposed as an installation source.  After successful bring-up and configuration of the cluster, a number of exercises and smoke tests have be invoked to ensure the updated package did not introduce any regressions. A number of test iterations were carried out to catch any possible transient errors.\n\nPlease Note the list of installed packages at the top and bottom of the report.\n\nFor records of upstream test coverage of this update, please see the Jenkins links in the comments of the relevant  upstream code-review(s):\n\nTrunk review: https://review.openstack.org/7961\nStable review: https://review.openstack.org/8393\n\nAs per the provisional Micro Release Exception granted to this package by the Technical Board, we hope this contributes toward verification of this update.", 
            "date_created": "2012-08-30 07:45:46.988152+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "Test coverage log.", 
            "date_created": "2012-08-30 07:45:49.584337+00:00", 
            "author": "https://api.launchpad.net/1.0/~gandelman-a"
        }, 
        {
            "content": "This bug was fixed in the package nova - 2012.1.3+stable-20120827-4d2a4afe-0ubuntu1\n\n---------------\nnova (2012.1.3+stable-20120827-4d2a4afe-0ubuntu1) precise-proposed; urgency=low\n\n  * New upstream snapshot, fixes FTBFS in -proposed. (LP: #1041120)\n  * Resynchronize with stable/essex (4d2a4afe):\n    - [5d63601] Inappropriate exception handling on kvm live/block migration\n      (LP: #917615)\n    - [ae280ca] Deleted floating ips can cause instance delete to fail\n      (LP: #1038266)\n\nnova (2012.1.3+stable-20120824-86fb7362-0ubuntu1) precise-proposed; urgency=low\n\n  * New upstream snapshot. (LP: #1041120)\n  * Dropped, superseded by new snapshot:\n    - debian/patches/CVE-2012-3447.patch: [d9577ce]\n    - debian/patches/CVE-2012-3371.patch: [25f5bd3]\n    - debian/patches/CVE-2012-3360+3361.patch: [b0feaff]\n  * Resynchronize with stable/essex (86fb7362):\n    - [86fb736] Libvirt driver reports incorrect error when volume-detach fails\n      (LP: #1029463)\n    - [272b98d] nova delete lxc-instance umounts the wrong rootfs (LP: #971621)\n    - [09217ab] Block storage connections are NOT restored on system reboot\n      (LP: #1036902)\n    - [d9577ce] CVE-2012-3361 not fully addressed (LP: #1031311)\n    - [e8ef050] pycrypto is unused and the existing code is potentially insecure\n      to use (LP: #1033178)\n    - [3b4ac31] cannot umount guestfs  (LP: #1013689)\n    - [f8255f3] qpid_heartbeat setting in ineffective (LP: #1030430)\n    - [413c641] Deallocation of fixed IP occurs before security group refresh\n      leading to potential security issue in error / race conditions\n      (LP: #1021352)\n    - [219c5ca] Race condition in network/deallocate_for_instance() leads to\n      security issue (LP: #1021340)\n    - [f2bc403] cleanup_file_locks does not remove stale sentinel files\n      (LP: #1018586)\n    - [4c7d671] Deleting Flavor currently in use by instance creates error\n      (LP: #994935)\n    - [7e88e39] nova testsuite errors on newer versions of python-boto (e.g.\n      2.5.2) (LP: #1027984)\n    - [80d3026] NoMoreFloatingIps: Zero floating ips available after repeatedly\n      creating and destroying instances over time (LP: #1017418)\n    - [4d74631] Launching with source groups under load produces lazy load error\n      (LP: #1018721)\n    - [08e5128] API 'v1.1/{tenant_id}/os-hosts' does not return a list of hosts\n      (LP: #1014925)\n    - [801b94a] Restarting nova-compute removes ip packet filters (LP: #1027105)\n    - [f6d1f55] instance live migration should create virtual_size disk image\n      (LP: #977007)\n    - [4b89b4f] [nova][volumes] Exceeding volumes, gigabytes and floating_ips\n      quotas returns general uninformative HTTP 500 error (LP: #1021373)\n    - [6e873bc] [nova][volumes] Exceeding volumes, gigabytes and floating_ips\n      quotas returns general uninformative HTTP 500 error (LP: #1021373)\n    - [7b215ed] Use default qemu-img cluster size in libvirt connection driver\n    - [d3a87a2] Listing flavors with marker set returns 400 (LP: #956096)\n    - [cf6a85a] nova-rootwrap hardcodes paths instead of using\n      /sbin:/usr/sbin:/usr/bin:/bin (LP: #1013147)\n    - [2efc87c] affinity filters don't work if scheduler_hints is None\n      (LP: #1007573)\n    - [48e5f46] metadata injection is broken in xen (LP: #1022036)\n    - [25f5bd3] scheduler hang (DOS) possible with\n      DifferentHostFilter/SameHostFilter  (LP: #1017795)\n    - [1c1b858] cannot umount guestfs  (LP: #1013689)\n    - [835ba4f] not able to get host total memory in xen with libvirt\n      (LP: #1004298)\n    - [00e5104] Call to network_get_all_by_uuids missing 'db' (LP: #986922)\n    - [4c49df7] [nova][volumes] Exceeding volumes, gigabytes and floating_ips\n      quotas returns general uninformative HTTP 500 error (LP: #1021373)\n    - [19631f3] [nova][volumes] Exceeding volumes quotas logs\n      \"VolumeSizeTooLarge\" instead of \"VolumeLimitExceeded\"  (LP: #1020634)\n    - [b0feaff] Remote arbitrary file corruption / creation flaw via injected\n      files (LP: #1015531)\n    - [3cb6e57] NoMoreFixedIps: Zero fixed ips available. Nova seems leaking\n      them. (LP: #1014769)\n    - [5d8431b] ram_allocation_ratio does not work (LP: #1016273)\n    - [410060f] test_get_console_output_file requires sudo NOPASSWD\n      (LP: #992805)\n    - [33c2575] Stop/start a KVM instance with volumes attached produces an\n      error state (LP: #1013782)\n    - [6c01c01] Backport tox settings to unbreak jenkins jobs.\n    - [344125f] Set defaultbranch in .gitreview to stable/essex\n    - [9b789be] floating ips are not disassociated from instances on deletion\n      (LP: #997763)\n    - [d89c2f3] qpid timeout causing compute service to crash (LP: #999698)\n    - [caae0e9] floating ips do not display in 'nova list' after association to\n      instance (LP: #939122)\n    - [1dc9f19] impl_qpid doesn't ACK messages (LP: #1012374)\n    - [bc621bc] Restarting nova-network removes ip packet filters\n      (LP: #1000853)\n    - [7870157] Add caching to openstack.common.cfg\n    - [27133ee] Firewall rules from nova-compute are not refreshed after host\n      reboot (LP: #985162)\n    - [3ee026e] Source group based security group rule without protocol and port\n      causes failures (LP: #1010514)\n    - [f0a9f47] [SRU] dns_domains table mysql charset is 'latin1'. Should be\n      'utf8' (LP: #993663)\n    - [cc8fd97] euca-describe-keypair NonExistent returns 200 (LP: #1006664)\n    - [9f9e9da] Security groups fail to be set correctly if incorrect case is\n      used for protocol specification (LP: #985184)\n -- Adam Gandelman <email address hidden>   Mon, 27 Aug 2012 14:50:40 -0700", 
            "date_created": "2012-09-03 15:18:08.793162+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "The verification of this Stable Release Update has completed successfully and the package has now been released to -updates.  Subsequently, the Ubuntu Stable Release Updates Team is being unsubscribed and will not receive messages about this bug report.  In the event that you encounter a regression using the package from -updates please report a new bug using ubuntu-bug and tag the bug report regression-update so we can easily find any regresssions.", 
            "date_created": "2012-09-03 15:23:38.381033+00:00", 
            "author": "https://api.launchpad.net/1.0/~clint-fewbar"
        }
    ], 
    "closed": "2012-07-04 08:26:23.694457+00:00"
}
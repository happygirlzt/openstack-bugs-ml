{
    "status": "Fix Released", 
    "last_updated": "2013-09-26 20:08:25.261596+00:00", 
    "description": "OS: ubuntu 12.04, 64 bit.\nlibvirt_type=qemu\nI installed openstack via devstack.\nWhen I try to attach a volume to a running instance, the mountpoint of this instance always starts from /dev/vdb, though  I can specify the mountpoint, e.g. /dev/vdd. \nIt seems the mountpoint of the instance is taken in a certain order, from /dev/vdb, then /dev/vdc... to /dev/vdz. The mountpoint specified by the command does not take effect.", 
    "tags": [], 
    "importance": "High", 
    "heat": 58, 
    "link": "https://bugs.launchpad.net/nova/+bug/1004328", 
    "owner": "https://api.launchpad.net/1.0/~vishvananda", 
    "id": 1004328, 
    "index": 692, 
    "openned": "2012-05-25 06:44:18.821466+00:00", 
    "created": "2012-05-25 06:44:18.821466+00:00", 
    "title": "mountpoint doesn't work when a volume is attached to an instance", 
    "comments": [
        {
            "content": "OS: ubuntu 12.04, 64 bit.\nlibvirt_type=qemu\nI installed openstack via devstack.\nWhen I try to attach a volume to a running instance, the mountpoint of this instance always starts from /dev/vdb, though  I can specify the mountpoint, e.g. /dev/vdd. \nIt seems the mountpoint of the instance is taken in a certain order, from /dev/vdb, then /dev/vdc... to /dev/vdz. The mountpoint specified by the command does not take effect.", 
            "date_created": "2012-05-25 06:44:18.821466+00:00", 
            "author": "https://api.launchpad.net/1.0/~houshengbo"
        }, 
        {
            "content": "Can you tell us how to step by step reproduce this? \r\n\r\nThakns\r\nchuck", 
            "date_created": "2012-06-07 18:31:02.224050+00:00", 
            "author": "https://api.launchpad.net/1.0/~zulcss"
        }, 
        {
            "content": "This is an issue that will require a fix in libvirt and perhaps the virtio driver.  Libvirt/virtio doesn't pass enough information via the attach for the guest os to figure out where it should show up in the guest.", 
            "date_created": "2012-06-07 23:20:27.647724+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "Thank you for the explanations.\n\nTo reproduce: \nSimply attach a volume to a VM.\nTry command like: \nnova volume-attach $VM_ID $VOL_ID /dev/vdc\nOr attach a volume to a VM from the horizon UI, set the mountpoint to /dev/vdc\n\nExpected result:\n/dev/vdc should be found in the VM.\n\nActual result:\n/dev/vdb is found in the VM. No /dev/vdc is there.\n \nI have checked the log. Can anyone help me to analyze it a bit?\n\n===========Log of nova-compute========================\n2012-06-08 10:36:04 DEBUG nova.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-36c08116-7cfc-4254-\nb896-44a1b6f058fb', u'_context_quota_class': None, u'args': {u'instance_uuid': u'ec4de940-0524-411c-859d-b8b2a12964a1', u'mountpoint':\n u'/dev/vdc', u'volume_id': u'7e7a36a2-8b51-43b7-b8b9-130e702fdb7a'}, u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': Tru\ne, u'version': u'1.0', u'_context_project_id': u'ce27e4b763244d54b665315121ad89f5', u'_context_timestamp': u'2012-06-08T02:36:04.30783\n5', u'_context_read_deleted': u'no', u'_context_user_id': u'c2c604090e664f068bd9a1031a6ae4e9', u'method': u'attach_volume', u'_context\n_remote_address': u'9.119.148.201'} from (pid=4955) _safe_log /opt/stack/nova/nova/rpc/common.py:198\n2012-06-08 10:36:04 DEBUG nova.rpc.amqp [-] unpacked context: {'user_id': u'c2c604090e664f068bd9a1031a6ae4e9', 'roles': [u'admin'], 't\nimestamp': u'2012-06-08T02:36:04.307835', 'auth_token': '<SANITIZED>', 'remote_address': u'9.119.148.201', 'quota_class': None, 'is_ad\nmin': True, 'request_id': u'req-36c08116-7cfc-4254-b896-44a1b6f058fb', 'project_id': u'ce27e4b763244d54b665315121ad89f5', 'read_delete\nd': u'no'} from (pid=4955) _safe_log /opt/stack/nova/nova/rpc/common.py:198\n2012-06-08 10:36:04 INFO nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244\nd54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] check_instance_lock: decorating: |<function attach_volume at 0x1\n88faa0>|\n2012-06-08 10:36:04 INFO nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244\nd54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] check_instance_lock: arguments: |<nova.compute.manager.ComputeMa\nnager object at 0xd6ad10>| |<nova.rpc.amqp.RpcContext object at 0x35909d0>|\n2012-06-08 10:36:04 DEBUG nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b76324\n4d54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] Getting locked state from (pid=4955) get_lock /opt/stack/nova/n\nova/compute/manager.py:1677\n2012-06-08 10:36:04 INFO nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244\nd54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] check_instance_lock: locked: |False|\n2012-06-08 10:36:04 INFO nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244\nd54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] check_instance_lock: admin: |True|\n2012-06-08 10:36:04 INFO nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244\nd54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] check_instance_lock: executing: |<function attach_volume at 0x18\n8faa0>|\n2012-06-08 10:36:04 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Reloading cached file /etc/nova/policy.json from (pid=4955) read_cached_file /opt/stack/nova/nova/utils.py:1197\n2012-06-08 10:36:04 AUDIT nova.compute.manager [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b76324\n4d54b665315121ad89f5] [instance: ec4de940-0524-411c-859d-b8b2a12964a1] Attaching volume 7e7a36a2-8b51-43b7-b8b9-130e702fdb7a to /dev/v\ndc\n2012-06-08 10:36:04 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Running cmd (subprocess): sudo /usr/local/bin/nova-rootwrap cat /etc/iscsi/initiatorname.iscsi from (pid=4955) execute /op\nt/stack/nova/nova/utils.py:176\nException AssertionError: AssertionError() in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\n2012-06-08 10:36:05 DEBUG nova.rpc.amqp [-] Making asynchronous call on volume.openstacknode ... from (pid=4955) multicall /opt/stack/\nnova/nova/rpc/amqp.py:349\n2012-06-08 10:36:05 DEBUG nova.rpc.amqp [-] MSG_ID is f89cb75138744d47a3e4e2a0ab22c320 from (pid=4955) multicall /opt/stack/nova/nova/\nrpc/amqp.py:352\n2012-06-08 10:36:07 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Attempting to grab semaphore \"connect_volume\" for method \"connect_volume\"... from (pid=4955) inner /opt/stack/nova/nova/ut\nils.py:741\n2012-06-08 10:36:07 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Got semaphore \"connect_volume\" for method \"connect_volume\"... from (pid=4955) inner /opt/stack/nova/nova/utils.py:745\n2012-06-08 10:36:07 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Running cmd (subprocess): sudo /usr/local/bin/nova-rootwrap iscsiadm -m node -T iqn.2010-10.org.openstack:volume-7e7a36a2-\n8b51-43b7-b8b9-130e702fdb7a -p 9.119.148.201:3260 from (pid=4955) execute /opt/stack/nova/nova/utils.py:176\nException AssertionError: AssertionError() in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\n2012-06-08 10:36:07 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Result was 255 from (pid=4955) execute /opt/stack/nova/nova/utils.py:192\n2012-06-08 10:36:07 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Running cmd (subprocess): sudo /usr/local/bin/nova-rootwrap iscsiadm -m node -T iqn.2010-10.org.openstack:volume-7e7a36a2-\n8b51-43b7-b8b9-130e702fdb7a -p 9.119.148.201:3260 --op new from (pid=4955) execute /opt/stack/nova/nova/utils.py:176\nException AssertionError: AssertionError() in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\n2012-06-08 10:36:09 DEBUG nova.virt.libvirt.volume [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b7\n63244d54b665315121ad89f5] iscsiadm ('--login',): stdout=Logging in to [iface: default, target: iqn.2010-10.org.openstack:volume-7e7a36\na2-8b51-43b7-b8b9-130e702fdb7a, portal: 9.119.148.201,3260]\nLogin to [iface: default, target: iqn.2010-10.org.openstack:volume-7e7a36a2-8b51-43b7-b8b9-130e702fdb7a, portal: 9.119.148.201,3260]: \nsuccessful\n stderr= from (pid=4955) _run_iscsiadm /opt/stack/nova/nova/virt/libvirt/volume.py:108\n2012-06-08 10:36:09 DEBUG nova.utils [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b763244d54b66531\n5121ad89f5] Running cmd (subprocess): sudo /usr/local/bin/nova-rootwrap iscsiadm -m node -T iqn.2010-10.org.openstack:volume-7e7a36a2-\n8b51-43b7-b8b9-130e702fdb7a -p 9.119.148.201:3260 --op update -n node.startup -v automatic from (pid=4955) execute /opt/stack/nova/nov\na/utils.py:176\nException AssertionError: AssertionError() in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\n2012-06-08 10:36:10 DEBUG nova.virt.libvirt.volume [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b7\n63244d54b665315121ad89f5] iscsiadm ('--op', 'update', '-n', 'node.startup', '-v', 'automatic'): stdout= stderr= from (pid=4955) _run_i\nscsiadm /opt/stack/nova/nova/virt/libvirt/volume.py:108\n2012-06-08 10:36:10 DEBUG nova.virt.libvirt.config [req-36c08116-7cfc-4254-b896-44a1b6f058fb c2c604090e664f068bd9a1031a6ae4e9 ce27e4b7\n63244d54b665315121ad89f5] Generated XML <disk type=\"block\" device=\"disk\">\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\"/>\n  <source dev=\"/dev/disk/by-path/ip-9.119.148.201:3260-iscsi-iqn.2010-10.org.openstack:volume-7e7a36a2-8b51-43b7-b8b9-130e702fdb7a-lun\n-1\"/>\n  <target bus=\"virtio\" dev=\"vdc\"/>\n</disk>\n  from (pid=4955) to_xml /opt/stack/nova/nova/virt/libvirt/config.py:62\n2012-06-08 10:36:12 DEBUG nova.rpc.amqp [-] Making asynchronous call on volume.openstacknode ... from (pid=4955) multicall /opt/stack/nova/nova/rpc/amqp.py:349\n2012-06-08 10:36:12 DEBUG nova.rpc.amqp [-] MSG_ID is 17f14621807041c2a9722fb2bd6039e8 from (pid=4955) multicall /opt/stack/nova/nova/rpc/amqp.py:352\n2012-06-08 10:36:50 DEBUG nova.manager [-] Running periodic task ComputeManager._publish_service_capabilities from (pid=4955) periodic_tasks /opt/stack/nova/nova/manager.py:164\n2012-06-08 10:36:50 DEBUG nova.manager [-] Notifying Schedulers of capabilities ... from (pid=4955) _publish_service_capabilities /opt/stack/nova/nova/manager.py:216\n2012-06-08 10:36:50 DEBUG nova.rpc.amqp [-] Making asynchronous fanout cast... from (pid=4955) fanout_cast /opt/stack/nova/nova/rpc/amqp.py:382\n2012-06-08 10:36:50 DEBUG nova.manager [-] Running periodic task ComputeManager._poll_rescued_instances from (pid=4955) periodic_tasks /opt/stack/nova/nova/manager.py:164\n2012-06-08 10:36:50 DEBUG nova.manager [-] Skipping ComputeManager._sync_power_states, 7 ticks left until next run from (pid=4955) periodic_tasks /opt/stack/nova/nova/manager.py:159\n2012-06-08 10:36:50 DEBUG nova.manager [-] Running periodic task ComputeManager._poll_bandwidth_usage from (pid=4955) periodic_tasks /opt/stack/nova/nova/manager.py:164", 
            "date_created": "2012-06-08 02:44:50.213140+00:00", 
            "author": "https://api.launchpad.net/1.0/~houshengbo"
        }, 
        {
            "content": "How to reproduce:\n1. Start a VM $VM_ID from an image.\n2. Create a volume with $VOL_ID.\n3.Try command like: nova volume-attach $VM_ID $VOL_ID /dev/vdc\nOr attach a volume to a VM from the horizon UI, set the mountpoint to /dev/vdc\n\nExpected result:\n/dev/vdc should be found in the VM. This can be checked after ssh this VM.\n\nActual result:\n/dev/vdb is found in the VM. No /dev/vdc is there.\n", 
            "date_created": "2012-06-08 09:25:45.518661+00:00", 
            "author": "https://api.launchpad.net/1.0/~houshengbo"
        }, 
        {
            "content": "This is something that from what I gather is NOT likely to change in libvirt.  Wondering if it would be possible to switch to using /dev/disk/by-id?\n\nThen we eliminate the confusion we have in this case, and we can auotmatically derive the mountpoint via the volume now that we've switched to uuid's?", 
            "date_created": "2012-08-01 19:06:55.456447+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/10908", 
            "date_created": "2012-08-06 19:21:34.699420+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/10908\nCommitted: http://github.com/openstack/nova/commit/e44751162b09c5b57557b89db27656b5bd23341c\nSubmitter: Jenkins\nBranch:    master\n\ncommit e44751162b09c5b57557b89db27656b5bd23341c\nAuthor: Vishvananda Ishaya <email address hidden>\nDate:   Mon Aug 6 12:17:43 2012 -0700\n\n    Allow nova to guess device if not passed to attach\n    \n    partial fix for bug 1004328\n    \n    Only the xen hypervisor actually respects the device name that\n    is passed in attach_volume. For other hypervisors it makes much\n    more sense to automatically generate a unique name.\n    \n    This patch generates a non-conflicting device name if one is not\n    passed to attach_volume. It also validates the passed in volume\n    name to make sure another device isn't already attached there.\n    \n    A corresponding change to novaclient and horizon will greatly\n    improve the user experience of attaching a volume.\n    \n    It moves some common code out of metadata/base so that it can\n    be used to get a list of block devices. The code was functionally\n    tested as well and block device name generation works properly.\n    \n    This adds a new method to the rpcapi to validate a device name. It\n    also adds server_id to the volumes extension, since it was omitted\n    by mistake.\n    \n    The next step is to modify the libvirt driver to match the serial\n    number of the device to the volume uuid so that the volume can\n    always be found at /dev/disk/by-id/virtio-<uuid>.\n    \n    DocImpact\n    \n    Change-Id: I0b9454fc50a5c93b4aea38545dcee98f68d7e511\n", 
            "date_created": "2012-08-16 01:16:08.070618+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/11492", 
            "date_created": "2012-08-16 18:48:34.583118+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/11493", 
            "date_created": "2012-08-16 18:48:41.050241+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "So many patches for this bug? Which one is THE ONE to be checked?", 
            "date_created": "2012-08-20 04:40:40.700405+00:00", 
            "author": "https://api.launchpad.net/1.0/~houshengbo"
        }, 
        {
            "content": "This fix will most likely be isolated to Nova compute/libvirt code only, wait until this closes on the Nova side to determine if there is a needed patch for Cinder", 
            "date_created": "2012-08-21 20:16:23.723681+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/11492\nCommitted: http://github.com/openstack/nova/commit/1e7769cf5587c1ce92f206b39fe646975b19fc95\nSubmitter: Jenkins\nBranch:    master\n\ncommit 1e7769cf5587c1ce92f206b39fe646975b19fc95\nAuthor: Vishvananda Ishaya <email address hidden>\nDate:   Thu Aug 16 10:58:33 2012 -0700\n\n    Adds support for serial to libvirt config disks.\n    \n    In order for users to find a volume that they have attached to\n    a vm, it is valuable to be able to find it in a consistent\n    location. A following patch wil accomplish this by setting\n    the serial number of the device to the uuid of the volume.\n    \n    This patch prepares for that change by allowing serial numbers\n    to be set in the libvirt config disk object.\n    \n    Prepares to fix bug 1004328\n    \n    Change-Id: Iecdfc17b45e1c38df50f844f127c0e95558ab22c\n", 
            "date_created": "2012-08-23 03:40:07.086472+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/11493\nCommitted: http://github.com/openstack/nova/commit/3a47c02c58cefed0e230190b4bcef14527c82709\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3a47c02c58cefed0e230190b4bcef14527c82709\nAuthor: Vishvananda Ishaya <email address hidden>\nDate:   Thu Aug 16 11:28:27 2012 -0700\n\n    Allows libvirt to set a serial number for a volume\n    \n    The serial number defaults to the volume_id of the volume being\n    attached. We may expose a method in the future to set a different\n    serial number when creating or attaching a volume.\n    \n    The purpose of this change is to give users a consistent place\n    they can find their volume. It should show up now in most flavors\n    of linux under /disk/by-id/virtio-<volume_uuid>\n    \n    Fixes bug 1004328\n    \n    Change-Id: Id1c56b5b23d799deb7da2d39ae57ecb48965c55f\n", 
            "date_created": "2012-08-23 18:23:10.226010+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This workaround does not work with heat templates that assume that the volumes they create can be attached with the device names they have assigned. This means some working EC2 templates will need to be substantially reworked to work on openstack.", 
            "date_created": "2013-09-26 19:45:18.574121+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-fox-y"
        }, 
        {
            "content": "In fact, this workaround doesn't seem to work for me in grizzly. I attached 4 volumes and only two of them got virtio serial numbers:\n\n[root@mongodbreplicasetmembertest3 ~]# ls -l /dev/vd*\nbrw-rw----. 1 root disk 252,  0 Sep 26 16:00 /dev/vda\nbrw-rw----. 1 root disk 252,  1 Sep 26 16:00 /dev/vda1\nbrw-rw----. 1 root disk 252,  2 Sep 26 16:00 /dev/vda2\nbrw-rw----. 1 root disk 252, 16 Sep 26 16:00 /dev/vdb\nbrw-rw----. 1 root disk 252, 32 Sep 26 16:00 /dev/vdc\nbrw-rw----. 1 root disk 252, 48 Sep 26 16:00 /dev/vdd\nbrw-rw----. 1 root disk 252, 64 Sep 26 16:00 /dev/vde\n[root@mongodbreplicasetmembertest3 ~]# ls -l /dev/disk/by-id/virtio-*\nlrwxrwxrwx. 1 root root 9 Sep 26 16:00 /dev/disk/by-id/virtio-cef82203-43de-4915-8 -> ../../vdb\nlrwxrwxrwx. 1 root root 9 Sep 26 16:00 /dev/disk/by-id/virtio-def5e6dd-c536-4667-9 -> ../../vdc\n", 
            "date_created": "2013-09-26 20:08:24.329270+00:00", 
            "author": "https://api.launchpad.net/1.0/~kevin-fox-y"
        }
    ], 
    "closed": "2012-09-19 06:33:09.308177+00:00"
}
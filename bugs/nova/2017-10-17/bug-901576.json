{
    "status": "Invalid", 
    "last_updated": "2012-02-02 11:27:54.708679+00:00", 
    "description": "this is the nova-compute.log on the compute node :\n\n\n######################################################\n\n2011-12-08 15:52:46,998 nova.rpc: received {u'_context_request_id': u'9FNVLGNTMY                                                             \n\nOPEY-H9--E', u'_context_read_deleted': False, u'args': {u'instance_id': 55}, u'_                                                             \n\ncontext_is_admin': True, u'_context_timestamp': u'2011-12-08T07:52:46Z', u'_cont                                                             \n\next_user': u'admin', u'method': u'terminate_instance', u'_context_project': u'cl                                                             \n\noud', u'_context_remote_address': u'192.168.1.91'}\n2011-12-08 15:52:46,999 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n\n7:52:46Z', 'remote_address': u'192.168.1.91', 'project': u'cloud', 'is_admin': T                                                             \n\nrue, 'user': u'admin', 'request_id': u'9FNVLGNTMYOPEY-H9--E', 'read_deleted': Fa                                                             \n\nlse}\n2011-12-08 15:52:47,001 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: decorating: |<function terminate_instance at 0xa8fc17                                                             \n\nc>|\n2011-12-08 15:52:47,002 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: arguments: |<nova.compute.manager.ComputeManager obje                                                             \n\nct at 0xa8fb90c>| |<nova.context.RequestContext object at 0xb6524ec>| |55|\n2011-12-08 15:52:47,003 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] instance 55: getting locked state\n2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: locked: |False|\n2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: admin: |True|\n2011-12-08 15:52:47,075 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: executing: |<function terminate_instance at 0xa8fc17c                                                             \n\n>|\n2011-12-08 15:52:47,113 nova.compute.manager: AUDIT [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] Terminating instance 55\n2011-12-08 15:52:47,123 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] Deallocating address 10.0.0.2\n2011-12-08 15:52:47,147 nova.virt.libvirt_conn: Error encountered when destroyin                                                             \n\ng instance '55': Instance instance-00000037 not found\n2011-12-08 15:52:47,217 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:47,218 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:47,227 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t filter\n2011-12-08 15:52:47,261 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:47,299 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t nat\n2011-12-08 15:52:47,335 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:47,402 nova.virt.libvirt_conn: instance instance-00000037: dele                                                             \n\nting instance files /var/lib/nova/instances/instance-00000037\n2011-12-08 15:52:51,302 nova.rpc: received {u'_context_request_id': u'UMX3F9KRTC                                                             \n\nO4JMAYABE0', u'_context_read_deleted': False, u'args': {u'instance_id': 56, u'in                                                             \n\njected_files': None, u'availability_zone': None}, u'_context_is_admin': True, u'                                                             \n\n_context_timestamp': u'2011-12-08T07:52:50Z', u'_context_user': u'admin', u'meth                                                             \n\nod': u'run_instance', u'_context_project': u'cloud', u'_context_remote_address':                                                              \n\nu'192.168.1.91'}\n2011-12-08 15:52:51,304 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n\n7:52:50Z', 'remote_address': u'192.168.1.91', 'project': u'cloud', 'is_admin': T                                                             \n\nrue, 'user': u'admin', 'request_id': u'UMX3F9KRTCO4JMAYABE0', 'read_deleted': Fa                                                             \n\nlse}\n2011-12-08 15:52:51,354 nova.compute.manager: AUDIT [UMX3F9KRTCO4JMAYABE0 admin                                                              \n\ncloud] instance 56: starting...\n2011-12-08 15:52:51,485 nova.rpc: Making asynchronous call on network.nova-cc ..                                                             \n\n.\n2011-12-08 15:52:51,486 nova.rpc: MSG_ID is 5203ad18acbf405cbe51733809b7eab6\n2011-12-08 15:52:52,237 nova.utils: Attempting to grab semaphore \"ensure_bridge\"                                                              \n\nfor method \"ensure_bridge\"...\n2011-12-08 15:52:52,237 nova.utils: Attempting to grab file lock \"ensure_bridge\"                                                              \n\nfor method \"ensure_bridge\"...\n2011-12-08 15:52:52,237 nova.utils: Running cmd (subprocess): ip link show dev b                                                             \n\nr0\n2011-12-08 15:52:52,249 nova.utils: Running cmd (subprocess): sudo route -n\n2011-12-08 15:52:52,277 nova.utils: Running cmd (subprocess): sudo ip addr show                                                              \n\ndev eth0 scope global\n2011-12-08 15:52:52,310 nova.utils: Running cmd (subprocess): sudo brctl addif b                                                             \n\nr0 eth0\n2011-12-08 15:52:52,342 nova.utils: Result was 1\n2011-12-08 15:52:52,418 nova.virt.libvirt_conn: instance instance-00000038: star                                                             \n\nting toXML method\n2011-12-08 15:52:52,514 nova.virt.libvirt_conn: instance instance-00000038: fini                                                             \n\nshed toXML method\n2011-12-08 15:52:52,589 nova: called setup_basic_filtering in nwfilter\n2011-12-08 15:52:52,589 nova: ensuring static filters\n2011-12-08 15:52:52,654 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n\ne object at 0xb47bb6c>\n2011-12-08 15:52:52,655 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n\ne object at 0xb47be8c>\n2011-12-08 15:52:52,665 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:52,665 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:52,675 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t filter\n2011-12-08 15:52:52,707 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:52,744 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t nat\n2011-12-08 15:52:52,777 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:52,869 nova.utils: Running cmd (subprocess): mkdir -p /var/lib/                                                             \n\nnova/instances/instance-00000038/\n2011-12-08 15:52:52,880 nova.virt.libvirt_conn: instance instance-00000038: Crea                                                             \n\nting image\n2011-12-08 15:52:52,984 nova.utils: Attempting to grab semaphore \"0000000d\" for                                                              \n\nmethod \"call_if_not_exists\"...\n2011-12-08 15:52:52,985 nova.utils: Running cmd (subprocess): cp /var/lib/nova/i                                                             \n\nnstances/_base/0000000d /var/lib/nova/instances/instance-00000038/kernel\n2011-12-08 15:52:53,042 nova.utils: Attempting to grab semaphore \"0000000e_sm\" f                                                             \n\nor method \"call_if_not_exists\"...\n2011-12-08 15:52:53,043 nova.utils: Running cmd (subprocess): qemu-img create -f                                                              \n\nqcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/0000000e_sm                                                              \n\n/var/lib/nova/instances/instance-00000038/disk\n2011-12-08 15:52:53,257 nova.virt.libvirt_conn: instance instance-00000038: inje                                                             \n\ncting key into image 14\n2011-12-08 15:52:53,276 nova.utils: Running cmd (subprocess): sudo qemu-nbd -c /                                                             \n\ndev/nbd15 /var/lib/nova/instances/instance-00000038/disk\n2011-12-08 15:52:54,332 nova.utils: Running cmd (subprocess): sudo tune2fs -c 0                                                              \n\n-i 0 /dev/nbd15\n2011-12-08 15:52:54,365 nova.utils: Result was 1\n2011-12-08 15:52:54,366 nova.utils: Running cmd (subprocess): sudo qemu-nbd -d /                                                             \n\ndev/nbd15\n2011-12-08 15:52:54,392 nova.virt.libvirt_conn: instance instance-00000038: igno                                                             \n\nring error injecting data into image 14 (Unexpected error while running command.\nCommand: sudo tune2fs -c 0 -i 0 /dev/nbd15\nExit code: 1\nStdout: 'tune2fs 1.41.14 (22-Dec-2010)\\n'\nStderr: \"tune2fs: Invalid argument while trying to open /dev/nbd15\\nCouldn't fin                                                             \n\nd valid filesystem superblock.\\n\")\n2011-12-08 15:53:26,317 nova.exception: Uncaught exception\n(nova.exception): TRACE: Traceback (most recent call last):\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 120, in _wrap\n(nova.exception): TRACE:     return f(*args, **kw)\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 617, in spawn\n(nova.exception): TRACE:     domain = self._create_new_domain(xml)\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 1079, in _create_new_domain\n(nova.exception): TRACE:     domain.createWithFlags(launch_flags)\n(nova.exception): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 337, in createWithFlags\n(nova.exception): TRACE:     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n(nova.exception): TRACE: libvirtError: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n(nova.exception): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n(nova.exception): TRACE:\n(nova.exception): TRACE:\n2011-12-08 15:53:26,318 nova.compute.manager: ERROR [UMX3F9KRTCO4JMAYABE0 admin cloud] Instance '56' failed to spawn. Is virtualization \n\nenabled in the BIOS?\n(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/compute/manager.py\", line 234, in run_instance\n(nova.compute.manager): TRACE:     self.driver.spawn(instance_ref)\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 126, in _wrap\n(nova.compute.manager): TRACE:     raise Error(str(e))\n(nova.compute.manager): TRACE: Error: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n(nova.compute.manager): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n(nova.compute.manager): TRACE:\n(nova.compute.manager): TRACE:\n2011-12-08 15:53:26,538 nova.compute.manager: Found instance 'instance-00000038' in DB but no VM. State=5, so setting state to shutoff.\n\n\n\n\n\n###################################################", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/901576", 
    "owner": "None", 
    "id": 901576, 
    "index": 3689, 
    "openned": "2011-12-08 08:29:47.882754+00:00", 
    "created": "2011-12-08 08:29:47.882754+00:00", 
    "title": "I can not start the instance on the compute node", 
    "comments": [
        {
            "content": "this is the nova-compute.log on the compute node :\n\n\n######################################################\n\n2011-12-08 15:52:46,998 nova.rpc: received {u'_context_request_id': u'9FNVLGNTMY                                                             \n\nOPEY-H9--E', u'_context_read_deleted': False, u'args': {u'instance_id': 55}, u'_                                                             \n\ncontext_is_admin': True, u'_context_timestamp': u'2011-12-08T07:52:46Z', u'_cont                                                             \n\next_user': u'admin', u'method': u'terminate_instance', u'_context_project': u'cl                                                             \n\noud', u'_context_remote_address': u'192.168.1.91'}\n2011-12-08 15:52:46,999 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n\n7:52:46Z', 'remote_address': u'192.168.1.91', 'project': u'cloud', 'is_admin': T                                                             \n\nrue, 'user': u'admin', 'request_id': u'9FNVLGNTMYOPEY-H9--E', 'read_deleted': Fa                                                             \n\nlse}\n2011-12-08 15:52:47,001 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: decorating: |<function terminate_instance at 0xa8fc17                                                             \n\nc>|\n2011-12-08 15:52:47,002 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: arguments: |<nova.compute.manager.ComputeManager obje                                                             \n\nct at 0xa8fb90c>| |<nova.context.RequestContext object at 0xb6524ec>| |55|\n2011-12-08 15:52:47,003 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] instance 55: getting locked state\n2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: locked: |False|\n2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: admin: |True|\n2011-12-08 15:52:47,075 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n\nloud] check_instance_lock: executing: |<function terminate_instance at 0xa8fc17c                                                             \n\n>|\n2011-12-08 15:52:47,113 nova.compute.manager: AUDIT [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] Terminating instance 55\n2011-12-08 15:52:47,123 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n\ncloud] Deallocating address 10.0.0.2\n2011-12-08 15:52:47,147 nova.virt.libvirt_conn: Error encountered when destroyin                                                             \n\ng instance '55': Instance instance-00000037 not found\n2011-12-08 15:52:47,217 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:47,218 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:47,227 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t filter\n2011-12-08 15:52:47,261 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:47,299 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t nat\n2011-12-08 15:52:47,335 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:47,402 nova.virt.libvirt_conn: instance instance-00000037: dele                                                             \n\nting instance files /var/lib/nova/instances/instance-00000037\n2011-12-08 15:52:51,302 nova.rpc: received {u'_context_request_id': u'UMX3F9KRTC                                                             \n\nO4JMAYABE0', u'_context_read_deleted': False, u'args': {u'instance_id': 56, u'in                                                             \n\njected_files': None, u'availability_zone': None}, u'_context_is_admin': True, u'                                                             \n\n_context_timestamp': u'2011-12-08T07:52:50Z', u'_context_user': u'admin', u'meth                                                             \n\nod': u'run_instance', u'_context_project': u'cloud', u'_context_remote_address':                                                              \n\nu'192.168.1.91'}\n2011-12-08 15:52:51,304 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n\n7:52:50Z', 'remote_address': u'192.168.1.91', 'project': u'cloud', 'is_admin': T                                                             \n\nrue, 'user': u'admin', 'request_id': u'UMX3F9KRTCO4JMAYABE0', 'read_deleted': Fa                                                             \n\nlse}\n2011-12-08 15:52:51,354 nova.compute.manager: AUDIT [UMX3F9KRTCO4JMAYABE0 admin                                                              \n\ncloud] instance 56: starting...\n2011-12-08 15:52:51,485 nova.rpc: Making asynchronous call on network.nova-cc ..                                                             \n\n.\n2011-12-08 15:52:51,486 nova.rpc: MSG_ID is 5203ad18acbf405cbe51733809b7eab6\n2011-12-08 15:52:52,237 nova.utils: Attempting to grab semaphore \"ensure_bridge\"                                                              \n\nfor method \"ensure_bridge\"...\n2011-12-08 15:52:52,237 nova.utils: Attempting to grab file lock \"ensure_bridge\"                                                              \n\nfor method \"ensure_bridge\"...\n2011-12-08 15:52:52,237 nova.utils: Running cmd (subprocess): ip link show dev b                                                             \n\nr0\n2011-12-08 15:52:52,249 nova.utils: Running cmd (subprocess): sudo route -n\n2011-12-08 15:52:52,277 nova.utils: Running cmd (subprocess): sudo ip addr show                                                              \n\ndev eth0 scope global\n2011-12-08 15:52:52,310 nova.utils: Running cmd (subprocess): sudo brctl addif b                                                             \n\nr0 eth0\n2011-12-08 15:52:52,342 nova.utils: Result was 1\n2011-12-08 15:52:52,418 nova.virt.libvirt_conn: instance instance-00000038: star                                                             \n\nting toXML method\n2011-12-08 15:52:52,514 nova.virt.libvirt_conn: instance instance-00000038: fini                                                             \n\nshed toXML method\n2011-12-08 15:52:52,589 nova: called setup_basic_filtering in nwfilter\n2011-12-08 15:52:52,589 nova: ensuring static filters\n2011-12-08 15:52:52,654 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n\ne object at 0xb47bb6c>\n2011-12-08 15:52:52,655 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n\ne object at 0xb47be8c>\n2011-12-08 15:52:52,665 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:52,665 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n\nmethod \"apply\"...\n2011-12-08 15:52:52,675 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t filter\n2011-12-08 15:52:52,707 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:52,744 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n\n-t nat\n2011-12-08 15:52:52,777 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n\nore\n2011-12-08 15:52:52,869 nova.utils: Running cmd (subprocess): mkdir -p /var/lib/                                                             \n\nnova/instances/instance-00000038/\n2011-12-08 15:52:52,880 nova.virt.libvirt_conn: instance instance-00000038: Crea                                                             \n\nting image\n2011-12-08 15:52:52,984 nova.utils: Attempting to grab semaphore \"0000000d\" for                                                              \n\nmethod \"call_if_not_exists\"...\n2011-12-08 15:52:52,985 nova.utils: Running cmd (subprocess): cp /var/lib/nova/i                                                             \n\nnstances/_base/0000000d /var/lib/nova/instances/instance-00000038/kernel\n2011-12-08 15:52:53,042 nova.utils: Attempting to grab semaphore \"0000000e_sm\" f                                                             \n\nor method \"call_if_not_exists\"...\n2011-12-08 15:52:53,043 nova.utils: Running cmd (subprocess): qemu-img create -f                                                              \n\nqcow2 -o cluster_size=2M,backing_file=/var/lib/nova/instances/_base/0000000e_sm                                                              \n\n/var/lib/nova/instances/instance-00000038/disk\n2011-12-08 15:52:53,257 nova.virt.libvirt_conn: instance instance-00000038: inje                                                             \n\ncting key into image 14\n2011-12-08 15:52:53,276 nova.utils: Running cmd (subprocess): sudo qemu-nbd -c /                                                             \n\ndev/nbd15 /var/lib/nova/instances/instance-00000038/disk\n2011-12-08 15:52:54,332 nova.utils: Running cmd (subprocess): sudo tune2fs -c 0                                                              \n\n-i 0 /dev/nbd15\n2011-12-08 15:52:54,365 nova.utils: Result was 1\n2011-12-08 15:52:54,366 nova.utils: Running cmd (subprocess): sudo qemu-nbd -d /                                                             \n\ndev/nbd15\n2011-12-08 15:52:54,392 nova.virt.libvirt_conn: instance instance-00000038: igno                                                             \n\nring error injecting data into image 14 (Unexpected error while running command.\nCommand: sudo tune2fs -c 0 -i 0 /dev/nbd15\nExit code: 1\nStdout: 'tune2fs 1.41.14 (22-Dec-2010)\\n'\nStderr: \"tune2fs: Invalid argument while trying to open /dev/nbd15\\nCouldn't fin                                                             \n\nd valid filesystem superblock.\\n\")\n2011-12-08 15:53:26,317 nova.exception: Uncaught exception\n(nova.exception): TRACE: Traceback (most recent call last):\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 120, in _wrap\n(nova.exception): TRACE:     return f(*args, **kw)\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 617, in spawn\n(nova.exception): TRACE:     domain = self._create_new_domain(xml)\n(nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 1079, in _create_new_domain\n(nova.exception): TRACE:     domain.createWithFlags(launch_flags)\n(nova.exception): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 337, in createWithFlags\n(nova.exception): TRACE:     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n(nova.exception): TRACE: libvirtError: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n(nova.exception): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n(nova.exception): TRACE:\n(nova.exception): TRACE:\n2011-12-08 15:53:26,318 nova.compute.manager: ERROR [UMX3F9KRTCO4JMAYABE0 admin cloud] Instance '56' failed to spawn. Is virtualization \n\nenabled in the BIOS?\n(nova.compute.manager): TRACE: Traceback (most recent call last):\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/compute/manager.py\", line 234, in run_instance\n(nova.compute.manager): TRACE:     self.driver.spawn(instance_ref)\n(nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 126, in _wrap\n(nova.compute.manager): TRACE:     raise Error(str(e))\n(nova.compute.manager): TRACE: Error: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n(nova.compute.manager): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n(nova.compute.manager): TRACE:\n(nova.compute.manager): TRACE:\n2011-12-08 15:53:26,538 nova.compute.manager: Found instance 'instance-00000038' in DB but no VM. State=5, so setting state to shutoff.\n\n\n\n\n\n###################################################", 
            "date_created": "2011-12-08 08:29:47.882754+00:00", 
            "author": "https://api.launchpad.net/1.0/~wulianmeng4643639"
        }, 
        {
            "content": "Looks like an error with an invalid image. What did you use as the image to boot ?", 
            "date_created": "2011-12-21 13:24:02.438344+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "ok,I got this down.But now run into another problem when use vm live_migration.\n\n\n\n\nafter following the guide on http://docs.openstack.org/diablo/openst ... html,still can not migrate running instance between compute nodes,(after a process which seems the vm is down,it is still running on the src compute node).\n\n(1)\nthe nova-compute.log on src compute node:\n************************************************************\n2011-12-19 14:16:37,153 nova.rpc: received {u'_msg_id': u'c0585a5ff89646c88d5c6c7960979abf', u'_context_read_deleted': False, u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'args': {u'filename': u'tmpItM9kK'}, u'_context_is_admin': True, u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_user': None, u'method': u'check_shared_storage_test_file', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:37,155 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:38,393 nova.rpc: received {u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'_context_read_deleted': False, u'args': {u'instance_id': 97, u'dest': u'nova-compute1'}, u'_context_is_admin': True, u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_user': None, u'method': u'live_migration', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:38,394 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:38,448 nova.rpc: Making asynchronous call on compute.nova-compute1 ...\n2011-12-19 14:16:38,448 nova.rpc: MSG_ID is 540bc88b09ef4d10ad3976f9f9e721b5\n*************************************************************************\nthe nova-compute.log on dest compute node\n\n*************************************************************************\n2011-12-19 14:16:36,707 nova.rpc: received {u'_msg_id': u'47e74582cb744768821188d297573f35', u'_context_read_deleted': False, u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_is_admin': True, u'_context_user': None, u'method': u'create_shared_storage_test_file', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:36,709 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:36,757 nova.compute.manager: Creating tmpfile /var/lib/nova/instances/tmpItM9kK to notify to other compute nodes that they should mount the same storage.\n2011-12-19 14:16:37,519 nova.rpc: received {u'_msg_id': u'44e0f50f02024646a26fb01498b4cfee', u'_context_read_deleted': False, u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'args': {u'filename': u'tmpItM9kK'}, u'_context_is_admin': True, u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_user': None, u'method': u'cleanup_shared_storage_test_file', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:37,520 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:37,924 nova.rpc: received {u'_msg_id': u'19bba6964f5b4ec69d37d2bbedbc0817', u'_context_read_deleted': False, u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'args': {u'cpu_info': u'{\"vendor\": \"AMD\", \"model\": \"Opteron_G3\", \"arch\": \"i686\", \"features\": [\"wdt\", \"skinit\", \"osvw\", \"3dnowprefetch\", \"cr8legacy\", \"extapic\", \"cmp_legacy\", \"3dnow\", \"3dnowext\", \"pdpe1gb\", \"fxsr_opt\", \"mmxext\", \"ht\", \"vme\"], \"topology\": {\"cores\": \"4\", \"threads\": \"1\", \"sockets\": \"1\"}}'}, u'_context_is_admin': True, u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_user': None, u'method': u'compare_cpu', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:37,926 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:37,927 nova.virt.libvirt_conn: Instance launched has CPU info:\n{\"vendor\": \"AMD\", \"model\": \"Opteron_G3\", \"arch\": \"i686\", \"features\": [\"wdt\", \"skinit\", \"osvw\", \"3dnowprefetch\", \"cr8legacy\", \"extapic\", \"cmp_legacy\", \"3dnow\", \"3dnowext\", \"pdpe1gb\", \"fxsr_opt\", \"mmxext\", \"ht\", \"vme\"], \"topology\": {\"cores\": \"4\", \"threads\": \"1\", \"sockets\": \"1\"}}\n2011-12-19 14:16:37,929 nova.virt.libvirt_conn: to xml...\n:<cpu>\n<arch>i686</arch>\n<model>Opteron_G3</model>\n<vendor>AMD</vendor>\n<topology sockets=\"1\" cores=\"4\" threads=\"1\"/>\n<features name=\"wdt\" />\n<features name=\"skinit\" />\n<features name=\"osvw\" />\n<features name=\"3dnowprefetch\" />\n<features name=\"cr8legacy\" />\n<features name=\"extapic\" />\n<features name=\"cmp_legacy\" />\n<features name=\"3dnow\" />\n<features name=\"3dnowext\" />\n<features name=\"pdpe1gb\" />\n<features name=\"fxsr_opt\" />\n<features name=\"mmxext\" />\n<features name=\"ht\" />\n<features name=\"vme\" />\n</cpu>\n\n2011-12-19 14:16:38,633 nova.rpc: received {u'_msg_id': u'540bc88b09ef4d10ad3976f9f9e721b5', u'_context_read_deleted': False, u'_context_request_id': u'WCEGAO1A5TAK6QSJIU7C', u'args': {u'instance_id': 97}, u'_context_is_admin': True, u'_context_timestamp': u'2011-12-19T06:16:36Z', u'_context_user': None, u'method': u'pre_live_migration', u'_context_project': None, u'_context_remote_address': None}\n2011-12-19 14:16:38,634 nova.rpc: unpacked context: {'timestamp': u'2011-12-19T06:16:36Z', 'remote_address': None, 'project': None, 'is_admin': True, 'user': None, 'request_id': u'WCEGAO1A5TAK6QSJIU7C', 'read_deleted': False}\n2011-12-19 14:16:38,724 nova.compute.manager: i-00000061 has no volume.\n2011-12-19 14:16:38,748 nova.utils: Attempting to grab semaphore \"ensure_bridge\" for method \"ensure_bridge\"...\n2011-12-19 14:16:38,749 nova.utils: Attempting to grab file lock \"ensure_bridge\" for method \"ensure_bridge\"...\n2011-12-19 14:16:38,749 nova.utils: Running cmd (subprocess): ip link show dev br0\n2011-12-19 14:16:38,760 nova.utils: Running cmd (subprocess): sudo route -n\n2011-12-19 14:16:38,781 nova.utils: Running cmd (subprocess): sudo ip addr show dev eth0 scope global\n2011-12-19 14:16:38,814 nova.utils: Running cmd (subprocess): sudo brctl addif br0 eth0\n2011-12-19 14:16:38,846 nova.utils: Result was 1\n2011-12-19 14:16:38,890 nova: called setup_basic_filtering in nwfilter\n2011-12-19 14:16:38,890 nova: ensuring static filters\n2011-12-19 14:16:38,994 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0xb45bd6c>\n2011-12-19 14:16:38,994 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRule object at 0xb45b6ec>\n2011-12-19 14:16:39,004 nova.utils: Attempting to grab semaphore \"iptables\" for method \"apply\"...\n2011-12-19 14:16:39,004 nova.utils: Attempting to grab file lock \"iptables\" for method \"apply\"...\n2011-12-19 14:16:39,014 nova.utils: Running cmd (subprocess): sudo iptables-save -t filter\n2011-12-19 14:16:39,047 nova.utils: Running cmd (subprocess): sudo iptables-restore\n2011-12-19 14:16:39,084 nova.utils: Running cmd (subprocess): sudo iptables-save -t nat\n2011-12-19 14:16:39,118 nova.utils: Running cmd (subprocess): sudo iptables-restore\n************************************************************\nfrom nova-scheduler.log on the controller node ,there seems no error.\n\n\n(2)\non the controller node,when I use \u201cid nova\u201d the output is \"uid=118(nova) gid=65534(nogroup) groups=65534(nogroup),128(libvirtd)\"\nwhile on the compute node \"uid=117(nova) gid=65534(nogroup) groups=65534(nogroup),126(libvirtd)\"\n\n(3)\nand some of  my nova service is not running as root ,they are running as nova,\n*******************************************************************\nnova      6733     1  0 10:41 ?        00:00:00 su -c nova-network --flagfile=/etc/nova/nova.conf nova\nnova      6743  6733  2 10:41 ?        00:09:07 /usr/bin/python /usr/bin/nova-network --flagfile=/etc/nova/nova.conf\nnova      6754     1  0 10:41 ?        00:00:00 su -c nova-api --flagfile=/etc/nova/nova.conf nova\nroot      6784  6774  0 10:41 ?        00:00:00 /usr/bin/python /usr/bin/nova-objectstore --uid 118 --gid 65534 --pidfile /var/run/nova/nova-objectstore.pid --flagfile=/etc/nova/nova.conf --nodaemon --logfile=/var/log/nova/nova-objectstore.log\nnova      6803     1  0 10:41 ?        00:00:00 su -c nova-scheduler --flagfile=/etc/nova/nova.conf nova\nnova      6807  6754  0 10:41 ?        00:00:00 /usr/bin/python /usr/bin/nova-api --flagfile=/etc/nova/nova.conf\nnova      6826  6803  2 10:41 ?        00:06:38 /usr/bin/python /usr/bin/nova-scheduler --flagfile=/etc/nova/nova.conf\nnova     22023     1  0 15:51 ?        00:00:00 su -c nova-compute --flagfile=/etc/nova/nova.conf nova\nnova     22030 22023 20 15:51 ?        00:00:02 /usr/bin/python /usr/bin/nova-compute --flagfile=/etc/nova/nova.conf\nnova     22563     1  0 15:51 ?        00:00:00 su -c nova-volume --flagfile=/etc/nova/nova.conf nova\nnova     22570 22563 27 15:51 ?        00:00:00 /usr/bin/python /usr/bin/nova-volume --flagfile=/etc/nova/nova.conf\nroot     22578  7444  0 15:51 pts/0    00:00:00 grep --color=auto nova\n******************************************************************\n\n\nI wonder what caused the failure of the live_migration between the compute nodes.if it is caused by (2),could you tell me how to set them equal on the controller node and the compute node,while if it caused by (3),do I have to edit the /etc/init/nova-compute.conf just as /etc/init/nova-objectstore.conf(it seems is running as root)?\nAt 2011-12-21 21:24:02,\"Thierry Carrez\" <email address hidden> wrote:\n>Looks like an error with an invalid image. What did you use as the image\n>to boot ?\n>\n>** Changed in: nova\n>       Status: New => Incomplete\n>\n>-- \n>You received this bug notification because you are subscribed to the bug\n>report.\n>https://bugs.launchpad.net/bugs/901576\n>\n>Title:\n>  I can not start the instance on the compute node\n>\n>Status in OpenStack Compute (Nova):\n>  Incomplete\n>\n>Bug description:\n>  this is the nova-compute.log on the compute node :\n>\n>  \n>  ######################################################\n>\n>  2011-12-08 15:52:46,998 nova.rpc: received {u'_context_request_id':\n>  u'9FNVLGNTMY\n>\n>  OPEY-H9--E', u'_context_read_deleted': False, u'args':\n>  {u'instance_id': 55}, u'_\n>\n>  context_is_admin': True, u'_context_timestamp':\n>  u'2011-12-08T07:52:46Z', u'_cont\n>\n>  ext_user': u'admin', u'method': u'terminate_instance',\n>  u'_context_project': u'cl\n>\n>  oud', u'_context_remote_address': u'192.168.1.91'}\n>  2011-12-08 15:52:46,999 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n>\n>  7:52:46Z', 'remote_address': u'192.168.1.91', 'project': u'cloud',\n>  'is_admin': T\n>\n>  rue, 'user': u'admin', 'request_id': u'9FNVLGNTMYOPEY-H9--E',\n>  'read_deleted': Fa\n>\n>  lse}\n>  2011-12-08 15:52:47,001 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n>\n>  loud] check_instance_lock: decorating: |<function terminate_instance\n>  at 0xa8fc17\n>\n>  c>|\n>  2011-12-08 15:52:47,002 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n>\n>  loud] check_instance_lock: arguments:\n>  |<nova.compute.manager.ComputeManager obje\n>\n>  ct at 0xa8fb90c>| |<nova.context.RequestContext object at 0xb6524ec>| |55|\n>  2011-12-08 15:52:47,003 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n>\n>  cloud] instance 55: getting locked state\n>  2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n>\n>  loud] check_instance_lock: locked: |False|\n>  2011-12-08 15:52:47,074 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n>\n>  loud] check_instance_lock: admin: |True|\n>  2011-12-08 15:52:47,075 nova.compute.manager: INFO [9FNVLGNTMYOPEY-H9--E admin c                                                             \n>\n>  loud] check_instance_lock: executing: |<function terminate_instance at\n>  0xa8fc17c\n>\n>  >|\n>  2011-12-08 15:52:47,113 nova.compute.manager: AUDIT [9FNVLGNTMYOPEY-H9--E admin                                                              \n>\n>  cloud] Terminating instance 55\n>  2011-12-08 15:52:47,123 nova.compute.manager: DEBUG [9FNVLGNTMYOPEY-H9--E admin                                                              \n>\n>  cloud] Deallocating address 10.0.0.2\n>  2011-12-08 15:52:47,147 nova.virt.libvirt_conn: Error encountered when destroyin                                                             \n>\n>  g instance '55': Instance instance-00000037 not found\n>  2011-12-08 15:52:47,217 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n>\n>  method \"apply\"...\n>  2011-12-08 15:52:47,218 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n>\n>  method \"apply\"...\n>  2011-12-08 15:52:47,227 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n>\n>  -t filter\n>  2011-12-08 15:52:47,261 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n>\n>  ore\n>  2011-12-08 15:52:47,299 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n>\n>  -t nat\n>  2011-12-08 15:52:47,335 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n>\n>  ore\n>  2011-12-08 15:52:47,402 nova.virt.libvirt_conn: instance instance-00000037: dele                                                             \n>\n>  ting instance files /var/lib/nova/instances/instance-00000037\n>  2011-12-08 15:52:51,302 nova.rpc: received {u'_context_request_id': u'UMX3F9KRTC                                                             \n>\n>  O4JMAYABE0', u'_context_read_deleted': False, u'args':\n>  {u'instance_id': 56, u'in\n>\n>  jected_files': None, u'availability_zone': None},\n>  u'_context_is_admin': True, u'\n>\n>  _context_timestamp': u'2011-12-08T07:52:50Z', u'_context_user':\n>  u'admin', u'meth\n>\n>  od': u'run_instance', u'_context_project': u'cloud',\n>  u'_context_remote_address':\n>\n>  u'192.168.1.91'}\n>  2011-12-08 15:52:51,304 nova.rpc: unpacked context: {'timestamp': u'2011-12-08T0                                                             \n>\n>  7:52:50Z', 'remote_address': u'192.168.1.91', 'project': u'cloud',\n>  'is_admin': T\n>\n>  rue, 'user': u'admin', 'request_id': u'UMX3F9KRTCO4JMAYABE0',\n>  'read_deleted': Fa\n>\n>  lse}\n>  2011-12-08 15:52:51,354 nova.compute.manager: AUDIT [UMX3F9KRTCO4JMAYABE0 admin                                                              \n>\n>  cloud] instance 56: starting...\n>  2011-12-08 15:52:51,485 nova.rpc: Making asynchronous call on network.nova-cc ..                                                             \n>\n>  .\n>  2011-12-08 15:52:51,486 nova.rpc: MSG_ID is 5203ad18acbf405cbe51733809b7eab6\n>  2011-12-08 15:52:52,237 nova.utils: Attempting to grab semaphore \"ensure_bridge\"                                                              \n>\n>  for method \"ensure_bridge\"...\n>  2011-12-08 15:52:52,237 nova.utils: Attempting to grab file lock \"ensure_bridge\"                                                              \n>\n>  for method \"ensure_bridge\"...\n>  2011-12-08 15:52:52,237 nova.utils: Running cmd (subprocess): ip link show dev b                                                             \n>\n>  r0\n>  2011-12-08 15:52:52,249 nova.utils: Running cmd (subprocess): sudo route -n\n>  2011-12-08 15:52:52,277 nova.utils: Running cmd (subprocess): sudo ip addr show                                                              \n>\n>  dev eth0 scope global\n>  2011-12-08 15:52:52,310 nova.utils: Running cmd (subprocess): sudo brctl addif b                                                             \n>\n>  r0 eth0\n>  2011-12-08 15:52:52,342 nova.utils: Result was 1\n>  2011-12-08 15:52:52,418 nova.virt.libvirt_conn: instance instance-00000038: star                                                             \n>\n>  ting toXML method\n>  2011-12-08 15:52:52,514 nova.virt.libvirt_conn: instance instance-00000038: fini                                                             \n>\n>  shed toXML method\n>  2011-12-08 15:52:52,589 nova: called setup_basic_filtering in nwfilter\n>  2011-12-08 15:52:52,589 nova: ensuring static filters\n>  2011-12-08 15:52:52,654 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n>\n>  e object at 0xb47bb6c>\n>  2011-12-08 15:52:52,655 nova: <nova.db.sqlalchemy.models.SecurityGroupIngressRul                                                             \n>\n>  e object at 0xb47be8c>\n>  2011-12-08 15:52:52,665 nova.utils: Attempting to grab semaphore \"iptables\" for                                                              \n>\n>  method \"apply\"...\n>  2011-12-08 15:52:52,665 nova.utils: Attempting to grab file lock \"iptables\" for                                                              \n>\n>  method \"apply\"...\n>  2011-12-08 15:52:52,675 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n>\n>  -t filter\n>  2011-12-08 15:52:52,707 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n>\n>  ore\n>  2011-12-08 15:52:52,744 nova.utils: Running cmd (subprocess): sudo iptables-save                                                              \n>\n>  -t nat\n>  2011-12-08 15:52:52,777 nova.utils: Running cmd (subprocess): sudo iptables-rest                                                             \n>\n>  ore\n>  2011-12-08 15:52:52,869 nova.utils: Running cmd (subprocess): mkdir -p /var/lib/                                                             \n>\n>  nova/instances/instance-00000038/\n>  2011-12-08 15:52:52,880 nova.virt.libvirt_conn: instance instance-00000038: Crea                                                             \n>\n>  ting image\n>  2011-12-08 15:52:52,984 nova.utils: Attempting to grab semaphore \"0000000d\" for                                                              \n>\n>  method \"call_if_not_exists\"...\n>  2011-12-08 15:52:52,985 nova.utils: Running cmd (subprocess): cp /var/lib/nova/i                                                             \n>\n>  nstances/_base/0000000d /var/lib/nova/instances/instance-00000038/kernel\n>  2011-12-08 15:52:53,042 nova.utils: Attempting to grab semaphore \"0000000e_sm\" f                                                             \n>\n>  or method \"call_if_not_exists\"...\n>  2011-12-08 15:52:53,043 nova.utils: Running cmd (subprocess): qemu-img create -f                                                              \n>\n>  qcow2 -o\n>  cluster_size=2M,backing_file=/var/lib/nova/instances/_base/0000000e_sm\n>\n>  /var/lib/nova/instances/instance-00000038/disk\n>  2011-12-08 15:52:53,257 nova.virt.libvirt_conn: instance instance-00000038: inje                                                             \n>\n>  cting key into image 14\n>  2011-12-08 15:52:53,276 nova.utils: Running cmd (subprocess): sudo qemu-nbd -c /                                                             \n>\n>  dev/nbd15 /var/lib/nova/instances/instance-00000038/disk\n>  2011-12-08 15:52:54,332 nova.utils: Running cmd (subprocess): sudo tune2fs -c 0                                                              \n>\n>  -i 0 /dev/nbd15\n>  2011-12-08 15:52:54,365 nova.utils: Result was 1\n>  2011-12-08 15:52:54,366 nova.utils: Running cmd (subprocess): sudo qemu-nbd -d /                                                             \n>\n>  dev/nbd15\n>  2011-12-08 15:52:54,392 nova.virt.libvirt_conn: instance instance-00000038: igno                                                             \n>\n>  ring error injecting data into image 14 (Unexpected error while running command.\n>  Command: sudo tune2fs -c 0 -i 0 /dev/nbd15\n>  Exit code: 1\n>  Stdout: 'tune2fs 1.41.14 (22-Dec-2010)\\n'\n>  Stderr: \"tune2fs: Invalid argument while trying to open /dev/nbd15\\nCouldn't fin                                                             \n>\n>  d valid filesystem superblock.\\n\")\n>  2011-12-08 15:53:26,317 nova.exception: Uncaught exception\n>  (nova.exception): TRACE: Traceback (most recent call last):\n>  (nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 120, in _wrap\n>  (nova.exception): TRACE:     return f(*args, **kw)\n>  (nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 617, in spawn\n>  (nova.exception): TRACE:     domain = self._create_new_domain(xml)\n>  (nova.exception): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/virt/libvirt_conn.py\", line 1079, in _create_new_domain\n>  (nova.exception): TRACE:     domain.createWithFlags(launch_flags)\n>  (nova.exception): TRACE:   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 337, in createWithFlags\n>  (nova.exception): TRACE:     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n>  (nova.exception): TRACE: libvirtError: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n>  (nova.exception): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n>  (nova.exception): TRACE:\n>  (nova.exception): TRACE:\n>  2011-12-08 15:53:26,318 nova.compute.manager: ERROR [UMX3F9KRTCO4JMAYABE0 admin cloud] Instance '56' failed to spawn. Is virtualization \n>\n>  enabled in the BIOS?\n>  (nova.compute.manager): TRACE: Traceback (most recent call last):\n>  (nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/compute/manager.py\", line 234, in run_instance\n>  (nova.compute.manager): TRACE:     self.driver.spawn(instance_ref)\n>  (nova.compute.manager): TRACE:   File \"/usr/lib/pymodules/python2.7/nova/exception.py\", line 126, in _wrap\n>  (nova.compute.manager): TRACE:     raise Error(str(e))\n>  (nova.compute.manager): TRACE: Error: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1\n>  (nova.compute.manager): TRACE: qemu: could not load kernel '/var/lib/nova/instances/instance-00000038/kernel': Inappropriate ioctl for device\n>  (nova.compute.manager): TRACE:\n>  (nova.compute.manager): TRACE:\n>  2011-12-08 15:53:26,538 nova.compute.manager: Found instance 'instance-00000038' in DB but no VM. State=5, so setting state to shutoff.\n>\n>\n>\n>  ###################################################\n>\n>To manage notifications about this bug go to:\n>https://bugs.launchpad.net/nova/+bug/901576/+subscriptions\n\n", 
            "date_created": "2011-12-22 01:23:05+00:00", 
            "author": "https://api.launchpad.net/1.0/~wulianmeng4643639"
        }
    ], 
    "closed": "2012-02-02 11:27:53.652568+00:00"
}
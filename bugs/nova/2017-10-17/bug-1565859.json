{
    "status": "Invalid", 
    "last_updated": "2017-08-11 09:46:49.119219+00:00", 
    "description": "Steps to Reproduce:\n1. Setup instance with a libvirt/KVM host and SVC storage\n2. Spawn a VM\n3. Attach a volume to the VM\n4. Wait for volume attachment to complete successfully\n5. Detach the volume\n\nExpected Result:\n1. The volume is detached from the VM\n2. The volume's status becomes \"Available\"\n3. The volume can be deleted.\n\nActual result:\n1. Volume remains attached to the VM (waited over 10 minutes)\n2. The volume's state stays \"In-Use\"\n\nLogs:\n016-03-24 16:34:13.852 143842 INFO nova.compute.resource_tracker [-] Final resource view: name=C387f19U21-KVM phys_ram=260533MB used_ram=4608MB phys_disk=545GB used_disk=40GB total_vcpus=160 used_vcpus=2 pci_stats=[]\n2016-03-24 16:34:14.081 143842 INFO nova.compute.resource_tracker [-] Compute_service record updated for C387f19U21_KVM:C387f19U21-KVM\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall [-] Dynamic interval looping call 'oslo_service.loopingcall._func' failed\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall Traceback (most recent call last):\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     result = func(*self.args, **self.kw)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     return self._sleep_time\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     self.force_reraise()\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     six.reraise(self.type_, self.value, self.tb)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     result = f(*args, **kwargs)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     reason=reason)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [req-fab1608e-cffe-40f9-82d0-a4c7a9cebf10 3ebcf1a38bc7b4977b7f8da32faad97bdef843372a670bb2817f8a066f042b9b e10bc17f58d8499a8fab1b05687123e5 - - -] [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] Failed to detach volume 4221ccad-0f98-4f78-ad06-92ea4941afc1 from /dev/vdb\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] Traceback (most recent call last):\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 4767, in _driver_detach_volume\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     encryption=encryption)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 1469, in detach_volume\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     wait_for_detach()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 385, in func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return evt.wait()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/eventlet/event.py\", line 121, in wait\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return hubs.get_hub().switch()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 294, in switch\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return self.greenlet.switch()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     result = func(*self.args, **self.kw)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return self._sleep_time\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     self.force_reraise()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     six.reraise(self.type_, self.value, self.tb)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     result = f(*args, **kwargs)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     reason=reason)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]", 
    "tags": [
        "libvirt", 
        "storwize", 
        "volumes"
    ], 
    "importance": "Undecided", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1565859", 
    "owner": "None", 
    "id": 1565859, 
    "index": 7405, 
    "openned": "2016-04-04 15:42:42.790459+00:00", 
    "created": "2016-04-04 15:42:42.790459+00:00", 
    "title": "Can't detach SVC volume from an instance; guest detach device times out", 
    "comments": [
        {
            "content": "Steps to Reproduce:\n1. Setup instance with a libvirt/KVM host and SVC storage\n2. Spawn a VM\n3. Attach a volume to the VM\n4. Wait for volume attachment to complete successfully\n5. Detach the volume\n\nExpected Result:\n1. The volume is detached from the VM\n2. The volume's status becomes \"Available\"\n3. The volume can be deleted.\n\nActual result:\n1. Volume remains attached to the VM (waited over 10 minutes)\n2. The volume's state stays \"In-Use\"\n\nLogs:\n016-03-24 16:34:13.852 143842 INFO nova.compute.resource_tracker [-] Final resource view: name=C387f19U21-KVM phys_ram=260533MB used_ram=4608MB phys_disk=545GB used_disk=40GB total_vcpus=160 used_vcpus=2 pci_stats=[]\n2016-03-24 16:34:14.081 143842 INFO nova.compute.resource_tracker [-] Compute_service record updated for C387f19U21_KVM:C387f19U21-KVM\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall [-] Dynamic interval looping call 'oslo_service.loopingcall._func' failed\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall Traceback (most recent call last):\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     result = func(*self.args, **self.kw)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     return self._sleep_time\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     self.force_reraise()\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     six.reraise(self.type_, self.value, self.tb)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     result = f(*args, **kwargs)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall     reason=reason)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-03-24 16:34:44.919 143842 ERROR oslo.service.loopingcall\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [req-fab1608e-cffe-40f9-82d0-a4c7a9cebf10 3ebcf1a38bc7b4977b7f8da32faad97bdef843372a670bb2817f8a066f042b9b e10bc17f58d8499a8fab1b05687123e5 - - -] [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] Failed to detach volume 4221ccad-0f98-4f78-ad06-92ea4941afc1 from /dev/vdb\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] Traceback (most recent call last):\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 4767, in _driver_detach_volume\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     encryption=encryption)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 1469, in detach_volume\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     wait_for_detach()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 385, in func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return evt.wait()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/eventlet/event.py\", line 121, in wait\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return hubs.get_hub().switch()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 294, in switch\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return self.greenlet.switch()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     result = func(*self.args, **self.kw)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     return self._sleep_time\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     self.force_reraise()\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     six.reraise(self.type_, self.value, self.tb)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     result = f(*args, **kwargs)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]     reason=reason)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6] DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-03-24 16:34:44.921 143842 ERROR nova.compute.manager [instance: 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6]", 
            "date_created": "2016-04-04 15:42:42.790459+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmtaylor"
        }, 
        {
            "content": "Are there errors in the cinder logs? Does the volume actually get detached via the cinder svc driver?", 
            "date_created": "2016-04-04 20:09:17.564525+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "What version of nova/cinder are you using?", 
            "date_created": "2016-04-04 20:09:27.481947+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Nevermind about detaching the volume from cinder, the compute manager tries to detach in the virt driver first, and then when that is done the compute manager terminates the connection via cinder API and then detaches the volume via cinder API.\n\nFollowing:\n\nhttps://github.com/openstack/nova/blob/master/nova/compute/manager.py#L4785\n\nDo you see anything in the libvirtd logs for the volume/instance uuid here?", 
            "date_created": "2016-04-04 20:14:25.854452+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Also, what version of nova/libvirt are you using?", 
            "date_created": "2016-04-04 20:15:16.920209+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Version of libvirt:   libvirtd (libvirt) 1.2.16\n\nrpm -qa| grep qemu\nqemu-kvm-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nqemu-img-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nlibvirt-daemon-qemu-1.2.16-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nipxe-roms-qemu-20130517-6.gitc4bce43.el7_1.1.noarch\nqemu-system-x86-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nqemu-common-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nlibvirt-daemon-driver-qemu-1.2.16-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nqemu-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nqemu-system-ppc-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\nqemu-kvm-tools-2.3-1.el7_1.pkvm3_1_0.4900.1.ppc64le\n\n\n", 
            "date_created": "2016-04-05 15:49:23.010487+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmtaylor"
        }, 
        {
            "content": "Version of Nova: 13.0.0\n\nrpm -qa| grep nova\nopenstack-nova-common-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-compute-prereqs-2013.1-201510201319.2.x86_64\nopenstack-nova-api-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-scheduler-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-conductor-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-cert-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-compute-13.0.0.0b1-201604011241.el7.271.noarch\npython-nova-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-network-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-13.0.0.0b1-201604011241.el7.271.noarch\npython-novaclient-3.3.0-1.el7.noarch\nopenstack-nova-novncproxy-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-cells-13.0.0.0b1-201604011241.el7.271.noarch\nopenstack-nova-console-13.0.0.0b1-201604011241.el7.271.noarch\n\n", 
            "date_created": "2016-04-05 15:53:30.354483+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmtaylor"
        }, 
        {
            "content": "Version of Cinder: 8.0.0\n\nrpm -qa | grep cinder\nopenstack-cinder-8.0.0.0b1-201604011230.el7.298.noarch\npython-cinder-8.0.0.0b1-201604011230.el7.298.noarch\npython-cinderclient-1.6.0-2.el7.noarch\n", 
            "date_created": "2016-04-05 15:54:28.689226+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmtaylor"
        }, 
        {
            "content": "Not seeing anything in the libvirt logs:\n\n2016-03-24 16:31:20.891+0000: starting up libvirt version: 1.2.16, package: 1.el7_1.pkvm3_1_0.4900.1 (Koji, 2016-01-12-15:09:11, bldple2), qemu version: 2.3.0\nLC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin QEMU_AUDIO_DRV=none /usr/bin/qemu-kvm -name tempest.commo-88724ae0-0000001a -S -machine pseries-2.4,accel=kvm,usb=off -m 2048 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 88724ae0-38d6-4b06-9236-7d5f4d6d6cd6 -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/tempest.commo-88724ae0-0000001a.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -no-shutdown -boot strict=on -device pci-ohci,id=usb,bus=pci.0,addr=0x2 -device spapr-vscsi,id=scsi0,reg=0x2000 -device spapr-vscsi,id=scsi1,reg=0x3000 -device spapr-vscsi,id=scsi2,reg=0x4000 -device spapr-vscsi,id=scsi3,reg=0x5000 -drive file=/mnt/nfs/nova/instances/88724ae0-38d6-4b06-9236-7d5f4d6d6cd6/disk,if=none,id=drive-virtio-disk0,format=qcow2,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x3,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive file=/mnt/nfs/nova/instances/88724ae0-38d6-4b06-9236-7d5f4d6d6cd6/disk.config,if=none,id=drive-scsi3-0-0-4,readonly=on,format=raw,cache=none -device scsi-cd,bus=scsi3.0,channel=0,scsi-id=0,lun=4,drive=drive-scsi3-0-0-4,id=scsi3-0-0-4 -netdev tap,fd=30,id=hostnet0,vhost=on,vhostfd=32 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:f0:cb:49,bus=pci.0,addr=0x1 -chardev pty,id=charserial0 -device spapr-vty,chardev=charserial0,reg=0x30000000 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4 -msg timestamp=on\nchar device redirected to /dev/pts/1 (label charserial0)\nqemu: terminating on signal 15 from pid 143764\n2016-03-24 16:38:01.731+0000: shutting down\n", 
            "date_created": "2016-04-05 16:07:39.248918+00:00", 
            "author": "https://api.launchpad.net/1.0/~lmtaylor"
        }, 
        {
            "content": "Enable debug logging for libvirt, recreate and then attach the nova-compute logs (with debug logs on) and the libvirtd logs (with debug logs on), plus the volume and instance uuids.\n\nYou can see how to enable debug logging for libvirt by looking at how it's done in devstack:\n\nhttps://github.com/openstack-dev/devstack/blob/master/lib/nova_plugins/functions-libvirt#L105", 
            "date_created": "2016-04-09 12:41:25.077060+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "", 
            "date_created": "2016-04-11 18:09:50.973420+00:00", 
            "author": "https://api.launchpad.net/1.0/~cjvolzka"
        }, 
        {
            "content": "", 
            "date_created": "2016-04-11 18:18:15.305558+00:00", 
            "author": "https://api.launchpad.net/1.0/~cjvolzka"
        }, 
        {
            "content": "attached the nova-compute and libvirt logs. The nova-compute log start just before an attempted detach.  The libvirt log includes everything from instance creation through the detach error.\n\nInstance UUID: a4806355-40df-48c8-9b73-8f8037a3aaa5\nVolume UUID: c16946aa-3018-4bdf-9586-a1cdfe8c8cb5\n\n(approximate timestamps +- a few seconds)\n15:14:52 Made instance\n15:16:46 Created volume\n15:18:05 attached volume\n15:19:59 attempted dettach\n15:21:39 DeviceDetachFailed in nova-compute.log", 
            "date_created": "2016-04-11 18:21:21.038980+00:00", 
            "author": "https://api.launchpad.net/1.0/~cjvolzka"
        }, 
        {
            "content": "The error itself seems to be coming because variable config always has a value at https://github.com/openstack/nova/blob/bc5035343d366a18cae587f92ecb4e871aba974a/nova/virt/libvirt/guest.py#L335. \n\nAt that point device is 'vdb' and config.to_xml() returns: '<disk type=\"block\" device=\"disk\">\\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\\n  <target bus=\"virtio\" dev=\"vdb\"/>\\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\\n</disk>\\n'. \n\nFrom what I can see the call to self.detach_device(config, persistent=False, live=live) just below that isn't having an effect although I don't know why.", 
            "date_created": "2016-04-11 18:49:57.095431+00:00", 
            "author": "https://api.launchpad.net/1.0/~cjvolzka"
        }, 
        {
            "content": "I have review the log pkvm-nova-compute.log and find that:\n\nThe nova VM tried to de-attach the volume at 15:19:58.688 and failed at 15:20:08.736 (The timeout of the operation)\n\n2016-04-11 15:19:58.688 39506 INFO nova.compute.manager [req-d1f30d7f-521a-48fe-8a19-a71adede8965 0688b01e6439ca32d698d20789d52169126fb41fb1a4ddafcebb97d854e836c9 0527fabbc19949fb8ced0cb1add35c63 - - -] [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5] Detach volume c16946aa-3018-4bdf-9586-a1cdfe8c8cb5 from mountpoint /dev/vdb\n2016-04-11 15:19:58.704 39506 DEBUG nova.virt.libvirt.config [req-d1f30d7f-521a-48fe-8a19-a71adede8965 0688b01e6439ca32d698d20789d52169126fb41fb1a4ddafcebb97d854e836c9 0527fabbc19949fb8ced0cb1add35c63 - - -] Generated XML ('<disk type=\"block\" device=\"disk\">\\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\\n  <target bus=\"virtio\" dev=\"vdb\"/>\\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\\n</disk>\\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82\n2016-04-11 15:20:03.722 39506 DEBUG oslo.service.loopingcall [req-d1f30d7f-521a-48fe-8a19-a71adede8965 0688b01e6439ca32d698d20789d52169126fb41fb1a4ddafcebb97d854e836c9 0527fabbc19949fb8ced0cb1add35c63 - - -] Waiting for function nova.virt.libvirt.guest._do_wait_and_retry_detach to return. func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:384\n2016-04-11 15:20:03.726 39506 DEBUG nova.virt.libvirt.config [-] Generated XML ('<disk type=\"block\" device=\"disk\">\\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\\n  <target bus=\"virtio\" dev=\"vdb\"/>\\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\\n</disk>\\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82\n2016-04-11 15:20:08.736 39506 DEBUG oslo.service.loopingcall [-] Exception which is in the suggested list of exceptions occurred while invoking function: nova.virt.libvirt.guest._do_wait_and_retry_detach. _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:362\n\nThen, nova VM retried 7 times to make sue that the operation really failed:\n\n2016-04-11 15:20:10.737 39506 DEBUG oslo.service.loopingcall [-] Invoking nova.virt.libvirt.guest._do_wait_and_retry_detach; retry count is 1. _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:355\n2016-04-11 15:20:10.740 39506 DEBUG nova.virt.libvirt.config [-] Generated XML ('<disk type=\"block\" device=\"disk\">\\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\\n  <target bus=\"virtio\" dev=\"vdb\"/>\\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\\n</disk>\\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82\n2016-04-11 15:20:15.750 39506 DEBUG oslo.service.loopingcall [-] Exception which is in the suggested list of exceptions occurred while invoking function: nova.virt.libvirt.guest._do_wait_and_retry_detach. _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:362\n\n....\n\n\n2016-04-11 15:21:34.832 39506 DEBUG oslo.service.loopingcall [-] Invoking nova.virt.libvirt.guest._do_wait_and_retry_detach; retry count is 7. _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:355\n2016-04-11 15:21:34.836 39506 DEBUG nova.virt.libvirt.config [-] Generated XML ('<disk type=\"block\" device=\"disk\">\\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\\n  <target bus=\"virtio\" dev=\"vdb\"/>\\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\\n</disk>\\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82\n2016-04-11 15:21:39.848 39506 DEBUG oslo.service.loopingcall [-] Exception which is in the suggested list of exceptions occurred while invoking function: nova.virt.libvirt.guest._do_wait_and_retry_detach. _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:362\n2016-04-11 15:21:39.848 39506 DEBUG oslo.service.loopingcall [-] Cannot retry nova.virt.libvirt.guest._do_wait_and_retry_detach upon suggested exception since retry count (7) reached max retry count (7). _func /usr/lib/python2.7/site-packages/oslo_service/loopingcall.py:372\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall [-] Dynamic interval looping call 'oslo_service.loopingcall._func' failed\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall Traceback (most recent call last):\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     result = func(*self.args, **self.kw)\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     return self._sleep_time\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     self.force_reraise()\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     six.reraise(self.type_, self.value, self.tb)\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     result = f(*args, **kwargs)\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall     reason=reason)\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-04-11 15:21:39.848 39506 ERROR oslo.service.loopingcall \n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [req-d1f30d7f-521a-48fe-8a19-a71adede8965 0688b01e6439ca32d698d20789d52169126fb41fb1a4ddafcebb97d854e836c9 0527fabbc19949fb8ced0cb1add35c63 - - -] [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5] Failed to detach volume c16946aa-3018-4bdf-9586-a1cdfe8c8cb5 from /dev/vdb\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5] Traceback (most recent call last):\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 4767, in _driver_detach_volume\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     encryption=encryption)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 1469, in detach_volume\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     wait_for_detach()\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 385, in func\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     return evt.wait()\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/eventlet/event.py\", line 121, in wait\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     return hubs.get_hub().switch()\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 294, in switch\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     return self.greenlet.switch()\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 136, in _run_loop\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     result = func(*self.args, **self.kw)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 377, in _func\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     return self._sleep_time\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 220, in __exit__\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     self.force_reraise()\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 196, in force_reraise\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     six.reraise(self.type_, self.value, self.tb)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/oslo_service/loopingcall.py\", line 356, in _func\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     result = f(*args, **kwargs)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 342, in _do_wait_and_retry_detach\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5]     reason=reason)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5] DeviceDetachFailed: Device detach failed for vdb: Unable to detach from guest transient domain.)\n2016-04-11 15:21:39.850 39506 ERROR nova.compute.manager [instance: a4806355-40df-48c8-9b73-8f8037a3aaa5] \n\nBut in the last try (7th), the libvirt.driver skipping the vdisk (volume) /dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5 (vdb)  and the reason is \n\n2016-04-11 15:21:36.059 39506 DEBUG nova.virt.libvirt.driver [-] skipping disk /dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5 (vdb) - unable to determine if volume _get_instance_disk_info /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6946\n\n\nHow can I determine where the problem is?\n\nHow to check the libvirt log?", 
            "date_created": "2016-04-11 20:21:17.090646+00:00", 
            "author": "https://api.launchpad.net/1.0/~xinli-v"
        }, 
        {
            "content": "Following up on my previous comment: I believe self.detach_device(config, persistent=False, live=live) is eventually calling /usr/lib64/python2.7/site-packages/libvirt.detachDeviceFlags() which itself calls /usr/lib64/python2.7/site-packages/libvirtmod.so. From there I can't see what happens next. From libvirt.detachDeviceFlags() I see:\n        ret = libvirtmod.virDomainDetachDeviceFlags(self._o, xml, flags)\n        if ret == -1: raise libvirtError ('virDomainDetachDeviceFlags() failed', dom=self)\n        return ret\nbut I'm not seeing an error in the libvirt log. The most relevant thing I can find in the log is:\n2016-04-11 15:20:19.756+0000: 38234: debug : virDomainDetachDeviceFlags:8560 : dom=0x3fff9c00b2e0, (VM: name=2_cvj_vm_test-a4806355-0000001c, uuid=a4806355-40df-48c8-9b73-8f8037a3aaa5), xml=<disk type=\"block\" device=\"disk\">\n  <driver name=\"qemu\" type=\"raw\" cache=\"none\" io=\"native\"/>\n  <source dev=\"/dev/disk/by-id/wwn-0x60050768028110a4880000000000bec5\"/>\n  <target bus=\"virtio\" dev=\"vdb\"/>\n  <serial>c16946aa-3018-4bdf-9586-a1cdfe8c8cb5</serial>\n</disk>\n, flags=1\n\n\n I've spoken to a libvirt on PowerKVM expert. It appears the problem is that the packages were out of date on the VMs that were created which caused them to not fully detach. After updating the OS I was able to detach the volumes with out issue. Invalidating the bug.\n\n", 
            "date_created": "2016-04-12 16:21:33.455101+00:00", 
            "author": "https://api.launchpad.net/1.0/~cjvolzka"
        }, 
        {
            "content": "\u4e00\u5e2e\u4eba\u5e94\u8be5\u6ca1\u6709\u5206\u6790\u5230\u70b9\u4e0a\n\n\u697c\u4e3b\u5e94\u8be5\u662f\u7528\u865a\u62df\u673a\u542f\u7528\u7684\u865a\u62df\u673a\n\u7b2c\u4e8c\u5c42\u542f\u52a8\u7684\u865a\u62df\u673a\u662f\u4e0d\u662fqemu\u7684\u4e0d\u662fkvm\u7684\uff1f\n\u7528\u4e86qemu\u865a\u62df\u673a\uff0c\u8fd9\u6837\u4f7f\u7528\u4e86tcg\n\n\u800c\u4e14\u5916\u9762\u4e00\u5c42\u548c\u91cc\u9762\u4e00\u5c42\u7684\u865a\u62df\u5316\u7248\u672c\u662f\u4e0d\u662f\u4e0d\u4e00\u81f4\uff1f\n\n\u53e6\u5916\uff0cnova\u9ed8\u8ba4\u4efb\u52a1cpu_mode\u4e3ahost_model\n\u4e24\u5c42\u865a\u62df\u5316\u5bfc\u81f4\u6307\u4ee4\u96c6\u4e0d\u7edf\u4e00\uff0chost_model\u4e0b\u4f1a\u8ba9\u91cc\u9762\u4e00\u5c42\u7684\u865a\u62df\u673aos\u8d77\u4e0d\u6765\n\u53ef\u4ee5vnc\u8fdb\u53bb\u770b\u770b\uff0cos\u8d77\u6765\u6ca1\nos\u8d77\u4e0d\u6765acpi\u7684\u4e2d\u65ad\u5c31\u6253\u4e0d\u8fdb\u53bb\uff0c\u70ed\u63d2\u62d4\u76d8\u5c31\u4e0d\u53ef\u7528\n\n\u4fee\u6539\u65b9\u6cd5\u6709\u4e09\u4e2a\uff1a\n1.\u865a\u62df\u5316\u5f00\u542fneseted\uff0c\u5177\u4f53\u53bbgoole,\u7528kvm\u865a\u62df\u673a\n2.\u4e24\u5c42\u865a\u62df\u5316\u7248\u672c\u4fdd\u6301\u4e00\u81f4\n3.nova.conf\u7684[libvirt]\u4e0b\u914d\u7f6ecpu_mode=none\u89c4\u907f\n\njust used to using Chinese,so what?\n\n\n\n\n", 
            "date_created": "2017-08-11 08:40:20.110831+00:00", 
            "author": "https://api.launchpad.net/1.0/~2005hanbaoying"
        }, 
        {
            "content": "sorry\ni made a mistake\nusing the same hypervisor version can not make it.\nso the second solution is wrong \n\ni checked at libvirt2.0.0/qemu2.6.0\nmake a kvm vm(let's call vm1) on host\nthen install libvirt2.0.0/qemu2.6.0 on vm1\n\nthen create qemu vm on vm1,let's called vm2\nat last ,vm2 can not be booted.\n\n\nBut all things seems ok when using libivrt1.2.17/qemu1.5.3\n\n\n\n\n", 
            "date_created": "2017-08-11 09:46:48.567121+00:00", 
            "author": "https://api.launchpad.net/1.0/~2005hanbaoying"
        }
    ], 
    "closed": "2016-04-12 16:21:43.056246+00:00"
}
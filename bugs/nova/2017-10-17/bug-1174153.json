{
    "status": "Fix Released", 
    "last_updated": "2015-04-30 10:06:23.077734+00:00", 
    "description": "At the moment the baremetal driver resets the partition table on the first hard disk, but doesn't wipe the data. This has two holes: other disks have their partition tables preserved; tenant data is able to be read by the new instance.\n\nWiping disks can be slow (particularly in cases where TRIM cannot be relied on),  so we probably want to only do it when the new instance is for a new tenant.", 
    "tags": [
        "baremetal"
    ], 
    "importance": "High", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1174153", 
    "owner": "https://api.launchpad.net/1.0/~joshnang", 
    "id": 1174153, 
    "index": 1064, 
    "openned": "2013-04-29 05:09:22.394629+00:00", 
    "created": "2013-04-29 05:09:22.394629+00:00", 
    "title": "data from previous tenants accessible with nova baremetal", 
    "comments": [
        {
            "content": "At the moment the baremetal driver resets the partition table on the first hard disk, but doesn't wipe the data. This has two holes: other disks have their partition tables preserved; tenant data is able to be read by the new instance.\n\nWiping disks can be slow (particularly in cases where TRIM cannot be relied on),  so we probably want to only do it when the new instance is for a new tenant.", 
            "date_created": "2013-04-29 05:09:22.394629+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "Looks like a pretty significant vulnerability to me, or am I missing something ?", 
            "date_created": "2013-05-02 12:18:44.486955+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "We failed to communicate the experimental nature of baremetal during Grizzly: it was not intended to be supported at all. It is a limitation, not a vulnerability IMO. There are a raft of caveats with cross-tenant use of the same hardware, of which this is just one.", 
            "date_created": "2013-05-03 19:56:51.664434+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "I think the experience from Essex shows that people will still be using Grizzly in a couple of years, and we'll still be explaining why we didn't intend people to use this feature. I think we either need to fix bugs as they are reported, or clearly document that we don't expect anyone to use it.", 
            "date_created": "2013-05-03 22:27:46.115761+00:00", 
            "author": "https://api.launchpad.net/1.0/~mikal"
        }, 
        {
            "content": "The fix for this can be backported once it's done; I'm merely arguing we don't need to treat it as a security vulnerability and fire-drill it : without secure boot [not supported in Grizzly, and likely too large to backport sensibly], and without full openflow hardware networking during the boot process [definitely not a Grizzly thing], it is impossible to trust multiple tenants on baremetal at all - because the vectors for attack are so low level that instances may be running in a virtual environment and unaware of it, with the virtual environment capturing secrets, forcing entropy pools to be predictable and other such hostile behaviour.\n\nMy suggestion for Grizzly is that we document these caveats thoroughly - I'd be delighted to do that - and then as and when we tackle them in H and I and beyond update the documentation accordingly.\n\nBaremetal's primary use case *today* is for deploying a cluster run by a single group of sysadmins : that is one tenant from a security perspective. For that, it is totally usable.", 
            "date_created": "2013-05-05 06:23:12.592287+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "@Robert, I'm fine with documenting that. We actually have a process (OSSN) to document that sort of thing (we issued one for libvirt-lxc drivers and how they should not be used in multi-tenant environments either).\n\nLet me plug the OSSN crew in.", 
            "date_created": "2013-05-06 10:04:22.637549+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "The gist of what needs to be issued as an OSSN is:\n\n\"without secure boot, and without full openflow hardware networking during the boot process, it is impossible to trust multiple tenants on baremetal at all - because the vectors for attack are so low level that instances may be running in a virtual environment and unaware of it, with the virtual environment capturing secrets, forcing entropy pools to be predictable and other such hostile behaviour.\"", 
            "date_created": "2013-05-06 10:06:03.678094+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I've just seen this, we're happy to issue an OSSN and will draft something today.\n\n-Rob", 
            "date_created": "2013-05-13 11:42:04.220433+00:00", 
            "author": "https://api.launchpad.net/1.0/~robert-clark"
        }, 
        {
            "content": "Nova Baremetal Exposes Previous Tenant Data \n-----\n\n### Summary ###\nData of previous tenants may be exposed to new ones when using Nova Baremetal\n\n### Affected Services / Software ###\nKeystone, Databases\n\n### Discussion ###\nNova Baremetal is intended for testing and development only, it is not intended to be production ready. Experience has shown that despite that warning the OpenStack community is keen to embrace new technologies and deploy at-risk. This OSSN serves to signpost some of the risks.\n\nWithout secure boot, and without full openflow hardware networking during the boot process, it is impossible to trust multiple tenants on baremetal at all - because the vectors for attack are so low level that instances may be running in a virtual environment and unaware of it, with the virtual environment capturing secrets, forcing entropy pools to be predictable and other such hostile behaviour.\n\n### Recommended Actions ###\nDo not use Nova Baremetal where secure separation of tenants on hardware is a requirement without a full verifiable boot chain and network hardware. \n\n### Contacts / References ###\nThis OSSN: https://bugs.launchpad.net/ossn/+bug/1174153\n", 
            "date_created": "2013-06-11 15:05:37.099674+00:00", 
            "author": "https://api.launchpad.net/1.0/~robert-clark"
        }, 
        {
            "content": "Sounds good.", 
            "date_created": "2013-06-12 13:44:37.299699+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Nova Baremetal Exposes Previous Tenant Data\n-----\n\n### Summary ###\nData of previous tenants may be exposed to new ones when using Nova Baremetal\n\n### Affected Services / Software ###\nKeystone, Databases\n\n### Discussion ###\nNova Baremetal is intended for testing and development only, it is not intended to be production ready. Experience has shown that despite that warning the OpenStack community is keen to embrace new technologies and deploy at-risk. This OSSN serves to signpost some of the risks.\n\nWithout secure boot, and without full openflow hardware networking during the boot process, it is impossible to trust multiple tenants on baremetal at all - because the vectors for attack are so low level that instances may be running in a virtual environment and unaware of it, with the virtual environment capturing secrets, forcing entropy pools to be predictable and other such hostile behaviour.\n\n### Recommended Actions ###\nDo not use Nova Baremetal where secure separation of tenants on hardware is a requirement without a full verifiable boot chain and network hardware.\n\n### Contacts / References ###\nThis OSSN : https://bugs.launchpad.net/ossn/+bug/1174153\nOpenStack Security ML : <email address hidden>\nOpenStack Security Group : https://launchpad.net/~openstack-ossg", 
            "date_created": "2013-07-02 10:37:33.402611+00:00", 
            "author": "https://api.launchpad.net/1.0/~robert-clark"
        }, 
        {
            "content": "Crossposted to OpenStack/OpenStack Dev - 2nd July 2013", 
            "date_created": "2013-07-02 10:39:10.011408+00:00", 
            "author": "https://api.launchpad.net/1.0/~robert-clark"
        }, 
        {
            "content": "I think wiping the node is the thing that should be discussed.\nThere are multiple approaches for performing that. Overwriting a block device over the network does not look like a good idea because it's extremely slow. I mentioned some thoughts in on of the sessions proposal here: http://summit.openstack.org/cfp/details/57 and going to repost that here.\n", 
            "date_created": "2013-10-11 13:00:07.225741+00:00", 
            "author": "https://api.launchpad.net/1.0/~romcheg"
        }, 
        {
            "content": "Possible solutions\n--------------------------\n- Build a special undeploy image and use it for either\n  - Securely erasing the volume on the node side\n  - Exporting a volume to manager and perform erasing on the manager side\n- Create a separate boot configuration on the node that loads a kernel and a ramdisk with undeploy scripts in it\n\nFood For Thought\n--------------------------\n- Should wiping be a part of deploying or undeploying?\n- Should we wipe all nodes or wipe them on-demand?\n  - Wiping all nodes might be ot required for everyone\n  - Securely wiping a node requires a lot of time", 
            "date_created": "2013-10-11 13:00:13.821840+00:00", 
            "author": "https://api.launchpad.net/1.0/~romcheg"
        }, 
        {
            "content": "Is there anything we'll have to do on Nova side?", 
            "date_created": "2014-08-13 11:28:12.695872+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "We've had to deal with this problem before in Nova with the libvirt driver with its LVM volume backend.\n\nIn that case we will wipe the data at VM teardown, to ensure future VMs don't see any data from previous tenants\n\ncommit 9d2ea970422591f8cdc394001be9a2deca499a5f\nAuthor: P\u00e1draig Brady <email address hidden>\nDate:   Fri Nov 23 14:59:13 2012 +0000\n\n    Don't leak info from libvirt LVM backed instances\n    \n    * nova/virt/libvirt/utils.py (remove_logical_volumes):\n    Overwrite each logical volume with zero\n    (clear_logical_volume): LV obfuscation implementation.\n    (logical_volume_size): A utility function used by\n    clear_logical_volume()\n    \n    Fixes bug: 1070539\n    Change-Id: I4e1024de8dfe9b0be3b0d6437c836d2042862f85\n\n\nWe made this behaviour configurable with a nova.conf setting\n\ncommit 71946855591a41dcc87ef59656a8a340774eeaf2\nAuthor: P\u00e1draig Brady <email address hidden>\nDate:   Tue Feb 11 11:51:39 2014 +0000\n\n    libvirt: support configurable wipe methods for LVM backed instances\n    \n    Provide configurable methods to clear these volumes.\n    The new 'volume_clear' and 'volume_clear_size' options\n    are the same as currently supported by cinder.\n    \n    * nova/virt/libvirt/imagebackend.py: Define the new options.\n    * nova/virt/libvirt/utils.py (clear_logical_volume): Support the\n    new options. Refactor the existing dd method out to\n    _zero_logic_volume().\n    * nova/tests/virt/libvirt/test_libvirt_utils.py: Add missing test cases\n    for the existing clear_logical_volume code, and for the new code\n    supporting the new clearing methods.\n    * etc/nova/nova.conf.sample: Add the 2 new config descriptions\n    to the [libvirt] section.\n    \n    Change-Id: I5551197f9ec89ae2f9b051696bccdeb1af2c031f\n    Closes-Bug: #889299\n\n\nIMHO we should move this config setting & code out of the libvirt section into the general nova.conf section and re-use the logic for baremetal.\n", 
            "date_created": "2014-08-13 11:44:08.067329+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "I just want to point out that this is being worked on, and expected to be completed in Kilo:\n\nhttps://review.openstack.org/#/q/status:open+branch:master+topic:bp/decom-nodes,n,z\n\nThis patch series covers more than wiping disks (e.g. firmware updates), but disk erase is in there and configurable at the ironic level, as I understand it.", 
            "date_created": "2014-11-20 13:38:37.260116+00:00", 
            "author": "https://api.launchpad.net/1.0/~jim-rollenhagen"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/155561\nCommitted: https://git.openstack.org/cgit/openstack/ironic/commit/?id=7589d1faaf5415c12b2e738c6c09b17b65993235\nSubmitter: Jenkins\nBranch:    master\n\ncommit 7589d1faaf5415c12b2e738c6c09b17b65993235\nAuthor: Josh Gachnang <email address hidden>\nDate:   Tue Feb 17 17:42:18 2015 -0800\n\n    Implement execute clean steps\n    \n    This implements executing the clean steps in the conductor. The RPC\n    API version is bumped to allow async clean steps to call back\n    to the conductor.\n    \n    Adds node.clean_step to store the current cleaning operation.\n    \n    The conductor will get a list of clean steps from the node's drivers,\n    order them by priority, and then have the drivers execute the steps\n    in order.\n    \n    Adds a config option to enable cleaning, defaulting to True.\n    \n    Related-bug: #1174153\n    \n    Implements blueprint implement-cleaning-states\n    Change-Id: I96af133c501f86a6e620c4684ee65abad2111f7b\n", 
            "date_created": "2015-03-16 16:21:39.896578+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "The feature work in Ironic has been done to address this long-standing bug, however Nova needs to be upated to understand the changes in Ironic.\n\nAddressed by the following:\n\nAdd support for cleaning in Ironic driver\n  https://review.openstack.org/#/c/161474/ \n\nAdjust resource tracker for new Ironic states\n  https://review.openstack.org/#/c/164313/\n", 
            "date_created": "2015-03-17 22:42:30.465651+00:00", 
            "author": "https://api.launchpad.net/1.0/~devananda"
        }, 
        {
            "content": "Looks like we still have 2 reviews in progress in **Nova**:\nhttps://review.openstack.org/#/c/161474/\nhttps://review.openstack.org/#/c/164313/", 
            "date_created": "2015-03-23 15:30:21.060427+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/161474\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=597507d66b7de6a4d17863b2713e6def2c436f7e\nSubmitter: Jenkins\nBranch:    master\n\ncommit 597507d66b7de6a4d17863b2713e6def2c436f7e\nAuthor: Josh Gachnang <email address hidden>\nDate:   Wed Mar 4 15:26:54 2015 -0800\n\n    Add support for cleaning in Ironic driver\n    \n    Ironic added a new state between deleting and available called cleaning\n    where tasks like erasing drives occur after each delete. This patch\n    ensures that instances can be considered deleted in Nova as soon as a\n    node enters cleaning state, otherwise, instances could be stuck in\n    deleting state in Nova for hours or days.\n    \n    This is necessary to implement the Ironic cleaning spec:\n    https://blueprints.launchpad.net/ironic/+spec/implement-cleaning-states\n    \n    Closes-Bug: 1174153\n    Change-Id: Ie04823c40efc08f887429a6b8e6219558c3e4efa\n", 
            "date_created": "2015-03-26 19:43:17.435305+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2015-04-10 09:19:15.909867+00:00"
}
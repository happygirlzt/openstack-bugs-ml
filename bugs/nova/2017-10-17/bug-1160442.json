{
    "status": "Fix Released", 
    "last_updated": "2014-07-12 19:44:08.109531+00:00", 
    "description": "i have install grizzly-g3, but quantum does not work well, when i boot 128 instances, i found one of instances got more than one fixed ip, howerver, when i boot 64 intances, it nerver happened, besides that , sometimes i can not ping vm with floatingip, i did not find any error message in my quantum log( all the files in the /var/log/quantum),  follows are the error output and configurations\n\n| 97a93600-38e2-4700-9851-15ef56c1d628 | slave  | ACTIVE | demo-int-net=172.16.100.4                   |\n| 99aeb6b8-4252-4839-a7d1-f87853116100 | slave  | ACTIVE | demo-int-net=172.16.100.117                 |\n| 9aa82a35-c9f1-4f44-a108-d14e74eec231 | slave  | ACTIVE | demo-int-net=172.16.100.108, 172.16.100.109 |\n| 9b6b1289-c450-4614-b647-e5ebdffff80a | slave  | ACTIVE | demo-int-net=172.16.100.5                   |\n| 9e0d3aa5-0f15-4b24-944a-6d6c3e18ce64 | slave  | ACTIVE | demo-int-net=172.16.100.35                  |\n| 9ea62124-9128-43cc-acdd-142f1e7743d6 | slave  | ACTIVE | demo-int-net=172.16.100.132                 |\n\n\nmy setup : one db host(db service), one glance host(glance service), on api host(keystone,nova-api,nova-scheduler, nova-conductor, quantum-server,quantum-dhcp, quantum-l3-agent,quantum-plugin-openvswitch-agent), eight compute host(each host with nova-compute, quantum-plugin-openvswitch-agent), i check that all the service on all hosts works well\n \ni used vlan type network and openvswitch plugin:\n\nmy quantum.conf \n\n[DEFAULT]\n# Default log level is INFO\n# verbose and debug has the same result.\n# One of them will set DEBUG log level output\ndebug = True\n\n# Address to bind the API server\nbind_host = 0.0.0.0\n\n# Port the bind the API server to\nbind_port = 9696\n\n\n# Quantum plugin provider module\n# core_plugin =\ncore_plugin = quantum.plugins.openvswitch.ovs_quantum_plugin.OVSQuantumPluginV2\n# Advanced service modules\n# service_plugins =\n\n# Paste configuration file\napi_paste_config = /etc/quantum/api-paste.ini\n\n# The strategy to be used for auth.\n# Supported values are 'keystone'(default), 'noauth'.\nauth_strategy = keystone\n\n\n# Modules of exceptions that are permitted to be recreated\n# upon receiving exception data from an rpc call.\n# allowed_rpc_exception_modules = quantum.openstack.common.exception, nova.exception\n# AMQP exchange to connect to if using RabbitMQ or QPID\ncontrol_exchange = quantum\n\n# RPC driver. DHCP agents needs it.\nnotification_driver = quantum.openstack.common.notifier.rpc_notifier\n\n# default_notification_level is used to form actual topic name(s) or to set logging level\ndefault_notification_level = INFO\n\n\n# Defined in rpc_notifier, can be comma separated values.\n# The actual topic names will be %s.%(default_notification_level)s\nnotification_topics = notifications\n\n[QUOTAS]\n# resource name(s) that are supported in quota features\n# quota_items = network,subnet,port\n\n# default number of resource allowed per tenant, minus for unlimited\n# default_quota = -1\n\n# number of networks allowed per tenant, and minus means unlimited\n# quota_network = 10\n\n# number of subnets allowed per tenant, and minus means unlimited\n# quota_subnet = 10\n\n# number of ports allowed per tenant, and minus means unlimited\nquota_port = 5000\nquota_floatingip = 5000\n\n# default driver to use for quota checks\n# quota_driver = quantum.quota.ConfDriver\n\n# =========== items for agent management extension =============\n# Seconds to regard the agent as down.\n# agent_down_time = 5\n# ===========  end of items for agent management extension =====\n\n[DEFAULT_SERVICETYPE]\n# Description of the default service type (optional)\n# description = \"default service type\"\n# Enter a service definition line for each advanced service provided\n# by the default service type.\n# Each service definition should be in the following format:\n# <service>:<plugin>[:driver]\n\n[SECURITYGROUP]\n# If set to true this allows quantum to receive proxied security group calls from nova\n# proxy_mode = False\n\n[AGENT]\nroot_helper = sudo quantum-rootwrap /etc/quantum/rootwrap.conf\n\n# =========== items for agent management extension =============\n# seconds between nodes reporting state to server, should be less than\n# agent_down_time\n# report_interval = 4\n\n# ===========  end of items for agent management extension =====\n\n[keystone_authtoken]\nauth_host = host-keystone\nauth_port = 35357\nauth_protocol = http\nadmin_tenant_name = demoTenant\nadmin_user = test\nadmin_password = 123456\nsigning_dir = /var/lib/quantum/keystone-signing\n\nmy dhcp_agent.ini\n\n[DEFAULT]\n# Where to store dnsmasq state files.  This directory must be writable by the\n# user executing the agent.\nstate_path = /var/lib/quantum\n\n# OVS based plugins(OVS, Ryu, NEC, NVP, BigSwitch/Floodlight)\ninterface_driver = quantum.agent.linux.interface.OVSInterfaceDriver\n\n# The agent can use other DHCP drivers.  Dnsmasq is the simplest and requires\n# no additional setup of the DHCP server.\ndhcp_driver = quantum.agent.linux.dhcp.Dnsmasq\n\nmy ovs_quantum_plugin.ini configure file\n\n[DATABASE]\n# This line MUST be changed to actually run the plugin.\nsql_connection = mysql://quantum:quantum@host-db/quantum\n\n# Database reconnection interval in seconds - if the initial connection to the\n# database fails\nreconnect_interval = 2\n\n[OVS]\n# (StrOpt) Type of network to allocate for tenant networks. The\n# default value 'local' is useful only for single-box testing and\n# provides no connectivity between hosts. You MUST either change this\n# to 'vlan' and configure network_vlan_ranges below or change this to\n# 'gre' and configure tunnel_id_ranges below in order for tenant\n# networks to provide connectivity between hosts. Set to 'none' to\n# disable creation of tenant networks.\n\ntenant_network_type=vlan\n\nnetwork_vlan_ranges = DemoNet:1:4094\n\nbridge_mappings = DemoNet:DemoBridge\n\n[AGENT]\n# Agent's polling interval in seconds\npolling_interval = 2\n\n[SECURITYGROUP]\n\n\nwhen i execute \"quantum router-gateway-set\" to add external network id as the router gateway, i found that the status of  port for external network id in the router is DOWN. does it matters,  if it does,  how can i fixed it.\n\n\nthis blocked me for serveral days , can someone help me sovle it, any help will be appreciated.", 
    "tags": [
        "quantum"
    ], 
    "importance": "Medium", 
    "heat": 38, 
    "link": "https://bugs.launchpad.net/nova/+bug/1160442", 
    "owner": "None", 
    "id": 1160442, 
    "index": 3318, 
    "openned": "2013-03-27 17:28:54.001925+00:00", 
    "created": "2013-03-26 15:45:26.542086+00:00", 
    "title": "when boot many vms with quantum, nova sometimes allocates two quantum ports rather than one", 
    "comments": [
        {
            "content": "i have install grizzly-g3, but quantum does not work well, when i boot 128 instances, i found one of instances got more than one fixed ip, howerver, when i boot 64 intances, it nerver happened, besides that , sometimes i can not ping vm with floatingip, i did not find any error message in my quantum log( all the files in the /var/log/quantum),  follows are the error output and configurations\n\n| 97a93600-38e2-4700-9851-15ef56c1d628 | slave  | ACTIVE | demo-int-net=172.16.100.4                   |\n| 99aeb6b8-4252-4839-a7d1-f87853116100 | slave  | ACTIVE | demo-int-net=172.16.100.117                 |\n| 9aa82a35-c9f1-4f44-a108-d14e74eec231 | slave  | ACTIVE | demo-int-net=172.16.100.108, 172.16.100.109 |\n| 9b6b1289-c450-4614-b647-e5ebdffff80a | slave  | ACTIVE | demo-int-net=172.16.100.5                   |\n| 9e0d3aa5-0f15-4b24-944a-6d6c3e18ce64 | slave  | ACTIVE | demo-int-net=172.16.100.35                  |\n| 9ea62124-9128-43cc-acdd-142f1e7743d6 | slave  | ACTIVE | demo-int-net=172.16.100.132                 |\n\n\nmy setup : one db host(db service), one glance host(glance service), on api host(keystone,nova-api,nova-scheduler, nova-conductor, quantum-server,quantum-dhcp, quantum-l3-agent,quantum-plugin-openvswitch-agent), eight compute host(each host with nova-compute, quantum-plugin-openvswitch-agent), i check that all the service on all hosts works well\n \ni used vlan type network and openvswitch plugin:\n\nmy quantum.conf \n\n[DEFAULT]\n# Default log level is INFO\n# verbose and debug has the same result.\n# One of them will set DEBUG log level output\ndebug = True\n\n# Address to bind the API server\nbind_host = 0.0.0.0\n\n# Port the bind the API server to\nbind_port = 9696\n\n\n# Quantum plugin provider module\n# core_plugin =\ncore_plugin = quantum.plugins.openvswitch.ovs_quantum_plugin.OVSQuantumPluginV2\n# Advanced service modules\n# service_plugins =\n\n# Paste configuration file\napi_paste_config = /etc/quantum/api-paste.ini\n\n# The strategy to be used for auth.\n# Supported values are 'keystone'(default), 'noauth'.\nauth_strategy = keystone\n\n\n# Modules of exceptions that are permitted to be recreated\n# upon receiving exception data from an rpc call.\n# allowed_rpc_exception_modules = quantum.openstack.common.exception, nova.exception\n# AMQP exchange to connect to if using RabbitMQ or QPID\ncontrol_exchange = quantum\n\n# RPC driver. DHCP agents needs it.\nnotification_driver = quantum.openstack.common.notifier.rpc_notifier\n\n# default_notification_level is used to form actual topic name(s) or to set logging level\ndefault_notification_level = INFO\n\n\n# Defined in rpc_notifier, can be comma separated values.\n# The actual topic names will be %s.%(default_notification_level)s\nnotification_topics = notifications\n\n[QUOTAS]\n# resource name(s) that are supported in quota features\n# quota_items = network,subnet,port\n\n# default number of resource allowed per tenant, minus for unlimited\n# default_quota = -1\n\n# number of networks allowed per tenant, and minus means unlimited\n# quota_network = 10\n\n# number of subnets allowed per tenant, and minus means unlimited\n# quota_subnet = 10\n\n# number of ports allowed per tenant, and minus means unlimited\nquota_port = 5000\nquota_floatingip = 5000\n\n# default driver to use for quota checks\n# quota_driver = quantum.quota.ConfDriver\n\n# =========== items for agent management extension =============\n# Seconds to regard the agent as down.\n# agent_down_time = 5\n# ===========  end of items for agent management extension =====\n\n[DEFAULT_SERVICETYPE]\n# Description of the default service type (optional)\n# description = \"default service type\"\n# Enter a service definition line for each advanced service provided\n# by the default service type.\n# Each service definition should be in the following format:\n# <service>:<plugin>[:driver]\n\n[SECURITYGROUP]\n# If set to true this allows quantum to receive proxied security group calls from nova\n# proxy_mode = False\n\n[AGENT]\nroot_helper = sudo quantum-rootwrap /etc/quantum/rootwrap.conf\n\n# =========== items for agent management extension =============\n# seconds between nodes reporting state to server, should be less than\n# agent_down_time\n# report_interval = 4\n\n# ===========  end of items for agent management extension =====\n\n[keystone_authtoken]\nauth_host = host-keystone\nauth_port = 35357\nauth_protocol = http\nadmin_tenant_name = demoTenant\nadmin_user = test\nadmin_password = 123456\nsigning_dir = /var/lib/quantum/keystone-signing\n\nmy dhcp_agent.ini\n\n[DEFAULT]\n# Where to store dnsmasq state files.  This directory must be writable by the\n# user executing the agent.\nstate_path = /var/lib/quantum\n\n# OVS based plugins(OVS, Ryu, NEC, NVP, BigSwitch/Floodlight)\ninterface_driver = quantum.agent.linux.interface.OVSInterfaceDriver\n\n# The agent can use other DHCP drivers.  Dnsmasq is the simplest and requires\n# no additional setup of the DHCP server.\ndhcp_driver = quantum.agent.linux.dhcp.Dnsmasq\n\nmy ovs_quantum_plugin.ini configure file\n\n[DATABASE]\n# This line MUST be changed to actually run the plugin.\nsql_connection = mysql://quantum:quantum@host-db/quantum\n\n# Database reconnection interval in seconds - if the initial connection to the\n# database fails\nreconnect_interval = 2\n\n[OVS]\n# (StrOpt) Type of network to allocate for tenant networks. The\n# default value 'local' is useful only for single-box testing and\n# provides no connectivity between hosts. You MUST either change this\n# to 'vlan' and configure network_vlan_ranges below or change this to\n# 'gre' and configure tunnel_id_ranges below in order for tenant\n# networks to provide connectivity between hosts. Set to 'none' to\n# disable creation of tenant networks.\n\ntenant_network_type=vlan\n\nnetwork_vlan_ranges = DemoNet:1:4094\n\nbridge_mappings = DemoNet:DemoBridge\n\n[AGENT]\n# Agent's polling interval in seconds\npolling_interval = 2\n\n[SECURITYGROUP]\n\n\nwhen i execute \"quantum router-gateway-set\" to add external network id as the router gateway, i found that the status of  port for external network id in the router is DOWN. does it matters,  if it does,  how can i fixed it.\n\n\nthis blocked me for serveral days , can someone help me sovle it, any help will be appreciated.", 
            "date_created": "2013-03-26 15:45:26.542086+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "Note: most of the interesting details are in the thread of: https://answers.launchpad.net/quantum/+question/225158\n\nBasically, when a large number of vms are booted at once, each of which should have a single nic (and thus a single quantum port), every once and a while one of the VMs gets two quantum ports allocated.  This is then visible to the tenant as two fixed IPs on the VM (since both quantum ports have a device_id of the same VM).  \n\nIt may be that the allocate_for_instance logic is running twice for the same VM-id.  unclear if this is happening on the same nova-compute host, or multiple.  Since the ports are created back-to-back, my best guess is that its on the same nova-compute.  \n\nxinxin-shu, can you also provide the quantum-server log that covers the time period when the two ports are created?   I'm trying to understand if there is an error on the quantum side that prompts nova to try to create the port again.  ", 
            "date_created": "2013-03-26 15:53:17.038575+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "From the info provided on the question, it seems likely that the required change will be in nova. ", 
            "date_created": "2013-03-27 17:30:50.343921+00:00", 
            "author": "https://api.launchpad.net/1.0/~danwent"
        }, 
        {
            "content": "Hi Gary & Dan,\n\nCan you guys take a look at it? I have other stuff depending on this in\norder to review/commit, etc. Can't find contacts of other core devstack\ndevelopers.\n\nThanks,\nRobert\n\nOn 3/18/13 4:38 PM, \"Robert Li (baoli)\" <email address hidden> wrote:\n\n>Public bug reported:\n>\n>Currently, lib/quantum supports two config files on the command line:\n>quantum config file and core plugin config file. Ideally it should be\n>able to support as many as indicated by the plugin script. For example,\n>cisco plugin may need to load openvswitch plugin configuration file as\n>well.\n>\n>** Affects: devstack\n>     Importance: Undecided\n>     Assignee: Baodong (Robert) Li (baoli)\n>         Status: New\n>\n>** Changed in: devstack\n>     Assignee: (unassigned) => Baodong (Robert) Li (baoli)\n>\n>-- \n>You received this bug notification because you are subscribed to the bug\n>report.\n>https://bugs.launchpad.net/bugs/1156831\n>\n>Title:\n>  support multiple config files from cmd line\n>\n>Status in devstack - openstack dev environments:\n>  New\n>\n>Bug description:\n>  Currently, lib/quantum supports two config files on the command line:\n>  quantum config file and core plugin config file. Ideally it should be\n>  able to support as many as indicated by the plugin script. For\n>  example, cisco plugin may need to load openvswitch plugin\n>  configuration file as well.\n>\n>To manage notifications about this bug go to:\n>https://bugs.launchpad.net/devstack/+bug/1156831/+subscriptions\n\n", 
            "date_created": "2013-04-02 14:59:44+00:00", 
            "author": "https://api.launchpad.net/1.0/~baoli"
        }, 
        {
            "content": "I am not able to reproduce this bug in my setup. I am using almost the same network configuration that the one posted in this bug report but I am using stable/grizzly with only one compute node, I am able to reach up to 200 VMs with no problems at all:\n\n| 284a318b-84df-4008-a386-d4a7b0510dee | vm-178 | ACTIVE | None       | Running     | private=10.0.0.178 |\n| b2e1bbec-e93c-4b48-9843-b45a45a33b87 | vm-179 | ACTIVE | None       | Running     | private=10.0.0.182 |\n| c8f5d908-79c3-4020-adca-6348f76f44cf | vm-18  | ACTIVE | None       | Running     | private=10.0.0.22  |\n| 2f452943-92f5-41c1-a294-f5e52ef97052 | vm-180 | ACTIVE | None       | Running     | private=10.0.0.184 |\n| 43314b6c-7f6f-4da3-8054-5a8158a48ef9 | vm-181 | ACTIVE | None       | Running     | private=10.0.0.183 |\n| 3821636d-c94b-4030-95d4-47c183893227 | vm-182 | ACTIVE | None       | Running     | private=10.0.0.187 |\n| f6b27836-acf3-49a9-a960-a2a6835df534 | vm-183 | ACTIVE | None       | Running     | private=10.0.0.185 |\n| c798b778-b471-4a1f-b43c-f9d69873354d | vm-184 | ACTIVE | None       | Running     | private=10.0.0.186 |\n| 99a023ec-aeff-4451-9e3c-27316d1ffd78 | vm-185 | ACTIVE | None       | Running     | private=10.0.0.188 |\n| 4475818f-801a-4a59-9a85-1113e54e0c18 | vm-186 | ACTIVE | None       | Running     | private=10.0.0.192 |\n| 56145c8c-a645-4bdf-84b8-5078c19c39a4 | vm-187 | ACTIVE | None       | Running     | private=10.0.0.189 |\n| f2c3074a-43aa-4506-b3a9-43f2ff3f461c | vm-188 | ACTIVE | None       | Running     | private=10.0.0.190 |\n| f12c882a-c25b-46f2-8550-7ec807d3b0b7 | vm-189 | ACTIVE | None       | Running     | private=10.0.0.191 |\n| 9a40b604-0f9e-44d0-a080-59bfb9adedf5 | vm-19  | ACTIVE | None       | Running     | private=10.0.0.20  |\n| 952a25a6-501b-44de-9149-0cb25b217d96 | vm-190 | ACTIVE | None       | Running     | private=10.0.0.194 |\n| 5012700f-83a7-4f9d-85a7-796a65043aed | vm-191 | ACTIVE | None       | Running     | private=10.0.0.193 |\n| 9d7bbc67-55e1-49e8-8c7b-6f6bd5b2183a | vm-192 | ACTIVE | None       | Running     | private=10.0.0.195 |\n| 13192f4b-541f-4e92-b3f5-51e51d28408a | vm-193 | ACTIVE | None       | Running     | private=10.0.0.198 |\n| 4ff125dc-8f1d-40b1-9db6-738665cdb8e7 | vm-194 | ACTIVE | None       | Running     | private=10.0.0.197 |\n| d8f366e2-5867-421c-ab86-c15c06802f48 | vm-195 | ACTIVE | None       | Running     | private=10.0.0.196 |\n| 5ea852aa-02a8-409a-9dc0-8abd5dcd6dc0 | vm-196 | ACTIVE | None       | Running     | private=10.0.0.201 |\n| 2c60b14a-d60a-4fbc-9f1c-509c5fb5ed8c | vm-197 | ACTIVE | None       | Running     | private=10.0.0.199 |\n| eacddd5d-0e3d-4c7d-9bdb-2f9010ff83e3 | vm-198 | ACTIVE | None       | Running     | private=10.0.0.202 |\n| 5dc1f542-abef-4d28-946c-e87b8f5cd48f | vm-199 | ACTIVE | None       | Running     | private=10.0.0.200 |\n| bba098a3-de0a-40b0-b09e-4b6916287a97 | vm-2   | ACTIVE | None       | Running     | private=10.0.0.4   |\n| 5192cdf0-d4ce-4c02-948e-c5659271da80 | vm-20  | ACTIVE | None       | Running     | private=10.0.0.23  |\n| c13cccd3-9d43-4511-93ea-d023e03f8f36 | vm-200 | ACTIVE | None       | Running     | private=10.0.0.203 |\n\nI am using a very simple scripts to boot all these VMs:\n\nemagana@rock:~$ more performance.sh\n#!/bin/bash\nCOUNTER=0\nfor i in {1..200}\ndo\n   echo \"Deploy VM vm-$i\"\n   nova boot --flavor 42 --image tty-quantum --nic net-id=fced80ba-100f-43f0-ab2e-a2bf657d15ab vm-$i\n   let COUNTER=$COUNTER+1\n   if [ $COUNTER -eq 300 ]\n      then\n      sleep 5\n      COUNTER=0\n   fi\ndone", 
            "date_created": "2013-05-22 21:55:52.388010+00:00", 
            "author": "https://api.launchpad.net/1.0/~emagana"
        }, 
        {
            "content": "Recently I upgrade my openstack setup from grizzly-g3 to grizzly stable, I have not got this bug yet.\r\n\r\n-----Original Message-----\r\nFrom: <email address hidden> [mailto:<email address hidden>] On Behalf Of Edgar Magana\r\nSent: Thursday, May 23, 2013 5:56 AM\r\nTo: Shu, Xinxin\r\nSubject: [Bug 1160442] Re: when boot many vms with quantum, nova sometimes allocates two quantum ports rather than one\r\n\r\nI am not able to reproduce this bug in my setup. I am using almost the same network configuration that the one posted in this bug report but I am using stable/grizzly with only one compute node, I am able to reach up to 200 VMs with no problems at all:\r\n\r\n| 284a318b-84df-4008-a386-d4a7b0510dee | vm-178 | ACTIVE | None       | Running     | private=10.0.0.178 |\r\n| b2e1bbec-e93c-4b48-9843-b45a45a33b87 | vm-179 | ACTIVE | None       | Running     | private=10.0.0.182 |\r\n| c8f5d908-79c3-4020-adca-6348f76f44cf | vm-18  | ACTIVE | None       | Running     | private=10.0.0.22  |\r\n| 2f452943-92f5-41c1-a294-f5e52ef97052 | vm-180 | ACTIVE | None       | Running     | private=10.0.0.184 |\r\n| 43314b6c-7f6f-4da3-8054-5a8158a48ef9 | vm-181 | ACTIVE | None       | Running     | private=10.0.0.183 |\r\n| 3821636d-c94b-4030-95d4-47c183893227 | vm-182 | ACTIVE | None       | Running     | private=10.0.0.187 |\r\n| f6b27836-acf3-49a9-a960-a2a6835df534 | vm-183 | ACTIVE | None       | Running     | private=10.0.0.185 |\r\n| c798b778-b471-4a1f-b43c-f9d69873354d | vm-184 | ACTIVE | None       | Running     | private=10.0.0.186 |\r\n| 99a023ec-aeff-4451-9e3c-27316d1ffd78 | vm-185 | ACTIVE | None       | Running     | private=10.0.0.188 |\r\n| 4475818f-801a-4a59-9a85-1113e54e0c18 | vm-186 | ACTIVE | None       | Running     | private=10.0.0.192 |\r\n| 56145c8c-a645-4bdf-84b8-5078c19c39a4 | vm-187 | ACTIVE | None       | Running     | private=10.0.0.189 |\r\n| f2c3074a-43aa-4506-b3a9-43f2ff3f461c | vm-188 | ACTIVE | None       | Running     | private=10.0.0.190 |\r\n| f12c882a-c25b-46f2-8550-7ec807d3b0b7 | vm-189 | ACTIVE | None       | Running     | private=10.0.0.191 |\r\n| 9a40b604-0f9e-44d0-a080-59bfb9adedf5 | vm-19  | ACTIVE | None       | Running     | private=10.0.0.20  |\r\n| 952a25a6-501b-44de-9149-0cb25b217d96 | vm-190 | ACTIVE | None       | Running     | private=10.0.0.194 |\r\n| 5012700f-83a7-4f9d-85a7-796a65043aed | vm-191 | ACTIVE | None       | Running     | private=10.0.0.193 |\r\n| 9d7bbc67-55e1-49e8-8c7b-6f6bd5b2183a | vm-192 | ACTIVE | None       | Running     | private=10.0.0.195 |\r\n| 13192f4b-541f-4e92-b3f5-51e51d28408a | vm-193 | ACTIVE | None       | Running     | private=10.0.0.198 |\r\n| 4ff125dc-8f1d-40b1-9db6-738665cdb8e7 | vm-194 | ACTIVE | None       | Running     | private=10.0.0.197 |\r\n| d8f366e2-5867-421c-ab86-c15c06802f48 | vm-195 | ACTIVE | None       | Running     | private=10.0.0.196 |\r\n| 5ea852aa-02a8-409a-9dc0-8abd5dcd6dc0 | vm-196 | ACTIVE | None       | Running     | private=10.0.0.201 |\r\n| 2c60b14a-d60a-4fbc-9f1c-509c5fb5ed8c | vm-197 | ACTIVE | None       | Running     | private=10.0.0.199 |\r\n| eacddd5d-0e3d-4c7d-9bdb-2f9010ff83e3 | vm-198 | ACTIVE | None       | Running     | private=10.0.0.202 |\r\n| 5dc1f542-abef-4d28-946c-e87b8f5cd48f | vm-199 | ACTIVE | None       | Running     | private=10.0.0.200 |\r\n| bba098a3-de0a-40b0-b09e-4b6916287a97 | vm-2   | ACTIVE | None       | Running     | private=10.0.0.4   |\r\n| 5192cdf0-d4ce-4c02-948e-c5659271da80 | vm-20  | ACTIVE | None       | Running     | private=10.0.0.23  |\r\n| c13cccd3-9d43-4511-93ea-d023e03f8f36 | vm-200 | ACTIVE | None       | Running     | private=10.0.0.203 |\r\n\r\nI am using a very simple scripts to boot all these VMs:\r\n\r\nemagana@rock:~$ more performance.sh\r\n#!/bin/bash\r\nCOUNTER=0\r\nfor i in {1..200}\r\ndo\r\n   echo \"Deploy VM vm-$i\"\r\n   nova boot --flavor 42 --image tty-quantum --nic net-id=fced80ba-100f-43f0-ab2e-a2bf657d15ab vm-$i\r\n   let COUNTER=$COUNTER+1\r\n   if [ $COUNTER -eq 300 ]\r\n      then\r\n      sleep 5\r\n      COUNTER=0\r\n   fi\r\ndone\r\n\r\n--\r\nYou received this bug notification because you are subscribed to the bug report.\r\nhttps://bugs.launchpad.net/bugs/1160442\r\n\r\nTitle:\r\n  when boot many vms with quantum, nova sometimes allocates two quantum\r\n  ports rather than one\r\n\r\nStatus in OpenStack Compute (Nova):\r\n  Triaged\r\nStatus in OpenStack Quantum (virtual network service):\r\n  Incomplete\r\n\r\nBug description:\r\n  i have install grizzly-g3, but quantum does not work well, when i boot\r\n  128 instances, i found one of instances got more than one fixed ip,\r\n  howerver, when i boot 64 intances, it nerver happened, besides that ,\r\n  sometimes i can not ping vm with floatingip, i did not find any error\r\n  message in my quantum log( all the files in the /var/log/quantum),\r\n  follows are the error output and configurations\r\n\r\n  | 97a93600-38e2-4700-9851-15ef56c1d628 | slave  | ACTIVE | demo-int-net=172.16.100.4                   |\r\n  | 99aeb6b8-4252-4839-a7d1-f87853116100 | slave  | ACTIVE | demo-int-net=172.16.100.117                 |\r\n  | 9aa82a35-c9f1-4f44-a108-d14e74eec231 | slave  | ACTIVE | demo-int-net=172.16.100.108, 172.16.100.109 |\r\n  | 9b6b1289-c450-4614-b647-e5ebdffff80a | slave  | ACTIVE | demo-int-net=172.16.100.5                   |\r\n  | 9e0d3aa5-0f15-4b24-944a-6d6c3e18ce64 | slave  | ACTIVE | demo-int-net=172.16.100.35                  |\r\n  | 9ea62124-9128-43cc-acdd-142f1e7743d6 | slave  | ACTIVE | demo-int-net=172.16.100.132                 |\r\n\r\n  \r\n  my setup : one db host(db service), one glance host(glance service), on api host(keystone,nova-api,nova-scheduler, nova-conductor, quantum-server,quantum-dhcp, quantum-l3-agent,quantum-plugin-openvswitch-agent), eight compute host(each host with nova-compute, quantum-plugin-openvswitch-agent), i check that all the service on all hosts works well\r\n   \r\n  i used vlan type network and openvswitch plugin:\r\n\r\n  my quantum.conf\r\n\r\n  [DEFAULT]\r\n  # Default log level is INFO\r\n  # verbose and debug has the same result.\r\n  # One of them will set DEBUG log level output\r\n  debug = True\r\n\r\n  # Address to bind the API server\r\n  bind_host = 0.0.0.0\r\n\r\n  # Port the bind the API server to\r\n  bind_port = 9696\r\n\r\n  \r\n  # Quantum plugin provider module\r\n  # core_plugin =\r\n  core_plugin = quantum.plugins.openvswitch.ovs_quantum_plugin.OVSQuantumPluginV2\r\n  # Advanced service modules\r\n  # service_plugins =\r\n\r\n  # Paste configuration file\r\n  api_paste_config = /etc/quantum/api-paste.ini\r\n\r\n  # The strategy to be used for auth.\r\n  # Supported values are 'keystone'(default), 'noauth'.\r\n  auth_strategy = keystone\r\n\r\n  \r\n  # Modules of exceptions that are permitted to be recreated\r\n  # upon receiving exception data from an rpc call.\r\n  # allowed_rpc_exception_modules = quantum.openstack.common.exception, nova.exception\r\n  # AMQP exchange to connect to if using RabbitMQ or QPID\r\n  control_exchange = quantum\r\n\r\n  # RPC driver. DHCP agents needs it.\r\n  notification_driver = quantum.openstack.common.notifier.rpc_notifier\r\n\r\n  # default_notification_level is used to form actual topic name(s) or to set logging level\r\n  default_notification_level = INFO\r\n\r\n  \r\n  # Defined in rpc_notifier, can be comma separated values.\r\n  # The actual topic names will be %s.%(default_notification_level)s\r\n  notification_topics = notifications\r\n\r\n  [QUOTAS]\r\n  # resource name(s) that are supported in quota features\r\n  # quota_items = network,subnet,port\r\n\r\n  # default number of resource allowed per tenant, minus for unlimited\r\n  # default_quota = -1\r\n\r\n  # number of networks allowed per tenant, and minus means unlimited\r\n  # quota_network = 10\r\n\r\n  # number of subnets allowed per tenant, and minus means unlimited\r\n  # quota_subnet = 10\r\n\r\n  # number of ports allowed per tenant, and minus means unlimited\r\n  quota_port = 5000\r\n  quota_floatingip = 5000\r\n\r\n  # default driver to use for quota checks\r\n  # quota_driver = quantum.quota.ConfDriver\r\n\r\n  # =========== items for agent management extension =============\r\n  # Seconds to regard the agent as down.\r\n  # agent_down_time = 5\r\n  # ===========  end of items for agent management extension =====\r\n\r\n  [DEFAULT_SERVICETYPE]\r\n  # Description of the default service type (optional)\r\n  # description = \"default service type\"\r\n  # Enter a service definition line for each advanced service provided\r\n  # by the default service type.\r\n  # Each service definition should be in the following format:\r\n  # <service>:<plugin>[:driver]\r\n\r\n  [SECURITYGROUP]\r\n  # If set to true this allows quantum to receive proxied security group calls from nova\r\n  # proxy_mode = False\r\n\r\n  [AGENT]\r\n  root_helper = sudo quantum-rootwrap /etc/quantum/rootwrap.conf\r\n\r\n  # =========== items for agent management extension =============\r\n  # seconds between nodes reporting state to server, should be less than\r\n  # agent_down_time\r\n  # report_interval = 4\r\n\r\n  # ===========  end of items for agent management extension =====\r\n\r\n  [keystone_authtoken]\r\n  auth_host = host-keystone\r\n  auth_port = 35357\r\n  auth_protocol = http\r\n  admin_tenant_name = demoTenant\r\n  admin_user = test\r\n  admin_password = 123456\r\n  signing_dir = /var/lib/quantum/keystone-signing\r\n\r\n  my dhcp_agent.ini\r\n\r\n  [DEFAULT]\r\n  # Where to store dnsmasq state files.  This directory must be writable by the\r\n  # user executing the agent.\r\n  state_path = /var/lib/quantum\r\n\r\n  # OVS based plugins(OVS, Ryu, NEC, NVP, BigSwitch/Floodlight)\r\n  interface_driver = quantum.agent.linux.interface.OVSInterfaceDriver\r\n\r\n  # The agent can use other DHCP drivers.  Dnsmasq is the simplest and requires\r\n  # no additional setup of the DHCP server.\r\n  dhcp_driver = quantum.agent.linux.dhcp.Dnsmasq\r\n\r\n  my ovs_quantum_plugin.ini configure file\r\n\r\n  [DATABASE]\r\n  # This line MUST be changed to actually run the plugin.\r\n  sql_connection = mysql://quantum:quantum@host-db/quantum\r\n\r\n  # Database reconnection interval in seconds - if the initial connection to the\r\n  # database fails\r\n  reconnect_interval = 2\r\n\r\n  [OVS]\r\n  # (StrOpt) Type of network to allocate for tenant networks. The\r\n  # default value 'local' is useful only for single-box testing and\r\n  # provides no connectivity between hosts. You MUST either change this\r\n  # to 'vlan' and configure network_vlan_ranges below or change this to\r\n  # 'gre' and configure tunnel_id_ranges below in order for tenant\r\n  # networks to provide connectivity between hosts. Set to 'none' to\r\n  # disable creation of tenant networks.\r\n\r\n  tenant_network_type=vlan\r\n\r\n  network_vlan_ranges = DemoNet:1:4094\r\n\r\n  bridge_mappings = DemoNet:DemoBridge\r\n\r\n  [AGENT]\r\n  # Agent's polling interval in seconds\r\n  polling_interval = 2\r\n\r\n  [SECURITYGROUP]\r\n\r\n  \r\n  when i execute \"quantum router-gateway-set\" to add external network id as the router gateway, i found that the status of  port for external network id in the router is DOWN. does it matters,  if it does,  how can i fixed it.\r\n\r\n  \r\n  this blocked me for serveral days , can someone help me sovle it, any help will be appreciated.\r\n\r\nTo manage notifications about this bug go to:\r\nhttps://bugs.launchpad.net/nova/+bug/1160442/+subscriptions\r\n", 
            "date_created": "2013-05-23 00:46:38+00:00", 
            "author": "https://api.launchpad.net/1.0/~xinxin-shu"
        }, 
        {
            "content": "Edger how about :\n\n#!/bin/bash\nCOUNTER=0\nfor i in {1..200}\ndo\n   (\n   echo \"Deploy VM vm-$i\"\n   nova boot --flavor 42 --image tty-quantum --nic net-id=fced80ba-100f-43f0-ab2e-a2bf657d15ab vm-$i\n   let COUNTER=$COUNTER+1\n   if [ $COUNTER -eq 300 ]\n      then\n      sleep 5\n      COUNTER=0\n   fi\n) & \ndone\n\n:) \n", 
            "date_created": "2013-05-23 03:15:05.312861+00:00", 
            "author": "https://api.launchpad.net/1.0/~arosen"
        }, 
        {
            "content": "When running the test pushing all VMs at the same time instead of sequencial as Aaron suggested I was able to reproduce the bug:\n\n 22bdea7f-0215-4810-82e6-1292fbbddadd | vm-54  | ACTIVE | None       | Running     | private=10.0.0.56, 10.0.0.78   |\n| 5041f4ce-52d4-40f9-b9cc-0e8eda5bb1bd | vm-55  | ACTIVE | None       | Running     | private=10.0.0.102             |\n| 9f105432-a74d-4fe7-acac-8ed423f99ef8 | vm-58  | ACTIVE | None       | Running     | private=10.0.0.115             |\n\nquantum port-list |grep 10.0.0.56\n| b39a4766-d8e9-4f86-98a5-41697bed30db |      | fa:16:3e:5e:1d:5d | {\"subnet_id\": \"b9ded0f5-faa7-40f8-abde-a1a400b056c7\", \"ip_address\": \"10.0.0.56\"}  |\n\nBoth Ports have the same device_id:\nemagana@rock:/opt/stack/logs/screen$ quantum port-show b39a4766-d8e9-4f86-98a5-41697bed30db\n+-----------------+----------------------------------------------------------------------------------+\n| Field           | Value                                                                            |\n+-----------------+----------------------------------------------------------------------------------+\n| admin_state_up  | True                                                                             |\n| device_id       | 22bdea7f-0215-4810-82e6-1292fbbddadd                                             |\n| device_owner    | compute:None                                                                     |\n| fixed_ips       | {\"subnet_id\": \"b9ded0f5-faa7-40f8-abde-a1a400b056c7\", \"ip_address\": \"10.0.0.56\"} |\n| id              | b39a4766-d8e9-4f86-98a5-41697bed30db                                             |\n| mac_address     | fa:16:3e:5e:1d:5d                                                                |\n| name            |                                                                                  |\n| network_id      | aedb8668-e23a-463b-932a-7e95f0d005b1                                             |\n| security_groups | eea80a63-eb12-4d77-bac9-95a5195c98fb                                             |\n| status          | DOWN                                                                             |\n| tenant_id       | 3bb1aac4f1a547f48af504db0a6d64e2                                                 |\n+-----------------+----------------------------------------------------------------------------------+\nemagana@rock:/opt/stack/logs/screen$ quantum port-show d8aac4ec-e22b-4547-98d5-ce4c0c88ba2c\n+-----------------+----------------------------------------------------------------------------------+\n| Field           | Value                                                                            |\n+-----------------+----------------------------------------------------------------------------------+\n| admin_state_up  | True                                                                             |\n| device_id       | 22bdea7f-0215-4810-82e6-1292fbbddadd                                             |\n| device_owner    | compute:None                                                                     |\n| fixed_ips       | {\"subnet_id\": \"b9ded0f5-faa7-40f8-abde-a1a400b056c7\", \"ip_address\": \"10.0.0.78\"} |\n| id              | d8aac4ec-e22b-4547-98d5-ce4c0c88ba2c                                             |\n| mac_address     | fa:16:3e:bb:02:2b                                                                |\n| name            |                                                                                  |\n| network_id      | aedb8668-e23a-463b-932a-7e95f0d005b1                                             |\n| security_groups | eea80a63-eb12-4d77-bac9-95a5195c98fb                                             |\n| status          | ACTIVE                                                                           |\n| tenant_id       | 3bb1aac4f1a547f48af504db0a6d64e2                                                 |\n+-----------------+----------------------------------------------------------------------------------+\n\nIt seems that allocate_for_instance() is being called just once: (I did not find another call for this VM with the two ports in the logs)\n\nallocate_for_instance() for vm-54 allocate_for_instance /opt/stack/nova/nova/network/quantumv2/api.py:153\n\nI see some ERRORs (Time out) in Quantum:\n\nTraceback (most recent call last):\n  File \"/opt/stack/quantum/quantum/api/v2/resource.py\", line 82, in resource\n    result = method(request=request, **args)\n  File \"/opt/stack/quantum/quantum/api/v2/base.py\", line 239, in index\n    return self._items(request, True, parent_id)\n  File \"/opt/stack/quantum/quantum/api/v2/base.py\", line 192, in _items\n    obj_list = obj_getter(request.context, **kwargs)\n  File \"/opt/stack/quantum/quantum/plugins/openvswitch/ovs_quantum_plugin.py\", line 554, in get_networks\n    limit, marker, page_reverse)\n  File \"/opt/stack/quantum/quantum/db/db_base_plugin_v2.py\", line 1019, in get_networks\n    page_reverse=page_reverse)\n  File \"/opt/stack/quantum/quantum/db/db_base_plugin_v2.py\", line 240, in _get_collection\n    items = [dict_func(c, fields) for c in query.all()]\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py\", line 2115, in all\n    return list(self)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py\", line 2227, in __iter__\n    return self._execute_and_instances(context)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py\", line 2240, in _execute_and_instances\n    close_with_result=True)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py\", line 2231, in _connection_from_session\n    **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 777, in connection\n    close_with_result=close_with_result)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 781, in _connection_for_bind\n    return self.transaction._connection_for_bind(engine)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 306, in _connection_for_bind\n    conn = bind.contextual_connect()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 2489, in contextual_connect\n    self.pool.connect(),\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 236, in connect\n    return _ConnectionFairy(self).checkout()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 401, in __init__\n    rec = self._connection_record = pool._do_get()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 738, in _do_get\n    (self.size(), self.overflow(), self._timeout))\nTimeoutError: QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30\n2013-05-22 22:28:24    DEBUG [quantum.policy] loading policy file at /etc/quantum/policy.json\n\nThe logs for nova-cpu related to that device_id (22bdea7f-0215-4810-82e6-1292fbbddadd):\n\nemagana@rock:/opt/stack/logs/screen$ grep -r \"22bdea7f-0215-4810-82e6-1292fbbddadd\" screen-n-cpu.log\n\n2013-05-22 22:26:31.905 26460 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'anotherrole', u'Member', u'admin'], u'_context_request_id': u'req-597f6296-0a89-4f67-972c-95d3c60883b0', u'_context_quota_class': None, u'_context_project_name':\n u'demo', u'_context_service_catalog': [{u'endpoints': [{u'adminURL': u'http://10.1.1.200:8776/v1/3bb1aac4f1a547f48af504db0a6d64e2', u'region': u'RegionOne', u'id': u'12755c1ae8e44155ba0a3865d267b6a4', u'internalURL': u'http://10.1.1.200:8776/v1/3bb1aac4f1a547f48a\nf504db0a6d64e2', u'publicURL': u'http://10.1.1.200:8776/v1/3bb1aac4f1a547f48af504db0a6d64e2'}], u'endpoints_links': [], u'type': u'volume', u'name': u'cinder'}], u'_context_user_name': u'demo', u'_context_auth_token': '<SANITIZED>', u'args': {u'node': u'rock.plumg\nrid.com', u'request_spec': {u'block_device_mapping': [], u'image': {u'status': u'active', u'name': u'tty-quantum', u'deleted': False, u'container_format': u'ami', u'created_at': u'2013-05-23T05:14:40.000000', u'disk_format': u'ami', u'updated_at': u'2013-05-23T05:\n14:40.000000', u'id': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'owner': u'f612f4b5a5ff4e129cf18705fa4b9f53', u'min_ram': 0, u'checksum': u'bd0818c1e5bf8267f944f5667692e5fc', u'min_disk': 0, u'is_public': True, u'deleted_at': None, u'properties': {u'kernel_id': u'\n6321291c-bad5-4e3f-b8ee-808820bc8c74', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729'}, u'size': 25165824}, u'instance_type': {u'disabled': False, u'root_gb': 0, u'name': u'm1.nano', u'flavorid': u'42', u'deleted': 0, u'created_at': u'2013-05-23T05:14:44.0\n00000', u'ephemeral_gb': 0, u'updated_at': None, u'memory_mb': 64, u'vcpus': 1, u'extra_specs': {}, u'swap': 0, u'rxtx_factor': 1.0, u'is_public': True, u'deleted_at': None, u'vcpu_weight': None, u'id': 6}, u'instance_properties': {u'vm_state': u'building', u'avai\nlability_zone': None, u'launch_index': 0, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data': None, u'vm_mode': None, u'reservation_id': u'r-oj8cm0i0', u'config_drive_id': u'', u'root_device_name': None, u'user_id': u'3b36a6a76935439cbfa18de2417d61e2', u'dis\nplay_description': u'vm-54', u'key_data': None, u'power_state': 0, u'progress': 0, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'config_drive': u'', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'access_ip_v6': None, u'access_ip_v4': None, u'ker\nnel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key_name': None, u'display_name': u'vm-54', u'system_metadata': {u'image_kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'instance_type_memory_mb': 64, u'instance_type_swap': 0, u'instance_type_vcpu_weight'\n: None, u'instance_type_root_gb': 0, u'instance_type_name': u'm1.nano', u'image_ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'instance_type_id': 6, u'instance_type_ephemeral_gb': 0, u'instance_type_rxtx_factor': 1.0, u'instance_type_flavorid': u'42', u'i\nnstance_type_vcpus': 1, u'image_base_image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b'}, u'root_gb': 0, u'locked': False, u'launch_time': u'2013-05-23T05:26:20Z', u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'architectur\ne': None, u'auto_disk_config': None, u'os_type': None, u'metadata': {}}, u'security_group': [u'default'], u'instance_uuids': [u'22bdea7f-0215-4810-82e6-1292fbbddadd']}, u'requested_networks': [[u'aedb8668-e23a-463b-932a-7e95f0d005b1', None, None]], u'filter_proper\nties': {u'config_options': {}, u'limits': {u'memory_mb': 24036.0}, u'request_spec': {u'block_device_mapping': [], u'image': {u'status': u'active', u'name': u'tty-quantum', u'deleted': False, u'container_format': u'ami', u'created_at': u'2013-05-23T05:14:40.000000'\n, u'disk_format': u'ami', u'updated_at': u'2013-05-23T05:14:40.000000', u'id': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'owner': u'f612f4b5a5ff4e129cf18705fa4b9f53', u'min_ram': 0, u'checksum': u'bd0818c1e5bf8267f944f5667692e5fc', u'min_disk': 0, u'is_public': Tr\nue, u'deleted_at': None, u'properties': {u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729'}, u'size': 25165824}, u'instance_type': {u'disabled': False, u'root_gb': 0, u'name': u'm1.nano', u'flavorid': u'4\n2', u'deleted': 0, u'created_at': u'2013-05-23T05:14:44.000000', u'ephemeral_gb': 0, u'updated_at': None, u'memory_mb': 64, u'vcpus': 1, u'extra_specs': {}, u'swap': 0, u'rxtx_factor': 1.0, u'is_public': True, u'deleted_at': None, u'vcpu_weight': None, u'id': 6},\nu'instance_properties': {u'vm_state': u'building', u'availability_zone': None, u'launch_index': 0, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data': None, u'vm_mode': None, u'reservation_id': u'r-oj8cm0i0', u'config_drive_id': u'', u'root_device_name': Non\ne, u'user_id': u'3b36a6a76935439cbfa18de2417d61e2', u'display_description': u'vm-54', u'key_data': None, u'power_state': 0, u'progress': 0, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'config_drive': u'', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2\n729', u'access_ip_v6': None, u'access_ip_v4': None, u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key_name': None, u'display_name': u'vm-54', u'system_metadata': {u'image_kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'instance_type_memory_mb': 6\n4, u'instance_type_swap': 0, u'instance_type_vcpu_weight': None, u'instance_type_root_gb': 0, u'instance_type_name': u'm1.nano', u'image_ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'instance_type_id': 6, u'instance_type_ephemeral_gb': 0, u'instance_type\n_rxtx_factor': 1.0, u'instance_type_flavorid': u'42', u'instance_type_vcpus': 1, u'image_base_image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b'}, u'root_gb': 0, u'locked': False, u'launch_time': u'2013-05-23T05:26:20Z', u'memory_mb': 64, u'vcpus': 1, u'image_ref\n': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'architecture': None, u'auto_disk_config': None, u'os_type': None, u'metadata': {}}, u'security_group': [u'default'], u'instance_uuids': [u'22bdea7f-0215-4810-82e6-1292fbbddadd']}, u'instance_type': {u'disabled': False,\n u'root_gb': 0, u'name': u'm1.nano', u'flavorid': u'42', u'deleted': 0, u'created_at': u'2013-05-23T05:14:44.000000', u'ephemeral_gb': 0, u'updated_at': None, u'memory_mb': 64, u'vcpus': 1, u'extra_specs': {}, u'swap': 0, u'rxtx_factor': 1.0, u'is_public': True, u\n'deleted_at': None, u'vcpu_weight': None, u'id': 6}, u'retry': {u'num_attempts': 1, u'hosts': [[u'rock', u'rock.plumgrid.com']]}, u'os_type': None, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'scheduler_hints': {}}, u'instance': {u'vm_state': u'building',\n u'availability_zone': None, u'terminated_at': None, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data': None, u'vm_mode': None, u'deleted_at': None, u'reservation_id': u'r-oj8cm0i0', u'id': 49, u'security_groups': [], u'disable_terminate': False, u'user_id'\n: u'3b36a6a76935439cbfa18de2417d61e2', u'uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'default_swap_device': None, u'info_cache': {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated\n_at': None, u'network_info': u'[]', u'deleted_at': None, u'id': 49}, u'hostname': u'vm-54', u'launched_on': None, u'display_description': u'vm-54', u'key_data': None, u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'power_state': 0, u'default_ephemeral_dev\nice': None, u'progress': 0, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'launched_at': None, u'scheduled_at': u'2013-05-23T05:26:31.760378', u'node': None, u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'access_ip_v6': None, u'access_ip_v4': Non\ne, u'deleted': 0, u'key_name': None, u'updated_at': u'2013-05-23T05:26:31.814354', u'host': None, u'architecture': None, u'display_name': u'vm-54', u'system_metadata': [{u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'201\n3-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key': u'image_kernel_id', u'deleted_at': None, u'id': 625}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T0\n5:26:25.000000', u'updated_at': None, u'value': u'64', u'key': u'instance_type_memory_mb', u'deleted_at': None, u'id': 626}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': Non\ne, u'value': u'0', u'key': u'instance_type_swap', u'deleted_at': None, u'id': 627}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': None, u'key': u'instance_typ\ne_vcpu_weight', u'deleted_at': None, u'id': 628}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_root_gb', u'deleted_at': None, u\n'id': 629}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6', u'key': u'instance_type_id', u'deleted_at': None, u'id': 630}, {u'instance_uuid': u'22bdea7f-0\n215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'key': u'image_ramdisk_id', u'deleted_at': None, u'id': 631}, {u'instance_uuid': u'22bdea7f-0215-4810\n-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'm1.nano', u'key': u'instance_type_name', u'deleted_at': None, u'id': 632}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0,\n u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_ephemeral_gb', u'deleted_at': None, u'id': 633}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:2\n6:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_type_rxtx_factor', u'deleted_at': None, u'id': 634}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None,\n u'value': u'42', u'key': u'instance_type_flavorid', u'deleted_at': None, u'id': 635}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_\ntype_vcpus', u'deleted_at': None, u'id': 636}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'key': u'image_base_ima\nge_ref', u'deleted_at': None, u'id': 637}], u'task_state': u'scheduling', u'shutdown_terminate': False, u'cell_name': None, u'root_gb': 0, u'locked': False, u'name': u'instance-00000031', u'created_at': u'2013-05-23T05:26:25.000000', u'launch_index': 0, u'metadata\n': [], u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'root_device_name': None, u'auto_disk_config': None, u'os_type': None, u'config_drive': u''}, u'admin_password': '<SANITIZED>', u'injected_files': [], u'is_first_time': T\nrue}, u'_context_tenant': u'3bb1aac4f1a547f48af504db0a6d64e2', u'_context_instance_lock_checked': False, u'_context_timestamp': u'2013-05-23T05:26:00.440653', u'_context_is_admin': False, u'version': u'2.19', u'_context_project_id': u'3bb1aac4f1a547f48af504db0a6d6\n4e2', u'_context_user': u'3b36a6a76935439cbfa18de2417d61e2', u'_unique_id': u'56a64367199640ff8627095c9bfee52c', u'_context_read_deleted': u'no', u'_context_user_id': u'3b36a6a76935439cbfa18de2417d61e2', u'method': u'run_instance', u'_context_remote_address': u'10\n.1.1.200'} _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:276\n2013-05-22 22:26:32.391 DEBUG nova.openstack.common.lockutils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Got semaphore \"22bdea7f-0215-4810-82e6-1292fbbddadd\" for method \"do_run_instance\"... inner /opt/stack/nova/nova/openstack/common/lockutils.py:186\n2013-05-22 22:26:32.907 AUDIT nova.compute.manager [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Starting instance...\n2013-05-22 22:26:51.152 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Attempting claim: memory 64 MB, disk 0 GB, VCPUs 1\n2013-05-22 22:26:51.152 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Total Memory: 16024 MB, used: 3584 MB\n2013-05-22 22:26:51.153 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Memory limit: 24036 MB, free: 20452 MB\n2013-05-22 22:26:51.153 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Total Disk: 884 GB, used: 0 GB\n2013-05-22 22:26:51.154 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Disk limit not specified, defaulting to unlimited\n2013-05-22 22:26:51.154 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Total CPU: 8 VCPUs, used: 48 VCPUs\n2013-05-22 22:26:51.154 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] CPU limit not specified, defaulting to unlimited\n2013-05-22 22:26:51.155 AUDIT nova.compute.claims [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Claim successful\n2013-05-22 22:30:19.153 DEBUG nova.compute.manager [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Instance network_info: |[VIF({'ovs_interfaceid': u'd8aac4ec-e22b-4547-98d5-ce4c0c88ba2c', 'network': Network({'\nbridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.0.0.78'})], 'version': 4, 'meta': {'dhcp_server': u'10.0.0.2'}, 'dns': [], 'routes': [], 'cidr': u'10.0.0.0/24', 'gateway': IP({'\nmeta': {}, 'version': 4, 'type': 'gateway', 'address': u'10.0.0.1'})})], 'meta': {'injected': False, 'tenant_id': u'3bb1aac4f1a547f48af504db0a6d64e2'}, 'id': u'aedb8668-e23a-463b-932a-7e95f0d005b1', 'label': u'private'}), 'devname': u'tapd8aac4ec-e2', 'qbh_params'\n: None, 'meta': {}, 'address': u'fa:16:3e:bb:02:2b', 'type': u'ovs', 'id': u'd8aac4ec-e22b-4547-98d5-ce4c0c88ba2c', 'qbg_params': None})]| _allocate_network /opt/stack/nova/nova/compute/manager.py:1078\n2013-05-22 22:30:42.928 26460 INFO nova.compute.manager [-] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] During sync_power_state the instance has a pending task. Skip.\n2013-05-22 22:31:39.106 INFO nova.virt.libvirt.driver [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Creating image\n2013-05-22 22:31:39.112 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): cp /opt/stack/data/nova/instances/_base/6321291c-bad5-4e3f-b8ee-808820bc8c74 /opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd\n/kernel execute /opt/stack/nova/nova/utils.py:208\n2013-05-22 22:31:39.183 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/kernel execute /opt/stack/nova/nova/utils.\npy:208\n2013-05-22 22:31:39.248 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): cp /opt/stack/data/nova/instances/_base/6c283423-2469-4fa5-996e-4b526c2e2729 /opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd\n/ramdisk execute /opt/stack/nova/nova/utils.py:208\n2013-05-22 22:31:39.304 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/ramdisk execute /opt/stack/nova/nova/utils\n.py:208\n2013-05-22 22:31:39.446 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): qemu-img create -f qcow2 -o backing_file=/opt/stack/data/nova/instances/_base/cfcf361456f6839132c91dac5f2666b8262da3b8 /opt/stack/data/nova/inst\nances/22bdea7f-0215-4810-82e6-1292fbbddadd/disk execute /opt/stack/nova/nova/utils.py:208\n2013-05-22 22:31:39.789 DEBUG nova.virt.libvirt.driver [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Start to_xml instance={u'vm_state': u'building', u'availability_zone': None, u'terminated_at': None, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data\n': None, u'vm_mode': None, u'deleted_at': None, u'reservation_id': u'r-oj8cm0i0', u'id': 49, u'security_groups': [], u'disable_terminate': False, u'root_device_name': None, u'user_id': u'3b36a6a76935439cbfa18de2417d61e2', u'uuid': u'22bdea7f-0215-4810-82e6-1292fbb\nddadd', u'default_swap_device': None, u'info_cache': {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': u'2013-05-23T05:30:18.000000', u'network_info': u'[{\"ovs_interfaceid\": \"d8a\nac4ec-e22b-4547-98d5-ce4c0c88ba2c\", \"network\": {\"bridge\": \"br-int\", \"subnets\": [{\"ips\": [{\"meta\": {}, \"version\": 4, \"type\": \"fixed\", \"floating_ips\": [], \"address\": \"10.0.0.78\"}], \"version\": 4, \"meta\": {\"dhcp_server\": \"10.0.0.2\"}, \"dns\": [], \"routes\": [], \"cidr\": \"\n10.0.0.0/24\", \"gateway\": {\"meta\": {}, \"version\": 4, \"type\": \"gateway\", \"address\": \"10.0.0.1\"}}], \"meta\": {\"injected\": false, \"tenant_id\": \"3bb1aac4f1a547f48af504db0a6d64e2\"}, \"id\": \"aedb8668-e23a-463b-932a-7e95f0d005b1\", \"label\": \"private\"}, \"devname\": \"tapd8aac4e\nc-e2\", \"qbh_params\": null, \"meta\": {}, \"address\": \"fa:16:3e:bb:02:2b\", \"type\": \"ovs\", \"id\": \"d8aac4ec-e22b-4547-98d5-ce4c0c88ba2c\", \"qbg_params\": null}]', u'deleted_at': None, u'id': 49}, u'hostname': u'vm-54', u'launched_on': u'rock', u'display_description': u'vm\n-54', u'key_data': None, u'deleted': 0, u'scheduled_at': u'2013-05-23T05:26:31.000000', u'power_state': 0, u'default_ephemeral_device': None, u'progress': 0, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'launched_at': None, u'config_drive': u'', u'node': u\n'rock.plumgrid.com', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'access_ip_v6': None, u'access_ip_v4': None, u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key_name': None, u'updated_at': u'2013-05-23T05:30:52.158725', u'host': u'rock', u'd\nisplay_name': u'vm-54', u'system_metadata': [{u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key': u'image_kernel_id'\n, u'deleted_at': None, u'id': 625}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'64', u'key': u'instance_type_memory_mb', u'deleted_at': None, u'id': 626},\n {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_swap', u'deleted_at': None, u'id': 627}, {u'instance_uuid': u'22bdea7f-0215-4810-\n82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': None, u'key': u'instance_type_vcpu_weight', u'deleted_at': None, u'id': 628}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0,\n u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_root_gb', u'deleted_at': None, u'id': 629}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.\n000000', u'updated_at': None, u'value': u'6', u'key': u'instance_type_id', u'deleted_at': None, u'id': 630}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6\nc283423-2469-4fa5-996e-4b526c2e2729', u'key': u'image_ramdisk_id', u'deleted_at': None, u'id': 631}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'm1.nano',\n u'key': u'instance_type_name', u'deleted_at': None, u'id': 632}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_ephemeral_gb', u\n'deleted_at': None, u'id': 633}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_type_rxtx_factor', u'deleted_at': None, u'id': 634}, {\nu'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'42', u'key': u'instance_type_flavorid', u'deleted_at': None, u'id': 635}, {u'instance_uuid': u'22bdea7f-0215-48\n10-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_type_vcpus', u'deleted_at': None, u'id': 636}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'\ncreated_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'key': u'image_base_image_ref', u'deleted_at': None, u'id': 637}], u'task_state': u'spawning', u'shutdown_terminate': False, u'cell_name': None, u'\nroot_gb': 0, u'locked': False, u'name': u'instance-00000031', u'created_at': u'2013-05-23T05:26:25.000000', u'launch_index': 0, u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'architecture': None, u'auto_disk_config': None,\nu'os_type': None, u'metadata': []} network_info=[({'injected': False, 'bridge': 'br-int', 'multi_host': False, 'bridge_interface': None, 'cidr_v6': None, 'cidr': u'10.0.0.0/24', 'vlan': None, 'id': u'aedb8668-e23a-463b-932a-7e95f0d005b1'}, {'vif_type': u'ovs', 'sh\nould_create_bridge': False, 'vif_devname': u'tapd8aac4ec-e2', 'broadcast': '10.0.0.255', 'ips': [{'ip': u'10.0.0.78', 'netmask': '255.255.255.0', 'enabled': '1', 'gateway': u'10.0.0.1'}], 'mac': u'fa:16:3e:bb:02:2b', 'should_create_vlan': False, 'gateway': u'10.0.\n0.1', 'ovs_interfaceid': u'd8aac4ec-e22b-4547-98d5-ce4c0c88ba2c', 'rxtx_cap': 0, 'label': u'private', 'qbh_params': None, 'vif_uuid': u'd8aac4ec-e22b-4547-98d5-ce4c0c88ba2c', 'dns': [], 'dhcp_server': u'10.0.0.2', 'qbg_params': None})] disk_info={'disk_bus': 'virt\nio', 'cdrom_bus': 'ide', 'mapping': {'disk': {'bus': 'virtio', 'type': 'disk', 'dev': 'vda'}, 'root': {'bus': 'virtio', 'type': 'disk', 'dev': 'vda'}}} image_meta={'status': u'active', 'name': u'tty-quantum', 'deleted': False, 'container_format': u'ami', 'created_\nat': datetime.datetime(2013, 5, 23, 5, 14, 40, tzinfo=<iso8601.iso8601.Utc object at 0x3234850>), 'disk_format': u'ami', 'updated_at': datetime.datetime(2013, 5, 23, 5, 14, 40, tzinfo=<iso8601.iso8601.Utc object at 0x3234850>), 'id': u'431b6066-3bc8-4948-b234-6b54\n55cd4a5b', 'owner': u'f612f4b5a5ff4e129cf18705fa4b9f53', 'min_ram': 0, 'checksum': u'bd0818c1e5bf8267f944f5667692e5fc', 'min_disk': 0, 'is_public': True, 'deleted_at': None, 'properties': {u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'ramdisk_id': u'6c2\n83423-2469-4fa5-996e-4b526c2e2729'}, 'size': 25165824} rescue=Noneblock_device_info={'block_device_mapping': [], 'root_device_name': None, 'ephemerals': [], 'swap': None} to_xml /opt/stack/nova/nova/virt/libvirt/driver.py:2333\n  <uuid>22bdea7f-0215-4810-82e6-1292fbbddadd</uuid>\n      <entry name=\"uuid\">22bdea7f-0215-4810-82e6-1292fbbddadd</entry>\n    <kernel>/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/kernel</kernel>\n    <initrd>/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/ramdisk</initrd>\n      <source file=\"/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/disk\"/>\n      <source path=\"/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/console.log\"/>\n2013-05-22 22:32:05.351 DEBUG nova.virt.libvirt.driver [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] End to_xml instance={u'vm_state': u'building', u'availability_zone': None, u'terminated_at': None, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data':\n None, u'vm_mode': None, u'deleted_at': None, u'reservation_id': u'r-oj8cm0i0', u'id': 49, u'security_groups': [], u'disable_terminate': False, u'root_device_name': None, u'user_id': u'3b36a6a76935439cbfa18de2417d61e2', u'uuid': u'22bdea7f-0215-4810-82e6-1292fbbdd\nadd', u'default_swap_device': None, u'info_cache': {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': u'2013-05-23T05:30:18.000000', u'network_info': u'[{\"ovs_interfaceid\": \"d8aac\n4ec-e22b-4547-98d5-ce4c0c88ba2c\", \"network\": {\"bridge\": \"br-int\", \"subnets\": [{\"ips\": [{\"meta\": {}, \"version\": 4, \"type\": \"fixed\", \"floating_ips\": [], \"address\": \"10.0.0.78\"}], \"version\": 4, \"meta\": {\"dhcp_server\": \"10.0.0.2\"}, \"dns\": [], \"routes\": [], \"cidr\": \"10\n.0.0.0/24\", \"gateway\": {\"meta\": {}, \"version\": 4, \"type\": \"gateway\", \"address\": \"10.0.0.1\"}}], \"meta\": {\"injected\": false, \"tenant_id\": \"3bb1aac4f1a547f48af504db0a6d64e2\"}, \"id\": \"aedb8668-e23a-463b-932a-7e95f0d005b1\", \"label\": \"private\"}, \"devname\": \"tapd8aac4ec-\ne2\", \"qbh_params\": null, \"meta\": {}, \"address\": \"fa:16:3e:bb:02:2b\", \"type\": \"ovs\", \"id\": \"d8aac4ec-e22b-4547-98d5-ce4c0c88ba2c\", \"qbg_params\": null}]', u'deleted_at': None, u'id': 49}, u'hostname': u'vm-54', u'launched_on': u'rock', u'display_description': u'vm-5\n4', u'key_data': None, u'deleted': 0, u'scheduled_at': u'2013-05-23T05:26:31.000000', u'power_state': 0, u'default_ephemeral_device': None, u'progress': 0, u'project_id': u'3bb1aac4f1a547f48af504db0a6d64e2', u'launched_at': None, u'config_drive': u'', u'node': u'r\nock.plumgrid.com', u'ramdisk_id': u'6c283423-2469-4fa5-996e-4b526c2e2729', u'access_ip_v6': None, u'access_ip_v4': None, u'kernel_id': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key_name': None, u'updated_at': u'2013-05-23T05:30:52.158725', u'host': u'rock', u'dis\nplay_name': u'vm-54', u'system_metadata': [{u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6321291c-bad5-4e3f-b8ee-808820bc8c74', u'key': u'image_kernel_id',\nu'deleted_at': None, u'id': 625}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'64', u'key': u'instance_type_memory_mb', u'deleted_at': None, u'id': 626}, {\nu'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_swap', u'deleted_at': None, u'id': 627}, {u'instance_uuid': u'22bdea7f-0215-4810-82\ne6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': None, u'key': u'instance_type_vcpu_weight', u'deleted_at': None, u'id': 628}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u\n'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_root_gb', u'deleted_at': None, u'id': 629}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.00\n0000', u'updated_at': None, u'value': u'6', u'key': u'instance_type_id', u'deleted_at': None, u'id': 630}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'6c2\n83423-2469-4fa5-996e-4b526c2e2729', u'key': u'image_ramdisk_id', u'deleted_at': None, u'id': 631}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'm1.nano', u\n'key': u'instance_type_name', u'deleted_at': None, u'id': 632}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'0', u'key': u'instance_type_ephemeral_gb', u'd\neleted_at': None, u'id': 633}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_type_rxtx_factor', u'deleted_at': None, u'id': 634}, {u'\ninstance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'42', u'key': u'instance_type_flavorid', u'deleted_at': None, u'id': 635}, {u'instance_uuid': u'22bdea7f-0215-4810\n-82e6-1292fbbddadd', u'deleted': 0, u'created_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'1', u'key': u'instance_type_vcpus', u'deleted_at': None, u'id': 636}, {u'instance_uuid': u'22bdea7f-0215-4810-82e6-1292fbbddadd', u'deleted': 0, u'cr\neated_at': u'2013-05-23T05:26:25.000000', u'updated_at': None, u'value': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'key': u'image_base_image_ref', u'deleted_at': None, u'id': 637}], u'task_state': u'spawning', u'shutdown_terminate': False, u'cell_name': None, u'ro\not_gb': 0, u'locked': False, u'name': u'instance-00000031', u'created_at': u'2013-05-23T05:26:25.000000', u'launch_index': 0, u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'431b6066-3bc8-4948-b234-6b5455cd4a5b', u'architecture': None, u'auto_disk_config': None, u'\nos_type': None, u'metadata': []} xml=<domain type=\"kvm\">\n  <uuid>22bdea7f-0215-4810-82e6-1292fbbddadd</uuid>\n      <entry name=\"uuid\">22bdea7f-0215-4810-82e6-1292fbbddadd</entry>\n    <kernel>/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/kernel</kernel>\n    <initrd>/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/ramdisk</initrd>\n      <source file=\"/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/disk\"/>\n      <source path=\"/opt/stack/data/nova/instances/22bdea7f-0215-4810-82e6-1292fbbddadd/console.log\"/>\n2013-05-22 22:32:13.218 DEBUG nova.utils [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl -- --may-exist add-port br-int qvod8aac4ec-e2 -- set Interface qvod8aac4ec-e2 external-ids:\niface-id=d8aac4ec-e22b-4547-98d5-ce4c0c88ba2c external-ids:iface-status=active external-ids:attached-mac=fa:16:3e:bb:02:2b external-ids:vm-uuid=22bdea7f-0215-4810-82e6-1292fbbddadd execute /opt/stack/nova/nova/utils.py:208\n2013-05-22 22:32:20.955 26460 INFO nova.compute.manager [-] Lifecycle event 0 on VM 22bdea7f-0215-4810-82e6-1292fbbddadd\n2013-05-22 22:32:21.154 26460 INFO nova.compute.manager [-] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] During sync_power_state the instance has a pending task. Skip.\n2013-05-22 22:32:21.600 DEBUG nova.virt.libvirt.driver [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Instance is running spawn /opt/stack/nova/nova/virt/libvirt/driver.py:1529\n2013-05-22 22:32:23.515 26460 INFO nova.virt.libvirt.driver [-] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Instance spawned successfully.\n2013-05-22 22:32:23.517 DEBUG nova.compute.manager [req-597f6296-0a89-4f67-972c-95d3c60883b0 demo demo] [instance: 22bdea7f-0215-4810-82e6-1292fbbddadd] Checking state _get_power_state /opt/stack/nova/nova/compute/manager.py:616\n\n\nI dont see where the problem could be but in my set-up it seems that the issue is related to the Quantum DB Time out, any extra ideas for testing?\n\nThanks,\n\nEdgar", 
            "date_created": "2013-05-23 19:09:11.655256+00:00", 
            "author": "https://api.launchpad.net/1.0/~emagana"
        }, 
        {
            "content": "Awsome! (yea i as definitely guessing things would timeout trying to launch 100 instances at once :) )  So, we know that this can occur with just one nova-compute node. Is there any chance we could get a copy of the full logs uploaded to dropbox or w/e?  I think if we add some print statements to allocate_for_instance() showing the time and instance id we should be able to figure out what's going on. ", 
            "date_created": "2013-05-23 23:51:09.813104+00:00", 
            "author": "https://api.launchpad.net/1.0/~arosen"
        }, 
        {
            "content": "Ok, I will add the print statement to allocate_for_instance(), in the meantime I have posted the logs on Dropbox:\n\nhttps://www.dropbox.com/sh/rbwkqj3t9zzkjbu/Il6UKsuw9t", 
            "date_created": "2013-05-24 00:20:00.490673+00:00", 
            "author": "https://api.launchpad.net/1.0/~emagana"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/31657", 
            "date_created": "2013-06-04 15:20:07.940369+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "After investigation: Problem would be a timout in the quantum client accessing the quantum service. Once this was increased the issues disappeared but it exposed the actual problem:\n\n2013-06-04 10:46:20    ERROR [quantum.api.v2.resource] index failed\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/quantum/api/v2/resource.py\", line 82, in resource\n    result = method(request=request, **args)\n  File \"/usr/lib/python2.6/site-packages/quantum/api/v2/base.py\", line 239, in index\n    return self._items(request, True, parent_id)\n  File \"/usr/lib/python2.6/site-packages/quantum/api/v2/base.py\", line 192, in _items\n    obj_list = obj_getter(request.context, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/quantum/plugins/openvswitch/ovs_quantum_plugin.py\", line 597, in get_ports\n    page_reverse)\n  File \"/usr/lib/python2.6/site-packages/quantum/db/db_base_plugin_v2.py\", line 1433, in get_ports\n    items = [self._make_port_dict(c, fields) for c in query.all()]\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/query.py\", line 2115, in all\n    return list(self)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/query.py\", line 2227, in __iter__\n    return self._execute_and_instances(context)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/query.py\", line 2240, in _execute_and_instances\n    close_with_result=True)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/query.py\", line 2231, in _connection_from_session\n    **kw)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py\", line 730, in connection\n    close_with_result=close_with_result)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py\", line 734, in _connection_for_bind\n    return self.transaction._connection_for_bind(engine)\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py\", line 267, in _connection_for_bind\n    conn = bind.contextual_connect()\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/engine/base.py\", line 2490, in contextual_connect\n    self.pool.connect(),\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/pool.py\", line 224, in connect\n    return _ConnectionFairy(self).checkout()\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/pool.py\", line 387, in __init__\n    rec = self._connection_record = pool._do_get()\n  File \"/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/pool.py\", line 729, in _do_get\n    (self.size(), self.overflow(), self._timeout))\nTimeoutError: QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30\n", 
            "date_created": "2013-06-04 15:35:46.221071+00:00", 
            "author": "https://api.launchpad.net/1.0/~garyk"
        }, 
        {
            "content": "did you get to the bottom of why a timeout in python-quantumclient would have caused a port to be allocated twice for an instance?", 
            "date_created": "2013-06-04 17:30:44.438658+00:00", 
            "author": "https://api.launchpad.net/1.0/~salvatore-orlando"
        }, 
        {
            "content": "i am still investigating that. the \"TimeoutError: QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30\" on the quantum service has me thinking that the service continues to proceed with the opertion. i am still investigating", 
            "date_created": "2013-06-04 17:59:43.933396+00:00", 
            "author": "https://api.launchpad.net/1.0/~garyk"
        }, 
        {
            "content": "When setting the timeout to a value that is very small then the problem does not occur.  for example:\n_ENGINE = create_engine(sql_connection, pool_timeout=2, **engine_args)\nIn addition to this when the I increase the pool size and add in a overflow size the problem does not occur\n\nFor a temporary fix we can add the following configuration parameters:\n   - pool_size\n   - max_overflow\n   - pool_timeout\n\nI'll do this tomorrow\n\n", 
            "date_created": "2013-06-04 18:46:06.186986+00:00", 
            "author": "https://api.launchpad.net/1.0/~garyk"
        }, 
        {
            "content": "When the patch is through it is a matter of tuning the QueuePool parameters.\nThanks\nGary", 
            "date_created": "2013-06-05 09:05:01.698209+00:00", 
            "author": "https://api.launchpad.net/1.0/~garyk"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/31657\nCommitted: http://github.com/openstack/quantum/commit/725d636da5959aee1954a8e8d5ff383d2a8b7bb9\nSubmitter: Jenkins\nBranch:    master\n\ncommit 725d636da5959aee1954a8e8d5ff383d2a8b7bb9\nAuthor: Gary Kotton <email address hidden>\nDate:   Tue Jun 4 15:14:39 2013 +0000\n\n    Expose most common QueuePoll parameters from SQLAlchemy\n    \n    Fixes bug 1160442\n    \n    The defualt parameters are used unless otherwise configured. This\n    is applicable for:\n     - pool_size\n     - max_overflow\n     - pool_timeout\n    \n    By increasing the pool_size and the max_overflow and lowering the timeout\n    the issue reported in the bug is resolved.\n    \n    NOTE: the configuration values enable the user to make use of the default\n    QueuePool tuning values.\n    \n    Change-Id: Ic258442531a2caf2db4f05c4ddd6fbfb0c180f17\n", 
            "date_created": "2013-06-10 16:42:27.040591+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/grizzly\nReview: https://review.openstack.org/32484", 
            "date_created": "2013-06-10 21:41:13.245203+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "oslo-incubator patch: https://review.openstack.org/31753", 
            "date_created": "2013-06-11 11:03:37.071730+00:00", 
            "author": "https://api.launchpad.net/1.0/~markmc"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/31753\nCommitted: http://github.com/openstack/oslo-incubator/commit/ebaa578351c9c6b47c2f28ef6d74451e1483036b\nSubmitter: Jenkins\nBranch:    master\n\ncommit ebaa578351c9c6b47c2f28ef6d74451e1483036b\nAuthor: Gary Kotton <email address hidden>\nDate:   Wed Jun 5 08:06:15 2013 +0000\n\n    Enable user to configure pool_timeout\n    \n    Fixes bug 1160442\n    \n    In addition to this there are the following changes:\n    1. The set_default method can configure the QueuePool parameters\n    2. The max_pool_size is defaulted to the QueuePool default\n    \n    Change-Id: Ie99f7fc4edba68127c4af508290d8074d7094be9\n", 
            "date_created": "2013-06-11 11:08:47.933491+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/grizzly\nReview: https://review.openstack.org/32691", 
            "date_created": "2013-06-12 07:24:13.025195+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/32691\nCommitted: http://github.com/openstack/quantum/commit/8ad0295015d8d1077de4610a3e10fc25858f65b9\nSubmitter: Jenkins\nBranch:    stable/grizzly\n\ncommit 8ad0295015d8d1077de4610a3e10fc25858f65b9\nAuthor: Gary Kotton <email address hidden>\nDate:   Tue Jun 4 15:14:39 2013 +0000\n\n    Expose most common QueuePoll parameters from SQLAlchemy\n    \n    Fixes bug 1160442\n    \n    The defualt parameters are used unless otherwise configured. This\n    is applicable for:\n     - pool_size\n     - max_overflow\n     - pool_timeout\n    \n    By increasing the pool_size and the max_overflow and lowering the timeout\n    the issue reported in the bug is resolved.\n    \n    NOTE: the configuration values enable the user to make use of the default\n    QueuePool tuning values.\n    \n    (cherry picked from commit 725d636da5959aee1954a8e8d5ff383d2a8b7bb9)\n    \n    Change-Id: Ic258442531a2caf2db4f05c4ddd6fbfb0c180f17\n", 
            "date_created": "2013-06-12 16:30:25.501211+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "It's not clear that multiple fixed ips being allocated for a VM when Nova and Neutron are experiencing heavy load is solved by the merged patch or whether it was merely masked.  At the least, there needs to be documentation added that makes it clear how to set max_pool_size, max_overflow and pool_timeout according to expected load.\n\nSuggestions via irc suggest max_pool_size should be set to x+1 where x is the expected maximum concurrent expected requests, with pool_timeout = 2.\n\n", 
            "date_created": "2013-12-04 14:23:32.317072+00:00", 
            "author": "https://api.launchpad.net/1.0/~maru"
        }, 
        {
            "content": "We sure this isn't the issue Aaron's reported whereby if a machine is scheduled, attempts to start, fails to start and is rescheduled we can sometimes create a second port?  (To be solved, but last I heard not yet solved, by creating the port before scheduling, rather than in the compute node.)", 
            "date_created": "2013-12-05 16:14:17.670629+00:00", 
            "author": "https://api.launchpad.net/1.0/~ijw-ubuntu"
        }, 
        {
            "content": "Status changed to 'Confirmed' because the bug affects multiple users.", 
            "date_created": "2013-12-09 00:49:49.601845+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }, 
        {
            "content": "I can confirm Ian Wells' suspicion that the problem is related to scheduling.  If there is an error interacting with Neutron during scheduling, the vm will be rescheduled but a port that was created before the scheduling failure will persist.  Nova needs to be updated to remove or reuse the port in question.\n\n\nscreen-n-sch.log:2013-12-18 10:00:27.458 ERROR nova.scheduler.filter_scheduler [req-2e3e4274-8348-471a-b9cb-69d0fd8f4180 demo demo] [instance: 45d132e7-bf85-4c18-b751-738767e49eb5] Error from last host: precise-devstack (node precise-devstack): Traceback (most recent call last):\n\n  File \"/opt/stack/nova/nova/compute/manager.py\", line 1058, in _build_instance\n    set_access_ip=set_access_ip)\n\n  File \"/opt/stack/nova/nova/compute/manager.py\", line 1462, in _spawn\n    LOG.exception(_('Instance failed to spawn'), instance=instance)\n\n  File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n\n  File \"/opt/stack/nova/nova/compute/manager.py\", line 1459, in _spawn\n    block_device_info)\n\n  File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 2168, in spawn\n    admin_pass=admin_password)\n\n  File \"/opt/stack/nova/nova/virt/libvirt/driver.py\", line 2529, in _create_image\n    content=files, extra_md=extra_md, network_info=network_info)\n\n  File \"/opt/stack/nova/nova/api/metadata/base.py\", line 165, in __init__\n    ec2utils.get_ip_info_for_instance_from_nw_info(network_info)\n\n  File \"/opt/stack/nova/nova/api/ec2/ec2utils.py\", line 149, in get_ip_info_for_instance_from_nw_info\n    fixed_ips = nw_info.fixed_ips()\n\n  File \"/opt/stack/nova/nova/network/model.py\", line 368, in _sync_wrapper\n    self.wait()\n\n  File \"/opt/stack/nova/nova/network/model.py\", line 400, in wait\n    self[:] = self._gt.wait()\n\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 168, in wait\n    return self._exit_event.wait()\n\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 120, in wait\n    current.throw(*self._exc)\n\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 194, in main\n    result = function(*args, **kwargs)\n\n  File \"/opt/stack/nova/nova/compute/manager.py\", line 1243, in _allocate_network_async\n    dhcp_options=dhcp_options)\n\n  File \"/opt/stack/nova/nova/network/neutronv2/api.py\", line 352, in allocate_for_instance\n    LOG.exception(msg, port_id)\n\n  File \"/opt/stack/nova/nova/openstack/common/excutils.py\", line 68, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n\n  File \"/opt/stack/nova/nova/network/neutronv2/api.py\", line 329, in allocate_for_instance\n    security_group_ids, available_macs, dhcp_opts))\n\n  File \"/opt/stack/nova/nova/network/neutronv2/api.py\", line 180, in _create_port\n    port_id = port_client.create_port(port_req_body)['port']['id']\n\n  File \"/opt/stack/python-neutronclient/neutronclient/v2_0/client.py\", line 112, in with_params\n    ret = self.function(instance, *args, **kwargs)\n\n  File \"/opt/stack/python-neutronclient/neutronclient/v2_0/client.py\", line 317, in create_port\n    return self.post(self.ports_path, body=body)\n\n  File \"/opt/stack/python-neutronclient/neutronclient/v2_0/client.py\", line 1242, in post\n    headers=headers, params=params)\n\n  File \"/opt/stack/python-neutronclient/neutronclient/v2_0/client.py\", line 1157, in do_request\n    resp, replybody = self.httpclient.do_request(action, method, body=body)\n\n  File \"/opt/stack/python-neutronclient/neutronclient/client.py\", line 192, in do_request\n    **kwargs)\n\n  File \"/opt/stack/python-neutronclient/neutronclient/client.py\", line 154, in _cs_request\n    raise exceptions.ConnectionFailed(reason=e)\n\nConnectionFailed: Connection to neutron failed: timed out\n\n ", 
            "date_created": "2013-12-18 10:24:48.162570+00:00", 
            "author": "https://api.launchpad.net/1.0/~maru"
        }, 
        {
            "content": "There is a retry logic in httplib2 used by neutron that automatically retries requests. One instance when this kicks in is: client makes a connection \u2014> issues request to the server \u2014> server takes too long to respond (if busy) \u2014> client socket timesout waiting for response and client goes for a retry. The end result is that the request may get processed twice at the server. There is a parameter RETRIES used in _conn_request(\u2026) defined in __init__.py. This parameter is not configurable, hard-coded to 2.\n\nOne possible way to deal with this is to increase the timeout value (which is configurable) to a larger value. The default is 30 seconds and increasing it to larger value helps. This will not solve the issue, but could make it less likely to occur.\n\nHere are Google references to the retry logic in httplib2:\nhttps://code.google.com/p/httplib2/issues/detail?id=109\nhttps://groups.google.com/forum/#!msg/google-api-python-client/VE1d7a7Wevg/elmt1MDsjmAJ\nhttp://httplib2.googlecode.com/hg/doc/html/libhttplib2.html", 
            "date_created": "2014-01-10 00:35:10.546993+00:00", 
            "author": "https://api.launchpad.net/1.0/~sudhi-vm"
        }, 
        {
            "content": "Saw this report just now,  but we had the same issue and were caused by slow quantum server.  In addition to port created by scheduler retrying request on a different compute, we also have a bug in the quantum client as reported here https://bugs.launchpad.net/neutron/+bug/1267649.  We were able to fix this temporarily by increasing the timeout value for the quantum_url_timeout property in nova compute to larger values, 2 minutes in our case. ", 
            "date_created": "2014-01-10 00:36:58.924356+00:00", 
            "author": "https://api.launchpad.net/1.0/~ashw7n"
        }, 
        {
            "content": "Preventing the timeout is a stop-gap, but a better solution might be for port creation to be idempotent such that multiple creates for the same port do not result in multiple ports.  Perhaps it should be possible for Nova to pass in the maximum number of ports that should be created for an instance in the creation request so that Neutron could avoid exceeding that limit?  Or Nova could provide the uuid for port creation and a duplicate index could indicate that one already exists?  There are drawbacks to either approach, but I think it's worth thinking about alternatives to the current approach.", 
            "date_created": "2014-02-04 17:13:25.888095+00:00", 
            "author": "https://api.launchpad.net/1.0/~maru"
        }, 
        {
            "content": "Fix proposed to branch: stable/havana\nReview: https://review.openstack.org/93061", 
            "date_created": "2014-05-09 15:33:29.958998+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "nova was fixed in https://review.openstack.org/#/c/87915/", 
            "date_created": "2014-07-12 19:43:14.671903+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }
    ], 
    "closed": "2014-07-12 19:44:05.241567+00:00"
}
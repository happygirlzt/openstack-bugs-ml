{
    "status": "Won't Fix", 
    "last_updated": "2014-10-13 20:54:48.674343+00:00", 
    "description": "Hi guys i'm trying to use live-migration with this scenario:\n\nOS: Ubuntu 12.04.3\nOpenStack version: Grizzly\nNova version: 2013.1.2-0ubuntu1~cloud0\nShared storage with GFS2\n\nMy migration configuration in nova-compute.conf is:\n\n...\nlive_migration_bandwidth=0\nlive_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE\nlive_migration_retry_count=30\nlive_migration_uri=qemu+tcp://%s/system\n...\n\nWhen i try to migrate an instance i get some kind of RPC time out (i sent a couple of mails to the operators list with the subject \"Migrating instances in grizzly\"):\n\nnova-compute.log\n...\n2013-09-02 15:35:45.784 4601 DEBUG nova.openstack.common.rpc.common [-] Timed out waiting for RPC response: timed out _error_callback /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py:628\n2013-09-02 15:35:45.790 4601 ERROR nova.utils [-] in fixed duration looping call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils Traceback (most recent call last):\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 594, in _inner\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.f(*self.args, **self.kw)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 3129, in wait_for_live_migration\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migrate_data)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 3208, in _post_live_migration\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\", line 664, in network_migrate_instance_start\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\", line 415, in network_migrate_instance_start\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return self.call(context, msg, version='1.41')\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 80, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return rpc.call(context, self._get_topic(topic), msg, timeout)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/__init__.py\", line 140, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return _get_impl().call(CONF, context, topic, msg, timeout)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 798, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     rpc_amqp.get_connection_pool(conf, Connection))\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 612, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     rv = list(rv)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 554, in __iter__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.done()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.gen.next()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 551, in __iter__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self._iterator.next()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 648, in iterconsume\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     yield self.ensure(_error_callback, _consume)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 566, in ensure\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     error_callback(e)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 629, in _error_callback\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     raise rpc_common.Timeout()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils Timeout: Timeout while waiting on RPC response.\n2013-09-02 15:35:45.790 4601 TRACE nova.utils\n...\n\n-The VM state never changes back to ACTIVE from MIGRATING\n-The VM actually moves from one node to the other (i can see it with \"virsh list\" on the new node) and it still answers ping\n\nAny clues?", 
    "tags": [
        "compute", 
        "libvirt"
    ], 
    "importance": "High", 
    "heat": 22, 
    "link": "https://bugs.launchpad.net/nova/+bug/1227836", 
    "owner": "None", 
    "id": 1227836, 
    "index": 1244, 
    "openned": "2013-09-19 18:49:52.567419+00:00", 
    "created": "2013-09-19 18:49:52.567419+00:00", 
    "title": "Grizzly live-migration fails in Ubuntu 12.04", 
    "comments": [
        {
            "content": "Hi guys i'm trying to use live-migration with this scenario:\n\nOS: Ubuntu 12.04.3\nOpenStack version: Grizzly\nNova version: 2013.1.2-0ubuntu1~cloud0\nShared storage with GFS2\n\nMy migration configuration in nova-compute.conf is:\n\n...\nlive_migration_bandwidth=0\nlive_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE\nlive_migration_retry_count=30\nlive_migration_uri=qemu+tcp://%s/system\n...\n\nWhen i try to migrate an instance i get some kind of RPC time out (i sent a couple of mails to the operators list with the subject \"Migrating instances in grizzly\"):\n\nnova-compute.log\n...\n2013-09-02 15:35:45.784 4601 DEBUG nova.openstack.common.rpc.common [-] Timed out waiting for RPC response: timed out _error_callback /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py:628\n2013-09-02 15:35:45.790 4601 ERROR nova.utils [-] in fixed duration looping call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils Traceback (most recent call last):\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 594, in _inner\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.f(*self.args, **self.kw)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 3129, in wait_for_live_migration\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migrate_data)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 3208, in _post_live_migration\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\", line 664, in network_migrate_instance_start\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\", line 415, in network_migrate_instance_start\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return self.call(context, msg, version='1.41')\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 80, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return rpc.call(context, self._get_topic(topic), msg, timeout)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/__init__.py\", line 140, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     return _get_impl().call(CONF, context, topic, msg, timeout)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 798, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     rpc_amqp.get_connection_pool(conf, Connection))\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 612, in call\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     rv = list(rv)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 554, in __iter__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.done()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.gen.next()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 551, in __iter__\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     self._iterator.next()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 648, in iterconsume\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     yield self.ensure(_error_callback, _consume)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 566, in ensure\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     error_callback(e)\n2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 629, in _error_callback\n2013-09-02 15:35:45.790 4601 TRACE nova.utils     raise rpc_common.Timeout()\n2013-09-02 15:35:45.790 4601 TRACE nova.utils Timeout: Timeout while waiting on RPC response.\n2013-09-02 15:35:45.790 4601 TRACE nova.utils\n...\n\n-The VM state never changes back to ACTIVE from MIGRATING\n-The VM actually moves from one node to the other (i can see it with \"virsh list\" on the new node) and it still answers ping\n\nAny clues?", 
            "date_created": "2013-09-19 18:49:52.567419+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "Are you sure it is not the issue about the RabbitMQ? \nI guess the root cause is the connection issue.", 
            "date_created": "2013-09-24 03:10:50.209624+00:00", 
            "author": "https://api.launchpad.net/1.0/~jeffrey4l"
        }, 
        {
            "content": "Do you mean this one https://bugs.launchpad.net/oslo/+bug/856764 ? I tried it last week, but didn't work. It might be something related to the message queue. I'd like to know if someone else is having the same problem with a deployment like this.", 
            "date_created": "2013-09-24 12:50:41.577932+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "Based on the reporter's ML thread, there is another user facing the same issue:\r\n\r\nhttp://lists.openstack.org/pipermail/openstack-operators/2013-September/003515.html", 
            "date_created": "2013-09-30 21:28:37.968391+00:00", 
            "author": "https://api.launchpad.net/1.0/~melwitt"
        }, 
        {
            "content": "I'm pretty sure this isn't related to libvirt, it looks like a problem with rabbit", 
            "date_created": "2013-11-11 20:38:08.196141+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "I'm seeing the same issue with Ubuntu 12.04 and Havana from cloudarchive, with a slightly different migration setup but same failure.  Instance does actually move but remains listed in MIGRATING state on the old-compute-node (even though it has moved to new-compute-node  and virsh no longer sees it on  old-compute-node).\n\nI need to reboot all my nova-compute nodes to fix an unrelated issue I'm having with a too old version of openvswitch, so getting migration to work is really important to me right now...", 
            "date_created": "2014-02-04 05:20:20.361371+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "here's the Havana version of the error BTW:\n\n2014-02-04 00:50:03.479 29816 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/loopingcall.py\", line 78, in _inner\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     self.f(*self.args, **self.kw)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 4066, in wait_for_live_migration\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     migrate_data)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 90, in wrapped\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     payload)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/exception.py\", line 73, in wrapped\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     return f(self, context, *args, **kw)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 4122, in _post_live_migration\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     instance_ref, block_migration, dest)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/compute/rpcapi.py\", line 473, in post_live_migration_at_destination\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     instance=instance_p, block_migration=block_migration)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 85, in call\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     return self._invoke(self.proxy.call, ctxt, method, **kwargs)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\", line 63, in _invoke\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     return cast_or_call(ctxt, msg, **self.kwargs)\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 130, in call\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall     exc.info, real_topic, msg.get('method'))\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall Timeout: Timeout while waiting on RPC response - topic: \"compute.blob-0\", RPC method: \"post_live_migration_at_destination\" info: \"<unknown>\"\n2014-02-04 00:50:03.479 29816 TRACE nova.openstack.common.loopingcall \n2014-02-04 00:50:03.496 29816 WARNING nova.openstack.common.rpc.amqp [-] No calling threads waiting for msg_id : 97a819ab005d430da4c05bfd9b4a972a, message : {u'_unique_id': u'96135bf5d12f4480b4bf0493985af903', u'failure': u'{\"args\": [\"Timeout while waiting on RPC response - topic: \\\\\"conductor\\\\\", RPC method: \\\\\"network_migrate_instance_finish\\\\\" info: \\\\\"<unknown>\\\\\"\"], \"module\": \"nova.openstack.common.rpc.common\", \"kwargs\": {\"info\": \"<unknown>\", \"topic\": \"conductor\", \"method\": \"network_migrate_instance_finish\"}, \"message\": \"Timeout while waiting on RPC response - topic: \\\\\"conductor\\\\\", RPC method: \\\\\"network_migrate_instance_finish\\\\\" info: \\\\\"<unknown>\\\\\"\", \"tb\": [\"Traceback (most recent call last):\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\\\\\", line 461, in _process_data\\\\n    **args)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py\\\\\", line 172, in dispatch\\\\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/exception.py\\\\\", line 90, in wrapped\\\\n    payload)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/exception.py\\\\\", line 73, in wrapped\\\\n    return f(self, context, *args, **kw)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\\\\\", line 4184, in post_live_migration_at_destination\\\\n    migration)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\\\\\", line 310, in network_migrate_instance_finish\\\\n    migration)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\\\\\", line 458, in network_migrate_instance_finish\\\\n    instance=instance_p, migration=migration_p)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\\\\\", line 85, in call\\\\n    return self._invoke(self.proxy.call, ctxt, method, **kwargs)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/rpcclient.py\\\\\", line 63, in _invoke\\\\n    return cast_or_call(ctxt, msg, **self.kwargs)\\\\n\", \"  File \\\\\"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\\\\\", line 130, in call\\\\n    exc.info, real_topic, msg.get(\\'method\\'))\\\\n\", \"Timeout: Timeout while waiting on RPC response - topic: \\\\\"conductor\\\\\", RPC method: \\\\\"network_migrate_instance_finish\\\\\" info: \\\\\"<unknown>\\\\\"\\\\n\"], \"class\": \"Timeout\"}', u'result': None}\n2014-02-04 00:50:03.496 29816 WARNING nova.openstack.common.rpc.amqp [-] _call_waiters: {}\n\n\n", 
            "date_created": "2014-02-04 05:56:13.903365+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "As the error involves the method 'network_migrate_instance_finish' i tmay be relevant to note that I'm using Neutron (OVS) networking.  I know operators who have this working with 12.04/Grizzly/nova-network ....", 
            "date_created": "2014-02-04 15:24:31.430038+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "Same scenario here: running OVS with quantum from grizzly cloud-archive.\n\n\n2014-02-04 Jon Proulx <email address hidden>:\n\n> As the error involves the method 'network_migrate_instance_finish' i\n> tmay be relevant to note that I'm using Neutron (OVS) networking.  I\n> know operators who have this working with 12.04/Grizzly/nova-network\n> ....\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/1227836\n>\n> Title:\n>   Grizzly live-migration fails in Ubuntu 12.04\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/1227836/+subscriptions\n>\n\n\n\n-- \nPavlik Salles Juan Jos\u00e9\n", 
            "date_created": "2014-02-04 15:37:20+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "The log error seems to come up on source system very close to 60sec after instance is created on destination.  Since this is teh default value of rpc_response_timeout and an rpc response seems to be what's timing out I tried increasing that to 360 an drestarting nova-compute, bu tfailure is still at 60sec.\n\nIs there another timeout that may be related?\n", 
            "date_created": "2014-02-04 20:36:27.180773+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "I have some rough plans to rework live migration to remove more of the rpc call operations, I guess it takes so long with neutron the RPC times out. But it does fight against the move to conductor work, which sucks a bit. :(\n\nIncreasing the timeout probably makes sense, I think you might be able to set the timeout on that explicit call, but I can't remember now, to be totally honest.", 
            "date_created": "2014-02-05 10:29:47.112198+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "PS, I don't think this is libvirt specific.", 
            "date_created": "2014-02-05 10:30:10.268728+00:00", 
            "author": "https://api.launchpad.net/1.0/~johngarbutt"
        }, 
        {
            "content": "Is it a timeout problem? Or something gets stuck?\nEl feb 5, 2014 7:36 a.m., \"John Garbutt\" <email address hidden> escribi\u00f3:\n\n> PS, I don't think this is libvirt specific.\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/1227836\n>\n> Title:\n>   Grizzly live-migration fails in Ubuntu 12.04\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/1227836/+subscriptions\n>\n", 
            "date_created": "2014-02-05 10:40:49+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "I can verify that it works for me:\n\nUbuntu 12.04 LTS, kernel 3.2.0-54\nopenvswitch 1.4.0-1ubuntu1.5\nquantum 1:2013.1.3-0ubuntu1~cloud0 / 1:2013.1.4-0ubuntu1~cloud0\n\nI've completed a live block migration between two hosts. Quantum is setup to configure gre tunneling and I confirmed that once the migration was complete the vm resumed successfully and was contactable.\n\n", 
            "date_created": "2014-02-05 12:32:25.833471+00:00", 
            "author": "https://api.launchpad.net/1.0/~jesse-pretorius"
        }, 
        {
            "content": "Jesse, was that on a loaded production system? It will take less time to move on an unloaded development or test cluster.\n\nI tried (rather ham fistedly) to increase the waiting time in virt/libvirt/driver.py (well setting initial_delay=120, for a 2min delay before starting to wait), but it failed in the same way (just after 3min rather than 1).  this may be the wrong place to wait, but the instance was active in libvirt on the remote end after about 60sec.\n\nMy failed kludge:\n\n diff virt/libvirt/driver.py virt/libvirt/driver-longwait.py\n4069c4069\n<         timer.start(interval=0.5).wait()\n---\n>         timer.start(interval=0.5,initial_delay=120).wait()\n\nLarger context of that:\n\n        # Waiting for completion of live_migration.                                                                                                 \n        timer = loopingcall.FixedIntervalLoopingCall(f=None)\n\n        def wait_for_live_migration():\n            \"\"\"waiting for live migration completion.\"\"\"\n            try:\n                self.get_info(instance)['state']\n            except exception.InstanceNotFound:\n                timer.stop()\n                post_method(context, instance, dest, block_migration,\n                            migrate_data)\n\n        timer.f = wait_for_live_migration\n        timer.start(interval=0.5,initial_delay=120).wait()\n", 
            "date_created": "2014-02-05 16:44:06.706122+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "Hi Jesse could you shre yor nova-compute.conf file?\n\n\n2014-02-05 Jon Proulx <email address hidden>:\n\n> Jesse, was that on a loaded production system? It will take less time to\n> move on an unloaded development or test cluster.\n>\n> I tried (rather ham fistedly) to increase the waiting time in\n> virt/libvirt/driver.py (well setting initial_delay=120, for a 2min delay\n> before starting to wait), but it failed in the same way (just after 3min\n> rather than 1).  this may be the wrong place to wait, but the instance\n> was active in libvirt on the remote end after about 60sec.\n>\n> My failed kludge:\n>\n>  diff virt/libvirt/driver.py virt/libvirt/driver-longwait.py\n> 4069c4069\n> <         timer.start(interval=0.5).wait()\n> ---\n> >         timer.start(interval=0.5,initial_delay=120).wait()\n>\n> Larger context of that:\n>\n>         # Waiting for completion of live_migration.\n>         timer = loopingcall.FixedIntervalLoopingCall(f=None)\n>\n>         def wait_for_live_migration():\n>             \"\"\"waiting for live migration completion.\"\"\"\n>             try:\n>                 self.get_info(instance)['state']\n>             except exception.InstanceNotFound:\n>                 timer.stop()\n>                 post_method(context, instance, dest, block_migration,\n>                             migrate_data)\n>\n>         timer.f = wait_for_live_migration\n>         timer.start(interval=0.5,initial_delay=120).wait()\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/1227836\n>\n> Title:\n>   Grizzly live-migration fails in Ubuntu 12.04\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/1227836/+subscriptions\n>\n\n\n\n-- \nPavlik Salles Juan Jos\u00e9\n", 
            "date_created": "2014-02-05 17:37:14+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "Using local conductor on the compute node seems to fix this for me at least for 1st test.\n\n[database]\nconnection=mysql://$nova_user:$nova_db_pass@$db_host/nova\n\n[conductor]\nuse_local=true\n\ninstance launched on dest. node               13:03:13.299 \nmigration reports complete on src node 13:03:32.861 \n\nsince this whole thing takes 20sec. the apparent timeout puzzles me. My message queue isn't very busy and I've got many conductor instances, none of which seem very busy either.  Can't quite believe a few trips through that add >40sec to a 20sec operation.", 
            "date_created": "2014-02-05 18:54:21.227714+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "I can't wait to test this on our cloud, unluckily i left the office and\nwon't be back until March (happy hollidays :D). I will try this as soon as\ni get back. Why would this be problem? Is it a RPC problem with\nnova-conductor (i'm using it).\n\n\n2014-02-05 Jon Proulx <email address hidden>:\n\n> Using local conductor on the compute node seems to fix this for me at\n> least for 1st test.\n>\n> [database]\n> connection=mysql://$nova_user:$nova_db_pass@$db_host/nova\n>\n> [conductor]\n> use_local=true\n>\n> instance launched on dest. node               13:03:13.299\n> migration reports complete on src node 13:03:32.861\n>\n> since this whole thing takes 20sec. the apparent timeout puzzles me. My\n> message queue isn't very busy and I've got many conductor instances,\n> none of which seem very busy either.  Can't quite believe a few trips\n> through that add >40sec to a 20sec operation.\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/1227836\n>\n> Title:\n>   Grizzly live-migration fails in Ubuntu 12.04\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/1227836/+subscriptions\n>\n\n\n\n-- \nPavlik Salles Juan Jos\u00e9\n", 
            "date_created": "2014-02-05 19:52:27+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "Is this still valid? Grizzly is not supported upstream any more.", 
            "date_created": "2014-09-09 23:05:16.111071+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Now you mentioned it, I tried live migration in the same cloud a few days ago and although there's still a time out problem the migration took place. We scripted a workaround that takes care of the compute node change  so everything is tidy.\n\nSeems that we'll need to move forward to havana. ", 
            "date_created": "2014-09-10 01:15:35.247804+00:00", 
            "author": "https://api.launchpad.net/1.0/~jjpavlik"
        }, 
        {
            "content": "My cloud is now on Icehouse but the local conductor kludge has been\ncarried forward since Grizzly so I'm not sure if it is a current\nproblem or not.  I'll try and get some time to setup a test.\n\nOn Tue, Sep 9, 2014 at 9:15 PM, Juan Pavlik <email address hidden> wrote:\n> Now you mentioned it, I tried live migration in the same cloud a few\n> days ago and although there's still a time out problem the migration\n> took place. We scripted a workaround that takes care of the compute node\n> change  so everything is tidy.\n>\n> Seems that we'll need to move forward to havana.\n>\n> --\n> You received this bug notification because you are subscribed to the bug\n> report.\n> https://bugs.launchpad.net/bugs/1227836\n>\n> Title:\n>   Grizzly live-migration fails in Ubuntu 12.04\n>\n> Status in OpenStack Compute (Nova):\n>   Incomplete\n>\n> Bug description:\n>   Hi guys i'm trying to use live-migration with this scenario:\n>\n>   OS: Ubuntu 12.04.3\n>   OpenStack version: Grizzly\n>   Nova version: 2013.1.2-0ubuntu1~cloud0\n>   Shared storage with GFS2\n>\n>   My migration configuration in nova-compute.conf is:\n>\n>   ...\n>   live_migration_bandwidth=0\n>   live_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE\n>   live_migration_retry_count=30\n>   live_migration_uri=qemu+tcp://%s/system\n>   ...\n>\n>   When i try to migrate an instance i get some kind of RPC time out (i\n>   sent a couple of mails to the operators list with the subject\n>   \"Migrating instances in grizzly\"):\n>\n>   nova-compute.log\n>   ...\n>   2013-09-02 15:35:45.784 4601 DEBUG nova.openstack.common.rpc.common [-] Timed out waiting for RPC response: timed out _error_callback /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py:628\n>   2013-09-02 15:35:45.790 4601 ERROR nova.utils [-] in fixed duration looping call\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils Traceback (most recent call last):\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/utils.py\", line 594, in _inner\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.f(*self.args, **self.kw)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py\", line 3129, in wait_for_live_migration\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     migrate_data)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 3208, in _post_live_migration\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/api.py\", line 664, in network_migrate_instance_start\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     migration)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py\", line 415, in network_migrate_instance_start\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     return self.call(context, msg, version='1.41')\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py\", line 80, in call\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     return rpc.call(context, self._get_topic(topic), msg, timeout)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/__init__.py\", line 140, in call\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     return _get_impl().call(CONF, context, topic, msg, timeout)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 798, in call\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     rpc_amqp.get_connection_pool(conf, Connection))\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 612, in call\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     rv = list(rv)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 554, in __iter__\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.done()\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     self.gen.next()\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py\", line 551, in __iter__\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     self._iterator.next()\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 648, in iterconsume\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     yield self.ensure(_error_callback, _consume)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 566, in ensure\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     error_callback(e)\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils   File \"/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py\", line 629, in _error_callback\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils     raise rpc_common.Timeout()\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils Timeout: Timeout while waiting on RPC response.\n>   2013-09-02 15:35:45.790 4601 TRACE nova.utils\n>   ...\n>\n>   -The VM state never changes back to ACTIVE from MIGRATING\n>   -The VM actually moves from one node to the other (i can see it with \"virsh list\" on the new node) and it still answers ping\n>\n>   Any clues?\n>\n> To manage notifications about this bug go to:\n> https://bugs.launchpad.net/nova/+bug/1227836/+subscriptions\n", 
            "date_created": "2014-09-10 03:23:57+00:00", 
            "author": "https://api.launchpad.net/1.0/~jproulx"
        }, 
        {
            "content": "Grizzly isn't supported upstream anymore, closing bug. ", 
            "date_created": "2014-10-13 20:54:30.242257+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }
    ], 
    "closed": "2014-10-13 20:54:45.524095+00:00"
}
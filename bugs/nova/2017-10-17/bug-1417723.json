{
    "status": "Fix Released", 
    "last_updated": "2017-06-16 17:23:37.969833+00:00", 
    "description": "According to \"http://specs.openstack.org/openstack/nova-specs/specs/juno/approved/virt-driver-cpu-pinning.html\", the topology of the guest is set up as follows:\n\n\"In the absence of an explicit vCPU topology request, the virt drivers typically expose all vCPUs as sockets with 1 core and 1 thread. When strict CPU pinning is in effect the guest CPU topology will be setup to match the topology of the CPUs to which it is pinned.\"\n\nWhat I'm seeing is that when strict CPU pinning is in use the guest seems to be configuring multiple threads, even if the host doesn't have theading enabled.\n\nAs an example, I set up a flavor with 2 vCPUs and enabled dedicated cpus.  I then booted up an instance of this flavor on two separate compute nodes, one with hyperthreading enabled and one with hyperthreading disabled.  In both cases, \"virsh dumpxml\" gave the following topology:\n\n<topology sockets='1' cores='1' threads='2'/>\n\nWhen running on the system with hyperthreading disabled, this should presumably have been set to \"cores=2 threads=1\".\n\nTaking this a bit further, even if hyperthreading is enabled on the host it would be more accurate to only specify multiple threads in the guest topology if the vCPUs are actually affined to multiple threads of the same host core.  Otherwise it would be more accurate to specify the guest topology with multiple cores of one thread each.", 
    "tags": [
        "compute"
    ], 
    "importance": "Medium", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1417723", 
    "owner": "https://api.launchpad.net/1.0/~stephenfinucane", 
    "id": 1417723, 
    "index": 4144, 
    "openned": "2015-02-03 18:55:32.168570+00:00", 
    "created": "2015-02-03 18:55:32.168570+00:00", 
    "title": "when using dedicated cpus, the guest topology doesn't match the host", 
    "comments": [
        {
            "content": "According to \"http://specs.openstack.org/openstack/nova-specs/specs/juno/approved/virt-driver-cpu-pinning.html\", the topology of the guest is set up as follows:\n\n\"In the absence of an explicit vCPU topology request, the virt drivers typically expose all vCPUs as sockets with 1 core and 1 thread. When strict CPU pinning is in effect the guest CPU topology will be setup to match the topology of the CPUs to which it is pinned.\"\n\nWhat I'm seeing is that when strict CPU pinning is in use the guest seems to be configuring multiple threads, even if the host doesn't have theading enabled.\n\nAs an example, I set up a flavor with 2 vCPUs and enabled dedicated cpus.  I then booted up an instance of this flavor on two separate compute nodes, one with hyperthreading enabled and one with hyperthreading disabled.  In both cases, \"virsh dumpxml\" gave the following topology:\n\n<topology sockets='1' cores='1' threads='2'/>\n\nWhen running on the system with hyperthreading disabled, this should presumably have been set to \"cores=2 threads=1\".\n\nTaking this a bit further, even if hyperthreading is enabled on the host it would be more accurate to only specify multiple threads in the guest topology if the vCPUs are actually affined to multiple threads of the same host core.  Otherwise it would be more accurate to specify the guest topology with multiple cores of one thread each.", 
            "date_created": "2015-02-03 18:55:32.168570+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "Hey Chris - thanks for the bug report!\n\nwould it be possible to get the actual resulting XML of running virsh capabilites on the two hosts (or at least the interesting bits about their CPUs and NUMA topology) and the resulting instance XMLs for each host (so that we can see which actual CPUs instances got pinned to).\n\nRelated: There are some patches in flight that change behaviour regarding exposing threads to guests starting at https://review.openstack.org/#/c/229573/. It might be worth trying them out and seeing if they fix this problem.", 
            "date_created": "2015-11-10 08:25:43.560494+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }, 
        {
            "content": "Unfortunately I don't have current master running on a system with multiple numa nodes, nor do I have a lot of spare time.  I'll try to set up a test, but I don't know for sure when I'll be able to get it done.\n\nThe host XML would look something like this for the hyperthreading-disabled case:\n    <cpu>\n      <arch>x86_64</arch>\n      <model>SandyBridge</model>\n      <vendor>Intel</vendor>\n      <topology sockets='1' cores='10' threads='1'/>\n      <feature name='invtsc'/>\n      <feature name='erms'/>\n      <feature name='smep'/>\n      <feature name='fsgsbase'/>\n      <feature name='pdpe1gb'/>\n      <feature name='rdrand'/>\n      <feature name='f16c'/>\n      <feature name='osxsave'/>\n      <feature name='dca'/>\n      <feature name='pcid'/>\n      <feature name='pdcm'/>\n      <feature name='xtpr'/>\n      <feature name='tm2'/>\n      <feature name='est'/>\n      <feature name='smx'/>\n      <feature name='vmx'/>\n      <feature name='ds_cpl'/>\n      <feature name='monitor'/>\n      <feature name='dtes64'/>\n      <feature name='pbe'/>\n      <feature name='tm'/>\n      <feature name='ht'/>\n      <feature name='ss'/>\n      <feature name='acpi'/>\n      <feature name='ds'/>\n      <feature name='vme'/>\n      <pages unit='KiB' size='4'/>\n      <pages unit='KiB' size='2048'/>\n      <pages unit='KiB' size='1048576'/>\n    </cpu>\n    <topology>\n      <cells num='2'>\n        <cell id='0'>\n          <memory unit='KiB'>16696468</memory>\n          <pages unit='KiB' size='4'>3287845</pages>\n          <pages unit='KiB' size='2048'>5315</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n          <distances>\n            <sibling id='0' value='10'/>\n            <sibling id='1' value='21'/>\n          </distances>\n          <cpus num='10'>\n            <cpu id='0' socket_id='0' core_id='0' siblings='0'/>\n            <cpu id='1' socket_id='0' core_id='1' siblings='1'/>\n            <cpu id='2' socket_id='0' core_id='2' siblings='2'/>\n            <cpu id='3' socket_id='0' core_id='3' siblings='3'/>\n            <cpu id='4' socket_id='0' core_id='4' siblings='4'/>\n            <cpu id='5' socket_id='0' core_id='8' siblings='5'/>\n            <cpu id='6' socket_id='0' core_id='9' siblings='6'/>\n            <cpu id='7' socket_id='0' core_id='10' siblings='7'/>\n            <cpu id='8' socket_id='0' core_id='11' siblings='8'/>\n            <cpu id='9' socket_id='0' core_id='12' siblings='9'/>\n          </cpus>\n        </cell>\n        <cell id='1'>\n          <memory unit='KiB'>16777216</memory>\n          <pages unit='KiB' size='4'>3814400</pages>\n          <pages unit='KiB' size='2048'>6374</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n          <distances>\n            <sibling id='0' value='21'/>\n            <sibling id='1' value='10'/>\n          </distances>\n          <cpus num='10'>\n            <cpu id='10' socket_id='1' core_id='0' siblings='10'/>\n            <cpu id='11' socket_id='1' core_id='1' siblings='11'/>\n            <cpu id='12' socket_id='1' core_id='2' siblings='12'/>\n            <cpu id='13' socket_id='1' core_id='3' siblings='13'/>\n            <cpu id='14' socket_id='1' core_id='4' siblings='14'/>\n            <cpu id='15' socket_id='1' core_id='8' siblings='15'/>\n            <cpu id='16' socket_id='1' core_id='9' siblings='16'/>\n            <cpu id='17' socket_id='1' core_id='10' siblings='17'/>\n            <cpu id='18' socket_id='1' core_id='11' siblings='18'/>\n            <cpu id='19' socket_id='1' core_id='12' siblings='19'/>\n          </cpus>\n        </cell>\n      </cells>\n    </topology>\n\n\nAnd for the hyperthreading-enabled case it would look something like this:\n      <topology sockets='1' cores='10' threads='2'/>\n      <feature name='invtsc'/>\n      <feature name='erms'/>\n      <feature name='smep'/>\n      <feature name='fsgsbase'/>\n      <feature name='pdpe1gb'/>\n      <feature name='rdrand'/>\n      <feature name='f16c'/>\n      <feature name='osxsave'/>\n      <feature name='dca'/>\n      <feature name='pcid'/>\n      <feature name='pdcm'/>\n      <feature name='xtpr'/>\n      <feature name='tm2'/>\n      <feature name='est'/>\n      <feature name='smx'/>\n      <feature name='vmx'/>\n      <feature name='ds_cpl'/>\n      <feature name='monitor'/>\n      <feature name='dtes64'/>\n      <feature name='pbe'/>\n      <feature name='tm'/>\n      <feature name='ht'/>\n      <feature name='ss'/>\n      <feature name='acpi'/>\n      <feature name='ds'/>\n      <feature name='vme'/>\n      <pages unit='KiB' size='4'/>\n      <pages unit='KiB' size='2048'/>\n      <pages unit='KiB' size='1048576'/>\n    </cpu>\n    <topology>\n      <cells num='2'>\n        <cell id='0'>\n          <memory unit='KiB'>16696472</memory>\n          <pages unit='KiB' size='4'>3292966</pages>\n          <pages unit='KiB' size='2048'>5305</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n          <distances>\n            <sibling id='0' value='10'/>\n            <sibling id='1' value='21'/>\n          </distances>\n          <cpus num='20'>\n            <cpu id='0' socket_id='0' core_id='0' siblings='0,20'/>\n            <cpu id='1' socket_id='0' core_id='1' siblings='1,21'/>\n            <cpu id='2' socket_id='0' core_id='2' siblings='2,22'/>\n            <cpu id='3' socket_id='0' core_id='3' siblings='3,23'/>\n            <cpu id='4' socket_id='0' core_id='4' siblings='4,24'/>\n            <cpu id='5' socket_id='0' core_id='8' siblings='5,25'/>\n            <cpu id='6' socket_id='0' core_id='9' siblings='6,26'/>\n            <cpu id='7' socket_id='0' core_id='10' siblings='7,27'/>\n            <cpu id='8' socket_id='0' core_id='11' siblings='8,28'/>\n            <cpu id='9' socket_id='0' core_id='12' siblings='9,29'/>\n            <cpu id='20' socket_id='0' core_id='0' siblings='0,20'/>\n            <cpu id='21' socket_id='0' core_id='1' siblings='1,21'/>\n            <cpu id='22' socket_id='0' core_id='2' siblings='2,22'/>\n            <cpu id='23' socket_id='0' core_id='3' siblings='3,23'/>\n            <cpu id='24' socket_id='0' core_id='4' siblings='4,24'/>\n            <cpu id='25' socket_id='0' core_id='8' siblings='5,25'/>\n            <cpu id='26' socket_id='0' core_id='9' siblings='6,26'/>\n            <cpu id='27' socket_id='0' core_id='10' siblings='7,27'/>\n            <cpu id='28' socket_id='0' core_id='11' siblings='8,28'/>\n            <cpu id='29' socket_id='0' core_id='12' siblings='9,29'/>\n          </cpus>\n        </cell>\n        <cell id='1'>\n          <memory unit='KiB'>16777216</memory>\n          <pages unit='KiB' size='4'>3820032</pages>\n          <pages unit='KiB' size='2048'>6363</pages>\n          <pages unit='KiB' size='1048576'>1</pages>\n          <distances>\n            <sibling id='0' value='21'/>\n            <sibling id='1' value='10'/>\n          </distances>\n          <cpus num='20'>\n            <cpu id='10' socket_id='1' core_id='0' siblings='10,30'/>\n            <cpu id='11' socket_id='1' core_id='1' siblings='11,31'/>\n            <cpu id='12' socket_id='1' core_id='2' siblings='12,32'/>\n            <cpu id='13' socket_id='1' core_id='3' siblings='13,33'/>\n            <cpu id='14' socket_id='1' core_id='4' siblings='14,34'/>\n            <cpu id='15' socket_id='1' core_id='8' siblings='15,35'/>\n            <cpu id='16' socket_id='1' core_id='9' siblings='16,36'/>\n            <cpu id='17' socket_id='1' core_id='10' siblings='17,37'/>\n            <cpu id='18' socket_id='1' core_id='11' siblings='18,38'/>\n            <cpu id='19' socket_id='1' core_id='12' siblings='19,39'/>\n            <cpu id='30' socket_id='1' core_id='0' siblings='10,30'/>\n            <cpu id='31' socket_id='1' core_id='1' siblings='11,31'/>\n            <cpu id='32' socket_id='1' core_id='2' siblings='12,32'/>\n            <cpu id='33' socket_id='1' core_id='3' siblings='13,33'/>\n            <cpu id='34' socket_id='1' core_id='4' siblings='14,34'/>\n            <cpu id='35' socket_id='1' core_id='8' siblings='15,35'/>\n            <cpu id='36' socket_id='1' core_id='9' siblings='16,36'/>\n            <cpu id='37' socket_id='1' core_id='10' siblings='17,37'/>\n            <cpu id='38' socket_id='1' core_id='11' siblings='18,38'/>\n            <cpu id='39' socket_id='1' core_id='12' siblings='19,39'/>\n          </cpus>\n        </cell>\n      </cells>\n    </topology>\n", 
            "date_created": "2015-11-16 16:58:54.915265+00:00", 
            "author": "https://api.launchpad.net/1.0/~cbf123"
        }, 
        {
            "content": "So I investigated this and it seems that it's still a bug. You can sidestep the issue using the `require` CPU thread policy, though that's not a real solution. Findings below.\n\n---\n\n# Platform\n\nTesting was conducted on two single-node, Fedora 23-based\n(4.3.5-300.fc23.x86_64) OpenStack instances (built with devstack). The system\nis a dual-socket, ten core, HT-enabled system (2 sockets * 10 cores * 2 threads\n= 40 \"pCPUs\". 0-9,20-29 = node0, 10-19,30-39 = node1).\n\nCommit `8bafc9` of Nova was used.\n\n# Steps\n\n## Create flavors\n\n    $ openstack flavor create pinned.prefer \\\n        --id 101 --ram 2048 --disk 0 --vcpus 4\n    $ openstack flavor set pinned.prefer \\\n        --property \"hw:cpu_policy=dedicated\" \\\n        --property \"hw:cpu_thread_policy=prefer\"\n\n## Validate a HT-enabled node\n\nSeeing as we're not running any other instances on this host, the policy should\nensure that thread siblings are preferred and that this information is passed\nup to the host. As such, the guest should see two sockets with one core per\nsocket and two threads per core.\n\n    $ openstack server create --flavor=pinned.prefer \\\n        --image=cirros-0.3.4-x86_64-uec --wait test1\n\n    $ sudo virsh list\n     Id    Name                           State\n    ----------------------------------------------------\n     1     instance-00000001              running\n\n    $ sudo virsh dumpxml 1\n    <domain type='kvm' id='1'>\n      <name>instance-00000001</name>\n      ...\n      <vcpu placement='static'>4</vcpu>\n      <cputune>\n        <shares>4096</shares>\n        <vcpupin vcpu='0' cpuset='1'/>\n        <vcpupin vcpu='1' cpuset='21'/>\n        <vcpupin vcpu='2' cpuset='0'/>\n        <vcpupin vcpu='3' cpuset='20'/>\n        <emulatorpin cpuset='0-1,20-21'/>\n      </cputune>\n      <numatune>\n        <memory mode='strict' nodeset='0'/>\n        <memnode cellid='0' mode='strict' nodeset='0'/>\n      </numatune>\n      ...\n      <cpu>\n        <topology sockets='2' cores='1' threads='2'/>\n        <numa>\n          <cell id='0' cpus='0-3' memory='2097152' unit='KiB'/>\n        </numa>\n      </cpu>\n      ...\n    </domain>\n\n    $ openstack server delete test1\n\nNo issues here.\n\n## Validate a HT-disabled node\n\nThis is the exact same as the configuration for the HT-enabled node, but should\nresult in a different output as there are no threads. We should see four\nsockets with one cores per socket and one thread per core.\n\n    $ openstack server create --flavor=pinned.prefer \\\n        --image=cirros-0.3.4-x86_64-uec --wait test1\n\n    $ sudo virsh list\n     Id    Name                           State\n    ----------------------------------------------------\n     1     instance-00000001              running\n\n    $ sudo virsh dumpxml 1\n    <domain type='kvm' id='1'>\n      <name>instance-00000001</name>\n      ...\n      <vcpu placement='static'>4</vcpu>\n      <cputune>\n        <shares>4096</shares>\n        <vcpupin vcpu='0' cpuset='0'/>\n        <vcpupin vcpu='1' cpuset='1'/>\n        <vcpupin vcpu='2' cpuset='2'/>\n        <vcpupin vcpu='3' cpuset='3'/>\n        <emulatorpin cpuset='0-3'/>\n      </cputune>\n      <numatune>\n        <memory mode='strict' nodeset='0'/>\n        <memnode cellid='0' mode='strict' nodeset='0'/>\n      </numatune>\n      ...\n      <cpu>\n        <topology sockets='2' cores='1' threads='2'/>\n        <numa>\n          <cell id='0' cpus='0-3' memory='2097152' unit='KiB'/>\n        </numa>\n      </cpu>\n      ...\n    </domain>\n\n    $ openstack server delete test1", 
            "date_created": "2016-02-26 10:22:20.756781+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "In case it's not clear to anyone, the above is incorrect as the latter case has a 2-1-2 socket-core-thread configuration, when it should have a 4-1-1 configuration.", 
            "date_created": "2016-02-26 10:23:31.662744+00:00", 
            "author": "https://api.launchpad.net/1.0/~stephenfinucane"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/285321", 
            "date_created": "2016-02-26 14:17:47.344754+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/285321\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=0b2e34f92507fd490faaec3285049b28446dc94c\nSubmitter: Jenkins\nBranch:    master\n\ncommit 0b2e34f92507fd490faaec3285049b28446dc94c\nAuthor: Stephen Finucane <email address hidden>\nDate:   Fri Feb 26 13:07:56 2016 +0000\n\n    virt/hardware: Fix 'isolate' case on non-SMT hosts\n    \n    The 'isolate' policy is supposed to function on both hosts with an\n    SMT architecture (e.g. HyperThreading) and those without. The former\n    is true, but the latter is broken due to a an underlying implementation\n    detail in how vCPUs are \"packed\" onto pCPUs.\n    \n    The '_pack_instance_onto_cores' function expects to work with a list of\n    sibling sets. Since non-SMT hosts don't have siblings, the function is\n    being given a list of all cores as one big sibling set. However, this\n    conflicts with the idea that, in the 'isolate' case, only one sibling\n    from each sibling set should be used. Using one sibling from the one\n    available sibling set means it is not possible to schedule instances\n    with more than one vCPU.\n    \n    Resolve this mismatch by instead providing the function with a list of\n    multiple sibling sets, each containing a single core.\n    \n    This also resolves another bug. When booting instances on a non-HT\n    host, the resulting NUMA topology should not define threads. By\n    correctly considering the cores on these systems as non-siblings,\n    the resulting instance topology will contain multiple cores with only\n    a single thread in each.\n    \n    Change-Id: I2153f25fdb6382ada8e62fddf4215d9a0e3a6aa7\n    Closes-bug: #1550317\n    Closes-bug: #1417723\n", 
            "date_created": "2016-05-11 22:28:21.540150+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 14.0.0.0b1 development milestone.", 
            "date_created": "2016-06-02 19:33:52.264948+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Fix proposed to branch: stable/mitaka\nReview: https://review.openstack.org/326944", 
            "date_created": "2016-06-08 09:20:43.465800+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/326944\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=57638a8c83f98a8c139378b3b1e920b4f1c49a95\nSubmitter: Jenkins\nBranch:    stable/mitaka\n\ncommit 57638a8c83f98a8c139378b3b1e920b4f1c49a95\nAuthor: Stephen Finucane <email address hidden>\nDate:   Fri Feb 26 13:07:56 2016 +0000\n\n    virt/hardware: Fix 'isolate' case on non-SMT hosts\n    \n    The 'isolate' policy is supposed to function on both hosts with an\n    SMT architecture (e.g. HyperThreading) and those without. The former\n    is true, but the latter is broken due to a an underlying implementation\n    detail in how vCPUs are \"packed\" onto pCPUs.\n    \n    The '_pack_instance_onto_cores' function expects to work with a list of\n    sibling sets. Since non-SMT hosts don't have siblings, the function is\n    being given a list of all cores as one big sibling set. However, this\n    conflicts with the idea that, in the 'isolate' case, only one sibling\n    from each sibling set should be used. Using one sibling from the one\n    available sibling set means it is not possible to schedule instances\n    with more than one vCPU.\n    \n    Resolve this mismatch by instead providing the function with a list of\n    multiple sibling sets, each containing a single core.\n    \n    This also resolves another bug. When booting instances on a non-HT\n    host, the resulting NUMA topology should not define threads. By\n    correctly considering the cores on these systems as non-siblings,\n    the resulting instance topology will contain multiple cores with only\n    a single thread in each.\n    \n    Change-Id: I2153f25fdb6382ada8e62fddf4215d9a0e3a6aa7\n    Closes-bug: #1550317\n    Closes-bug: #1417723\n    (cherry picked from commit 0b2e34f92507fd490faaec3285049b28446dc94c)\n", 
            "date_created": "2016-08-03 22:32:54.588889+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/nova 13.1.1 release.", 
            "date_created": "2016-08-09 18:25:23.828152+00:00", 
            "author": "https://api.launchpad.net/1.0/~doug-hellmann"
        }
    ], 
    "closed": "2016-05-11 22:28:18.911002+00:00"
}
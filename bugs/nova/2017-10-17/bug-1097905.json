{
    "status": "Invalid", 
    "last_updated": "2013-02-27 01:40:45.639697+00:00", 
    "description": "When running an OpenStack VM on a host machine that uses LVM Mirroring for the filesystem that is hosting /var/lib/nova, performance can be 1/10th of native hard drive speeds due to some latency issue with LVM Mirroring.\n\nTo reproduce the problem:\n\n1. Install OpenStack controller/compute node on a single machine. Ensure that the root filesystem, which hosts /var/lib/nova/, is backed by an LVM Mirror. The setup I had was 4 drives, 1 master and 3 mirrors.\n2. Run dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct on the host and ensure near-native disk write speeds. My test showed ~124MB/s.\n3. Start an OpenStack VM and run dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct in the VM and you should get terrible disk write speeds. My test showed ~13MB/s.\n\nTo solve the problem:\n\n1. On the host machine, do lvconvert -m0 for the root filesystem. Ensure near-native disk write speeds by running the dd command above.\n2. On the VM, run the dd command above. Disk speeds should be at least 50% or more of the host's native disk write speeds.\n\nThis is most likely a libvirt or LVM2 issue, but it only surfaced when using OpenStack and LVM2 Mirroring together.\n\nOther important configuration details:\n\nHost VM:  Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-35-generic x86_64)\n\n\u00a0# dpkg -l \"*nova*\" | grep nova\nii  nova-ajax-console-proxy          2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - AJAX console proxy - transitional package\nii  nova-api                         2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - API frontend\nii  nova-cert                        2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - certificate management\nii  nova-common                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - common files\nii  nova-compute                     2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - compute node\nii  nova-compute-kvm                 2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - compute node (KVM)\nii  nova-consoleauth                 2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - Console Authenticator\nii  nova-doc                         2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - documentation\nii  nova-network                     2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - Network manager\nii  nova-scheduler                   2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - virtual machine scheduler\nii  nova-volume                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - storage\nii  python-nova                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute Python libraries\nii  python-novaclient                2012.1-0ubuntu1                            client library for OpenStack Compute API\n\n# dpkg -l \"*lvm*\"\nii  lvm2                             2.02.66-4ubuntu7.1                         The Linux Logical Volume Manager\n\n# dpkg -l \"*virt*\" | grep libvirt\nii  libvirt-bin                      0.9.8-2ubuntu17.4                          programs for the libvirt library\nii  libvirt0                         0.9.8-2ubuntu17.4                          library for interfacing with different virtualization systems\nii  python-libvirt                   0.9.8-2ubuntu17.4                          libvirt Python bindings\n\nHere's the question I asked on the #openstack IRC channel, and nobody seemed to know the answer to it:\n\n\"\"\"\nI'm having virtio disk read/write slowness issues, and I'm trying to debug if libvirt is setup correctly.\nWe are using libvirt via OpenStack. We have two OpenStack setups that are showing the same poor read performance.\nWe're running Ubuntu 12.04 for the hosts and the VMs, OpenStack Essex. From what I can tell, the VMs are using virtio. Host is using ext4, VMs are using ext4. Raw disk speed for the host machine is 120MB/s, but VMs top out at 10-20MB/s.\nThe command I'm using to benchmark is dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct\nI have double-checked the libvirt.xml files to ensure that they have the appropriate entries for <driver type='qcow2' cache='none'/> and <target dev='vda' bus='virtio'/>\nThe VM kernel log says \"Booting paravirtualized kernel on KVM\", and has the following VirtIO drivers via lspci: 00:03.0 Ethernet controller: Red Hat, Inc Virtio network device, 00:04.0 SCSI storage controller: Red Hat, Inc Virtio block device, 00:05.0 RAM memory: Red Hat, Inc Virtio memory balloon\nI have also booted just a plain 'ol cirros image via 'kvm -m 1024 -drive file=cirros.img,if=virtio,index=0 -boot c -net nic -net user -nographic -vnc :0' and gotten dismal read/write speeds  1-2MB/s\nSo, this leads me to believe that libvirt may be setup incorrectly, but I don't know how where to start looking for issues... anyone on here have any pointers?\n\"\"\"\n\nFurther testing showed the host system could do 120MB/s of throughput, but the disk latencies were quite high (via bonnie++):\n\n(LVM Mirrored)\nVersion\u00a0 1.96\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ------Sequential Output------ --Sequential Input- --Random-\nConcurrency\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 -Per Chr- --Block-- -Rewrite- -Per Chr- --Block-- --Seeks--\nMachine\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Size K/sec %CP K/sec %CP K/sec %CP K/sec %CP K/sec %CP\u00a0 /sec %CP\nproduction-1 31G\u00a0 1004\u00a0 88 120741\u00a0 23 56172\u00a0 18\u00a0 3479\u00a0 62 141812\u00a0 20\u00a0 72.7\u00a0\u00a0 2\nLatency\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 22025us\u00a0\u00a0 10328ms\u00a0\u00a0 14211ms\u00a0\u00a0\u00a0\u00a0 157ms\u00a0\u00a0\u00a0\u00a0 233ms\u00a0\u00a0\u00a0 1104ms                      <------------ !!!HIGH LATENCIES!!!\nVersion\u00a0 1.96\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ------Sequential Create------ --------Random Create--------\nproduction-1\u00a0\u00a0\u00a0\u00a0 -Create-- --Read--- -Delete-- -Create-- --Read--- -Delete--\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 files\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 16\u00a0 5570\u00a0\u00a0 5 +++++ +++ 11761\u00a0\u00a0 9 20993\u00a0 16 +++++ +++ 18943\u00a0 13\nLatency\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 528us\u00a0\u00a0\u00a0 1127us\u00a0\u00a0\u00a0\u00a0 232ms\u00a0\u00a0\u00a0\u00a0 522us\u00a0\u00a0\u00a0\u00a0\u00a0 59us\u00a0\u00a0\u00a0\u00a0 735us\n1.96,1.96,production-1,1,1357612517,31G,,1004,88,120741,23,56172,18,3479,62,141812,20,72.7,2,16,,,,,5570,5,+++++,+++,11761,9,20993,16,+++++,+++,18943,13,22025us,10328ms,14211ms,157ms,233ms,1104ms,528us,1127us,232ms,522us,59us,735us\n\nSo, I moved /var/lib/nova to a ramdisk and that helped performance tremendously (540MB/s throughput from inside the VMs). I then mounted a simple disk with ext3 on /var/lib/nova and that showed good throughput as well (128MB/s on the host, 65MB/s on the VM). I then tested drive + LVM + ext3 (same good performance). That left LVM mirroring on the main OpenStack VM host as the only culprit. I removed LVM mirroring via lvcreate -m0 and disk throughput for all of the VMs jumped from 13MB/s up to 65MB/s - 85MB/s. Note that I only had one VM running at a time to ensure that this wasn't disk contention between multiple VMs.", 
    "tags": [], 
    "importance": "Undecided", 
    "heat": 6, 
    "link": "https://bugs.launchpad.net/nova/+bug/1097905", 
    "owner": "None", 
    "id": 1097905, 
    "index": 4572, 
    "openned": "2013-01-09 19:50:00.257680+00:00", 
    "created": "2013-01-09 19:50:00.257680+00:00", 
    "title": "Poor VM disk performance on host using LVM Mirroring", 
    "comments": [
        {
            "content": "When running an OpenStack VM on a host machine that uses LVM Mirroring for the filesystem that is hosting /var/lib/nova, performance can be 1/10th of native hard drive speeds due to some latency issue with LVM Mirroring.\n\nTo reproduce the problem:\n\n1. Install OpenStack controller/compute node on a single machine. Ensure that the root filesystem, which hosts /var/lib/nova/, is backed by an LVM Mirror. The setup I had was 4 drives, 1 master and 3 mirrors.\n2. Run dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct on the host and ensure near-native disk write speeds. My test showed ~124MB/s.\n3. Start an OpenStack VM and run dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct on the host and terrible disk write speeds. My test showed ~13MB/s.\n\nTo solve the problem:\n\n1. On the host machine, do lvconvert -m0 for the root filesystem. Ensure near-native disk write speeds by running the dd command above.\n2. On the VM, run the dd command above. Disk speeds should be at least 50% or more of the host's native disk write speeds.\n\nThis is most likely a libvirt or LVM2 issue, but it only surfaced when using OpenStack and LVM2 Mirroring together.\n\nOther important configuration details:\n\nHost VM:  Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-35-generic x86_64)\n\n # dpkg -l \"*nova*\" | grep nova\nii  nova-ajax-console-proxy          2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - AJAX console proxy - transitional package\nii  nova-api                         2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - API frontend\nii  nova-cert                        2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - certificate management\nii  nova-common                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - common files\nii  nova-compute                     2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - compute node\nii  nova-compute-kvm                 2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - compute node (KVM)\nii  nova-consoleauth                 2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - Console Authenticator\nii  nova-doc                         2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - documentation\nii  nova-network                     2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - Network manager\nii  nova-scheduler                   2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - virtual machine scheduler\nii  nova-volume                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute - storage\nii  python-nova                      2012.1.3+stable-20120827-4d2a4afe-0ubuntu1 OpenStack Compute Python libraries\nii  python-novaclient                2012.1-0ubuntu1                            client library for OpenStack Compute API\n\n# dpkg -l \"*lvm*\"\nii  lvm2                             2.02.66-4ubuntu7.1                         The Linux Logical Volume Manager\n\n# dpkg -l \"*virt*\" | grep libvirt\nii  libvirt-bin                      0.9.8-2ubuntu17.4                          programs for the libvirt library\nii  libvirt0                         0.9.8-2ubuntu17.4                          library for interfacing with different virtualization systems\nii  python-libvirt                   0.9.8-2ubuntu17.4                          libvirt Python bindings\n\nHere's the question I asked on the #openstack IRC channel, and nobody seemed to know the answer to it:\n\n\"\"\"\nI'm having virtio disk read/write slowness issues, and I'm trying to debug if libvirt is setup correctly.\nWe are using libvirt via OpenStack. We have two OpenStack setups that are showing the same poor read performance. \nWe're running Ubuntu 12.04 for the hosts and the VMs, OpenStack Essex. From what I can tell, the VMs are using virtio. Host is using ext4, VMs are using ext4. Raw disk speed for the host machine is 120MB/s, but VMs top out at 10-20MB/s.\nThe command I'm using to benchmark is dd if=/dev/zero of=/tmp/test.dat bs=1G count=1 oflag=direct\nI have double-checked the libvirt.xml files to ensure that they have the appropriate entries for <driver type='qcow2' cache='none'/> and <target dev='vda' bus='virtio'/>\nThe VM kernel log says \"Booting paravirtualized kernel on KVM\", and has the following VirtIO drivers via lspci: 00:03.0 Ethernet controller: Red Hat, Inc Virtio network device, 00:04.0 SCSI storage controller: Red Hat, Inc Virtio block device, 00:05.0 RAM memory: Red Hat, Inc Virtio memory balloon\nI have also booted just a plain 'ol cirros image via 'kvm -m 1024 -drive file=cirros.img,if=virtio,index=0 -boot c -net nic -net user -nographic -vnc :0' and gotten dismal read/write speeds  1-2MB/s\nSo, this leads me to believe that libvirt may be setup incorrectly, but I don't know how where to start looking for issues... anyone on here have any pointers?\n\"\"\"\n\nFurther testing showed the host system could do 120MB/s of throughput, but the disk latencies were quite high (via bonnie++):\n\n(LVM Mirrored)\nVersion\u00a0 1.96\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ------Sequential Output------ --Sequential Input- --Random-\nConcurrency\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 -Per Chr- --Block-- -Rewrite- -Per Chr- --Block-- --Seeks--\nMachine\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Size K/sec %CP K/sec %CP K/sec %CP K/sec %CP K/sec %CP\u00a0 /sec %CP\nproduction-1 31G\u00a0 1004\u00a0 88 120741\u00a0 23 56172\u00a0 18\u00a0 3479\u00a0 62 141812\u00a0 20\u00a0 72.7\u00a0\u00a0 2\nLatency\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 22025us\u00a0\u00a0 10328ms\u00a0\u00a0 14211ms\u00a0\u00a0\u00a0\u00a0 157ms\u00a0\u00a0\u00a0\u00a0 233ms\u00a0\u00a0\u00a0 1104ms                      <------------ !!!HIGH LATENCIES!!!\nVersion\u00a0 1.96\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ------Sequential Create------ --------Random Create--------\nproduction-1\u00a0\u00a0\u00a0\u00a0 -Create-- --Read--- -Delete-- -Create-- --Read--- -Delete--\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 files\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\u00a0 /sec %CP\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 16\u00a0 5570\u00a0\u00a0 5 +++++ +++ 11761\u00a0\u00a0 9 20993\u00a0 16 +++++ +++ 18943\u00a0 13\nLatency\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 528us\u00a0\u00a0\u00a0 1127us\u00a0\u00a0\u00a0\u00a0 232ms\u00a0\u00a0\u00a0\u00a0 522us\u00a0\u00a0\u00a0\u00a0\u00a0 59us\u00a0\u00a0\u00a0\u00a0 735us\n1.96,1.96,production-1,1,1357612517,31G,,1004,88,120741,23,56172,18,3479,62,141812,20,72.7,2,16,,,,,5570,5,+++++,+++,11761,9,20993,16,+++++,+++,18943,13,22025us,10328ms,14211ms,157ms,233ms,1104ms,528us,1127us,232ms,522us,59us,735us\n\nSo, I moved /var/lib/nova to a ramdisk and that helped performance tremendously (540MB/s throughput from inside the VMs). I then mounted a simple disk with ext3 on /var/lib/nova and that showed good throughput as well (128MB/s on the host, 65MB/s on the VM). I then tested drive + LVM + ext3 (same good performance). That left LVM mirroring on the main OpenStack VM host as the only culprit. I removed LVM mirroring via lvcreate -m0 and disk throughput for all of the VMs jumped from 13MB/s up to 65MB/s - 85MB/s. Note that I only had one VM running at a time to ensure that this wasn't disk contention between multiple VMs.", 
            "date_created": "2013-01-09 19:50:00.257680+00:00", 
            "author": "https://api.launchpad.net/1.0/~msporny"
        }, 
        {
            "content": "I don't think this is something that we can deal with in openstack. This is likely a kernel/lvm issue.", 
            "date_created": "2013-02-27 00:42:04.931124+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "Ugh, no - please don't mark it as invalid. You could kick out a warning if you detect an LVM mirror backing the volume that the instances are being placed upon. At least put a warning in the documentation about this. I spent almost two weeks off and on, trying to figure out what was going wrong. I'd prefer it if nobody else had to go through that pain. ", 
            "date_created": "2013-02-27 01:40:44.409530+00:00", 
            "author": "https://api.launchpad.net/1.0/~msporny"
        }
    ], 
    "closed": "2013-02-27 00:42:13.758718+00:00"
}
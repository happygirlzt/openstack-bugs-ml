{
    "status": "Fix Released", 
    "last_updated": "2016-02-19 20:19:25.105778+00:00", 
    "description": "[Impact]\n\n * Users may sometimes fail to shutdown an instance if the associated qemu\n   process is on uninterruptable sleep (typically IO).\n\n[Test Case]\n\n * 1. create some IO load in a VM\n   2. look at the associated qemu, make sure it has STAT D in ps output\n   3. shutdown the instance\n   4. with the patch in place, nova will retry calling libvirt to shutdown\n      the instance 3 times to wait for the signal to be delivered to the \n      qemu process.\n\n[Regression Potential]\n\n * None\n\n\n\nmessage: \"Failed to terminate process\" AND message:'InstanceNotRescuable' AND message: 'Exception during message handling' AND tags:\"screen-n-cpu.txt\"\n\nThe above log stash-query reports back only the failed jobs, the 'Failed to terminate process' close other failed rescue tests,\nbut tempest does not always reports them as an error at the end.\n\nmessage: \"Failed to terminate process\" AND tags:\"screen-n-cpu.txt\"\n\nUsual console log:\nDetails: (ServerRescueTestJSON:test_rescue_unrescue_instance) Server 0573094d-53da-40a5-948a-747d181462f5 failed to reach RESCUE status and task state \"None\" within the required time (196 s). Current status: SHUTOFF. Current task state: None.\n\nhttp://logs.openstack.org/82/107982/2/gate/gate-tempest-dsvm-postgres-full/90726cb/console.html#_2014-08-07_03_50_26_520\n\nUsual n-cpu exception:\nhttp://logs.openstack.org/82/107982/2/gate/gate-tempest-dsvm-postgres-full/90726cb/logs/screen-n-cpu.txt.gz#_2014-08-07_03_32_02_855\n\n2014-08-07 03:32:02.855 ERROR oslo.messaging.rpc.dispatcher [req-39ce7a3d-5ceb-41f5-8f9f-face7e608bd1 ServerRescueTestJSON-2035684545 ServerRescueTestJSON-1017508309] Exception during message handling: Instance 0573094d-53da-40a5-948a-747d181462f5 cannot be rescued: Driver Error: Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 134, in _dispatch_and_reply\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 177, in _dispatch\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 123, in _do_dispatch\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 408, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 88, in wrapped\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     payload)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 71, in wrapped\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 292, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     pass\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 278, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 342, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 320, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 308, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3094, in rescue_instance\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     reason=_(\"Driver Error: %s\") % unicode(e))\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher InstanceNotRescuable: Instance 0573094d-53da-40a5-948a-747d181462f5 cannot be rescued: Driver Error: Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher", 
    "tags": [
        "compute", 
        "in-stable-kilo", 
        "kilo-backport-potential", 
        "libvirt", 
        "patch", 
        "testing"
    ], 
    "importance": "High", 
    "heat": 84, 
    "link": "https://bugs.launchpad.net/nova/+bug/1353939", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1353939, 
    "index": 1558, 
    "openned": "2014-08-07 10:19:57.269241+00:00", 
    "created": "2014-08-07 10:19:57.269241+00:00", 
    "title": "Rescue fails with 'Failed to terminate process: Device or resource busy' in the n-cpu log", 
    "comments": [
        {
            "content": "message: \"Failed to terminate process\" AND message:'InstanceNotRescuable' AND message: 'Exception during message handling' AND tags:\"screen-n-cpu.txt\"\n\nThe above log stash-query reports back only the failed jobs, the 'Failed to terminate process' close other failed rescue tests,\nbut tempest does not always reports them as an error at the end.\n\nmessage: \"Failed to terminate process\" AND tags:\"screen-n-cpu.txt\"\n\nUsual console log:\nDetails: (ServerRescueTestJSON:test_rescue_unrescue_instance) Server 0573094d-53da-40a5-948a-747d181462f5 failed to reach RESCUE status and task state \"None\" within the required time (196 s). Current status: SHUTOFF. Current task state: None.\n\nhttp://logs.openstack.org/82/107982/2/gate/gate-tempest-dsvm-postgres-full/90726cb/console.html#_2014-08-07_03_50_26_520\n\nUsual n-cpu exception:\nhttp://logs.openstack.org/82/107982/2/gate/gate-tempest-dsvm-postgres-full/90726cb/logs/screen-n-cpu.txt.gz#_2014-08-07_03_32_02_855\n\n2014-08-07 03:32:02.855 ERROR oslo.messaging.rpc.dispatcher [req-39ce7a3d-5ceb-41f5-8f9f-face7e608bd1 ServerRescueTestJSON-2035684545 ServerRescueTestJSON-1017508309] Exception during message handling: Instance 0573094d-53da-40a5-948a-747d181462f5 cannot be rescued: Driver Error: Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 134, in _dispatch_and_reply\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 177, in _dispatch\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 123, in _do_dispatch\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 408, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 88, in wrapped\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     payload)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 71, in wrapped\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 292, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     pass\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 278, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 342, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 320, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 308, in decorated_function\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3094, in rescue_instance\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher     reason=_(\"Driver Error: %s\") % unicode(e))\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher InstanceNotRescuable: Instance 0573094d-53da-40a5-948a-747d181462f5 cannot be rescued: Driver Error: Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:32:02.855 22829 TRACE oslo.messaging.rpc.dispatcher", 
            "date_created": "2014-08-07 10:19:57.269241+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRmFpbGVkIHRvIHRlcm1pbmF0ZSBwcm9jZXNzXCIgQU5EIG1lc3NhZ2U6XCJJbnN0YW5jZU5vdFJlc2N1YWJsZVwiIEFORCBtZXNzYWdlOlwiRXhjZXB0aW9uIGR1cmluZyBtZXNzYWdlIGhhbmRsaW5nXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1jcHUudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImN1c3RvbSIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJmcm9tIjoiMjAxNC0wNy0zMFQxNzo0NTozMiswMDowMCIsInRvIjoiMjAxNC0wOC0xM1QxNzo0NTozMiswMDowMCIsInVzZXJfaW50ZXJ2YWwiOiIwIn0sInN0YW1wIjoxNDA3OTUxOTU3NzU0fQ==", 
            "date_created": "2014-08-13 17:49:06.079738+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "\"Error from libvirt during destroy. Code=38 Error=Failed to terminate process 26425 with SIGKILL: Device or resource busy\" shows up in a lot of other test failures, not just the rescue tests, but doesn't always result in a failure.\n\nFrom the logs here:\n\nhttp://logs.openstack.org/82/107982/2/gate/gate-tempest-dsvm-postgres-full/90726cb/logs/screen-n-cpu.txt.gz?\n\nIt looks like there are other snapshot operations taking place at the same time for other concurrently running tests.", 
            "date_created": "2014-08-13 17:57:21.470406+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "query: message:\"SIGKILL\\: Device or resource busy\" AND tags:\"screen-n-cpu.txt\"\n\nHas a lot of hits but some on successful runs", 
            "date_created": "2014-08-30 01:14:13.084303+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Seen in gate, though only 3 times in 10 days, so marking as high.", 
            "date_created": "2014-09-12 13:16:38.679470+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "More useful is the stack trace of the original error, rather than the RPC error\n\n2014-08-07 03:31:46.400 ERROR nova.virt.libvirt.driver [req-39ce7a3d-5ceb-41f5-8f9f-face7e608bd1 ServerRescueTestJSON-2035684545 ServerRescueTestJSON-1017508309] [instance: 0573094d-53da-40a5-948a-747d181462f5] Error from libvirt during destroy. Code=38 Error=Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:31:46.400 ERROR nova.compute.manager [req-39ce7a3d-5ceb-41f5-8f9f-face7e608bd1 ServerRescueTestJSON-2035684545 ServerRescueTestJSON-1017508309] [instance: 0573094d-53da-40a5-948a-747d181462f5] Error trying to Rescue Instance\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5] Traceback (most recent call last):\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3088, in rescue_instance\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     rescue_image_meta, admin_password)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 2279, in rescue\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     self._destroy(instance)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 936, in _destroy\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     instance=instance)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/opt/stack/new/nova/nova/openstack/common/excutils.py\", line 82, in __exit__\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     six.reraise(self.type_, self.value, self.tb)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 908, in _destroy\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     virt_dom.destroy()\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 179, in doit\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 139, in proxy_call\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     rv = execute(f,*args,**kwargs)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/usr/lib/python2.7/dist-packages/eventlet/tpool.py\", line 77, in tworker\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     rv = meth(*args,**kwargs)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 918, in destroy\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]     if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5] libvirtError: Failed to terminate process 26425 with SIGKILL: Device or resource busy\n2014-08-07 03:31:46.400 22829 TRACE nova.compute.manager [instance: 0573094d-53da-40a5-948a-747d181462f5]\n\n\nSo this is the 'rescue' method, attempting to kill off the existing running guest, before starting the guest in its rescued configuration.\n\nWhen libvirt kills a process it sends it SIGTERM first and waits 10 seconds. If it hasn't gone it sends SIGKILL and waits another 5 seconds. If it still hasn't gone then you get this EBUSY error.\n\nUsually when a QEMU process fails to go away upon SIGKILL it is because it is stuck in an uninterruptable kernel sleep waiting on I/O from some non-responsive server.\n\nGiven the CPU load of the gate tests though, it is conceivable that the 15 second timeout is too short, particularly if the VM running tempest has a high steal time from the cloud host. ie 15 wallclock seconds may have passed, but the VM might have only have a few seconds of scheduled run time.\n\n", 
            "date_created": "2014-11-19 13:32:17.507647+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "One possible option would be to simply retry the libvirt  destroy() call a few times when we see the exception with  err.code = VIR_ERR_SYSTEM_ERROR and err.int1 == EBUSY", 
            "date_created": "2014-11-19 13:37:21.643389+00:00", 
            "author": "https://api.launchpad.net/1.0/~berrange"
        }, 
        {
            "content": "We appear to be hitting this bug in our environment when running stable/juno by doing the following:\n\n- boot instance w/ volume (nova boot --block-device source=image,id=<image id>,dest=volume,size=10,shutdown=remove,bootindex=0 ...)\n- whilst instance is active and up, rebuild instance (nova reimage ...)\n\nThe later, when issued w/ --poll, returns:\n\nError rebuilding server\nERROR (InstanceInErrorState): Failed to terminate process 10947 with SIGKILL: Device or resource busy\n\nInterestingly, I tried the above w/ an environment deployed w/ stable/icehouse and was not able to replicate the issue.\n\nI'm happy to provide any further information if required!\n\nThanks,\n\n--Matt", 
            "date_created": "2014-12-15 16:00:52.628426+00:00", 
            "author": "https://api.launchpad.net/1.0/~mattt416"
        }, 
        {
            "content": "If \"Setting instance vm_state to ERROR\"  appears in the log http://logs.openstack.org/41/145041/2/gate/gate-tempest-dsvm-neutron-full/b43d7c6/logs/screen-n-cpu.txt.gz#_2015-01-15_17_39_14_119, the issue should be considered triggered even if the gate job SUCCEEDS.\n\nThe end user would see the VM in ERROR state.", 
            "date_created": "2015-01-16 10:12:46.662985+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "The issue has bigger impact with nova network!\n\n+--------------------------------------+-------+--------+------------+-------------+------------------+\n| ID                                   | Name  | Status | Task State | Power State | Networks         |\n+--------------------------------------+-------+--------+------------+-------------+------------------+\n| 0c854e99-7ff2-4aeb-9b7b-5e32f42b500b | test  | ACTIVE | -          | Running     | private=10.1.0.2 |\n| 3a4c4f39-8283-4b69-9fd4-0a93a4984f40 | test  | ERROR  | -          | Running     | private=10.1.0.2 |\n| 4b143b19-3175-4313-8d3a-a1eb8520b70d | test3 | ACTIVE | -          | Running     | private=10.1.0.3 |\n| bc41e35e-76ad-4e43-9903-577f909ff1e2 | test3 | ERROR  | -          | Running     | private=10.1.0.3 |\n+--------------------------------------+-------+--------+------------+-------------+------------------+\n\nIf on this location an exception raised the IP become not allocated, but the security group rules/iptables are remaining there.\nA new instance can be created with the same fixedip which will use the previous/ERRORed  instances filtering rules,\ninstead of his own ones.\n\nChain nova-compute-local (1 references)\ntarget     prot opt source               destination         \nnova-compute-inst-36  all  --  anywhere             10.1.0.2            \nnova-compute-inst-37  all  --  anywhere             10.1.0.3            \nnova-compute-inst-38  all  --  anywhere             10.1.0.3         \n\n\nIf your env does not able to produce this issue normally, you can have it artificially:\n\ndiff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py\nindex 5aea859..c037542 100644\n--- a/nova/virt/libvirt/driver.py\n+++ b/nova/virt/libvirt/driver.py\n@@ -613,6 +613,9 @@ class LibvirtDriver(driver.ComputeDriver):\n         if virt_dom is not None:\n             try:\n                 old_domid = virt_dom.ID()\n+                err = libvirt.libvirtError(\"Trigger\")\n+                err.err = [libvirt.VIR_FROM_STREAMS,]\n+                raise err\n                 virt_dom.destroy()\n \n             except libvirt.libvirtError as e:\n\nAfter the patch applied restart the n-cpu and:\n$ nova boot test3 --image cirros-0.3.2-x86_64-uec --flavor 42\n$ nova delete test3  # goes to ERROR state\n$ nova boot test3 --image cirros-0.3.2-x86_64-uec --flavor 42\n\n\nThe tempest 'tox -eall test_rescued_vm_detach_volume' test can lead to the Code=38/ VIR_FROM_STREAMS exceptions\nhttp://logs.openstack.org/87/139287/14/check/check-tempest-dsvm-f21/5f3d210/logs/screen-n-cpu.txt.gz#_2015-01-16_03_27_05_790", 
            "date_created": "2015-01-22 13:57:20.455191+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Could you please provide 'dmesg' output after the crash?", 
            "date_created": "2015-02-18 16:21:09.331192+00:00", 
            "author": "https://api.launchpad.net/1.0/~pboldin"
        }, 
        {
            "content": "I have the same issue with Openstack Juno, Ubuntu 12.04 (3.13.0-40-generic, kvm 2.0.0+dfsg-2ubuntu1.9). I was running a long stress test, booting instance backed by volume and then deleting the instance, in a 'while true' loop.\n\nAfter a while I hit this issue.\n\nnova-compute.log\n-----------------------------------\n2015-04-09 14:58:20.987 8014 TRACE [..]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 918, in destroy\n2015-04-09 14:58:20.987 8014 TRACE [..]       if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)\n2015-04-09 14:58:20.987 8014 TRACE [..] libvirtError: Failed to terminate process 28634 with SIGKILL: Device or resource busy\n\nlibvirtd.log\n-------------------------------------\n2015-04-09 14:58:19.815: 27669: error : virProcessKillPainfully:320 : Failed to terminate process 28634 with SIGKILL: Device or resource busy\n2015-04-09 14:58:37.864: 27673: error : virProcessGetAffinity:433 : cannot get CPU affinity of process 28639: No such process\n2015-04-09 14:58:41.433: 27662: error : qemuMonitorIO:656 : internal error: End of file from monitor\n\ndmesg\n--------------------------------------\n<6>Apr  9 14:57:19 node-16 kernel: [612547.695013] kvm: zapping shadow pages for mmio generation wraparound\n<6>Apr  9 14:58:41 node-16 kernel: [612630.160522] qbrb4c910ab-72: port 2(tapb4c910ab-72) entered disabled state\n<6>Apr  9 14:58:41 node-16 kernel: [612630.160736] device tapb4c910ab-72 left promiscuous mode\n<6>Apr  9 14:58:41 node-16 kernel: [612630.160758] qbrb4c910ab-72: port 2(tapb4c910ab-72) entered disabled state\n\n", 
            "date_created": "2015-04-09 16:03:58.508337+00:00", 
            "author": "https://api.launchpad.net/1.0/~jordan-pittier"
        }, 
        {
            "content": "I see this on instance create sometimes in our internal CI running in nested virt (qemu) on RHEL 7.1.\n\nThis is the error from the nova compute logs:\n\n2015-05-09 17:09:35.298 4449 ERROR nova.compute.manager [req-f3ae72f5-6139-4031-b527-e506e321f4ab - - - - -] [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def] Instance failed to spawn\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def] Traceback (most recent call last):\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2501, in _build_resources\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     yield resources\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/compute/manager.py\", line 2365, in _build_and_run_instance\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     block_device_info=block_device_info)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 2354, in spawn\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     block_device_info=block_device_info)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 4391, in _create_domain_and_network\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     power_on=power_on)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 4322, in _create_domain\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     LOG.error(err)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/oslo_utils/excutils.py\", line 85, in __exit__\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     six.reraise(self.type_, self.value, self.tb)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 4312, in _create_domain\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     domain.createWithFlags(launch_flags)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 183, in doit\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     rv = execute(f, *args, **kwargs)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 122, in execute\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     six.reraise(c, e, tb)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 80, in tworker\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     rv = meth(*args, **kwargs)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]   File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 996, in createWithFlags\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def] libvirtError: internal error: Process exited prior to exec: libvirt:  error : Failed to terminate process 7231 with SIGKILL: Device or resource busy\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]\n2015-05-09 17:09:35.298 4449 TRACE nova.compute.manager [instance: 6e8122a8-0a18-408c-8518-5ed2f7470def]\n\n\nThis is the error from the qemu log for the instance/domain xml:\n\n2015-05-09 22:09:20.235+0000: starting up\nLC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name instance-00000002 -S -machine pc-i440fx-rhel7.0.0,accel=tcg,usb=off -m 512 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 6e8122a8-0a18-408c-8518-5ed2f7470def -smbios type=1,manufacturer=IBM,product=OpenStack Nova,version=2015.1-201505071728.ibm.el7.119,serial=a66bfaaf-bb7a-7736-c706-5e51fb70bdb3,uuid=6e8122a8-0a18-408c-8518-5ed2f7470def -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/instance-00000002.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -kernel /var/lib/nova/instances/6e8122a8-0a18-408c-8518-5ed2f7470def/kernel -initrd /var/lib/nova/instances/6e8122a8-0a18-408c-8518-5ed2f7470def/ramdisk -append root=/dev/vda console=tty0 console=ttyS0 no_timer_check -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/6e8122a8-0a18-408c-8518-5ed2f7470def/disk,if=none,id=drive-virtio-disk0,format=qcow2,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=23,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:d3:88:ad,bus=pci.0,addr=0x3 -chardev file,id=charserial0,path=/var/lib/nova/instances/6e8122a8-0a18-408c-8518-5ed2f7470def/console.log -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\nlibvirt:  error : Failed to terminate process 7231 with SIGKILL: Device or resource busy\n2015-05-09 22:09:35.293+0000: shutting down", 
            "date_created": "2015-05-11 01:08:49.558253+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/181781", 
            "date_created": "2015-05-11 01:49:16.547797+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/181781\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=3907867601d1044eaadebff68a590d176abff6cf\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3907867601d1044eaadebff68a590d176abff6cf\nAuthor: Matt Riedemann <email address hidden>\nDate:   Sun May 10 18:46:37 2015 -0700\n\n    libvirt: handle code=38 + sigkill (ebusy) in destroy()\n    \n    Handle the libvirt error during destroy when the sigkill fails due to an\n    EBUSY. This is taken from a comment by danpb in the bug report as a\n    potential workaround.\n    \n    Co-authored-by: Daniel Berrange (<email address hidden>)\n    \n    Closes-Bug: #1353939\n    \n    Change-Id: I128bf6b939fbbc85df521fd3fe23c3c6f93b1b2c\n", 
            "date_created": "2015-06-10 19:29:44.497015+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "And it looks like the above change isn't helping:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRXJyb3IgZnJvbSBsaWJ2aXJ0IGR1cmluZyBkZXN0cm95XCIgQU5EIG1lc3NhZ2U6XCJhdHRlbXB0XCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1jcHUudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6Ijg2NDAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQzMzk3MzEwMzQyN30=\n\nSeeing the 3 attempts failing.", 
            "date_created": "2015-06-10 21:52:24.220034+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Conversation in IRC about how ceilometer's compute- agent pollster is hitting libvirt directly and that might be contributing to the EBUSY here:\n\n(9:09:07 AM) sdague: there might be some additional libvirt bits not exposed, but we've actually told people that you shouldn't be talking to libvirt directly on a nova worker\n(9:09:46 AM) zz_dimtruck is now known as dimtruck\n(9:10:00 AM) cdent: we've been over this before. the ceilometer-compute-agent breaks all the rules and has done for a long time and for the time being it needs to until you and I find the free time to get nova to do the notifications that we've discussed wanting to do\n(9:10:59 AM) openstackgerrit: Sergey Skripnick proposed openstack-infra/project-config: Remove double curly brackets from cue builder  https://review.openstack.org/192693\n(9:11:16 AM) mriedem: fungi: we're looking to do the final icehouse release, tag icehouse-eol and drop the stable/icehouse branch - i see with the chef cookbook rename you have several changes to update .gitreview in those repos on stable/icehouse: https://review.openstack.org/#/q/status:open+branch:stable/icehouse,n,z\n(9:11:27 AM) mriedem: fungi: do we care about those? i'm also asking in #openstack-chef\n(9:11:58 AM) cdent: sdague: those items listed here as pollsters are some of those that are the result of the libvirt (and other VIRT_DRIVER) -based polling: http://docs.openstack.org/admin-guide-cloud/content/section_telemetry-compute-meters.html\n(9:12:50 AM) cdent: I agree that this is not how things should be, but they are what they are right now.\n(9:13:04 AM) cdent: So: a working devstack for ceilometer at least _wants_ libvirt\n(9:13:59 AM) cdent: What I'd like to do is make it acquire that in the least ugly and most flexible way. As far as I can tell from this discussion that means insuring that libvirt package is present before asking to install libvirt-python?\n(9:14:56 AM) fungi: mriedem: no, i pushed them with a script for all branches of all repos we renamed. feel free to abandon the ones you don't want and approve the ones you do want\n(9:15:06 AM) mriedem: fungi: ok, thanks\n(9:16:34 AM) sdague: cdent: so #3 is at least one path. I didn't realize this was polling libvirt directly, and I kind of wonder if that might be exasserbating https://bugs.launchpad.net/nova/+bug/1353939\n(9:16:36 AM) openstack: Launchpad bug 1353939 in OpenStack Compute (nova) \"Rescue fails with 'Failed to terminate process: Device or resource busy' in the n-cpu log\" [High,Fix committed] - Assigned to Matt Riedemann (mriedem)\n(9:17:26 AM) sdague: because one of the reasons we tell people not to talk to libvirt, is it's stability level seems to go down with multiple concurrent clients.\n(9:18:13 AM) mriedem: sdague: might want to capture that in the bug report b/c we pushed up a change to try and help that bug but it didn't really, at least with libvirt 1.2.2\n(9:18:20 AM) openstackgerrit: Merged openstack-infra/project-config: Move akanda functional tests out of experimental  https://review.openstack.org/190745\n(9:18:21 AM) mriedem: and danpb didn't see any reason why\n(9:18:31 AM) cdent: Good questions sdague. ceilo has been doing this kind of polling for well over a year (that's as long as I have experience) and presumably _much_ longer as it is sort of the core magic of the compute-agent\n(9:18:35 AM) openstackgerrit: David Shrewsbury proposed openstack-infra/shade: Return True/False for delete methods  https://review.openstack.org/192681\n(9:18:41 AM) mriedem: but apparently it doesn't hit as much in the centos6/fc21 jobs where libvirt is at 1.2.8 and 1.2.9 respectively\n(9:18:45 AM) Shrews: greghaynes: ^^^ context added\n(9:18:45 AM) ***cdent starts a branch to #3\n(9:19:05 AM) mriedem: cdent: we've had that bug in the gate for about a year too :)\n(9:19:15 AM) greghaynes: Shrews: mmm context\n(9:19:28 AM) cdent: mriedem: the second half of my statement is probably the more important part: far more than a year\n(9:19:52 AM) sdague: right, but we also bumped major os version in the last year, so that shuffles all the bugs around\n(9:20:19 AM) cdent: sure, I'm not trying to wave away it as being a contributor, just trying to reinforce that ceilo's contribution is not new", 
            "date_created": "2015-06-17 14:21:35.272018+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/193497", 
            "date_created": "2015-06-19 11:05:27.187120+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/193497\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=dc6af6bf861b510834122aa75750fd784578e197\nSubmitter: Jenkins\nBranch:    stable/kilo\n\ncommit dc6af6bf861b510834122aa75750fd784578e197\nAuthor: Matt Riedemann <email address hidden>\nDate:   Sun May 10 18:46:37 2015 -0700\n\n    libvirt: handle code=38 + sigkill (ebusy) in destroy()\n    \n    Handle the libvirt error during destroy when the sigkill fails due to an\n    EBUSY. This is taken from a comment by danpb in the bug report as a\n    potential workaround.\n    \n    Co-authored-by: Daniel Berrange (<email address hidden>)\n    \n    Closes-Bug: #1353939\n    \n    Conflicts:\n        nova/tests/unit/virt/libvirt/test_driver.py\n    \n        NOTE (kashyapc): 'stable/kilo' branch doesn't have the\n        'libvirt_guest' object, so, adjust the below unit tests accordingly:\n    \n            test_private_destroy_ebusy_timeout\n            test_private_destroy_ebusy_multiple_attempt_ok\n    \n    Change-Id: I128bf6b939fbbc85df521fd3fe23c3c6f93b1b2c\n    (cherry picked from commit 3907867601d1044eaadebff68a590d176abff6cf)\n", 
            "date_created": "2015-07-06 15:29:33.046085+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/221529", 
            "date_created": "2015-09-08 22:14:38.406352+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: stable/juno\nReview: https://review.openstack.org/221529\nReason: Juno is EOL soon.", 
            "date_created": "2015-12-02 03:14:47.398215+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This fix was made available in 1:2014.2.4-0ubuntu1~cloud4 of nova in the Ubuntu Cloud Archive for Juno.", 
            "date_created": "2015-12-15 19:38:18.829462+00:00", 
            "author": "https://api.launchpad.net/1.0/~billy-olsen"
        }, 
        {
            "content": "I also met with such issue:\n\nError: Resource DELETE failed: Error: Resource DELETE failed: Error: Server vm001 delete failed: (500) Failed to terminate process 17320 with SIGKILL: Device or resource busy \n\n", 
            "date_created": "2016-02-01 07:15:28.650707+00:00", 
            "author": "https://api.launchpad.net/1.0/~double12gzh"
        }, 
        {
            "content": "Delete the in-progress vm on dashboard and then it works.", 
            "date_created": "2016-02-01 08:05:36.719159+00:00", 
            "author": "https://api.launchpad.net/1.0/~double12gzh"
        }
    ], 
    "closed": "2015-06-24 12:14:27.178324+00:00"
}
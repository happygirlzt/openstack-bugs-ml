{
    "status": "Confirmed", 
    "last_updated": "2017-03-24 14:23:12.655081+00:00", 
    "description": "Seen here:\n\nhttp://logs.openstack.xenproject.org/35/246635/1/check/dsvm-tempest-xen/7bfd07c/console.html\n\nI see this in the n-cpu logs:\n\n2015-11-17 23:16:50.731 DEBUG nova.compute.manager [req-320c876b-00a2-4785-a9a4-3979d567302e tempest-ImagesOneServerTestJSON-1384909178 tempest-ImagesOneServerTestJSON-1707465920] [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] Cleaning up image 085d91cf-d1a2-412c-bbcb-aca06ac5bd29 decorated_function /opt/stack/new/nova/nova/compute/manager.py:420\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] Traceback (most recent call last):\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 416, in decorated_function\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3046, in snapshot_instance\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     task_states.IMAGE_SNAPSHOT)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3076, in _snapshot_instance\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     update_task_state)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1405, in snapshot\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     guest.save_memory_state()\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/virt/libvirt/guest.py\", line 313, in save_memory_state\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     self._domain.managedSave(0)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     rv = execute(f, *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     six.reraise(c, e, tb)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     rv = meth(*args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1397, in managedSave\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     if ret == -1: raise libvirtError ('virDomainManagedSave() failed', dom=self)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] libvirtError: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] \n\n\n\n\n2015-11-17 23:16:51.094 ERROR oslo_messaging.rpc.dispatcher [req-320c876b-00a2-4785-a9a4-3979d567302e tempest-ImagesOneServerTestJSON-1384909178 tempest-ImagesOneServerTestJSON-1707465920] Exception during message handling: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher Traceback (most recent call last):\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 142, in _dispatch_and_reply\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     executor_callback))\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 186, in _dispatch\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     executor_callback)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 129, in _do_dispatch\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     result = func(ctxt, **new_args)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 105, in wrapped\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     payload)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 88, in wrapped\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 350, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     LOG.warning(msg, e, instance=instance)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 323, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 378, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 366, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 426, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     instance=instance)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 416, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3046, in snapshot_instance\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     task_states.IMAGE_SNAPSHOT)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3076, in _snapshot_instance\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     update_task_state)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1405, in snapshot\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     guest.save_memory_state()\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/virt/libvirt/guest.py\", line 313, in save_memory_state\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     self._domain.managedSave(0)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     rv = execute(f, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(c, e, tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     rv = meth(*args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1397, in managedSave\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     if ret == -1: raise libvirtError ('virDomainManagedSave() failed', dom=self)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher libvirtError: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher", 
    "tags": [
        "libvirt", 
        "xen"
    ], 
    "importance": "Medium", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1517282", 
    "owner": "None", 
    "id": 1517282, 
    "index": 4388, 
    "openned": "2015-11-18 02:26:31.680428+00:00", 
    "created": "2015-11-18 02:26:31.680428+00:00", 
    "title": "test_create_delete_image fails in dsvm-tempest-xen due to 'libvirtError: internal error: Failed to save domain '42' with libxenlight'", 
    "comments": [
        {
            "content": "Seen here:\n\nhttp://logs.openstack.xenproject.org/35/246635/1/check/dsvm-tempest-xen/7bfd07c/console.html\n\nI see this in the n-cpu logs:\n\n2015-11-17 23:16:50.731 DEBUG nova.compute.manager [req-320c876b-00a2-4785-a9a4-3979d567302e tempest-ImagesOneServerTestJSON-1384909178 tempest-ImagesOneServerTestJSON-1707465920] [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] Cleaning up image 085d91cf-d1a2-412c-bbcb-aca06ac5bd29 decorated_function /opt/stack/new/nova/nova/compute/manager.py:420\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] Traceback (most recent call last):\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 416, in decorated_function\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3046, in snapshot_instance\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     task_states.IMAGE_SNAPSHOT)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3076, in _snapshot_instance\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     update_task_state)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1405, in snapshot\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     guest.save_memory_state()\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/opt/stack/new/nova/nova/virt/libvirt/guest.py\", line 313, in save_memory_state\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     self._domain.managedSave(0)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     rv = execute(f, *args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     six.reraise(c, e, tb)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     rv = meth(*args, **kwargs)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1397, in managedSave\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b]     if ret == -1: raise libvirtError ('virDomainManagedSave() failed', dom=self)\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] libvirtError: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:50.731 22089 ERROR nova.compute.manager [instance: fba53658-7192-4c2e-a7f7-8f7e186da13b] \n\n\n\n\n2015-11-17 23:16:51.094 ERROR oslo_messaging.rpc.dispatcher [req-320c876b-00a2-4785-a9a4-3979d567302e tempest-ImagesOneServerTestJSON-1384909178 tempest-ImagesOneServerTestJSON-1707465920] Exception during message handling: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher Traceback (most recent call last):\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 142, in _dispatch_and_reply\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     executor_callback))\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 186, in _dispatch\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     executor_callback)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 129, in _do_dispatch\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     result = func(ctxt, **new_args)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 105, in wrapped\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     payload)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/exception.py\", line 88, in wrapped\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return f(self, context, *args, **kw)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 350, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     LOG.warning(msg, e, instance=instance)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 323, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 378, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 366, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 426, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     instance=instance)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 197, in __exit__\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 416, in decorated_function\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3046, in snapshot_instance\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     task_states.IMAGE_SNAPSHOT)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 3076, in _snapshot_instance\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     update_task_state)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1405, in snapshot\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     guest.save_memory_state()\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/virt/libvirt/guest.py\", line 313, in save_memory_state\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     self._domain.managedSave(0)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     rv = execute(f, *args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     six.reraise(c, e, tb)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     rv = meth(*args, **kwargs)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/libvirt.py\", line 1397, in managedSave\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher     if ret == -1: raise libvirtError ('virDomainManagedSave() failed', dom=self)\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher libvirtError: internal error: Failed to save domain '42' with libxenlight\n2015-11-17 23:16:51.094 22089 ERROR oslo_messaging.rpc.dispatcher", 
            "date_created": "2015-11-18 02:26:31.680428+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "More about this bugs, in the libvirt/libxl logs, there is this:\n(starting at 2015-11-17 23:16:44 UTC)\nxc: detail: xc_domain_save: starting save of domid 42\nlibxl: debug: libxl_dom.c:1074:libxl__domain_suspend_common_callback: issuing PV suspend request via XenBus control node\nlibxl: debug: libxl_dom.c:1078:libxl__domain_suspend_common_callback: wait for the guest to acknowledge suspend request\nlibxl: debug: libxl_dom.c:1125:libxl__domain_suspend_common_callback: guest acknowledged suspend request\nlibxl: debug: libxl_dom.c:1129:libxl__domain_suspend_common_callback: wait for the guest to suspend\nlibxl: error: libxl_dom.c:1151:libxl__domain_suspend_common_callback: guest did not suspend\nxc: error: Suspend request failed: Internal error\nxc: error: Domain appears not to have suspended: Internal error\nxc: detail: Save exit of domid 42 with rc=1\nlibxl-save-helper: debug: complete r=1: Success\nlibxl: error: libxl_dom.c:1406:libxl__xc_domain_save_done: saving domain: domain responded to suspend request: Success\n\n\nAnd I think it is Linux itself that fail to suspend. I been trying to debug that for sometime, but so far, the main thing I've done is update the CirrOS image to include a newer kernel, and have the CI print the guest console when suspend fail. It appear that Linux fail to suspend if a suspend request arrive before it is fully booted.", 
            "date_created": "2017-03-24 14:23:11.110000+00:00", 
            "author": "https://api.launchpad.net/1.0/~anthony-perard"
        }
    ]
}
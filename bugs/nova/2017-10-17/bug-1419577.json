{
    "status": "In Progress", 
    "last_updated": "2017-08-01 10:03:01.350125+00:00", 
    "description": "Hi, guys\n\nWhen live-migrate failed with error, lun-id of connection_info column in Nova's block_deivce_mapping table couldn't be rollback.\nand failed VM can have others volume.\n\nmy test environment is following :\n\nOpenstack Version : Havana ( 2013.2.3)\nCompute Node OS : 3.5.0-23-generic #35~precise1-Ubuntu SMP Fri Jan 25 17:13:26 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\nCompute Node multipath : multipath-tools 0.4.9-3ubuntu7.2\n\ntest step is :\n\n1) create 2 Compute node (host#1 and host#2)\n2) create 1 VM on host#1 (vm01)\n3) create 1 cinder volume (vol01)\n4) attach 1 volume to vm01 (/dev/vdb)\n5) live-migrate vm01 from host#1 to host#2\n6) live-migrate success\n\u00a0\u00a0\u00a0\u00a0\u00a0- please check the mapper by using multipath command in host#1 (# multipath -ll), then you can find mapper is not deleted.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0and the status of devices is \"failed faulty\"\n\u00a0\u00a0\u00a0\u00a0\u00a0- please check the lun-id of vol01\n7) Again, live-migrate vm01 from host#2 to host#1 (vm01 was migrated to host#2 at step 4)\n8) live-migrate fail\n\u00a0\u00a0\u00a0\u00a0\u00a0- please check the mapper in host#1\n\u00a0\u00a0\u00a0\u00a0\u00a0- please check the lun-id of vol01, then you can find the lun hav \"two\" igroups\n\u00a0\u00a0\u00a0\u00a0\u00a0- please check the connection_info column in Nova's block_deivce_mapping table, then you can find lun-id couldn't be rollback\n\nThis Bug is critical security issue because the failed VM can have others volume.\n\nand every backend storage of cinder-volume can have same problem because this is the bug of live-migration's rollback process.\n\nI suggest below methods to solve issue :\n\n1) when live-migrate is complete, nova should delete mapper devices at origin host\n2) when live-migrate is failed, nova should rollback lun-id in connection_info column\n3) when live-migrate is failed, cinder should delete the mapping between lun and host (Netapp : igroup, EMC : storage_group ...)\n4) when volume-attach is requested , cinder volume driver of vendors should make lun-id randomly for reduce of probability of mis-mapping\n\nplease check this bug.\n\nThank you.", 
    "tags": [
        "live-migration", 
        "openstack-version.havana", 
        "security", 
        "volumes"
    ], 
    "importance": "Medium", 
    "heat": 32, 
    "link": "https://bugs.launchpad.net/nova/+bug/1419577", 
    "owner": "https://api.launchpad.net/1.0/~lyarwood", 
    "id": 1419577, 
    "index": 4148, 
    "openned": "2015-02-09 01:29:56.278932+00:00", 
    "created": "2015-02-09 01:29:56.278932+00:00", 
    "title": "when live-migrate failed, lun-id couldn't be rollback in havana", 
    "comments": [
        {
            "content": "Hi, guys\n\nWhen live-migrate failed with error, lun-id of connection_info column in Nova's block_deivce_mapping table couldn't be rollback.\nand failed VM can have others volume.\n\nmy test environment is following :\n\nOpenstack Version : Havana ( 2013.2.3)\nCompute Node OS : 3.5.0-23-generic #35~precise1-Ubuntu SMP Fri Jan 25 17:13:26 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\nCompute Node multipath : multipath-tools 0.4.9-3ubuntu7.2\n\ntest step is :\n\n1) create 2 Compute node (host#1 and host#2)\n2) create 1 VM on host#1 (vm01)\n3) create 1 cinder volume (vol01)\n4) attach 1 volume to vm01 (/dev/vdb)\n5) live-migrate vm01 from host#1 to host#2\n6) live-migrate success\n     - please check the mapper by using multipath command in host#1 (# multipath -ll), then you can find mapper is not deleted.\n       and the status of devices is \"failed faulty\"\n     - please check the lun-id of vol01 \n7) Again, live-migrate vm01 from host#2 to host#1 (vm01 was migrated to host#2 at step 4)\n8) live-migrate fail\n     - please check the mapper in host#1\n     - please check the lun-id of vol01, then you can find the lun hav \"two\" igroups\n     - please check the connection_info column in Nova's block_deivce_mapping table, then you can find lun-id couldn't be rollback\n\nThis Bug is critical security issue because the failed VM can have others volume.\n\nand every backend storage of cinder-volume can have same problem because this is the bug of live-migration's rollback process.\n\nI suggest bellow methods to solve issue : \n\n1) when live-migrate is complete, nova should delete mapper devices at origin host\n2) when live-migrate is failed, nova should rollback lun-id in connection_info column\n3) when live-migrate is failed, cinder should delete the mapping between lun and host (Netapp : igroup, EMC : storage_group ...)\n4) when volume-attach is requested , cinder volume driver of vendors should make lun-id randomly for reduce of probability of mis-mapping\n\nplease check this bug.\n\nThank you.", 
            "date_created": "2015-02-09 01:29:56.278932+00:00", 
            "author": "https://api.launchpad.net/1.0/~raymon-ha"
        }, 
        {
            "content": "Since this report concerns a possible security risk, an incomplete security advisory task has been added while the core security reviewers for the affected project or projects confirm the bug and discuss the scope of any vulnerability along with potential solutions.\n\nIs this only in Havana or does it also reproduce on Icehouse/Juno ? ", 
            "date_created": "2015-02-09 14:12:25.179278+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Hi, Tristan\n\nIcehouse and Juno has the same bug.\n\nplease see below that i reported :\nhttps://bugs.launchpad.net/cinder/+bug/1416314\n\nwhen live-migration failed, the information of mapping between host and lun can be tangled.\nthis bug can affect not only security vulnerability but also data stability.  (filesystem can be break when lun mis-mapped)\n\nThanks.\n", 
            "date_created": "2015-02-09 15:04:20.007800+00:00", 
            "author": "https://api.launchpad.net/1.0/~raymon-ha"
        }, 
        {
            "content": "I agree that's a bug with critical consequences, however unless I'm mistaken, it's not a situation that can be triggered or predicted by an attacker, in which case it may not be considered a vulnerability ?", 
            "date_created": "2015-02-12 15:10:04.189614+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I agree with ttx here, without a way to make the migration process fail, this is a bug with security consequence, but not a vulnerability.\n\n@Hahyun, is there a missing steps that an attacker can use to make the live migration (step 8) failed ?\n\nElse let's triage this as a class D type of report and remove the advisory task. ( https://wiki.openstack.org/wiki/Vulnerability_Management#Incident_report_taxonomy )", 
            "date_created": "2015-02-18 16:29:38.391658+00:00", 
            "author": "https://api.launchpad.net/1.0/~tristan-cacqueray"
        }, 
        {
            "content": "Hi, \nThank you for your comment.\n\nThe reason that I reported above issue as vulnerability is that attacker can attach others volume to his own VM on purpose.\nThere are many ways to make live-migration fail.\n\nFirstly, there are two issues on havana (tag : 2013.2.3)\nOne issue is the bug with multipath rescan.(https://bugs.launchpad.net/nova/+bug/1362916)\nThe other one is that when volume is detached, multipath device couldn\u2019t be deleted.\nDue to these reasons, live-migration process will be failed in the situation that one vm live-migrate to another compute-node and go back to the original compute-node.\n\nSecondly, if live-migration is executed while process keep using big size of memory by benchmark tool or something like that in VM instance \nand then the waiting status of live-migration could be persisted, eventually live-migration will be failed.\n\nThere are some ways to make live-migration fail except I explained above.\nMake NIC of compute-node down and then excute live-migration, live-migration is going to be failed for example (using multipath, iscsi)\n\nUsing rollback bug is just one way that attacker can attach others volume to his VM.\nI think the importance thing is that nova attach volume with lun-id so that if lun-id might be changed with errors or by attackers, it occurs critical security issues.\nplease think about below situation.\nattaker get the admin authority of nova DB.\nchange lun-id of connection_info in block_device_mapping table.\nreboot hard his VM with volume changed lun-id.\nfinally attacker get others volume on his vm easily.\n\nI think the root-cause of this bug is that nova use \u201clun-id\u201d for mapping VM with volume.\nlun-id is not unique and could be changed in attach/detach process because it is generaged dynamically.\nI'd like to suggest that nova should attach volume to vm with \"unique-id\" of lun not lun-id.\nAnd additionally, the bug that I reported should be fixed.\n\nUsers who have VM on Public cloud based on Openstack can feel their vm is unsafe, if they know about the possibility of volume mis-mapping because one compute-node have many different customers vm.\nSo, I think this issue should be triaged as a Class A type.\n\nThank you.", 
            "date_created": "2015-02-23 01:09:45.429868+00:00", 
            "author": "https://api.launchpad.net/1.0/~raymon-ha"
        }, 
        {
            "content": "Unless I'm misreading, you're suggesting potential exploits involving :\n\n1. disconnecting physical network interfaces\n\n2. gaining administrative access to the nova database\n\nOur report taxonomy is not based on feelings and user impressions, but rather on the feasibility of exploiting a bug weighed against its implied risks.", 
            "date_created": "2015-02-23 16:34:10.479080+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I tend to agree that it becomes a security vulnerability if live-migrate regularly fails. Even if the leak can't be triggered or controlled, it is still a privacy issue. We issued one in the past for OSSA-2013-006:\n\nhttp://security.openstack.org/ossa/OSSA-2013-006.html", 
            "date_created": "2015-02-26 15:07:48.165153+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Hi, \nLive-migrate regularly fails on Havana and  all branch before Havana.\nLive-migrate can be failed on Juno and Ice-House in specific condition as I reported above.\nThank you.", 
            "date_created": "2015-03-02 14:39:55.808654+00:00", 
            "author": "https://api.launchpad.net/1.0/~raymon-ha"
        }, 
        {
            "content": "Agree that it's a vulnerability in Havana (since live-migration fails so often there). I wouldn't consider it a vulnerability in Icehouse/Juno, since you can't trigger live migration failure without administrative or physical access to the machines.\n\nIt is a bug with security consequences there, and it should be fixed as soon as possible.", 
            "date_created": "2015-03-02 15:26:18.060418+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "As far as OSSA is going, I'd rate this class C1 or D.", 
            "date_created": "2015-03-02 15:33:40.716384+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "Agreed on C1: this wouldn't qualify for an advisory since Havana is no longer supported by the VMT, but it's still something a distro carrying Havana packages of Nova might fix on their own and request a corresponding CVE to track.", 
            "date_created": "2015-03-09 14:07:37.949989+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I'm going to go ahead and request a CVE for this on oss-sec at least for havana (which we [redhat] still support downstream) unless someone has a good reason not to? (or beats me to it)", 
            "date_created": "2015-03-17 03:19:12.555350+00:00", 
            "author": "https://api.launchpad.net/1.0/~gmollett"
        }, 
        {
            "content": "per http://seclists.org/oss-sec/2015/q1/990:\n\nFor purposes of CVE, we typically don't think of vulnerabilities in the way expressed in https://bugs.launchpad.net/nova/+bug/1419577/comments/4 \"without a way to make the migration process fail, this is a bug with security consequence, but not a vulnerability.\" In other words, for a CVE, the attacker can be a person who wishes to have an unauthorized volume attachment after the bug is triggered. The attacker does not need to be a person who has determined a reproducible way to trigger the bug.", 
            "date_created": "2015-04-03 22:21:57.443053+00:00", 
            "author": "https://api.launchpad.net/1.0/~edmondsw"
        }, 
        {
            "content": "And as I indicated in follow-up replies on that thread, the OpenStack VMT doesn't decide whether or not a bug is worthy of getting a CVE assigned (only whether or not we're going to embargo it and/or eventually issue a security advisory about it).", 
            "date_created": "2015-04-03 22:36:46.990989+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "I'm trying to sort this out a bit.\n\nLooking at the nova.virt.libvirt.driver.pre_live_migration() method, I see it's connecting to a volume and the connection_info dictionary is updated in the nova.virt.libvirt.volume code, but I don't see where that connection_info dict comes back to the virt driver's pre_live_migration method and persists the change to the database.\n\nThis is where pre_live_migration() connects the volume:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/driver.py?id=2015.1.0#n5813\n\nLet's assume we're using the LibvirtISCSIVolumeDriver volume driver, the connect_volume method in there will update the connection_info dict here:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/virt/libvirt/volume.py?id=2015.1.0#n483\n\nThat change never gets persisted back to the block_device_mapping table for the bdm instance, but we've connected the volume potentially on another host so if live migration fails and we never rollback the volume connection_info to the source host (before pre_live_migration), and reboot the instance, then the bdm will be recreated from what's in the database which will be wrong.", 
            "date_created": "2015-05-29 14:16:23.520152+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Should this bug remain open as it is targeted to a no longer supported version (havana)?", 
            "date_created": "2015-08-03 19:57:46.827620+00:00", 
            "author": "https://api.launchpad.net/1.0/~jshepher"
        }, 
        {
            "content": "I have a feeling that this is fixed via https://review.openstack.org/#/c/211051/ .", 
            "date_created": "2015-08-18 18:54:28.574381+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Per comment 17, maybe not given that's post live migration which is only called for a successful live migration, the rollback is called for a failed live migration.", 
            "date_created": "2015-08-18 18:57:06.374040+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "@Justin, per comment 16, this was reported against Havana but as far as I can tell this is not yet resolved in master (Liberty right now).", 
            "date_created": "2015-08-18 18:57:31.738729+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "I'm wondering if the fix (https://review.openstack.org/#/c/202770/) for bug 1475411 plays here, i.e. the case that the live migration from A to B is considered successful even though we didn't disconnect the correct volumes, and then that causes the migration from B back to A to fail.", 
            "date_created": "2015-09-28 14:56:59.079451+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Can anybody please verify if my bug is a duplicate of this one? https://bugs.launchpad.net/nova/+bug/1525802", 
            "date_created": "2015-12-14 07:38:27.819918+00:00", 
            "author": "https://api.launchpad.net/1.0/~tobias-urdin"
        }, 
        {
            "content": "Since this bug was switched from public back to public security with no comment explaining why, I have reset it to public again. Please, whenever moving a bug to a security type, add a comment with your reasoning.", 
            "date_created": "2016-01-27 13:46:03.072342+00:00", 
            "author": "https://api.launchpad.net/1.0/~fungi"
        }, 
        {
            "content": "We've seen this downstream against RHEL OSP 7 (kilo) documented (mostly privately) in the following RHBZ:\n\niscsi details changed for cinder volume using EMCCLIISCSIDriver\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1353147\n\nWe manually reverted the changes to the target_luns to workaround the issue in this case. This still looks possible against master so I'm going to propose a change refreshing connection_info on the source host during _rollback_live_migration.", 
            "date_created": "2016-07-07 12:12:46.717544+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/338929", 
            "date_created": "2016-07-07 12:15:10.916108+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/342111\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=b83cae02ece4c338e09c3606c6ae69b715bd6f8c\nSubmitter: Jenkins\nBranch:    master\n\ncommit b83cae02ece4c338e09c3606c6ae69b715bd6f8c\nAuthor: Lee Yarwood <email address hidden>\nDate:   Thu Jul 14 11:53:09 2016 +0100\n\n    block_device: Make refresh_conn_infos py3 compatible\n    \n    Also add a simple test ensuring that refresh_connection_info is called\n    for each DriverVolumeBlockDevice derived device provided.\n    \n    Related-Bug: #1419577\n    Partially-Implements: blueprint goal-python35\n    Change-Id: Ib1ff00e7f4f5b599317d7111c322ce9af8a9a2b1\n", 
            "date_created": "2016-10-18 04:45:54.627692+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/391598", 
            "date_created": "2016-10-29 11:35:59.117851+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Lee Yarwood (<email address hidden>) on branch: master\nReview: https://review.openstack.org/391598", 
            "date_created": "2016-11-07 20:05:30.854276+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "https://review.openstack.org/#/c/338929/ is the correct WIP review that I've been working on. It's currently waiting on https://review.openstack.org/#/c/389608/ so we can use the stashed bdms to reset connection_info without additional calls to Cinder.\n\n\n", 
            "date_created": "2016-12-09 16:25:45.021182+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Automatically discovered version havana in description. If this is incorrect, please update the description to include 'nova version: ...'", 
            "date_created": "2017-06-27 15:55:08.298920+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: master\nReview: https://review.openstack.org/338929\nReason: This review is > 4 weeks without comment, and is not mergable in it's current state. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and leaving a 'recheck' comment to get fresh test results.", 
            "date_created": "2017-08-01 10:03:00.692545+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ]
}
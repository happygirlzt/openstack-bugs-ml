{
    "status": "Fix Released", 
    "last_updated": "2012-09-27 15:27:18.620155+00:00", 
    "description": "Hello,\n\nI'm deploying OpenStack as Private Cloud implementation on multi-hw-node deployment with many Nova Volume (and other) services. \n\nAt the beginning I must say that I'm very disappointed of the QA of the OpenStack! OpenStack is now in 5. major (essex) release and his features are still very buggy, non-completed,... everything looks like it was never tested nor unit-tested! The whole OpenStack in this state is a \"tool\" for school project for one laptop/desktop only, or looks like Alpha version v0.2. OpenStack's database is consistently broken due to internal bugs, which are in simplest Cloud task like attaching volumes, detaching volumes, stopping/starting instances... Immediately when the bug occurs OpenStack log the bug/exception in middle of his \"task\" (why it is not transaction?) that keeps inconsistent DB entries, iSCSI and LVM stuff, ... That is not robust behaviour which is required for production deployment. How it is possible that this happen in fifth major release? Please do not implement any new features until current OpenStack's features are not fully functional and (that is important) robust!!!\n\nNow I will report one of the regressions/issues: \"Creating volume from snapshot on real/production/multicluster installation of OpenStack is broken\"\n\nError log from nova-volume service:\n\n2012-06-04 14:44:17 DEBUG nova.utils [] Running cmd (subprocess): sudo /usr/bin/nova-rootwrap dd if=/dev/mapper/nova--volumes-snap--00000005 of=/dev/mapper/nova--volumes-vol--000000f4 count=10240 bs=1M from (pid=11034) execute /usr/lib/python2.6/site-packages/nova/utils.py:220\n2012-06-04 14:44:17 DEBUG nova.utils [] Result was 1 from (pid=11034) execute /usr/lib/python2.6/site-packages/nova/utils.py:236\n2012-06-04 14:44:17 ERROR nova.rpc.amqp [] Exception during message handling\n2012-06-04 14:44:17 TRACE nova.rpc.amqp Traceback (most recent call last):\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/rpc/amqp.py\", line 253, in _process_data\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     rval = node_func(context=ctxt, **node_args)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/manager.py\", line 138, in create_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     volume_ref['id'], {'status': 'error'})\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib64/python2.6/contextlib.py\", line 23, in __exit__\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     self.gen.next()\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/manager.py\", line 127, in create_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     snapshot_ref)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/driver.py\", line 158, in create_volume_from_snapshot\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     snapshot['volume_size'])\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/driver.py\", line 116, in _copy_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     run_as_root=True)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/utils.py\", line 243, in execute\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     cmd=' '.join(cmd))\n2012-06-04 14:44:17 TRACE nova.rpc.amqp ProcessExecutionError: Unexpected error while running command.\n2012-06-04 14:44:17 TRACE nova.rpc.amqp Command: sudo /opt/openstack/bin/nova-rootwrap dd if=/dev/mapper/nova--volumes-snap--00000005 of=/dev/mapper/nova--volumes-vol--000000f4 count=10240 bs=1M\n\nTask: create instance with volume root  \"euca-run-instances ami-0000001 -z foo -g default -t m1.small --block-device-mapping /dev/vda=snap-00000006:10:true\"\n\nThe problem is this - OpenStack use local copy command (\"dd\") on multi-node deployment with many volume services. So the source snapshot is on different node which is accessible over iSCSI and TCP/IP but not directly by \"dd\".\n\nI hope that the QA of OpenStack will be better soon,\nJaroslav", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 14, 
    "link": "https://bugs.launchpad.net/nova/+bug/1008866", 
    "owner": "https://api.launchpad.net/1.0/~zrzhit", 
    "id": 1008866, 
    "index": 2931, 
    "openned": "2012-06-05 07:18:19.931896+00:00", 
    "created": "2012-06-05 07:18:19.931896+00:00", 
    "title": "Creating volume from snapshot on real/production/multicluster installation of OpenStack is broken", 
    "comments": [
        {
            "content": "Hello,\n\nI'm deploying OpenStack as Private Cloud implementation on multi-hw-node deployment with many Nova Volume (and other) services. \n\nAt the beginning I must say that I'm very disappointed of the QA of the OpenStack! OpenStack is now in 5. major (essex) release and his features are still very buggy, non-completed,... everything looks like it was never tested nor unit-tested! The whole OpenStack in this state is a \"tool\" for school project for one laptop/desktop only, or looks like Alpha version v0.2. OpenStack's database is consistently broken due to internal bugs, which are in simplest Cloud task like attaching volumes, detaching volumes, stopping/starting instances... Immediately when the bug occurs OpenStack log the bug/exception in middle of his \"task\" (why it is not transaction?) that keeps inconsistent DB entries, iSCSI and LVM stuff, ... That is not robust behaviour which is required for production deployment. How it is possible that this happen in fifth major release? Please do not implement any new features until current OpenStack's features are not fully functional and (that is important) robust!!!\n\nNow I will report one of the regressions/issues: \"Creating volume from snapshot on real/production/multicluster installation of OpenStack is broken\"\n\nError log from nova-volume service:\n\n2012-06-04 14:44:17 DEBUG nova.utils [] Running cmd (subprocess): sudo /usr/bin/nova-rootwrap dd if=/dev/mapper/nova--volumes-snap--00000005 of=/dev/mapper/nova--volumes-vol--000000f4 count=10240 bs=1M from (pid=11034) execute /usr/lib/python2.6/site-packages/nova/utils.py:220\n2012-06-04 14:44:17 DEBUG nova.utils [] Result was 1 from (pid=11034) execute /usr/lib/python2.6/site-packages/nova/utils.py:236\n2012-06-04 14:44:17 ERROR nova.rpc.amqp [] Exception during message handling\n2012-06-04 14:44:17 TRACE nova.rpc.amqp Traceback (most recent call last):\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/rpc/amqp.py\", line 253, in _process_data\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     rval = node_func(context=ctxt, **node_args)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/manager.py\", line 138, in create_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     volume_ref['id'], {'status': 'error'})\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib64/python2.6/contextlib.py\", line 23, in __exit__\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     self.gen.next()\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/manager.py\", line 127, in create_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     snapshot_ref)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/driver.py\", line 158, in create_volume_from_snapshot\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     snapshot['volume_size'])\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/volume/driver.py\", line 116, in _copy_volume\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     run_as_root=True)\n2012-06-04 14:44:17 TRACE nova.rpc.amqp   File \"/usr/lib/python2.6/site-packages/nova/utils.py\", line 243, in execute\n2012-06-04 14:44:17 TRACE nova.rpc.amqp     cmd=' '.join(cmd))\n2012-06-04 14:44:17 TRACE nova.rpc.amqp ProcessExecutionError: Unexpected error while running command.\n2012-06-04 14:44:17 TRACE nova.rpc.amqp Command: sudo /opt/openstack/bin/nova-rootwrap dd if=/dev/mapper/nova--volumes-snap--00000005 of=/dev/mapper/nova--volumes-vol--000000f4 count=10240 bs=1M\n\nTask: create instance with volume root  \"euca-run-instances ami-0000001 -z foo -g default -t m1.small --block-device-mapping /dev/vda=snap-00000006:10:true\"\n\nThe problem is this - OpenStack use local copy command (\"dd\") on multi-node deployment with many volume services. So the source snapshot is on different node which is accessible over iSCSI and TCP/IP but not directly by \"dd\".\n\nI hope that the QA of OpenStack will be better soon,\nJaroslav", 
            "date_created": "2012-06-05 07:18:19.931896+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "I'm now working on some solution and will paste patch when it is ready.", 
            "date_created": "2012-06-05 11:04:07.236958+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "I prepared solution of this issue in my OpenSack Nova fork on GitHUB at https://github.com/pulchart/nova/commit/76e59b9628153b6c71150797f7fe35804502624d\n\nHow it works:\n- each new snapshot is exported via iSCSI (like volumes)\n- nova-volume, on which we creating the new volume from snapshot, register this iSCSI export into local system (if iscsi_ip_preffix filter pass) \n- nova-volume can now use local copy\n- after that the iSCSI snapshot is unregistered from local system\n\nUnfortunately  \"host\" and \"provider_location\" are missing in DB schema so I created some hacks:\n- \"host\" is used from snapshot's volume (LVM snapshot is on same host as volume), there is a new API call \"snapshot_get_host\"\n- iSCSI discovery is used instead of using location from database\n\nFixes:\n- \"_run_iscsiadm\" from nova/volume/driver.py is synchronized with same function from libvirt\n ", 
            "date_created": "2012-06-05 18:44:23.395368+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "I forget to \"init_host\" function, so snapshot is not registered after server reboot. I will fix it and send updated patch.", 
            "date_created": "2012-06-06 09:47:12.296057+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "Fixed in 1008866_v2.patch \n- new: snapshot iSCSI exports are initialized by nova-volume start\n(https://github.com/pulchart/nova/commit/a2df1c340fe874f2d2100e3e59b89140974ac51c)", 
            "date_created": "2012-06-06 10:06:30.564401+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "1008866_v3.patch:\n- \"create_volume_from_snapshot\" function need to be synchronized\n\n....\n+    @utils.synchronized('create_volume_from_snapshot')\n     def create_volume_from_snapshot(self, volume, snapshot):\n....", 
            "date_created": "2012-06-06 17:56:23.256604+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "Rebased git source link: https://github.com/pulchart/nova/commit/e13c308e11b306109c04223c8f5d282f612d34e0", 
            "date_created": "2012-06-07 06:48:47.588514+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "daily updates :)\n\n1008866_v4.patch: https://github.com/pulchart/nova/commit/a0c858c22305772f8acd4c3194392f60bd78464b\n", 
            "date_created": "2012-06-07 14:35:03.781041+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "@Jaroslav: would you consider pushing the change to Gerrit ?\nSee http://wiki.openstack.org/HowToContribute#If_you.27re_a_developer.2C_start_here:", 
            "date_created": "2012-06-07 15:13:34.996778+00:00", 
            "author": "https://api.launchpad.net/1.0/~ttx"
        }, 
        {
            "content": "I have to handle one latest issue of this patch before I push it into \"world\". I try to find solution with any DB table/entry update, unfortunately it still not correctly works  without updating \"iscsi_targets\" table. \n\nSo current solution expect this:\n1/ all volume.(id)s on nova-volume on one hostX  != snapshot.(id)s on the same hostX\n2/ created snapshot.id < max(volume.id)\nThat is caused due to using/mixing \"iscsi_targets.volume_id\" with both \"volume.id\"/\"snapshot.id\". This workaround woks for my use case but can cause unxepected problems in some other situations.\n\nI have seen some blueprints about changing volume_id, snapshot_id to UUID which is uniq across whole stack, so I thing this migration to UUID will solve all problems of this issue.", 
            "date_created": "2012-06-08 06:17:16.618457+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "I wonder if a simpler solution might just be to force volumes created from snapshots onto the same host as the snapshot. In other words, skip the scheduler altogether and just send the message directly to the right host if snapshot_id is set.  Clearly this could lead to an overload of some hosts in the system, but it seems much more managable in the short term. Then we have more time to work on a robust solution for migrating/creating volumes on different hosts.", 
            "date_created": "2012-06-10 19:52:53.885311+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "That is simple solution for implementation but not usable for cloud use-case(s). We have to be able create volume from snapshot in every place of In the Private Cloud (= on any volume node in cloud). Attached patch is now used and it solve many problems that we had.\n\nQuestion: Do we use (or create) \"Cloud solution\" or \"a simple tool for managing local LVM stuff\"? \nMy answer is: We do NOT (!) need simple LVM tool manager for local volume node. We need cloud solution.\n\nI will have some time this week for updating attached patch and fix the last problem of proposed solution. Please keep tuned :).", 
            "date_created": "2012-06-11 06:48:21.286892+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "I understand that we want a better soution, but I think your patch is too large to backport into stable/essex and therefore should be done against the new cinder project.\n\nThe simple solution is appropriate for an essex backport. The more complex solution can be done in cinder, as it still requires some cleanup.\n\nFor example, the attach to snapshot code is essentially the same as attach to volume code and should be put into a little library of some sort. Also, I couldn't tell from the patch if it is using discovery by default, but if so it should be storing provider_location in the snapshots as well and connecting via an initialize_connection call in the same way it does for volumes.\n\nIn other words, this is a large set of changes that should be planned coherently in the cinder project.", 
            "date_created": "2012-06-11 21:38:50.482074+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "> I understand that we want a better soution, but I think your patch is too large to \n> backport into stable/essex and therefore should be\n> done against the new cinder project.\n\nYes I understand too. Unfortunately, proposed simple solution will not fix this regression of this feature for production environment with many nova-volume nodes. But, yes you can workaround \"the error/exception in log\" by this way (until volume space is not exhausted). In our use-case we will rollback the simple fix and apply our fix in stable/essex release in our build of OpenStack.\n\n> The simple solution is appropriate for an essex backport. \n> The more complex solution can be done in cinder, as it still requires some cleanup.\n\nYes completely agree, cleanups are needed for future release. This patch was created in hurry without thinking about best practices.\n\n> For example, the attach to snapshot code is essentially the same as \n> attach to volume code and should be put into a little library of some sort. \n> Also, I couldn't tell from the patch if it is using discovery by default, \n> but if so it should be storing provider_location in the snapshots as \n> well and connecting via an initialize_connection call in the same \n> way it does for volumes.\n\nI completely agree.\n\n> In other words, this is a large set of changes that should be planned coherently in the cinder project.\n\nYes, again I completely agree without comments because review comments was expected ;). Please fell free to re-factor this patch/code for cinder project. I will be happy man whit the best solution/code :). This patch was for essex = do it without any DB modifications, without rewriting current functions if it is possible, do it immediately because this regression was blocking issue for our Private Cloud based on Essex.", 
            "date_created": "2012-06-12 06:23:44.474207+00:00", 
            "author": "https://api.launchpad.net/1.0/~jaroslav-pulchart-4"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/10649", 
            "date_created": "2012-08-01 13:29:12.374214+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/9761\nCommitted: http://github.com/openstack/cinder/commit/99456bd690445443ae05c0d4fe1ec43ba6090f6f\nSubmitter: Jenkins\nBranch:    master\n\ncommit 99456bd690445443ae05c0d4fe1ec43ba6090f6f\nAuthor: ZhuRongze <email address hidden>\nDate:   Fri Jul 13 12:07:13 2012 +0000\n\n    Send 'create volume from snapshot' to the proper host\n    \n    A simple solution for bug 1008866. When creating volume from snapshot on\n    multicluster, in volume it will check if snapshot_id is set. If snapshot_id\n    is set, make the call create volume directly to the volume host where the\n    snapshot resides instead of passing it through the scheduler. So snapshot can\n    be copy to new volume.\n    \n    Change-Id: Ie9c1a77f62abc40e294b1d0c604cf885652728da\n", 
            "date_created": "2012-08-07 03:50:29.935114+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/10649\nCommitted: http://github.com/openstack/nova/commit/6795de644b8a8a1879543101d85ba90674219c8b\nSubmitter: Jenkins\nBranch:    master\n\ncommit 6795de644b8a8a1879543101d85ba90674219c8b\nAuthor: ZhuRongze <email address hidden>\nDate:   Wed Aug 1 13:23:13 2012 +0000\n\n    Send 'create volume from snapshot' to the proper host\n    \n    A simple solution for bug 1008866. When creating volume from snapshot on\n    multicluster, in volume it will check if snapshot_id is set. If snapshot_id\n    is set and FLAGS.snapshot_same_host is true, make the call create volume\n    directly to the volume host where the snapshot resides instead of passing it\n    through the scheduler. So snapshot can be copy to new volume. The same as\n    review 9761.\n    \n    Change-Id: Ic182eb4563b9462704c5969d5116629442df316a\n", 
            "date_created": "2012-08-09 23:16:40.744509+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/12675", 
            "date_created": "2012-09-09 08:21:56.642813+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/12675\nCommitted: http://github.com/openstack/cinder/commit/3eaf43a9f5a26a51a89347cffe39bfae2b12c2d6\nSubmitter: Jenkins\nBranch:    master\n\ncommit 3eaf43a9f5a26a51a89347cffe39bfae2b12c2d6\nAuthor: Rongze Zhu <email address hidden>\nDate:   Sun Sep 9 15:35:26 2012 +0800\n\n    Prevent from bug #1008866 is reverted\n    \n    Fixes bug #1047841.\n    \n    Commit 2f5360753308eb8b10581fc3c026c1b66f42ebdc (Adds new volume API\n    extensions) reverted a part of commit\n    99456bd690445443ae05c0d4fe1ec43ba6090f6f (Send 'create volume from\n    snapshot' to the proper host), so bug #1008866 is reproduced. I make\n    API.create_volume to call _cast_create_volume in cinder/volume/api.py,\n    it Prevent from bug #1008866 is reverted.\n    \n    Change-Id: I1bf0b7c5fc47da756bce95128f8fd770d14399b0\n", 
            "date_created": "2012-09-09 17:45:54.207657+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/12854", 
            "date_created": "2012-09-12 07:38:58.510227+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2012-09-26 13:51:06.462667+00:00"
}
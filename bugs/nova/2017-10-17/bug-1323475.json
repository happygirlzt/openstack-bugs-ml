{
    "status": "Won't Fix", 
    "last_updated": "2014-11-19 18:32:39.701536+00:00", 
    "description": "We are using stable/havana.\n\nFor some inexplicable reason, some instances lost network information. The result looks like:\n\n\n$ nova list\n| a8f8a437-d203-4265-aca2-7bd35539c5d1 | test                                              | ACTIVE | -                | Running     |          \n                      \n$ neutron port-list --device-id a8f8a437-d203-4265-aca2-7bd35539c5d1\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n| id                                   | name | mac_address       | fixed_ips                                                                          |\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n| 6b042778-76bb-45ca-86a8-abfdb1ba1a62 |      | fa:16:3e:67:9a:88 | {\"subnet_id\": \"90b338d3-7711-48fd-a0f6-11a27388cb42\", \"ip_address\": \"10.162.82.2\"} |\n| 9800fd03-5e07-4a54-8568-28d501073c5f |      | fa:16:3e:d0:86:4a | {\"subnet_id\": \"9a1fc59d-aec1-4e3a-bd88-99ea558e8b29\", \"ip_address\": \"192.168.0.5\"} |\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n\nneutron said there are two ports binding with the instance, but nova said the instance has no port.\n\nWe dug logs, and found somethings went wrong after running heal_instance_info_cache. One line log said the instance info_cache is [], but the previous log said the instance info_cache is filled. From that time, the info_cache lost, and can't self-healing. \n\nThe simple logs pasted below, and full log here: http://paste.openstack.org/show/81605/\n\n\n....\n2014-05-26 03:47:13.258 14884 DEBUG nova.network.api [-] Updating cache with info: [VIF({'ovs_interfaceid': u'5953e098-e131-48eb-b53c-5eb095f3bfee', 'network': Network({'bridge': 'br-int', 'subne\nts': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.162.81.4'})], 'version': 4, 'meta': {'dhcp_server': u'10.162.81.3'}, 'dns': [], 'rout\nes': [], 'cidr': u'10.162.81.0/28', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'c10373fb5d234e31af4d5d56527994f\nc'}, 'id': u'b0bb08c1-dc05-4e17-a021-f3b850a823ba', 'label': u'idc_c10373fb5d234e31af4d5d56527994fc'}), 'devname': u'tap5953e098-e1', 'qbh_params': None, 'meta': {}, 'address': u'fa:16:3e:40:34:4\nc', 'type': u'ovs', 'id': u'5953e098-e131-48eb-b53c-5eb095f3bfee', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/dist-packages/nova/network/api.py:71\n2014-05-26 03:47:13.263 14884 DEBUG nova.compute.manager [-] [instance: 49a806a9-986e-4ce3-ae9f-d3c4317255a3] Updated the info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/dist\n-packages/nova/compute/manager.py:5146\n.....\n2014-05-26 03:52:14.255 14884 DEBUG nova.network.api [-] Updating cache with info: [] update_instance_cache_with_nw_info /usr/lib/python2.7/dist-packages/nova/network/api.py:71\n.....\n\n\nI try hard but can't no re-product the bug manual, The key problem here is why the info_cache not showing up. But on the other hand, we'd better give nova the ability to self-healing in this case.", 
    "tags": [
        "network", 
        "neutron"
    ], 
    "importance": "Medium", 
    "heat": 26, 
    "link": "https://bugs.launchpad.net/nova/+bug/1323475", 
    "owner": "https://api.launchpad.net/1.0/~gtt116", 
    "id": 1323475, 
    "index": 3913, 
    "openned": "2014-05-27 03:55:55.552151+00:00", 
    "created": "2014-05-27 03:55:55.552151+00:00", 
    "title": "Losing network info cache sometimes", 
    "comments": [
        {
            "content": "We are using stable/havana.\n\nFor some inexplicable reason, some instances lost network information. The result looks like:\n\n\n$ nova list\n| a8f8a437-d203-4265-aca2-7bd35539c5d1 | test                                              | ACTIVE | -                | Running     |          \n                      \n$ neutron port-list --device-id a8f8a437-d203-4265-aca2-7bd35539c5d1\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n| id                                   | name | mac_address       | fixed_ips                                                                          |\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n| 6b042778-76bb-45ca-86a8-abfdb1ba1a62 |      | fa:16:3e:67:9a:88 | {\"subnet_id\": \"90b338d3-7711-48fd-a0f6-11a27388cb42\", \"ip_address\": \"10.162.82.2\"} |\n| 9800fd03-5e07-4a54-8568-28d501073c5f |      | fa:16:3e:d0:86:4a | {\"subnet_id\": \"9a1fc59d-aec1-4e3a-bd88-99ea558e8b29\", \"ip_address\": \"192.168.0.5\"} |\n+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+\n\nneutron said there are two ports binding with the instance, but nova said the instance has no port.\n\nWe dug logs, and found somethings went wrong after running heal_instance_info_cache. One line log said the instance info_cache is [], but the previous log said the instance info_cache is filled. From that time, the info_cache lost, and can't self-healing. \n\nThe simple logs pasted below, and full log here: http://paste.openstack.org/show/81605/\n\n\n....\n2014-05-26 03:47:13.258 14884 DEBUG nova.network.api [-] Updating cache with info: [VIF({'ovs_interfaceid': u'5953e098-e131-48eb-b53c-5eb095f3bfee', 'network': Network({'bridge': 'br-int', 'subne\nts': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.162.81.4'})], 'version': 4, 'meta': {'dhcp_server': u'10.162.81.3'}, 'dns': [], 'rout\nes': [], 'cidr': u'10.162.81.0/28', 'gateway': IP({'meta': {}, 'version': None, 'type': 'gateway', 'address': None})})], 'meta': {'injected': False, 'tenant_id': u'c10373fb5d234e31af4d5d56527994f\nc'}, 'id': u'b0bb08c1-dc05-4e17-a021-f3b850a823ba', 'label': u'idc_c10373fb5d234e31af4d5d56527994fc'}), 'devname': u'tap5953e098-e1', 'qbh_params': None, 'meta': {}, 'address': u'fa:16:3e:40:34:4\nc', 'type': u'ovs', 'id': u'5953e098-e131-48eb-b53c-5eb095f3bfee', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/dist-packages/nova/network/api.py:71\n2014-05-26 03:47:13.263 14884 DEBUG nova.compute.manager [-] [instance: 49a806a9-986e-4ce3-ae9f-d3c4317255a3] Updated the info_cache for instance _heal_instance_info_cache /usr/lib/python2.7/dist\n-packages/nova/compute/manager.py:5146\n.....\n2014-05-26 03:52:14.255 14884 DEBUG nova.network.api [-] Updating cache with info: [] update_instance_cache_with_nw_info /usr/lib/python2.7/dist-packages/nova/network/api.py:71\n.....\n\n\nI try hard but can't no re-product the bug manual, The key problem here is why the info_cache not showing up. But on the other hand, we'd better give nova the ability to self-healing in this case.", 
            "date_created": "2014-05-27 03:55:55.552151+00:00", 
            "author": "https://api.launchpad.net/1.0/~gtt116"
        }, 
        {
            "content": "Same behavior seen in grizzly as well.", 
            "date_created": "2014-07-16 04:55:07.512099+00:00", 
            "author": "https://api.launchpad.net/1.0/~egon-p"
        }, 
        {
            "content": "In this state, there aren't any actions including changing ports, deleting interfaces, rebooting, etc which cause the info_cache to be rebuilt.", 
            "date_created": "2014-07-16 20:03:05.807664+00:00", 
            "author": "https://api.launchpad.net/1.0/~egon-p"
        }, 
        {
            "content": "Inserting a record, or updating network_info to have at least [{\"network\": {\"id\": \"NETWORK_ID\"}}] allows the auto_heal process to work.", 
            "date_created": "2014-07-16 22:25:32.911000+00:00", 
            "author": "https://api.launchpad.net/1.0/~egon-p"
        }, 
        {
            "content": "One other reference to this incident: http://lists.openstack.org/pipermail/openstack/2014-March/006311.html", 
            "date_created": "2014-07-17 16:54:06.978985+00:00", 
            "author": "https://api.launchpad.net/1.0/~egon-p"
        }, 
        {
            "content": "Change abandoned by Tiantian Gao (<email address hidden>) on branch: master\nReview: https://review.openstack.org/98068\nReason: Havana is too old, so abandon", 
            "date_created": "2014-09-30 06:20:10.075878+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Havana is not supported anymore", 
            "date_created": "2014-11-19 18:32:32.527322+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }
    ], 
    "closed": "2014-11-19 18:32:37.000627+00:00"
}
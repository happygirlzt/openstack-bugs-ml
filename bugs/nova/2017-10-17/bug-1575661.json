{
    "status": "Invalid", 
    "last_updated": "2016-06-08 14:08:15.955138+00:00", 
    "description": "Using fresh master of DevStack, I can not deploy partition images to Ironic nodes via Nova.\n\nI have two images in Glance - kernel image and partition image with kernel_id property set.\n\nI have configured Ironic nodes and nova flavor with capabilities: \"boot_option: local\" as described in [0].\n\nWhen I try to boot nova instance with the partition image and the configured flavor, instance goes to error:\n\n$openstack server list\n+--------------------------------------+--------+--------+----------+\n| ID                                   | Name   | Status | Networks |\n+--------------------------------------+--------+--------+----------+\n| 6cde85d2-47ad-446b-9a1f-960dbcca5199 | parted | ERROR  |          |\n+--------------------------------------+--------+--------+----------+\n\nInstance is assigned to Ironic node but node is not moved to deploying state\n\n$openstack baremetal list\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name   | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n| 95d3353f-61a6-44ba-8485-2881d1138ce1 | node-0 | None                                 | power off   | available          | False       |\n| 48112a56-8f8b-42fc-b143-742cf4856e78 | node-1 | 6cde85d2-47ad-446b-9a1f-960dbcca5199 | power off   | available          | False       |\n| c66a1035-5edf-434b-9d09-39ecc9069e02 | node-2 | None                                 | power off   | available          | False       |\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n\nIn n-cpu.log I see the following errors:\n\n2016-04-27 15:26:13.190 ERROR ironicclient.common.http [req-077efca4-1776-443b-bd70-0769c09a0e54 demo demo] Error contacting Ironic server: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with\n a node, it cannot be associated with this other node c66a1035-5edf-434b-9d09-39ecc9069e02 (HTTP 409). Attempt 2 of 2\n2016-04-27 15:26:13.190 ERROR nova.compute.manager [req-077efca4-1776-443b-bd70-0769c09a0e54 demo demo] [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Instance failed to spawn\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Traceback (most recent call last):\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2209, in _build_resources\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     yield resources\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2055, in _build_and_run_instance\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     block_device_info=block_device_info)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/driver.py\", line 698, in spawn\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     self._add_driver_fields(node, instance, image_meta, flavor)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/driver.py\", line 366, in _add_driver_fields\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     retry_on_conflict=False)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/client_wrapper.py\", line 139, in call\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     return self._multi_getattr(client, method)(*args, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/v1/node.py\", line 198, in update\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     method=http_method)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/base.py\", line 171, in _update\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     resp, body = self.api.json_request(method, url, body=patch)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 552, in json_request\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     resp = self._http_request(url, method, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 189, in wrapper\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     return func(self, url, method, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 534, in _http_request\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     error_json.get('debuginfo'), method, url)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Conflict: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with a node, it cannot be associat\ned with this other node c66a1035-5edf-434b-9d09-39ecc9069e02 (HTTP 409)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] \n\nIn ir-cond.log the error is as follows:\n\n2016-04-27 15:26:13.183 ERROR oslo_messaging.rpc.dispatcher [req-ec2f0a30-13a8-4029-ac0b-e2852c0c67c9 None None] Exception during message handling: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associa\nted with a node, it cannot be associated with this other node c66a1035-5edf-434b-9d09-39ecc9069e02\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher Traceback (most recent call last):\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 138, in _dispatch_and_reply\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     incoming.message))\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 185, in _dispatch\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 127, in _do_dispatch\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     result = func(ctxt, **new_args)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 150, in inner\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     return func(*args, **kwargs)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/conductor/manager.py\", line 228, in update_node\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     node_obj.save()\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/objects/node.py\", line 340, in save\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     self.dbapi.update_node(self.uuid, updates)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/db/sqlalchemy/api.py\", line 399, in update_node\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     node=node_id)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher InstanceAssociated: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with a node, it cannot be associated with this other node c66a10\n35-5edf-434b-9d09-39ecc9069e02\n\nWhat's more, when I delete the failed server from Nova, Ironic node is left with orphaned instance assignment, which only can be deleted with node-update removing instance_uuid.\n\n[0] http://docs.openstack.org/developer/ironic/deploy/install-guide.html?highlight=local%20boot#enabling-local-boot-with-compute-service", 
    "tags": [
        "ironic"
    ], 
    "importance": "Undecided", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1575661", 
    "owner": "None", 
    "id": 1575661, 
    "index": 7449, 
    "openned": "2016-04-27 13:12:22.937484+00:00", 
    "created": "2016-04-27 13:12:22.937484+00:00", 
    "title": "can not deploy a partition image to Ironic node", 
    "comments": [
        {
            "content": "Using fresh master of DevStack, I can not deploy partition images to Ironic nodes via Nova.\n\nI have two images in Glance - kernel image and partition image with kernel_id property set.\n\nI have configured Ironic nodes and nova flavor with capabilities: \"boot_option: local\" as described in [0].\n\nWhen I try to boot nova instance with the partition image and the configured flavor, instance goes to error:\n\n$openstack server list\n+--------------------------------------+--------+--------+----------+\n| ID                                   | Name   | Status | Networks |\n+--------------------------------------+--------+--------+----------+\n| 6cde85d2-47ad-446b-9a1f-960dbcca5199 | parted | ERROR  |          |\n+--------------------------------------+--------+--------+----------+\n\nInstance is assigned to Ironic node but node is not moved to deploying state\n\n$openstack baremetal list\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name   | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n| 95d3353f-61a6-44ba-8485-2881d1138ce1 | node-0 | None                                 | power off   | available          | False       |\n| 48112a56-8f8b-42fc-b143-742cf4856e78 | node-1 | 6cde85d2-47ad-446b-9a1f-960dbcca5199 | power off   | available          | False       |\n| c66a1035-5edf-434b-9d09-39ecc9069e02 | node-2 | None                                 | power off   | available          | False       |\n+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+\n\nIn n-cpu.log I see the following errors:\n\n2016-04-27 15:26:13.190 ERROR ironicclient.common.http [req-077efca4-1776-443b-bd70-0769c09a0e54 demo demo] Error contacting Ironic server: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with\n a node, it cannot be associated with this other node c66a1035-5edf-434b-9d09-39ecc9069e02 (HTTP 409). Attempt 2 of 2\n2016-04-27 15:26:13.190 ERROR nova.compute.manager [req-077efca4-1776-443b-bd70-0769c09a0e54 demo demo] [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Instance failed to spawn\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Traceback (most recent call last):\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2209, in _build_resources\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     yield resources\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/compute/manager.py\", line 2055, in _build_and_run_instance\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     block_device_info=block_device_info)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/driver.py\", line 698, in spawn\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     self._add_driver_fields(node, instance, image_meta, flavor)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/driver.py\", line 366, in _add_driver_fields\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     retry_on_conflict=False)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/nova/nova/virt/ironic/client_wrapper.py\", line 139, in call\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     return self._multi_getattr(client, method)(*args, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/v1/node.py\", line 198, in update\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     method=http_method)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/base.py\", line 171, in _update\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     resp, body = self.api.json_request(method, url, body=patch)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 552, in json_request\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     resp = self._http_request(url, method, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 189, in wrapper\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     return func(self, url, method, **kwargs)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]   File \"/opt/stack/python-ironicclient/ironicclient/common/http.py\", line 534, in _http_request\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199]     error_json.get('debuginfo'), method, url)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] Conflict: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with a node, it cannot be associat\ned with this other node c66a1035-5edf-434b-9d09-39ecc9069e02 (HTTP 409)\n2016-04-27 15:26:13.190 TRACE nova.compute.manager [instance: 6cde85d2-47ad-446b-9a1f-960dbcca5199] \n\nIn ir-cond.log the error is as follows:\n\n2016-04-27 15:26:13.183 ERROR oslo_messaging.rpc.dispatcher [req-ec2f0a30-13a8-4029-ac0b-e2852c0c67c9 None None] Exception during message handling: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associa\nted with a node, it cannot be associated with this other node c66a1035-5edf-434b-9d09-39ecc9069e02\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher Traceback (most recent call last):\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 138, in _dispatch_and_reply\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     incoming.message))\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 185, in _dispatch\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/dispatcher.py\", line 127, in _do_dispatch\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     result = func(ctxt, **new_args)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/server.py\", line 150, in inner\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     return func(*args, **kwargs)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/conductor/manager.py\", line 228, in update_node\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     node_obj.save()\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/objects/node.py\", line 340, in save\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     self.dbapi.update_node(self.uuid, updates)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher   File \"/opt/stack/ironic/ironic/db/sqlalchemy/api.py\", line 399, in update_node\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher     node=node_id)\n2016-04-27 15:26:13.183 TRACE oslo_messaging.rpc.dispatcher InstanceAssociated: Instance 6cde85d2-47ad-446b-9a1f-960dbcca5199 is already associated with a node, it cannot be associated with this other node c66a10\n35-5edf-434b-9d09-39ecc9069e02\n\nWhat's more, when I delete the failed server from Nova, Ironic node is left with orphaned instance assignment, which only can be deleted with node-update removing instance_uuid.\n\n[0] http://docs.openstack.org/developer/ironic/deploy/install-guide.html?highlight=local%20boot#enabling-local-boot-with-compute-service", 
            "date_created": "2016-04-27 13:12:22.937484+00:00", 
            "author": "https://api.launchpad.net/1.0/~pshchelo"
        }, 
        {
            "content": "Hi!\n\nPlease try to find a specific error in the Ironic logs.\n\nDouble assignment of nodes is an issue in Nova indeed, and it is worked on FWIW.", 
            "date_created": "2016-05-12 10:44:35.542344+00:00", 
            "author": "https://api.launchpad.net/1.0/~divius"
        }, 
        {
            "content": "This is unclear that it is actually a Nova bug, as it's getting an error back from Ironic. Putting to incomplete for now. ", 
            "date_created": "2016-06-02 16:31:04.435826+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Hi,\n\nThanks for reporting the bug. Some points:\n\n* Leaving the orphan nodes behind. We have a bug about it: https://bugs.launchpad.net/nova/+bug/1477490\n\n* What driver was being used?\n\n* I see that you are requesting local boot, this requires the tenant image to have grub2 installed in it, which probably is not the case for cirros.\n\n(also, please post the ironic logs as requested by others)", 
            "date_created": "2016-06-02 16:44:42.912611+00:00", 
            "author": "https://api.launchpad.net/1.0/~lucasagomes"
        }, 
        {
            "content": "Lucas,\n\nI was using the agent_ipmitool driver in Ironic, and ubuntu images.\n\n\nAnyway, I can no longer reproduce this bug on latest master, so please close as invalid. Feel free to reopen if it resurfaces.", 
            "date_created": "2016-06-08 14:07:52.798041+00:00", 
            "author": "https://api.launchpad.net/1.0/~pshchelo"
        }
    ], 
    "closed": "2016-06-08 14:08:13.344933+00:00"
}
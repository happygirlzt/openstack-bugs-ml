{
    "status": "Invalid", 
    "last_updated": "2015-09-07 04:05:55.639723+00:00", 
    "description": "Description of problem:\nA snapshot of an instance is been created with the status of 'deleted'. \nThe instance was launched with an ISO image of RHEL 6.5 & with the following flavor configuration: \nFlavor Name: m1.small\nVCPUs: 1\nRAM: 2048MB\nRoot Disk: 20\nEphemeral Disk: 40\nSwap Disk: 0MB\n\nThe system topology is:\n1. cloud controller with the Nova services installed (with nova network).\n2. Glance stand alone server.\n3. Cinder & Swift installed on the same server.\n4. Nova compute stand alone.\n\nVersion-Release number of selected component (if applicable):\nopenstack-nova-conductor-2013.2.2-2.el6ost.noarch\nopenstack-nova-scheduler-2013.2.2-2.el6ost.noarch\npython-django-openstack-auth-1.1.2-2.el6ost.noarch\nopenstack-dashboard-2013.2.2-1.el6ost.noarch\nopenstack-selinux-0.1.3-2.el6ost.noarch\nopenstack-packstack-2013.2.1-0.25.dev987.el6ost.noarch\nopenstack-keystone-2013.2.2-1.el6ost.noarch\nopenstack-nova-common-2013.2.2-2.el6ost.noarch\nopenstack-nova-api-2013.2.2-2.el6ost.noarch\nopenstack-nova-console-2013.2.2-2.el6ost.noarch\nopenstack-nova-network-2013.2.2-2.el6ost.noarch\nopenstack-nova-cert-2013.2.2-2.el6ost.noarch\nopenstack-dashboard-theme-2013.2.2-1.el6ost.noarch\nredhat-access-plugin-openstack-4.0.0-0.el6ost.noarch\nopenstack-nova-compute-2013.2.2-2.el6ost.noarch\nopenstack-nova-novncproxy-2013.2.2-2.el6ost.noarch\n\nHow reproducible:\n100%\n\nSteps to Reproduce:\n1. Launch an instance with an ISO image (with the same flavor configuration as above) \n2. Install the OS of the ISO on the ephemeral disk.\n3. After the installation is done & the OS is up take a snapshot of the instance.\n\nActual results:\nThe snapshot is been created in deleted status.\n\nExpected results:\nThe snapshot should be available.\n\nLogs are attached.", 
    "tags": [
        "libvirt", 
        "snapshot"
    ], 
    "importance": "High", 
    "heat": 16, 
    "link": "https://bugs.launchpad.net/nova/+bug/1287047", 
    "owner": "https://api.launchpad.net/1.0/~noelnelson", 
    "id": 1287047, 
    "index": 1410, 
    "openned": "2014-03-03 08:21:07.958336+00:00", 
    "created": "2014-03-03 08:21:07.958336+00:00", 
    "title": "instance snapshot creation failed: libvirtError: block copy still active: domain has active block copy job", 
    "comments": [
        {
            "content": "Description of problem:\nA snapshot of an instance is been created with the status of 'deleted'. \nThe instance was launched with an ISO image of RHEL 6.5 & with the following flavor configuration: \nFlavor Name: m1.small\nVCPUs: 1\nRAM: 2048MB\nRoot Disk: 20\nEphemeral Disk: 40\nSwap Disk: 0MB\n\nThe system topology is:\n1. cloud controller with the Nova services installed (with nova network).\n2. Glance stand alone server.\n3. Cinder & Swift installed on the same server.\n4. Nova compute stand alone.\n\nVersion-Release number of selected component (if applicable):\nopenstack-nova-conductor-2013.2.2-2.el6ost.noarch\nopenstack-nova-scheduler-2013.2.2-2.el6ost.noarch\npython-django-openstack-auth-1.1.2-2.el6ost.noarch\nopenstack-dashboard-2013.2.2-1.el6ost.noarch\nopenstack-selinux-0.1.3-2.el6ost.noarch\nopenstack-packstack-2013.2.1-0.25.dev987.el6ost.noarch\nopenstack-keystone-2013.2.2-1.el6ost.noarch\nopenstack-nova-common-2013.2.2-2.el6ost.noarch\nopenstack-nova-api-2013.2.2-2.el6ost.noarch\nopenstack-nova-console-2013.2.2-2.el6ost.noarch\nopenstack-nova-network-2013.2.2-2.el6ost.noarch\nopenstack-nova-cert-2013.2.2-2.el6ost.noarch\nopenstack-dashboard-theme-2013.2.2-1.el6ost.noarch\nredhat-access-plugin-openstack-4.0.0-0.el6ost.noarch\nopenstack-nova-compute-2013.2.2-2.el6ost.noarch\nopenstack-nova-novncproxy-2013.2.2-2.el6ost.noarch\n\nHow reproducible:\n100%\n\nSteps to Reproduce:\n1. Launch an instance with an ISO image (with the same flavor configuration as above) \n2. Install the OS of the ISO on the ephemeral disk.\n3. After the installation is done & the OS is up take a snapshot of the instance.\n\nActual results:\nThe snapshot is been created in deleted status.\n\nExpected results:\nThe snapshot should be available.\n\nLogs are attached.", 
            "date_created": "2014-03-03 08:21:07.958336+00:00", 
            "author": "https://api.launchpad.net/1.0/~yrabl"
        }, 
        {
            "content": "", 
            "date_created": "2014-03-03 08:21:07.958336+00:00", 
            "author": "https://api.launchpad.net/1.0/~yrabl"
        }, 
        {
            "content": "Question: does this happen on a fresh install?  I know in the past we've had issues with the API showing a deleted X with the same id as a newly created X.", 
            "date_created": "2014-03-06 21:10:18.264573+00:00", 
            "author": "https://api.launchpad.net/1.0/~sross-7"
        }, 
        {
            "content": "It happen after a fresh install and on a system that had a week of uptime, I don't think the point is there.", 
            "date_created": "2014-03-09 11:43:04.816778+00:00", 
            "author": "https://api.launchpad.net/1.0/~yrabl"
        }, 
        {
            "content": "One more bit of information: can you indicate the commands that you use to create the VM?  It will be useful when people attempt to reproduce and fix the issue.", 
            "date_created": "2014-04-15 21:04:01.955475+00:00", 
            "author": "https://api.launchpad.net/1.0/~sross-7"
        }, 
        {
            "content": "I've launched the instance from the Horizon, so I recommend you'll do the following, in the Horizon:\n1. Edit the small flavor and add the 40 GB to the ephemeral disk. \n2. Launch the instance with that flavor.", 
            "date_created": "2014-04-22 07:31:02.423056+00:00", 
            "author": "https://api.launchpad.net/1.0/~yrabl"
        }, 
        {
            "content": "I believe this is not working because a snapshot only saves the root disk and not the ephemeral disk.  There would be no way to determine which disk contains the root filesystem from an OpenStack perspective, or control which disk the user installs the root filesystem on.\n\nThe logic in nova/virt/libvirt/driver.py snapshot() method calls nova/virt/libvirt/utils.py find_disk() to find the root disk.  find_disk returns the first devices/filesystem/source in the domain XML tree, which would be always vda/hda, since it is the first in the matching element tree.", 
            "date_created": "2014-04-23 16:57:06.388041+00:00", 
            "author": "https://api.launchpad.net/1.0/~thang-pham"
        }, 
        {
            "content": "I could not reproduce this bug in Devstack with the latest Juno code.\n\nSteps taken:\n\n1. Upload Debian net-inst ISO image into Glance\n2. Modify tiny flavor to add an ephemeral disk (4GB)\n3. Boot an instance with the uploaded ISO image\n4. Install Debian in test instance\n5. Take a snapshot: produces a valid snapshot (of the ISO image)\n6. Reboot the instance into new Debian install\n7. Take a snapshot: produces a valid snapshot (of the ISO image)\n\nFrom your original bug description I see that you are using 2013 packages which are probably Icehouse or earlier. It is possible that the bug has been fixed in Juno. Although, that the snapshots are of the installation ISO image rather than the root disk is certainly suboptimal.", 
            "date_created": "2014-07-30 22:51:14.358943+00:00", 
            "author": "https://api.launchpad.net/1.0/~daniel-genin"
        }, 
        {
            "content": "Just saw this issue.  The codeblock that's raising the error is this one, which hasn't really changed since grizzly-eol:\n\nhttps://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L1830:L1851\n\nWe're undefining the domain so that we can do a blockRebase operation, then we abort it when it's done and have a finally statement that redefines the domain.  The redefine fails though because libvirt thinks there's still an active block copy job.  My guess is that we're raising an exception inside the block sometime after the blockRebase gets kicked off, but this could also be a race condition within libvirt.\n\nIf I can get a repro, I'll try putting an except Exception around the block and logging any exceptions that get raised.", 
            "date_created": "2014-11-03 21:34:35.641770+00:00", 
            "author": "https://api.launchpad.net/1.0/~joelfriedly"
        }, 
        {
            "content": "I am not able to reproduce above bug in Juno version. \n\nWhen I tried to reproduce, Snapshot Image status is Active instead of  \u2018Deleted\u2019.  \n\nI followed the same steps as mentioned above, to reproduce the bug. \n\n Image file : Ubuntu-14.04.1-desktop-amd64.iso .\n\nIf anyone is affected with this bug then will debug further otherwise this bug has to be marked as invalid\n\nRegards\nNoel Nelson Dsouza", 
            "date_created": "2015-03-31 14:14:54.396874+00:00", 
            "author": "https://api.launchpad.net/1.0/~noelnelson"
        }, 
        {
            "content": "Seems like you're hitting an old bug[1] where 'blockcopy' (or\n'blockcommit') missed to execute a cleanup routine which destroys a\nreference to the active block operation -- resulting in the error you're\nseeing when you attempted to 'abort' the block operation manually.\n\nThis bug is fixed in libvirt-1.2.8 and above. I see you're using\nlibvirt-1.2.7, if you can update libvirt in your environment, that\nshould fix your issue.", 
            "date_created": "2015-05-15 13:10:46.249978+00:00", 
            "author": "https://api.launchpad.net/1.0/~haichuan0227"
        }
    ], 
    "closed": "2015-04-06 05:29:06.945608+00:00"
}
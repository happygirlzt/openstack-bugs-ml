{
    "status": "Expired", 
    "last_updated": "2015-10-27 04:17:26.034650+00:00", 
    "description": "Reproducing method as following:\n1\u3001create one instance\n\n[root@opencosf0ccfb2525a94ffa814d647f08e4d6a4 ~(keystone_admin)]# nova list\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n| ID                                   | Name    | Status | Task State | Power State | Networks          |\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n| dc7c8242-9e02-4acf-9ae4-08030380e629 | test_zy | ACTIVE | -          | Running     | net=192.168.0.111 |\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n2\u3001run \"nova volume-attach instance_id  volume_id \".\n\n3\u3001after step2, the volume attached the instance successfuly.\n\n4\u3001run \"nova volume-attach instance_id  volume_id \", you will find the exception as following:\n[root@opencosf0ccfb2525a94ffa814d647f08e4d6a4 ~(keystone_admin)]# nova volume-attach  dc7c8242-9e02-4acf-9ae4-08030380e629  1435df8a-c4d6-4993-a0fd-4f57de66a28e\nERROR (BadRequest): Invalid volume: volume '1435df8a-c4d6-4993-a0fd-4f57de66a28e' status must be 'available'. Currently in 'in-use' (HTTP 400) (Request-ID: req-45902cbb-1f00-432f-bfbf-b041bdcc2695)\n\n5\u3001Execute command : nova reboot --hard <instance_id> ,\n  then login to the instance , you will find the volume attached as /dev/vdb don't work ok", 
    "tags": [
        "volumes"
    ], 
    "importance": "Undecided", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1471167", 
    "owner": "None", 
    "id": 1471167, 
    "index": 6901, 
    "openned": "2015-07-03 09:41:13.005124+00:00", 
    "created": "2015-07-03 09:41:13.005124+00:00", 
    "title": "A volume attached one instance not working properly in K version", 
    "comments": [
        {
            "content": "Reproducing method as following:\n1\u3001create one instance\n\n[root@opencosf0ccfb2525a94ffa814d647f08e4d6a4 ~(keystone_admin)]# nova list\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n| ID                                   | Name    | Status | Task State | Power State | Networks          |\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n| dc7c8242-9e02-4acf-9ae4-08030380e629 | test_zy | ACTIVE | -          | Running     | net=192.168.0.111 |\n+--------------------------------------+---------+--------+------------+-------------+-------------------+\n2\u3001run \"nova volume-attach instance_id  volume_id \".\n\n3\u3001after step2, the volume attached the instance successfuly.\n\n4\u3001run \"nova volume-attach instance_id  volume_id \", you will find the exception as following:\n[root@opencosf0ccfb2525a94ffa814d647f08e4d6a4 ~(keystone_admin)]# nova volume-attach  dc7c8242-9e02-4acf-9ae4-08030380e629  1435df8a-c4d6-4993-a0fd-4f57de66a28e\nERROR (BadRequest): Invalid volume: volume '1435df8a-c4d6-4993-a0fd-4f57de66a28e' status must be 'available'. Currently in 'in-use' (HTTP 400) (Request-ID: req-45902cbb-1f00-432f-bfbf-b041bdcc2695)\n\n5\u3001Execute command : nova reboot --hard <instance_id> ,\n  then login to the instance , you will find the volume attached as /dev/vdb don't work ok", 
            "date_created": "2015-07-03 09:41:13.005124+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "@YaoZheng_ZTE:\n\nDid you try to :\n1) attach *another* volume to the instance or \n2) the *same* volume again to the instance where it is already attached to?\n\nThe volume which is available at \"/dev/vdb\", is it accessible/usable?\n\nI put the status of this bug to \"Incomplete\" as long as there are open questions. When you answered the open questions, please put the status back to \"New\".", 
            "date_created": "2015-07-06 07:48:24.029903+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Hi mzoeller:\n   the issuse fault is: after run attached the volume to the vm again, as attached failed , then rollback , but when rollback , delete the volume's bdm table's record(the record is added at the first attached), the second attached record will be residual, but it is not complete. so the volume cannot normally be used.", 
            "date_created": "2015-07-06 08:06:37.246791+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "Hi mzoeller:\n    if  1) attach *another* volume to the instance or\n        this will be ok.\n   2) the *same* volume again to the instance where it is already attached to?\n      this step will create new device_name /dev/vdc, but,when the step do finish,as for attach fail,so rollback,\n     the rollback will destory the /dev/vdb. \n", 
            "date_created": "2015-07-06 08:14:45.981669+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "@YaoZheng_ZTE:\n\nI tried to reproduce the issue as you described [1] and did not succeed.\nThe volume I attached to the instance was still available/usable after\nthe hard reboot. I used:\n* nova commit: b7ab3689 2015-08-05 Merge \"Imported Translations f[...]\n* libvirt driver with LVM as storage backend [2]\n\nOpen questions\n--------------\n* Which storage backend did you use?\n* Which hypervisor did you use?\n* Exact version of Nova/OpenStack you are running:\n  If this is from a distro please provide `dpkg -l | grep nova` or \n  `rpm -ql | grep nova`. \n  If this is from git, please provide `git log -1`.\n\nReferences\n----------\n[1] script: paste.openstack.org/show/409043/\n[2] devstack local.conf: http://paste.openstack.org/show/409044/", 
            "date_created": "2015-08-05 17:37:58.485192+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "Hi mzoeller:\n    I clear the reason of  you cann't reproduce the issue. you use libvirt driver with LVM as storage backend . because the connection_info  in BDM table is the same for all lvm volume. you use IPSAN  or FCSAN ,will should be reproduce the issue.", 
            "date_created": "2015-08-27 06:56:01.742602+00:00", 
            "author": "https://api.launchpad.net/1.0/~zheng-yao1"
        }, 
        {
            "content": "[Expired for OpenStack Compute (nova) because there has been no activity for 60 days.]", 
            "date_created": "2015-10-27 04:17:22.334873+00:00", 
            "author": "https://api.launchpad.net/1.0/~janitor"
        }
    ], 
    "closed": "2015-10-27 04:17:23.354800+00:00"
}
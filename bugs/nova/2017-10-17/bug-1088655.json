{
    "status": "Fix Released", 
    "last_updated": "2013-04-04 11:08:07.437816+00:00", 
    "description": "When bare metal partitioning fails, ops needs to look in the log to determine the cause - but user requests (such as too-large swap sizes) can cause failures, and so users should get a decent error status set on their instance.", 
    "tags": [
        "baremetal"
    ], 
    "importance": "High", 
    "heat": 20, 
    "link": "https://bugs.launchpad.net/nova/+bug/1088655", 
    "owner": "https://api.launchpad.net/1.0/~vishvananda", 
    "id": 1088655, 
    "index": 908, 
    "openned": "2012-12-10 21:17:19.337709+00:00", 
    "created": "2012-12-10 21:17:19.337709+00:00", 
    "title": "bare metal node partitioning does not handle errors well", 
    "comments": [
        {
            "content": "When bare metal partitioning fails, ops needs to look in the log to determine the cause - but user requests (such as too-large swap sizes) can cause failures, and so users should get a decent error status set on their instance.", 
            "date_created": "2012-12-10 21:17:19.337709+00:00", 
            "author": "https://api.launchpad.net/1.0/~lifeless"
        }, 
        {
            "content": "I suspect this happens because of the division between nova-compute and nova-baremetal-deploy-helper. n-cpu calls driver.spawn() and marks the instances as ACTIVE once the machine powers on, while a separate process (nova-baremetal-deploy-helper) does the partitioning and image deployment. Merging these processes would probably resolve this bug.", 
            "date_created": "2013-02-09 00:42:21.648863+00:00", 
            "author": "https://api.launchpad.net/1.0/~devananda"
        }, 
        {
            "content": "Another approach would be for nova-baremetal-deploy-helper to record its progress in nova_bm.bm_deployments table.", 
            "date_created": "2013-02-09 05:51:34.637179+00:00", 
            "author": "https://api.launchpad.net/1.0/~devananda"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/21564", 
            "date_created": "2013-02-09 09:41:30.226074+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/21564\nCommitted: http://github.com/openstack/nova/commit/48439b98a1a7ac2dded34c8899918773f70667f2\nSubmitter: Jenkins\nBranch:    master\n\ncommit 48439b98a1a7ac2dded34c8899918773f70667f2\nAuthor: Devananda van der Veen <email address hidden>\nDate:   Fri Feb 8 20:36:19 2013 -0800\n\n    Wait for baremetal deploy inside driver.spawn\n    \n    Previously, baremetal driver.spawn returned as soon as the\n    machine power turned on, but before the user-image was deployed to the\n    hardware node, and long before the node was available on the network.\n    This meant the nova instance was marked as ACTIVE before provisioning\n    had actually finished. If the deploy failed and the baremetal node was\n    set to an ERROR state, the nova instance could still be left as ACTIVE\n    and the user was never informed of the error.\n    \n    This patch introduces a LoopingCall to monitor the deployment status in\n    the baremetal database. As the deployment is performed by\n    nova-baremetal-deploy-helper, the database record is updated. Once the\n    deployment is complete, driver.spawn() sets the baremetal node status\n    and the nova instance status is also set properly. If an error occurs\n    during the deployment, an exception is raised within driver.spawn()\n    allowing nova to follow the normal cleanup and notify paths.\n    \n    This also allows the baremetal PXE driver to delete cached image files\n    when a baremetal deployment fails.\n    \n    Fixes bug 1088655.\n    \n    Change-Id: I4feefd462fd956c9780995ec8b05b13e78278c8b\n", 
            "date_created": "2013-02-21 00:30:01.712142+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2013-02-21 08:49:01.253618+00:00"
}
{
    "status": "Expired", 
    "last_updated": "2016-07-05 09:51:31.968136+00:00", 
    "description": "VM creation failed due to a `shortage` in fixed IP.\n\nThe fixed range is /24, tempest normally does not keeps up more than ~8 VM.\n\nmessage: \"FixedIpLimitExceeded\" AND filename:\"logs/screen-n-net.txt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkZpeGVkSXBMaW1pdEV4Y2VlZGVkXCIgQU5EIGZpbGVuYW1lOlwibG9ncy9zY3JlZW4tbi1uZXQudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDc0MTA0MzE3MTgsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=\n\nhttp://logs.openstack.org/23/112523/1/check/check-tempest-dsvm-postgres-full/acac6d9/logs/screen-n-cpu.txt.gz#_2014-08-07_09_42_18_481", 
    "tags": [
        "gate-failure", 
        "quotas", 
        "testing"
    ], 
    "importance": "Undecided", 
    "heat": 30, 
    "link": "https://bugs.launchpad.net/nova/+bug/1353962", 
    "owner": "None", 
    "id": 1353962, 
    "index": 6325, 
    "openned": "2014-08-07 11:23:30.308359+00:00", 
    "created": "2014-08-07 11:23:30.308359+00:00", 
    "title": "Test job fails with FixedIpLimitExceeded with nova network", 
    "comments": [
        {
            "content": "message: \"FixedIpLimitExceeded\" AND filename:\"logs/screen-n-net.txt\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkZpeGVkSXBMaW1pdEV4Y2VlZGVkXCIgQU5EIGZpbGVuYW1lOlwibG9ncy9zY3JlZW4tbi1uZXQudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDc0MTA0MzE3MTgsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=\n\nhttp://logs.openstack.org/23/112523/1/check/check-tempest-dsvm-postgres-full/acac6d9/logs/screen-n-cpu.txt.gz#_2014-08-07_09_42_18_481\n\nThe fixed range is /24, tempest normally does not keeps up more than ~8 VM.", 
            "date_created": "2014-08-07 11:23:30.308359+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Is it possible tempest is leaking servers?", 
            "date_created": "2014-09-09 20:36:10.324478+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "https://review.openstack.org/#/c/112581/ should help sort this one out", 
            "date_created": "2014-09-09 20:41:03.928090+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "marking as incomplete in nova, since unable to confirm this is a nova bug", 
            "date_created": "2014-09-09 20:41:23.965424+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "This looks like a duplicate of bug 1338844.", 
            "date_created": "2014-09-11 16:11:28.092733+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Nevermind, 1338844 was fixed in tempest (quota issue), so this is different.  There are 26 hits in 10 days.", 
            "date_created": "2014-09-11 16:14:33.359836+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Even if tempest leaked every server it created (which is definitely not happening) if the network allocates a /24 there should still be plenty of ips available. From the logs above tempest only allocated a total of 171 servers:\n\nhttp://logs.openstack.org/23/112523/1/check/check-tempest-dsvm-postgres-full/acac6d9/logs/qemu/\n\nBut the exception is a quota one, the default tenant quota for a fixed ips is definitely more than 2. (otherwise we'd have much larger issues) I'm pretty sure we set it to -1, but I'm not sure. Looking at the test class in question:\n\nhttp://git.openstack.org/cgit/openstack/tempest/tree/tempest/api/compute/admin/test_servers_negative.py\n\nThe the tenant for that class will only have 2 server create calls. One in setUpClass and one in this test. I have a hard time believing 2 fixed ips exceeded the quota limit. Looking at the tests running around the same time, starting at:\n\nhttp://logs.openstack.org/23/112523/1/check/check-tempest-dsvm-postgres-full/acac6d9/console.html.gz#_2014-08-07_09_42_03_634\n\nwhich was the last test from a previous class on the same thread, until the same thing after the failed test:\n\nhttp://logs.openstack.org/23/112523/1/check/check-tempest-dsvm-postgres-full/acac6d9/console.html.gz#_2014-08-07_09_42_27_778\n\nI don't see anything that should have accidentally changed the quota to be < 2. I guess there could be a more hidden cross interaction because there are a bunch of admin api tests running around the same time (the joys of a default alphabetical test order), but I don't see one yet.", 
            "date_created": "2014-09-12 14:50:10.803417+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "This happened 26 times between 9/10 and 9/13, and zero times between 9/13 and 9/16. Perhaps it is fixed but we should keep watching.", 
            "date_created": "2014-09-16 17:15:08.145345+00:00", 
            "author": "https://api.launchpad.net/1.0/~david-kranz"
        }, 
        {
            "content": "Just hit it again:\n\nhttp://logs.openstack.org/98/125398/11/check/check-tempest-dsvm-postgres-full/f0d9495/logs/screen-n-net.txt.gz?level=TRACE\n\nThere are 17 hits in the last 7 days, check and gate, all failures.", 
            "date_created": "2014-10-06 14:00:24.112554+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Taking a look at the code in this stacktrace:\n\n2014-10-06 08:00:23.848 DEBUG nova.network.manager [req-bf60b9d9-452c-4fbc-9f7a-cc90ed762536 ServersAdminTestJSON-1220859943 ServersAdminTestJSON-233825707] [instance: 50156a3a-8b14-40ea-affe-97e5a510ec32] Allocate fixed ip on network cbcee1f9-13d7-4ec6-a450-d777a3d11120 allocate_fixed_ip /opt/stack/new/nova/nova/network/manager.py:859\n2014-10-06 08:00:23.874 DEBUG nova.network.manager [req-bf60b9d9-452c-4fbc-9f7a-cc90ed762536 ServersAdminTestJSON-1220859943 ServersAdminTestJSON-233825707] Quota exceeded for 3292f7b35abf4565a53d099ad878a335, tried to allocate fixed IP allocate_fixed_ip /opt/stack/new/nova/nova/network/manager.py:875\n2014-10-06 08:00:23.874 ERROR oslo.messaging.rpc.dispatcher [req-bf60b9d9-452c-4fbc-9f7a-cc90ed762536 ServersAdminTestJSON-1220859943 ServersAdminTestJSON-233825707] Exception during message handling: Maximum number of fixed ips exceeded\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 134, in _dispatch_and_reply\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     incoming.message))\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 177, in _dispatch\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py\", line 123, in _do_dispatch\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/network/floating_ips.py\", line 114, in allocate_for_instance\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     **kwargs)\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/network/manager.py\", line 511, in allocate_for_instance\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     requested_networks=requested_networks)\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/network/manager.py\", line 192, in _allocate_fixed_ips\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     vpn=vpn, address=address)\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher   File \"/opt/stack/new/nova/nova/network/manager.py\", line 876, in allocate_fixed_ip\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher     raise exception.FixedIpLimitExceeded()\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher FixedIpLimitExceeded: Maximum number of fixed ips exceeded\n2014-10-06 08:00:23.874 25609 TRACE oslo.messaging.rpc.dispatcher \n\nWe're using the FlatDHCPManager:\n\nhttp://logs.openstack.org/98/125398/11/check/check-tempest-dsvm-postgres-full/f0d9495/logs/etc/nova/nova.conf.txt.gz\n\nSo we're using DB quotas (VlanManager uses the no-op quota driver).\n\nWe could be logging the wrong project_id:\n\nLOG.debug(\"Quota exceeded for %s, tried to allocate \"\n                      \"fixed IP\", context.project_id)\n\nfixed_ips is a per-project quota in the DB API:\n\nPER_PROJECT_QUOTAS = ['fixed_ips', 'floating_ips', 'networks']\n\nSo the user_id in this case doesn't matter.  I'm wondering if the actual project id used is the admin tenant?  I don't see 3292f7b35abf4565a53d099ad878a335 in tempest.conf though so it's probably not the admin tenant:\n\nhttp://logs.openstack.org/98/125398/11/check/check-tempest-dsvm-postgres-full/f0d9495/logs/tempest_conf.txt.gz\n\nFrom talking with Matt Treinish, it sounds like this test creates a tenant-isolated user/project with an admin role:\n\nhttp://git.openstack.org/cgit/openstack/tempest/tree/tempest/api/compute/admin/test_servers.py#n29\n\nSo yeah, we have an admin role and we're using a per-project resource for quotas (fixed_ips), so we need to find out what the quota limit is per project in a tempest run.\n\nI don't see the quota driver set in nova.conf so it defaults to the DB quota driver:\n\nhttp://logs.openstack.org/98/125398/11/check/check-tempest-dsvm-postgres-full/f0d9495/logs/etc/nova/nova.conf.txt.gz\n\nBut what are the limits?  Because the defaults are 10.  I don't see anything in devstack that changes quotas unless the virt driver is the fake driver, which isn't the case here.", 
            "date_created": "2014-10-06 14:44:12.147791+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looking at tests that extend BaseV2ComputeAdminTest because tempest.api.compute.admin.test_servers creates 2 test instances for the JSON run alone (and there is an xml version also).  Those would be on the same admin tenant, so if those are running concurrently we could have at least 4 instances running at the same time under that tenant.\n\nWe have 4 test workers, so we could have 2 other tests running concurrently.\n\nThere are 36 test classes that extend BaseV2ComputeAdminTest, not all of them create test servers though.\n\nLooking at the console log when our test fails:\n\nhttp://logs.openstack.org/98/125398/11/check/check-tempest-dsvm-postgres-full/f0d9495/console.html#_2014-10-06_08_00_28_464\n\nAnd looking at what's running before it, we see:\n\ntempest.api.compute.admin.test_quotas_negative.QuotasAdminNegativeTestXML\n\nand\n\ntempest.api.compute.admin.test_quotas.QuotaClassesAdminTestJSON\n\nWe've had race issues with QuotaClassesAdminTestJSON in the past where it would set the default quota class values to 0 so we'd hit over-quota.  That was changed awhile back to take the default quotas and add 100 to the limit, so that shouldn't be causing issues.\n\nQuotasAdminNegativeTestXML is running with force_tenant_isolation=False\n\nSo the quota admin tests should be running in tenant isolation and not impact ServersAdminTestJSON.", 
            "date_created": "2014-10-06 15:49:12.055194+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Chatted with Matt Treinish and the json and xml tests should have separate tenants because in the gate we running with allow_tenant_isolation=True and the credentials are created in setUpClass of each test, which is different for the json and xml runs.\n\nSo I guess I'm back to wondering what's going on with the fixed_ips over-quota if it's a per-project resource quota and the test is only creating at most 4 test servers.", 
            "date_created": "2014-10-06 16:13:05.056268+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/126401", 
            "date_created": "2014-10-06 19:33:45.757665+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/126402", 
            "date_created": "2014-10-06 19:33:54.072491+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/126401\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=044b18e7f02c588738bc21c43ad8066c9fcd7426\nSubmitter: Jenkins\nBranch:    master\n\ncommit 044b18e7f02c588738bc21c43ad8066c9fcd7426\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Oct 6 08:04:01 2014 -0700\n\n    nova-net: add more useful logging before raising FixedIpLimitExceeded\n    \n    The OverQuota exception that we get back from the DB API contains some\n    useful information about the quota limits for the project and resource\n    and the current usage, so we can log that before raising\n    FixedIpLimitExceeded. Also logs the actual project ID passed to\n    quotas.reserve rather than context.project_id since it's possible those\n    are different.\n    \n    This should help us figure out if we're leaking resources in gate runs.\n    \n    Related-Bug: #1353962\n    \n    Change-Id: Ia8a6d2336ddbbe28178be8d3ce4d97e6d4ab3787\n", 
            "date_created": "2014-10-07 00:41:04.794878+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "after further investigation with Matt Riedemann, it looks like there are some issues in the quotas code, although its still not clear if nova is that is the root cause of this bug or not.", 
            "date_created": "2014-10-07 17:21:26.791785+00:00", 
            "author": "https://api.launchpad.net/1.0/~jogo"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/126628", 
            "date_created": "2014-10-07 17:32:51.142856+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This tells us that there is something wrong with quotas in nova for fixed IPs:\n\nhttp://logs.openstack.org/28/126628/2/check/check-tempest-dsvm-postgres-full/a4b846b/logs/screen-n-cond.txt.gz#_2014-10-07_21_26_33_349", 
            "date_created": "2014-10-08 16:12:02.817059+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Bug 1284930 might be related, i.e. if we're not rolling back quota reservations on a fixed IP deallocate failure we could be leaking quotas that way.", 
            "date_created": "2014-10-09 14:03:31.182989+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/126402\nReason: I was confusing this for nova.quota.DbQuotaDriver.rollback but it's actually nova.objects.quotas.Quotas.rollback.\n\nYou were all right for not believing me!", 
            "date_created": "2014-10-09 14:17:35.609488+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/126628\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=ff2748990389deee62bed97456bf7a81cc43a47c\nSubmitter: Jenkins\nBranch:    master\n\ncommit ff2748990389deee62bed97456bf7a81cc43a47c\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Oct 8 08:28:32 2014 -0700\n\n    Log quota refresh in_use message at INFO level for logstash\n    \n    Commit 5aef26496786c02a1ed324bbd2eb0b9994a2018d added the log message to\n    give some idea when we're unnecessarily refreshing quota usage.\n    \n    Logstash only collects INFO+ level messages, so bump this to an INFO\n    level message so we can track gate runs to see if this is actually a\n    problem.\n    \n    Related-Bug: #1353962\n    \n    Change-Id: I60ffa9dbda22e252f290bf316fa2e472d3e92f2f\n", 
            "date_created": "2014-10-09 15:59:32.343993+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I hit it again here today\n\nhttp://logs.openstack.org/60/111660/13/check/check-tempest-dsvm-full/d0f4d9b/logs/screen-n-net.txt.gz\n\n\nreally confusing me is following log\n\nWARNING nova.network.manager [req-49960ae4-3327-4770-8ff1-e1002786ee79 ServersAdminNegativeTestXML-863236903 ServersAdminNegativeTestXML-1636622346] [instance: 7f330ee3-90fb-463f-a325-f4a8bd7c3a02] Quota exceeded for project 5e8c727ff61142d9a7ad424cdb8ca8b5, tried to allocate fixed IP. 0 of 99 are in use or are already reserved.\n\nfrom code analysis, it should not report an exception because the used count is 0\nI checked quota related code but I can't see any useful log to indicate why quota failed to allocate\n\nso ,I will ad more logs to _raise_overquota_exception and see whether we can know what happened", 
            "date_created": "2014-10-29 12:31:40.229357+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "@matt, can you guide me how to see logs in gate ? I am not familiar with it .... thanks", 
            "date_created": "2014-10-29 12:32:04.698405+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "BTW: I didn't see logs changed at  https://review.openstack.org/126628\nso maybe it can be another cause of the problem?", 
            "date_created": "2014-10-29 12:32:53.402684+00:00", 
            "author": "https://api.launchpad.net/1.0/~jichenjc"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/131737", 
            "date_created": "2014-10-29 12:52:55.692153+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/131737\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=5c9537b40092e701615e33e0dcf90a082d172f72\nSubmitter: Jenkins\nBranch:    master\n\ncommit 5c9537b40092e701615e33e0dcf90a082d172f72\nAuthor: jichenjc <email address hidden>\nDate:   Tue Oct 28 04:55:50 2014 +0800\n\n    Add debug log when over quota exception occurs\n    \n    There are some weird logs showed in bug 1353962, with\n    current logs we can't decide which part is wrong.\n    Add more logs will help operater to know the real reason\n    that make the quota exceed.\n    \n    If this change makes too much log out, after bug 1353962\n    is fixed, we can talk about remove those logs, but currently\n    it will be helpful.\n    \n    Change-Id: I2ab1fb62efc06b1cee213920539980e331870bf5\n    Related-Bug: #1353962\n", 
            "date_created": "2014-11-24 07:21:38.501165+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I'm not seeing anything tempest related in this bug. It looks like nova is just falling down and enforcing a quota even when the limits aren't even close.", 
            "date_created": "2014-11-26 21:04:17.075357+00:00", 
            "author": "https://api.launchpad.net/1.0/~treinish"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/167814", 
            "date_created": "2015-03-25 21:59:54.137817+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Here we go, we have better logging in the DB API when this fails now:\n\nhttp://logs.openstack.org/01/167101/5/check/check-tempest-dsvm-full/9f94b8c/logs/screen-n-cond.txt.gz#_2015-03-25_19_19_37_678\n\nRaise OverQuota exception because: project_quotas: {'fixed_ips': -1}, user_quotas: {'fixed_ips': 99L}, deltas: {u'fixed_ips': 1}, overs: [u'fixed_ips'], project_usages: {u'ram': {'total': 128L, 'reserved': 0L, 'in_use': 128L}, u'floating_ips': {'total': 1L, 'reserved': 0L, 'in_use': 1L}, u'instances': {'total': 2L, 'reserved': 0L, 'in_use': 2L}, u'cores': {'total': 2L, 'reserved': 0L, 'in_use': 2L}, u'fixed_ips': {'total': 1L, 'reserved': 0L, 'in_use': 1L}, u'security_groups': {'total': 1L, 'reserved': 0L, 'in_use': 1L}}, user_usages: {u'ram': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedba50>, u'floating_ips': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedb5d0>, u'instances': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedbc90>, u'cores': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedb750>, u'fixed_ips': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedb510>, u'security_groups': <nova.db.sqlalchemy.models.QuotaUsage object at 0x7faaccedbf90>}\n\nSo project_quotas and user_quotas are different:\n\nproject_quotas: {'fixed_ips': -1}, user_quotas: {'fixed_ips': 99L}\n\nThe user_quotas are what's passed back to the network manager which is why the log message is confusing:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/db/sqlalchemy/api.py?id=2015.1.0b3#n3578\n\nSo we see this in the n-net logs:\n\nhttp://logs.openstack.org/01/167101/5/check/check-tempest-dsvm-full/9f94b8c/logs/screen-n-net.txt.gz?level=TRACE#_2015-03-25_19_19_37_685\n\n2015-03-25 19:19:37.685 WARNING nova.network.manager [req-0f01e070-e2ac-4058-a7cb-9d619bb557d6 FloatingIPsTestJSON-1260806534 FloatingIPsTestJSON-109816172] [instance: c90d1a08-4be7-4f88-8a73-36424d70acd6] Quota exceeded for project e3bba3eb15c844cda054b8c73ec68c0b, tried to allocate fixed IP. 1 of 99 are in use or are already reserved.\n\n\nSo from project_usages, we have this:\n\nu'fixed_ips': {'total': 1L, 'reserved': 0L, 'in_use': 1L}\n\nWhich is why we go over-quota, because it thinks we have 1 total fixed_ip to use and it's already in use.", 
            "date_created": "2015-03-26 13:54:15.935010+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This might also be useful, we are being forced to refresh quotas in the gate but only for fixed_ips:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwicXVvdGFfdXNhZ2VzIG91dCBvZiBzeW5jLCB1cGRhdGluZ1wiIEFORCB0YWdzOlwic2NyZWVuLW4tY29uZC50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQyNzM4ODk2NDUyOCwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==\n\ne.g.\n\n[req-962680b8-2147-462e-a3bc-13d942b8989f MultipleCreateTestJSON-1247330694 MultipleCreateTestJSON-1141436516] quota_usages out of sync, updating. project_id: 5b4715def338466c9a6eea5886fb8f38, user_id: None, resource: fixed_ips, tracked usage: 0, actual usage: 1\n\n[req-75c73b51-9f2a-4275-8ab7-1e8c270a3848 MultipleCreateTestJSON-1809221276 MultipleCreateTestJSON-267241457] quota_usages out of sync, updating. project_id: 647717c69bba4377b9421b2c832f4991, user_id: None, resource: fixed_ips, tracked usage: -1, actual usage: 1", 
            "date_created": "2015-03-26 16:58:21.628313+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/167814\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=fe5003bc1400e4e467699ef857071717e75146e7\nSubmitter: Jenkins\nBranch:    master\n\ncommit fe5003bc1400e4e467699ef857071717e75146e7\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Mar 25 14:58:04 2015 -0700\n\n    Add some logging in the quota.reserve flow\n    \n    Try to figure out what's going on with a race in the gate\n    when we go over-quota trying to allocate a fixed IP.\n    \n    Change-Id: Ib23450826a5b4a995360d1958e6070b23a3e732c\n    Related-Bug: #1353962\n", 
            "date_created": "2015-03-26 19:16:41.422217+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/168158", 
            "date_created": "2015-03-26 21:03:23.817334+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/168158\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=af4ce3ee63e4e4af9113405396c3201eb3efe3c6\nSubmitter: Jenkins\nBranch:    master\n\ncommit af4ce3ee63e4e4af9113405396c3201eb3efe3c6\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Mar 26 13:46:40 2015 -0700\n\n    Add debug logging to quota_reserve flow\n    \n    There are some places in the quota_reserve flow that could use some\n    debug logging for helping to figure out the problem when we over quota:\n    \n    1. Log when we detect a desync in _is_quota_refresh_needed and force a\n       refresh.\n    2. There is a comment in quota_reserve after we call\n       _refresh_quota_usages saying we assume the sync was OK and\n       essentially everything is hunky dory. Log if we actually refreshed\n       without a change to usage.\n    3. Log the details from project/user quotas/usages when we go over\n       quota in _calculate_overquota.\n    4. Log if we update project_usages using values from user_usages in\n       quota_reserve.\n    5. Finally, before we raise the OverQuota exception in quota_reserve,\n       the user_usages dict values are UserQuota objects which are sqla\n       dict-like objects and don't stringify nicely for the logs, so\n       convert those to actual dict values for logging.\n    \n    Related-Bug: #1353962\n    \n    Change-Id: Ic6e3d5fdf51c0a13c97509d0888a3bba00fb3193\n", 
            "date_created": "2015-03-27 04:10:24.649326+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "It looks like this bug has nothing to do with Tempest. So moving the status of the bug to \"Invalid\".", 
            "date_created": "2015-06-30 13:08:10.504582+00:00", 
            "author": "https://api.launchpad.net/1.0/~ylobankov"
        }, 
        {
            "content": "\nThis is an automated cleanup. This bug report has been closed because it\nis older than 18 months and there is no open code change to fix this.\nAfter this time it is unlikely that the circumstances which lead to\nthe observed issue can be reproduced.\n\nIf you can reproduce the bug, please:\n* reopen the bug report (set to status \"New\")\n* AND add the detailed steps to reproduce the issue (if applicable)\n* AND leave a comment \"CONFIRMED FOR: <RELEASE_NAME>\"\n  Only still supported release names are valid (LIBERTY, MITAKA, OCATA, NEWTON).\n  Valid example: CONFIRMED FOR: LIBERTY\n", 
            "date_created": "2016-07-05 09:51:31.245179+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }
    ], 
    "closed": "2016-07-05 09:51:27.247526+00:00"
}
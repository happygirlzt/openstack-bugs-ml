{
    "status": "Won't Fix", 
    "last_updated": "2016-05-09 15:29:08.110039+00:00", 
    "description": "Logstash query:\nmessage:\"<class 'oslo_messaging.exceptions.MessagingTimeout'> (HTTP 500)\"\n\nGrenade failure:\n2016-02-12 10:31:02.846 | + openstack ip floating remove 172.24.5.2 cinder_server1\n2016-02-12 10:32:05.422 | Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n2016-02-12 10:32:05.422 | <class 'oslo_messaging.exceptions.MessagingTimeout'> (HTTP 500) (Request-ID: req-b1f17c4c-b814-4a21-9209-13db7915370f)\n2016-02-12 10:32:05.443 | + exit_trap\n\nExample from: http://logs.openstack.org/12/278012/3/check/gate-grenade-dsvm/ac87c30/logs/grenade.sh.txt.gz", 
    "tags": [
        "gate-failure", 
        "in-stable-mitaka"
    ], 
    "importance": "Undecided", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1545002", 
    "owner": "None", 
    "id": 1545002, 
    "index": 7305, 
    "openned": "2016-03-10 02:15:55.904090+00:00", 
    "created": "2016-02-12 14:34:10.597146+00:00", 
    "title": "Grenade Failure - MessagingTimeout on floating IP remove", 
    "comments": [
        {
            "content": "Logstash query:\nmessage:\"<class 'oslo_messaging.exceptions.MessagingTimeout'> (HTTP 500)\"\n\nGrenade failure:\n2016-02-12 10:31:02.846 | + openstack ip floating remove 172.24.5.2 cinder_server1\n2016-02-12 10:32:05.422 | Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n2016-02-12 10:32:05.422 | <class 'oslo_messaging.exceptions.MessagingTimeout'> (HTTP 500) (Request-ID: req-b1f17c4c-b814-4a21-9209-13db7915370f)\n2016-02-12 10:32:05.443 | + exit_trap\n\nExample from: http://logs.openstack.org/12/278012/3/check/gate-grenade-dsvm/ac87c30/logs/grenade.sh.txt.gz", 
            "date_created": "2016-02-12 14:34:10.597146+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Initial triage on openstack-oslo - http://eavesdrop.openstack.org/irclogs/%23openstack-oslo/%23openstack-oslo.2016-02-12.log.html#t2016-02-12T13:54:17", 
            "date_created": "2016-02-12 14:36:21.300195+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "The failure in n-api is here:\n\nhttp://logs.openstack.org/12/278012/3/check/gate-grenade-dsvm/ac87c30/logs/new/screen-n-api.txt.gz?level=TRACE#_2016-02-12_10_32_05_378\n\nThat timeout is at 10:32:05. Around the same time in the n-net logs we have this:\n\nhttp://logs.openstack.org/12/278012/3/check/gate-grenade-dsvm/ac87c30/logs/new/screen-n-net.txt.gz#_2016-02-12_10_31_05_380\n\n2016-02-12 10:31:05.380 DEBUG oslo_concurrency.lockutils [req-b1f17c4c-b814-4a21-9209-13db7915370f cinder_grenade cinder_grenade] Lock \"172.24.5.2\" acquired by \"nova.network.floating_ips.do_disassociate\" :: waited 0.000s inner /usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py:270\n2016-02-12 10:32:06.595 22979 DEBUG oslo.messaging._drivers.impl_rabbit [-] Received recoverable error from kombu: on_error /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/impl_rabbit.py:654\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit Traceback (most recent call last):\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 436, in _ensured\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     return fun(*args, **kwargs)\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 508, in __call__\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     return fun(*args, channel=channels[0], **kwargs), channels[0]\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/impl_rabbit.py\", line 704, in execute_method\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     method()\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/impl_rabbit.py\", line 1050, in _publish\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     producer.publish(msg, expiration=timeout)\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/kombu/messaging.py\", line 172, in publish\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     routing_key, mandatory, immediate, exchange, declare)\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/kombu/messaging.py\", line 188, in _publish\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     mandatory=mandatory, immediate=immediate,\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/amqp/channel.py\", line 2132, in basic_publish_confirm\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     self.wait([(60, 80), (60, 120)])\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/amqp/abstract_channel.py\", line 67, in wait\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     self.channel_id, allowed_methods, timeout)\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/amqp/connection.py\", line 241, in _wait_method\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     channel, method_sig, args, content = read_timeout(timeout)\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/amqp/connection.py\", line 330, in read_timeout\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     return self.method_reader.read_method()\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit   File \"/usr/local/lib/python2.7/dist-packages/amqp/method_framing.py\", line 189, in read_method\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit     raise m\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit IOError: Socket closed\n2016-02-12 10:32:06.595 22979 ERROR oslo.messaging._drivers.impl_rabbit \n2016-02-12 10:32:06.597 22979 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server 127.0.0.1:5672 closed the connection. Check login credentials: Socket closed", 
            "date_created": "2016-02-12 14:38:17.764718+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Also, FWIW, it looks like this started happening around 2/4:\n\nhttp://logstash.openstack.org/#dashboard/file/logstash.json?query=message%3A%5C%22cinder_grenade%5C%22%20AND%20message%3A%5C%22_disassociate_floating_ip%5C%22%20AND%20message%3A%5C%22MessagingTimeout%5C%22%20AND%20tags%3A%5C%22screen-n-api.txt%5C%22&from=30d", 
            "date_created": "2016-02-12 14:46:54.929991+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "oslo.messaging 4.1.0 was released on 2/4 so we suspect there was a regression in that:\n\nhttps://github.com/openstack/oslo.messaging/compare/4.0.0...4.1.0", 
            "date_created": "2016-02-12 14:53:05.533915+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "1 hit in last 7 days. Please reopen if this repeats again.", 
            "date_created": "2016-02-28 21:44:59.823090+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "This looks to me like the same deal, everything fails at the end of the run with rabbit timeouts:\n\nhttp://logs.openstack.org/15/290715/2/check/gate-grenade-dsvm/056bc19/logs/grenade.sh.txt.gz#_2016-03-09_19_21_22_820\n\nhttp://logs.openstack.org/15/290715/2/check/gate-grenade-dsvm/056bc19/logs/new/screen-n-api.txt.gz?level=TRACE#_2016-03-09_19_21_22_814\n\nhttp://logs.openstack.org/15/290715/2/check/gate-grenade-dsvm/056bc19/logs/new/screen-n-cpu.txt.gz?level=TRACE#_2016-03-09_19_20_41_431\n\nhttp://logs.openstack.org/15/290715/2/check/gate-grenade-dsvm/056bc19/logs/new/screen-n-cpu.txt.gz?level=TRACE#_2016-03-09_19_20_41_431", 
            "date_created": "2016-03-09 20:23:05.882407+00:00", 
            "author": "https://api.launchpad.net/1.0/~danms"
        }, 
        {
            "content": "Looks like 5 hits starting 7th. all on  build_node devstack-trusty-osic-cloud1-XYZ", 
            "date_created": "2016-03-09 20:51:32.159388+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "In each of the 5 hits i see - \"Function 'nova.servicegroup.drivers.db.DbDriver._report_state' run outlasted interval by 104.46 sec\"\n\nhttp://logs.openstack.org/30/285530/7/check/gate-grenade-dsvm/4575575/logs/new/screen-n-net.txt.gz?#_2016-03-08_18_36_23_074\nhttp://logs.openstack.org/54/268054/6/check/gate-grenade-dsvm-ceilometer/a253d5f/logs/new/screen-n-net.txt.gz?#_2016-03-08_08_01_31_912\nhttp://logs.openstack.org/14/288814/2/check/gate-grenade-dsvm/c80d3b0/logs/new/screen-n-net.txt.gz?#_2016-03-08_00_57_02_802\nhttp://logs.openstack.org/76/286976/4/gate/gate-grenade-dsvm/101e2a3/logs/new/screen-n-net.txt.gz?#_2016-03-07_22_42_49_151\nhttp://logs.openstack.org/19/225119/5/check/gate-grenade-dsvm/31baa7b/logs/new/screen-n-net.txt.gz?#_2016-03-07_11_08_44_475\n\nIt almost seems like something is locking things up?", 
            "date_created": "2016-03-10 02:19:27.130941+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "Adding \"gate-failure\" tag as this issue got noted in the gate.", 
            "date_created": "2016-03-10 09:58:35.931607+00:00", 
            "author": "https://api.launchpad.net/1.0/~mzoeller"
        }, 
        {
            "content": "This is not a nova issue, this is an oslo.messaging failure. The Nova API is just the first thing to get called.", 
            "date_created": "2016-03-10 15:44:44.438616+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/292181", 
            "date_created": "2016-03-14 00:44:33.211737+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/mitaka\nReview: https://review.openstack.org/292313", 
            "date_created": "2016-03-14 11:38:44.535890+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/292181\nCommitted: https://git.openstack.org/cgit/openstack/oslo.messaging/commit/?id=b6a8d51e6cdbe69b38a0c51401e7ffd07d67a9ba\nSubmitter: Jenkins\nBranch:    master\n\ncommit b6a8d51e6cdbe69b38a0c51401e7ffd07d67a9ba\nAuthor: Davanum Srinivas <email address hidden>\nDate:   Sun Mar 13 20:41:01 2016 -0400\n\n    Bump rabbit_transient_queues_ttl to 30 mins\n    \n    In I83a8d09dc0cdae24c12d7043ec810529a9ce57ab, we\n    made reply and fanout queues expire instead of auto-delete\n    and set the rabbit_transient_queues_ttl to 10 mins.\n    \n    In grenade, we seem to see the queues expiring just around\n    the time the new side is coming up which causes havoc. There\n    is no reason the rabbit_transient_queues_ttl should not be\n    higher. So let's bump up the expiry to 30 mins.\n    \n    Closes-Bug: #1545002\n    Change-Id: I70a29b762198129fe0a3e904d9f2a7d4242b322c\n", 
            "date_created": "2016-03-14 17:40:25.218695+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/292313\nCommitted: https://git.openstack.org/cgit/openstack/oslo.messaging/commit/?id=f8cad0e76d1d7ed0c60b0a7c2cd6c157382f0e9a\nSubmitter: Jenkins\nBranch:    stable/mitaka\n\ncommit f8cad0e76d1d7ed0c60b0a7c2cd6c157382f0e9a\nAuthor: Davanum Srinivas <email address hidden>\nDate:   Sun Mar 13 20:41:01 2016 -0400\n\n    Bump rabbit_transient_queues_ttl to 30 mins\n    \n    In I83a8d09dc0cdae24c12d7043ec810529a9ce57ab, we\n    made reply and fanout queues expire instead of auto-delete\n    and set the rabbit_transient_queues_ttl to 10 mins.\n    \n    In grenade, we seem to see the queues expiring just around\n    the time the new side is coming up which causes havoc. There\n    is no reason the rabbit_transient_queues_ttl should not be\n    higher. So let's bump up the expiry to 30 mins.\n    \n    Closes-Bug: #1545002\n    Change-Id: I70a29b762198129fe0a3e904d9f2a7d4242b322c\n", 
            "date_created": "2016-03-14 17:40:31.327132+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This issue was fixed in the openstack/oslo.messaging 4.5.1 release.", 
            "date_created": "2016-03-15 11:24:46.622394+00:00", 
            "author": "https://api.launchpad.net/1.0/~dims-v"
        }, 
        {
            "content": "This issue was fixed in the openstack/oslo.messaging 5.0.0 release.", 
            "date_created": "2016-05-09 15:29:07.462041+00:00", 
            "author": "https://api.launchpad.net/1.0/~doug-hellmann"
        }
    ], 
    "closed": "2016-03-10 15:44:16.556506+00:00"
}
{
    "status": "Fix Released", 
    "last_updated": "2015-06-19 12:51:04.783989+00:00", 
    "description": "When live-migrating an instance that has a Cinder volume (stored on NFS) attached, the operation fails if the volume size is bigger than the space left on the destination node. This should not happen, since this volume does not have to be migrated. Here is how to reproduce the bug on a cluster with one control node and two compute nodes, using the NFS backend of Cinder.\n\n\n$ nova boot --flavor m1.tiny --image 173241e-babb-45c7-a35f-b9b62e8ced78 test_vm\n...\n\n$ nova volume-create --display-name test_volume 100\n...\n| id                  | 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 |\n...\n\n\n$ nova volume-attach test_vm  6b9e1d03-3f53-4454-add9-a8c32d82c7e6 auto\n...\n\n$ nova show test_vm | grep OS-EXT-SRV-ATTR:host\n| OS-EXT-SRV-ATTR:host                 | t1-cpunode0                                                |\n\n$ nova service-list | grep nova-compute\n| nova-compute     | t1-cpunode0 | nova     | enabled | up    | 2014-08-13T19:14:40.000000 | -               |\n| nova-compute     | t1-cpunode1 | nova     | enabled | up    | 2014-08-13T19:14:41.000000 | -               |\n\nNow, let's say I want to live-migrate test_vm to t1-cpunode1:\n\n$ nova live-migration --block-migrate test_vm t1-cpunode1\nERROR: Migration pre-check error: Unable to migrate a0d9c991-7931-4710-8684-282b1df4cca6: Disk of instance is too large(available on destination host:46170898432 < need:108447924224) (HTTP 400) (Request-ID: req-b4f00867-df51-44be-8f97-577be385d536)\n\n\nIn nova/virt/libvirt/driver.py, _assert_dest_node_has_enough_disk() calls get_instance_disk_info(), which in turn, calls _get_instance_disk_info(). In this method, we see that volume devices are not taken into account when computing the amount of space needed to migrate an instance:\n\n...\n            if disk_type != 'file':\n                LOG.debug('skipping %s since it looks like volume', path)\n                continue\n\n            if target in volume_devices:\n                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '\n                          'volume', {'path': path, 'target': target})\n                continue\n...\n\nBut for some reason, we never get into these conditions.\n\nIf we ssh the compute where the instance currently lies, we can get more information about it:\n\n$ virsh dumpxml 11\n...\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source file='/var/lib/nova/mnt/84751739e625d0ea9609a65dd9c0a6f1/volume-6b9e1d03-3f53-4454-add9-a8c32d82c7e6'/>\n      <target dev='vdb' bus='virtio'/>\n      <serial>6b9e1d03-3f53-4454-add9-a8c32d82c7e6</serial>\n      <alias name='virtio-disk1'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>\n    </disk>\n...\n\nThe disk type is \"file\", which might explain why this volume is not skipped in the code snippet shown above. When we use the default Cinder backend, we get something such as:\n\n    <disk type='block' device='disk'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source dev='/dev/disk/by-path/ip-192.168.200.250:3260-iscsi-iqn.2010-10.org.openstack:volume-47ecc6a6-8af9-4011-a53f-14a71d14f50b-lun-1'/>\n      <target dev='vdb' bus='virtio'/>\n      <serial>47ecc6a6-8af9-4011-a53f-14a71d14f50b</serial>\n      <alias name='virtio-disk1'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'\nfunction='0x0'/>\n    </disk>\n\n\nI think that the code in LibvirtNFSVolumeDriver.connect_volume() might be wrong: conf.source_type should be set to something else than \"file\" (and some other changes might be needed), but I must admit I'm not a libvirt expert.\n\nAny thoughts ?", 
    "tags": [
        "in-stable-icehouse", 
        "in-stable-juno", 
        "juno-backport-potential", 
        "libvirt"
    ], 
    "importance": "Undecided", 
    "heat": 12, 
    "link": "https://bugs.launchpad.net/nova/+bug/1356552", 
    "owner": "https://api.launchpad.net/1.0/~cyril-roelandt", 
    "id": 1356552, 
    "index": 6342, 
    "openned": "2014-08-13 19:41:14.885693+00:00", 
    "created": "2014-08-13 19:41:14.885693+00:00", 
    "title": "Live migration: 'Disk of instance is too large' when using a volume stored on NFS", 
    "comments": [
        {
            "content": "When live-migrating an instance that has a Cinder volume (stored on NFS) attached, the operation fails if the volume size is bigger than the space left on the destination node. This should not happen, since this volume does not have to be migrated. Here is how to reproduce the bug on a cluster with one control node and two compute nodes, using the NFS backend of Cinder.\n\n\n$ nova boot --flavor m1.tiny --image 173241e-babb-45c7-a35f-b9b62e8ced78 test_vm\n...\n\n$ nova volume-create --display-name test_volume 100\n...\n| id                  | 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 |\n...\n\n\n$ nova volume-attach test_vm  6b9e1d03-3f53-4454-add9-a8c32d82c7e6 auto\n...\n\n$ nova show test_vm | grep OS-EXT-SRV-ATTR:host\n| OS-EXT-SRV-ATTR:host                 | t1-cpunode0                                                |\n\n$ nova service-list | grep nova-compute\n| nova-compute     | t1-cpunode0 | nova     | enabled | up    | 2014-08-13T19:14:40.000000 | -               |\n| nova-compute     | t1-cpunode1 | nova     | enabled | up    | 2014-08-13T19:14:41.000000 | -               |\n\nNow, let's say I want to live-migrate test_vm to t1-cpunode1:\n\n$ nova live-migration --block-migrate test_vm t1-cpunode1\nERROR: Migration pre-check error: Unable to migrate a0d9c991-7931-4710-8684-282b1df4cca6: Disk of instance is too large(available on destination host:46170898432 < need:108447924224) (HTTP 400) (Request-ID: req-b4f00867-df51-44be-8f97-577be385d536)\n\n\nIn nova/virt/libvirt/driver.py, _assert_dest_node_has_enough_disk() calls get_instance_disk_info(), which in turn, calls _get_instance_disk_info(). In this method, we see that volume devices are not taken into account when computing the amount of space needed to migrate an instance:\n\n...\n            if disk_type != 'file':\n                LOG.debug('skipping %s since it looks like volume', path)\n                continue\n\n            if target in volume_devices:\n                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '\n                          'volume', {'path': path, 'target': target})\n                continue\n...\n\nBut for some reason, we never get into these conditions.\n\nIf we ssh the compute where the instance currently lies, we can get more information about it:\n\n$ virsh dumpxml 11\n...\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source file='/var/lib/nova/mnt/84751739e625d0ea9609a65dd9c0a6f1/volume-6b9e1d03-3f53-4454-add9-a8c32d82c7e6'/>\n      <target dev='vdb' bus='virtio'/>\n      <serial>6b9e1d03-3f53-4454-add9-a8c32d82c7e6</serial>\n      <alias name='virtio-disk1'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>\n    </disk>\n...\n\nThe disk type is \"file\", which might explain why this volume is not skipped in the code snippet shown above. When we use the default Cinder backend, we get something such as:\n\n    <disk type='block' device='disk'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source dev='/dev/disk/by-path/ip-192.168.200.250:3260-iscsi-iqn.2010-10.org.openstack:volume-47ecc6a6-8af9-4011-a53f-14a71d14f50b-lun-1'/>\n      <target dev='vdb' bus='virtio'/>\n      <serial>47ecc6a6-8af9-4011-a53f-14a71d14f50b</serial>\n      <alias name='virtio-disk1'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'\nfunction='0x0'/>\n    </disk>\n\n\nI think that the code in LibvirtNFSVolumeDriver.connect_volume() might be wrong: conf.source_type should be set to something else than \"file\" (and some other changes might be needed), but I must admit I'm not a libvirt expert.\n\nAny thoughts ?", 
            "date_created": "2014-08-13 19:41:14.885693+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyril-roelandt"
        }, 
        {
            "content": "Also, I think it's worth noticing that_assert_dest_node_has_enough_disk() calls self.get_instance_disk_info(instance['name']), which means that get_instance_disk_info() has a block_device_info parameter equal to None, and _get_instance_disk_info() as well. In the end, block_device_info_get_mapping() returns an empty list, and volume_devices is an empty set, which explains why we never get in the following condition:\n\n            if target in volume_devices:\n                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '\n                          'volume', {'path': path, 'target': target})\n                continue\n\n\nSo, we either have a problem with the libvirt configuration written by Nova for NFS volumes, or with volume devices not being properly detected when calling _assert_dest_node_has_enough_disk().\n\n", 
            "date_created": "2014-08-14 10:09:07.828370+00:00", 
            "author": "https://api.launchpad.net/1.0/~cyril-roelandt"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/115041", 
            "date_created": "2014-08-18 17:49:16.274983+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/115041\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=671aa9f8b7ca5274696f83bde0d4822ee431b837\nSubmitter: Jenkins\nBranch:    master\n\ncommit 671aa9f8b7ca5274696f83bde0d4822ee431b837\nAuthor: Cyril Roelandt <email address hidden>\nDate:   Mon Aug 18 17:45:35 2014 +0000\n\n    libvirt: Make sure volumes are well detected during block migration\n    \n    Current implementation of live migration in libvirt incorrectly includes\n    block devices on shared storage (e.g., NFS) when computing destination\n    storage requirements. Since these volumes are already on shared storage\n    they do not need to be migrated. As a result, migration fails if the\n    amount of free space on the shared drive is less than the size of the\n    volume to be migrated. The problem is addressed by adding a\n    block_device_info parameter to check_can_live_migrate_source() to allow\n    volumes to be filtered correctly when computing migration space\n    requirements.\n    \n    This only fixes the issue on libvirt: it is unclear whether other\n    implementations suffer from the same issue.\n    \n    Thanks to Florent Flament for spotting and fixing an issue while trying out\n    this patch.\n    \n    Co-Authored-By: Florent Flament <email address hidden>\n    Change-Id: Iac7d2cd2a70800fd89864463ca45c030c47411b0\n    Closes-Bug: #1356552\n", 
            "date_created": "2014-10-15 15:06:29.896723+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Sean Dague (<email address hidden>) on branch: stable/icehouse\nReview: https://review.openstack.org/117631\nReason: This review is > 4 weeks without comment and currently blocked by a core reviewer with a -2. We are abandoning this for now. Feel free to reactivate the review by pressing the restore button and contacting the reviewer with the -2 on this review to ensure you address their concerns.", 
            "date_created": "2015-01-23 15:23:35.898665+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/155863", 
            "date_created": "2015-02-13 18:46:56.373447+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/icehouse\nReview: https://review.openstack.org/156937", 
            "date_created": "2015-02-18 10:48:10.343852+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/155863\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=42cae28241cd0c213201d036bfbe13fb118e4bee\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit 42cae28241cd0c213201d036bfbe13fb118e4bee\nAuthor: Cyril Roelandt <email address hidden>\nDate:   Mon Aug 18 17:45:35 2014 +0000\n\n    libvirt: Make sure volumes are well detected during block migration\n    \n    Current implementation of live migration in libvirt incorrectly includes\n    block devices on shared storage (e.g., NFS) when computing destination\n    storage requirements. Since these volumes are already on shared storage\n    they do not need to be migrated. As a result, migration fails if the\n    amount of free space on the shared drive is less than the size of the\n    volume to be migrated. The problem is addressed by adding a\n    block_device_info parameter to check_can_live_migrate_source() to allow\n    volumes to be filtered correctly when computing migration space\n    requirements.\n    \n    This only fixes the issue on libvirt: it is unclear whether other\n    implementations suffer from the same issue.\n    \n    Thanks to Florent Flament for spotting and fixing an issue while trying out\n    this patch.\n    \n    Co-Authored-By: Florent Flament <email address hidden>\n    Change-Id: Iac7d2cd2a70800fd89864463ca45c030c47411b0\n    Closes-Bug: #1356552\n    (cherry picked from commit 671aa9f8b7ca5274696f83bde0d4822ee431b837)\n", 
            "date_created": "2015-02-27 21:10:19.206320+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/156937\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=f513a282e125075dc43ee05ffcfe0a62c336c059\nSubmitter: Jenkins\nBranch:    stable/icehouse\n\ncommit f513a282e125075dc43ee05ffcfe0a62c336c059\nAuthor: Cyril Roelandt <email address hidden>\nDate:   Mon Aug 18 17:45:35 2014 +0000\n\n    libvirt: Make sure volumes are well detected during block migration\n    \n    Current implementation of live migration in libvirt incorrectly includes\n    block devices on shared storage (e.g., NFS) when computing destination\n    storage requirements. Since these volumes are already on shared storage\n    they do not need to be migrated. As a result, migration fails if the\n    amount of free space on the shared drive is less than the size of the\n    volume to be migrated. The problem is addressed by adding a\n    block_device_info parameter to check_can_live_migrate_source() to allow\n    volumes to be filtered correctly when computing migration space\n    requirements.\n    \n    This only fixes the issue on libvirt: it is unclear whether other\n    implementations suffer from the same issue.\n    \n    Thanks to Florent Flament for spotting and fixing an issue while trying out\n    this patch.\n    \n    (cherry picked from commit 42cae28241cd0c213201d036bfbe13fb118e4bee)\n    \n    Conflicts:\n    \tnova/tests/virt/libvirt/test_libvirt.py\n    \tnova/virt/driver.py\n    \tnova/virt/hyperv/driver.py\n    \tnova/virt/xenapi/driver.py\n    \n    Co-Authored-By: Florent Flament <email address hidden>\n    Closes-Bug: #1356552\n    Change-Id: Iac7d2cd2a70800fd89864463ca45c030c47411b0\n", 
            "date_created": "2015-04-15 11:00:36.799210+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2014-12-18 20:10:34.244762+00:00"
}
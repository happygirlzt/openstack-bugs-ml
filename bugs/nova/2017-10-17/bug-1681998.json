{
    "status": "In Progress", 
    "last_updated": "2017-06-28 12:11:36.349784+00:00", 
    "description": "Sometimes the following dirty BDM enty (1.row) can be seen in the database that multiple BDMs with the same image_id and instance_uuid.\n\nmysql> select * from block_device_mapping where volume_id='153bcab4-1f88-440c-9782-3c661a7502a8' \\G\n*************************** 1. row ***************************\n           created_at: 2017-02-02 02:28:45\n           updated_at: NULL\n           deleted_at: NULL\n                   id: 9754\n          device_name: /dev/vdb\ndelete_on_termination: 0\n          snapshot_id: NULL\n            volume_id: 153bcab4-1f88-440c-9782-3c661a7502a8\n          volume_size: NULL\n            no_device: NULL\n      connection_info: NULL\n        instance_uuid: b52f9264-d8b3-406a-bf9b-d7d7471b13fc\n              deleted: 0\n          source_type: volume\n     destination_type: volume\n         guest_format: NULL\n          device_type: NULL\n             disk_bus: NULL\n           boot_index: NULL\n             image_id: NULL\n*************************** 2. row ***************************\n           created_at: 2017-02-02 02:29:31\n           updated_at: 2017-02-27 10:59:42\n           deleted_at: NULL\n                   id: 9757\n          device_name: /dev/vdc\ndelete_on_termination: 0\n          snapshot_id: NULL\n            volume_id: 153bcab4-1f88-440c-9782-3c661a7502a8\n          volume_size: NULL\n            no_device: NULL\n      connection_info: {\"driver_volume_type\": \"rbd\", \"serial\": \"153bcab4-1f88-440c-9782-3c661a7502a8\", \"data\": {\"secret_type\": \"ceph\", \"name\": \"cinder-ceph/volume-153bcab4-1f88-440c-9782-3c661a7502a8\", \"secret_uuid\": null, \"qos_specs\": null, \"hosts\": [\"10.7.1.202\", \"10.7.1.203\", \"10.7.1.204\"], \"auth_enabled\": true, \"access_mode\": \"rw\", \"auth_username\": \"cinder-ceph\", \"ports\": [\"6789\", \"6789\", \"6789\"]}}\n        instance_uuid: b52f9264-d8b3-406a-bf9b-d7d7471b13fc\n              deleted: 0\n          source_type: volume\n     destination_type: volume\n         guest_format: NULL\n          device_type: disk\n             disk_bus: virtio\n           boot_index: NULL\n             image_id: NULL\n\nthen it cause we fail to detach the volume and see the following error since connection_info of row 1 is NULL.\n\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher self._detach_volume(context, instance, bdm)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 4801, in _detach_volume\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher connection_info = jsonutils.loads(bdm.connection_info)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/oslo_serialization/jsonutils.py\", line 215, in loads\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher return json.loads(encodeutils.safe_decode(s, encoding), **kwargs)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/oslo_utils/encodeutils.py\", line 33, in safe_decode\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher raise TypeError(\"%s can't be decoded\" % type(text))\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded\n\nThis kind of dirty data can be produced when happened to fail to run this line _attach_volume()#volume_bdm.destroy() [1], I think these conditions may cause it to happen:\n1, lose the database during the operation volume_bdm.destroy()\n2, lose an MQ connection or RPC timing out during the operation volume_bdm.destroy()\n\nIf you lose the database during any operation, things are going to be bad, so in general I'm not sure how realistic guarding for that case is. Losing an MQ connection or RPC timing out is probably more realistic. Seems the fix [2] is trying to solve the point 2.\n\nHowever, I'm thinking if we can bypass the dirty BDM entry according to the condition that connection_info is NULL no matter how it is produced.\n\n\n[1] https://github.com/openstack/nova/blob/master/nova/compute/api.py#L3724\n[2] https://review.openstack.org/#/c/290793", 
    "tags": [], 
    "importance": "Medium", 
    "heat": 8, 
    "link": "https://bugs.launchpad.net/nova/+bug/1681998", 
    "owner": "None", 
    "id": 1681998, 
    "index": 4796, 
    "openned": "2017-04-12 03:50:33.824333+00:00", 
    "created": "2017-04-12 03:50:33.824333+00:00", 
    "title": "Bypass the dirty BDM enty no matter how it is produced", 
    "comments": [
        {
            "content": "Sometimes the following dirty BDM enty (1.row) can be seen in the database that multiple BDMs with the same image_id and instance_uuid.\n\nmysql> select * from block_device_mapping where volume_id='153bcab4-1f88-440c-9782-3c661a7502a8' \\G\n*************************** 1. row ***************************\n           created_at: 2017-02-02 02:28:45\n           updated_at: NULL\n           deleted_at: NULL\n                   id: 9754\n          device_name: /dev/vdb\ndelete_on_termination: 0\n          snapshot_id: NULL\n            volume_id: 153bcab4-1f88-440c-9782-3c661a7502a8\n          volume_size: NULL\n            no_device: NULL\n      connection_info: NULL\n        instance_uuid: b52f9264-d8b3-406a-bf9b-d7d7471b13fc\n              deleted: 0\n          source_type: volume\n     destination_type: volume\n         guest_format: NULL\n          device_type: NULL\n             disk_bus: NULL\n           boot_index: NULL\n             image_id: NULL\n*************************** 2. row ***************************\n           created_at: 2017-02-02 02:29:31\n           updated_at: 2017-02-27 10:59:42\n           deleted_at: NULL\n                   id: 9757\n          device_name: /dev/vdc\ndelete_on_termination: 0\n          snapshot_id: NULL\n            volume_id: 153bcab4-1f88-440c-9782-3c661a7502a8\n          volume_size: NULL\n            no_device: NULL\n      connection_info: {\"driver_volume_type\": \"rbd\", \"serial\": \"153bcab4-1f88-440c-9782-3c661a7502a8\", \"data\": {\"secret_type\": \"ceph\", \"name\": \"cinder-ceph/volume-153bcab4-1f88-440c-9782-3c661a7502a8\", \"secret_uuid\": null, \"qos_specs\": null, \"hosts\": [\"10.7.1.202\", \"10.7.1.203\", \"10.7.1.204\"], \"auth_enabled\": true, \"access_mode\": \"rw\", \"auth_username\": \"cinder-ceph\", \"ports\": [\"6789\", \"6789\", \"6789\"]}}\n        instance_uuid: b52f9264-d8b3-406a-bf9b-d7d7471b13fc\n              deleted: 0\n          source_type: volume\n     destination_type: volume\n         guest_format: NULL\n          device_type: disk\n             disk_bus: virtio\n           boot_index: NULL\n             image_id: NULL\n\nthen it cause we fail to detach the volume and see the following error since connection_info of row 1 is NULL.\n\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher self._detach_volume(context, instance, bdm)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/nova/compute/manager.py\", line 4801, in _detach_volume\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher connection_info = jsonutils.loads(bdm.connection_info)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/oslo_serialization/jsonutils.py\", line 215, in loads\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher return json.loads(encodeutils.safe_decode(s, encoding), **kwargs)\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher File \"/usr/lib/python2.7/dist-packages/oslo_utils/encodeutils.py\", line 33, in safe_decode\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher raise TypeError(\"%s can't be decoded\" % type(text))\n2017-03-23 13:28:05.360 1865733 TRACE oslo_messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded\n\nThis kind of dirty data can be produced when happened to fail to run this line _attach_volume()#volume_bdm.destroy() [1], I think these conditions may cause it to happen:\n1, lose the database during the operation volume_bdm.destroy()\n2, lose an MQ connection or RPC timing out during the operation volume_bdm.destroy()\n\nIf you lose the database during any operation, things are going to be bad, so in general I'm not sure how realistic guarding for that case is. Losing an MQ connection or RPC timing out is probably more realistic. Seems the fix [2] is trying to solve the point 2.\n\nHowever, I'm thinking if we can bypass the dirty BDM entry according to the condition that connection_info is NULL no matter how it is produced.\n\n\n[1] https://github.com/openstack/nova/blob/master/nova/compute/api.py#L3724\n[2] https://review.openstack.org/#/c/290793", 
            "date_created": "2017-04-12 03:50:33.824333+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhhuabj"
        }, 
        {
            "content": "Yeah we did attempt to fix this with [2] but couldn't find a reasonable way to handle >1 bdm with the same instance_uuid and volume_id. \n\nI don't think connection_info being NULL is the correct way to avoid this as all newly created BDMs would meet this criteria, making it impossible for us to find the BDM later when calling intialize_connection etc.\n\nShould we mark this as a duplicate of bug#1427060 and continue there?", 
            "date_created": "2017-04-12 14:00:03.361202+00:00", 
            "author": "https://api.launchpad.net/1.0/~lyarwood"
        }, 
        {
            "content": "Hi Lee, \n\nThank you for your response. To illustrate my intentions to bypass the dirty BDM entry according to the condition that connection_info is NULL, I drafted a patch below just now:\n\ndiff --git a/nova/compute/manager.py b/nova/compute/manager.py\nindex d6efd18..efbb225 100644\n--- a/nova/compute/manager.py\n+++ b/nova/compute/manager.py\n@@ -4937,10 +4937,10 @@ class ComputeManager(manager.Manager):\n                                                     wr_req, wr_bytes,\n                                                     instance,\n                                                     update_totals=True)\n-\n-        self._driver_detach_volume(context, instance, bdm)\n-        connector = self.driver.get_volume_connector(instance)\n-        self.volume_api.terminate_connection(context, volume_id, connector)\n+        if bdm.connection_info:\n+            self._driver_detach_volume(context, instance, bdm)\n+            connector = self.driver.get_volume_connector(instance)\n+            self.volume_api.terminate_connection(context, volume_id, connector)\n\n         if destroy_bdm:\n             bdm.destroy()\n\nWhen bdm.connection_info is NULL (means it's dirty data), we will bypass _driver_detach_volume() and terminate_connection() and just invoke bdm.destroy() to delete this dirty data. So:\n\n1, If there are N (N > 1) BDMs with the same instance_uuid and volume_id, we can run 'nova detach' operation N times to delete N dirty BDMs.\n\n2, Because above logic is just added in detach operation, so it will not affect all newly created BDMs would meet this criteria.\n\nIf you guys think I'm on the right road, I can spend time to test above patch and continue to submit the code review. Hope to hear from you. thanks.", 
            "date_created": "2017-04-13 00:20:17.426217+00:00", 
            "author": "https://api.launchpad.net/1.0/~zhhuabj"
        }
    ]
}
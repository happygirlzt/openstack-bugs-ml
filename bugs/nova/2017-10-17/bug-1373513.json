{
    "status": "Invalid", 
    "last_updated": "2015-04-30 09:54:01.387286+00:00", 
    "description": "Managed to trigger a hang in lvm create\n\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.704164] INFO: task lvm:14805 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.705096]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.705839] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706871] lvm             D ffff8801ffd14440     0 14805  14804 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706876]  ffff880068f9dae0 0000000000000082 ffff8801a14bc800 ffff880068f9dfd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706879]  0000000000014440 0000000000014440 ffff8801a14bc800 ffff8801ffd14cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706881]  0000000000000000 ffff88004063c280 0000000000000000 ffff8801a14bc800\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706883] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706895]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706914]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706918]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706920]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706922]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706924]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706926]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706937]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706947]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706953]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706955]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706958]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706960]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706962]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706966]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706968] INFO: task lvs:14822 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.707774]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.708507] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709535] lvs             D ffff8801ffc14440     0 14822  14821 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709537]  ffff880009ffdae0 0000000000000082 ffff8800095e1800 ffff880009ffdfd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709539]  0000000000014440 0000000000014440 ffff8800095e1800 ffff8801ffc14cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709541]  0000000000000000 ffff880003d59900 0000000000000000 ffff8800095e1800\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709543] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709547]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709549]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709551]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709554]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709555]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709557]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709559]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709562]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709564]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709566]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709568]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709570]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709572]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709573]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709575]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709578] INFO: task lvs:14984 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.710411]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.711153] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712193] lvs             D ffff8801ffc94440     0 14984  14983 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712196]  ffff880101711ae0 0000000000000086 ffff880007d2b000 ffff880101711fd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712198]  0000000000014440 0000000000014440 ffff880007d2b000 ffff8801ffc94cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712200]  0000000000000000 ffff880017fd2300 0000000000000000 ffff880007d2b000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712202] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712219]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712221]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712224]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712226]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712228]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712230]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712231]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712234]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712236]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712238]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712249]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712256]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712264]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712270]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712276]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712278] INFO: task vgs:15086 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.713149]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.713870] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714919] vgs             D ffff8801ffc94440     0 15086  15061 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  ffff88014411fae0 0000000000000082 ffff8801f2f7e000 ffff88014411ffd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  0000000000014440 0000000000014440 ffff8801f2f7e000 ffff8801ffc94cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  0000000000000000 ffff880017fd2580 0000000000000000 ffff8801f2f7e000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714923]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714924]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714926]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714928]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714930]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714932]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714934]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714936]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714938]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714940]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714942]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714944]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\n\nLink to logs: http://logs.openstack.org/22/119522/9/check/check-tempest-dsvm-postgres-full/46a5f8c/logs/syslog.txt.gz#_Sep_22_21_19_01", 
    "tags": [
        "icehouse-backport-potential", 
        "juno-backport-potential"
    ], 
    "importance": "Critical", 
    "heat": 54, 
    "link": "https://bugs.launchpad.net/nova/+bug/1373513", 
    "owner": "None", 
    "id": 1373513, 
    "index": 212, 
    "openned": "2014-09-24 17:00:05.806191+00:00", 
    "created": "2014-09-24 17:00:05.806191+00:00", 
    "title": "Lvm hang during tempest tests", 
    "comments": [
        {
            "content": "Managed to trigger a hang in lvm create\n\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.704164] INFO: task lvm:14805 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.705096]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.705839] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706871] lvm             D ffff8801ffd14440     0 14805  14804 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706876]  ffff880068f9dae0 0000000000000082 ffff8801a14bc800 ffff880068f9dfd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706879]  0000000000014440 0000000000014440 ffff8801a14bc800 ffff8801ffd14cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706881]  0000000000000000 ffff88004063c280 0000000000000000 ffff8801a14bc800\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706883] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706895]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706914]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706918]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706920]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706922]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706924]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706926]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706937]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706947]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706953]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706955]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706958]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706960]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706962]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706966]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.706968] INFO: task lvs:14822 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.707774]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.708507] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709535] lvs             D ffff8801ffc14440     0 14822  14821 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709537]  ffff880009ffdae0 0000000000000082 ffff8800095e1800 ffff880009ffdfd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709539]  0000000000014440 0000000000014440 ffff8800095e1800 ffff8801ffc14cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709541]  0000000000000000 ffff880003d59900 0000000000000000 ffff8800095e1800\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709543] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709547]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709549]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709551]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709554]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709555]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709557]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709559]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709562]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709564]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709566]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709568]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709570]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709572]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709573]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709575]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.709578] INFO: task lvs:14984 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.710411]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.711153] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712193] lvs             D ffff8801ffc94440     0 14984  14983 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712196]  ffff880101711ae0 0000000000000086 ffff880007d2b000 ffff880101711fd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712198]  0000000000014440 0000000000014440 ffff880007d2b000 ffff8801ffc94cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712200]  0000000000000000 ffff880017fd2300 0000000000000000 ffff880007d2b000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712202] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712219]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712221]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712224]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712226]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712228]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712230]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712231]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712234]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712236]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712238]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712249]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712256]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712264]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712270]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712276]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.712278] INFO: task vgs:15086 blocked for more than 120 seconds.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.713149]       Not tainted 3.13.0-35-generic #62-Ubuntu\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.713870] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714919] vgs             D ffff8801ffc94440     0 15086  15061 0x00000000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  ffff88014411fae0 0000000000000082 ffff8801f2f7e000 ffff88014411ffd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  0000000000014440 0000000000014440 ffff8801f2f7e000 ffff8801ffc94cd8\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  0000000000000000 ffff880017fd2580 0000000000000000 ffff8801f2f7e000\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921] Call Trace:\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff81722a6d>] io_schedule+0x9d/0x140\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff811fac94>] do_blockdev_direct_IO+0x1ce4/0x2910\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714921]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714923]  [<ffffffff811fb915>] __blockdev_direct_IO+0x55/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714924]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714926]  [<ffffffff811f61f6>] blkdev_direct_IO+0x56/0x60\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714928]  [<ffffffff811f5b00>] ? I_BDEV+0x10/0x10\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714930]  [<ffffffff8115106b>] generic_file_aio_read+0x69b/0x700\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714932]  [<ffffffff811cca78>] ? path_openat+0x158/0x640\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714934]  [<ffffffff810f3c92>] ? from_kgid_munged+0x12/0x20\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714936]  [<ffffffff811f667b>] blkdev_aio_read+0x4b/0x70\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714938]  [<ffffffff811bc99a>] do_sync_read+0x5a/0x90\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714940]  [<ffffffff811bd035>] vfs_read+0x95/0x160\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714942]  [<ffffffff811bdb49>] SyS_read+0x49/0xa0\nSep 22 21:19:01 devstack-trusty-hpcloud-b5-2344254 kernel: [ 3120.714944]  [<ffffffff8172ed6d>] system_call_fastpath+0x1a/0x1f\n\nLink to logs: http://logs.openstack.org/22/119522/9/check/check-tempest-dsvm-postgres-full/46a5f8c/logs/syslog.txt.gz#_Sep_22_21_19_01", 
            "date_created": "2014-09-24 17:00:05.806191+00:00", 
            "author": "https://api.launchpad.net/1.0/~vishvananda"
        }, 
        {
            "content": "I wonder if this is a result of something happening in terms of overall system load and perhaps flushing to disk.  Experimenting with some ideas like changing the IO scheduler might be worth a shot.\n\nAlso Iv'e been considering a proposal to attach a Cinder Volume to our OpenStack instance and use that as our VG in gating rather than the loopback file.  Don't have any data to say this is better or worse, but an idea I've kicked around for a bit now.", 
            "date_created": "2014-09-25 01:15:31.634544+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "This is a related diagnostics patch that I need to update to make rootwrap work:\n\nhttps://review.openstack.org/#/c/126735/", 
            "date_created": "2014-10-13 16:45:18.336936+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Looking at a logstash query like this:\n\nmessage:\"Failed to delete resource\" AND message:\"within the required time\" AND tags:\"console\"\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRmFpbGVkIHRvIGRlbGV0ZSByZXNvdXJjZVwiIEFORCBtZXNzYWdlOlwid2l0aGluIHRoZSByZXF1aXJlZCB0aW1lXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTMyMTkyOTUxMDgsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=\n\nThat's coming from mostly volume-related tests, so I think they are all related to the same thing, something is doing a blocking (on i/o?) call in cinder on the delete flow (or something using the same lock that gets into the delete flow, like a volume update periodic task maybe?) and we timeout.\n\nIt would be nice if that Tempest error message gave the type of resource in the error message rather than just 'failed to delete resource'.  Then we could filter a bit better on the error message.", 
            "date_created": "2014-10-13 16:58:38.492111+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Opened tempest bug 1380712 to make the error message more specific so we can get an elastic-recheck query going for this.", 
            "date_created": "2014-10-13 17:01:42.047514+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "This could be related, libvirt domain delete/destroy fails because the backing volume is busy:\n\nhttp://logs.openstack.org/20/128020/2/check/check-tempest-dsvm-full/6b56bf1/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-10-13_18_12_06_653\n\n2014-10-13 18:12:06.653 ERROR nova.virt.libvirt.driver [req-017c0b4b-11ba-4f59-9895-3d0053481b00 VolumesV2ActionsTestXML-1527860598 VolumesV2ActionsTestXML-196352251] [instance: 6f733615-3124-400e-b890-b07468fa4777] Error from libvirt during destroy. Code=38 Error=Failed to terminate process 16365 with SIGKILL: Device or resource busy\n2014-10-13 18:12:08.000 ERROR nova.compute.manager [req-017c0b4b-11ba-4f59-9895-3d0053481b00 VolumesV2ActionsTestXML-1527860598 VolumesV2ActionsTestXML-196352251] [instance: 6f733615-3124-400e-b890-b07468fa4777] Setting instance vm_state to ERROR\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777] Traceback (most recent call last):\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2500, in do_terminate_instance\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     self._delete_instance(context, instance, bdms, quotas)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/hooks.py\", line 131, in inner\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     rv = f(*args, **kwargs)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2469, in _delete_instance\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     quotas.rollback()\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/oslo/utils/excutils.py\", line 82, in __exit__\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     six.reraise(self.type_, self.value, self.tb)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2446, in _delete_instance\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     self._shutdown_instance(context, instance, bdms)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2376, in _shutdown_instance\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     requested_networks)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/oslo/utils/excutils.py\", line 82, in __exit__\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     six.reraise(self.type_, self.value, self.tb)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/compute/manager.py\", line 2365, in _shutdown_instance\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     block_device_info)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1054, in destroy\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     self._destroy(instance)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 1010, in _destroy\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     instance=instance)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/oslo/utils/excutils.py\", line 82, in __exit__\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     six.reraise(self.type_, self.value, self.tb)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/opt/stack/new/nova/nova/virt/libvirt/driver.py\", line 979, in _destroy\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     virt_dom.destroy()\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 183, in doit\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 141, in proxy_call\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     rv = execute(f, *args, **kwargs)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 122, in execute\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     six.reraise(c, e, tb)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py\", line 80, in tworker\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     rv = meth(*args, **kwargs)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]   File \"/usr/lib/python2.7/dist-packages/libvirt.py\", line 918, in destroy\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777]     if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777] libvirtError: Failed to terminate process 16365 with SIGKILL: Device or resource busy\n2014-10-13 18:12:08.000 17350 TRACE nova.compute.manager [instance: 6f733615-3124-400e-b890-b07468fa4777] ", 
            "date_created": "2014-10-13 18:46:21.326526+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "The n-cpu error in comment 5 could be a separate issue, I'm not seeing in vishy's original build fail link:\n\nhttp://logs.openstack.org/22/119522/9/check/check-tempest-dsvm-postgres-full/46a5f8c/logs/screen-n-cpu.txt.gz?level=TRACE", 
            "date_created": "2014-10-13 18:48:05.558687+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Running a query on this:\n\nmessage:\"INFO: task lvm:\" AND message:\"blocked for more than 120 seconds\" AND tags:\"syslog\"\n\nThere are 111 hits in 7 days, check and gate, not all are failures though.  That's not surprising though, we've seen some of these fail within a window of 2 seconds (the tempest volume delete timeout is 196 seconds).\n\nWhen we see this though, it's in a failed job 98% of the time, so this is a good query.", 
            "date_created": "2014-10-13 19:15:53.973119+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "e-r query: https://review.openstack.org/#/c/128053/", 
            "date_created": "2014-10-13 19:19:32.372577+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "According to my investigation this is related to a SYNCHRONIZE_CACHE command being issued to the Cinder iSCSI-over-LVM driver. I'm working on a similar case with qemu hang-up.", 
            "date_created": "2014-10-14 21:35:25.933972+00:00", 
            "author": "https://api.launchpad.net/1.0/~pboldin"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/128482", 
            "date_created": "2014-10-14 23:43:27.934259+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Tempest bug 1380712 is fixed so now we'll get more specific timeout failures in Tempest, e.g.:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRmFpbGVkIHRvIGRlbGV0ZVwiIEFORCBtZXNzYWdlOlwid2l0aGluIHRoZSByZXF1aXJlZCB0aW1lXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjE0NDAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQxMzQwNTgzNDQ1NX0=\n\nLooks like they are mostly coming up volumes so far, as expected.", 
            "date_created": "2014-10-15 20:46:08.788977+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/128962", 
            "date_created": "2014-10-16 15:41:16.979425+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/128962\nCommitted: https://git.openstack.org/cgit/openstack/cinder/commit/?id=e3feb0297dfece1fc198b231d9aa39ae2920d877\nSubmitter: Jenkins\nBranch:    master\n\ncommit e3feb0297dfece1fc198b231d9aa39ae2920d877\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Oct 16 08:39:07 2014 -0700\n\n    Log a warning when getting lvs and vgs takes longer than 60 seconds\n    \n    We know something is causing lvs/vgs commands to block while deleting a\n    volume and this is causing Tempest to timeout while waiting for the\n    volume to be deleted.  What we don't have right now is very good\n    (specific) logging when this happens, unless we get messages in syslog\n    for lvm tasks taking more than 120 seconds, but that doesn't always\n    happen when we see the volume delete timeout in Tempest.\n    \n    This patch adds a check for when getting logical volumes and volume\n    groups takes longer than 60 seconds and logs a warning if that happens.\n    This is helpful in production also because the default interval for\n    periodic tasks is 60 seconds so having these take longer than that time\n    could cause periodic tasks to block up on each other and you'll get\n    warnings from the FixedIntervalLoopingCall in oslo which is controlling\n    the task runs.\n    \n    Related-Bug: #1373513\n    \n    Change-Id: I7ac5ddbbd9bda7744db33f0bb8d56879301f5538\n", 
            "date_created": "2014-10-17 15:57:55.726214+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "https://review.openstack.org/#/c/128962/ is merged but the logstash results aren't very helpful:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVG9va1wiIEFORCAobWVzc2FnZTpcInNlY29uZHMgdG8gZ2V0IGxvZ2ljYWwgdm9sdW1lc1wiIE9SIG1lc3NhZ2U6XCJzZWNvbmRzIHRvIGdldCB2b2x1bWUgZ3JvdXBzXCIpIEFORCB0YWdzOlwic2NyZWVuLWMtdm9sLnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI0MzIwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTM1NzEwMjMyNTIsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=\n\nThere are 1432 hits in the first 4 hours since it merged, but 99% of those jobs were successful.  If you filter by build_status:\"FAILURE\" there are 38 hits and the range of times vary quit a bit, e.g.:\n\n\"Took 67.2061200142 seconds to get volume groups.\"\n\n\"Took 117.149465799 seconds to get volume groups.\"\n\n--\n\n\"Took 63.6563539505 seconds to get logical volumes.\"\n\n\"Took 203.954236984 seconds to get logical volumes.\"", 
            "date_created": "2014-10-17 18:39:38.680974+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/126735\nReason: From the results of https://review.openstack.org/#/c/128962/ in the gate, we apparently see lvs/vgs take a long time in a high number of successful runs, so there must be something else going on, so I'm going to abandon this.", 
            "date_created": "2014-10-22 17:58:20.088024+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "I made this patch to try to troubleshoot this: https://review.openstack.org/#/c/130364/\n\nIgnoring the errors because I didn't use root_helper, there is a pattern here:\nhttp://logs.openstack.org/64/130364/1/check/check-tempest-dsvm-full/caf173e/logs/screen-c-vol.txt.gz#_2014-10-22_21_48_07_126\n\nThese locks seem to mostly point to instances of \"lvs\", \"vgs\", and \"lvcreate\" commands blocking against each other.  These all lock LVM VG metadata while running.\n\nPart of the length of time is these block indicates that something is probably wrong and needs further investigation.  (I suspect they are maybe scanning some dead devices, etc.)\n\nBut, this leads me back to a previous suggestion: it's possible that enabling lvmetad on these gate machines could alleviate this issue.  It caches some of the information collected during scans and so may help avoid this by having fewer long-running commands.", 
            "date_created": "2014-10-22 22:23:28.258378+00:00", 
            "author": "https://api.launchpad.net/1.0/~eharney"
        }, 
        {
            "content": "lvmetad experiment: https://review.openstack.org/130386", 
            "date_created": "2014-10-22 23:02:21.550439+00:00", 
            "author": "https://api.launchpad.net/1.0/~eharney"
        }, 
        {
            "content": "Related fix proposed to branch: master\nReview: https://review.openstack.org/130625", 
            "date_created": "2014-10-23 19:28:31.589171+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/130625\nCommitted: https://git.openstack.org/cgit/openstack/cinder/commit/?id=dcda67053df5dc0240c743537dc1b9c4a3231b61\nSubmitter: Jenkins\nBranch:    master\n\ncommit dcda67053df5dc0240c743537dc1b9c4a3231b61\nAuthor: Eric Harney <email address hidden>\nDate:   Thu Oct 23 11:28:29 2014 -0400\n\n    Brick LVM: Optimize get_volume\n    \n    When we only want the LV information for a single volume, call\n    \"lvs\" with the volume name specified.\n    \n    This should reduce unnecessary load of calling \"lvs\" for the\n    whole VG.\n    \n    Related-Bug: #1373513\n    \n    Change-Id: Ifbb679e9160f44fe52ecba7bf19aa0eb5bb133f1\n", 
            "date_created": "2014-10-28 23:06:08.934822+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Notice for potential backports: \"Brick LVM: Optimize get_volume\" caused regression bug 1390081", 
            "date_created": "2014-11-21 16:51:30.431318+00:00", 
            "author": "https://api.launchpad.net/1.0/~apevec"
        }, 
        {
            "content": "Here are some more logs, I'm not seeing any errors though.\n\nVolume delete starts: \n\nhttp://logs.openstack.org/90/131490/4/gate/gate-grenade-dsvm-partial-ncpu/0537e3b/logs/old/screen-c-vol.txt.gz#_2014-11-25_03_41_22_466\n\nNext time we see the volume uuid logged (3 min later):\n\nhttp://logs.openstack.org/90/131490/4/gate/gate-grenade-dsvm-partial-ncpu/0537e3b/logs/old/screen-c-vol.txt.gz#_2014-11-25_03_44_41_032\n\nThen tempest times out waiting on the volume to be deleted in that same run:\n\nhttp://logs.openstack.org/90/131490/4/gate/gate-grenade-dsvm-partial-ncpu/0537e3b/logs/grenade.sh.txt.gz#_2014-11-25_03_44_48_217\n\ne-r query:\n\nhttp://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiKFRlc3RWb2x1bWVCb290UGF0dGVybjpfcnVuX2NsZWFudXBzKSBGYWlsZWQgdG8gZGVsZXRlIHZvbHVtZVwiIEFORCBtZXNzYWdlOlwid2l0aGluIHRoZSByZXF1aXJlZCB0aW1lXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MTY5MjYwMDY1NTZ9\n\n94 hits in 7 days, check and gate, all failures.", 
            "date_created": "2014-11-25 14:45:24.135242+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Change abandoned by Mike Perez (<email address hidden>) on branch: master\nReview: https://review.openstack.org/128482", 
            "date_created": "2014-12-02 15:01:01.494500+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "This doesn't seem to be showing up in the query as of late.  Will give it a bit longer but wondering if something updated that fixed this.", 
            "date_created": "2015-01-07 20:29:55.895689+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Finally had a chance to look at this more closely since I can easily reproduce it with Rally.  It turns out that the issue is we're running vgscan on all disks on the system.  What happens though is that some of those disks are a result of a cinder create / attach, and we hit issues where the attachment is removed and deleted during the vgscan causing the scan operation to hang and if we're lucky timeout.\n\nWe can fix this by modifying lvm.conf to ONLY query devices that we specify as being used by cinders VG, this has risks though incase somebody wants to use the system for other purposes.  Turns out though you can create your own lvm.conf and set it via an env variable.\n\nRunning a number of tests iterations with Rally (which produces the original issue every time), this works and cleans up the issue.  I'll just need to figure out the best way to deal with env vars in Cinder commands.", 
            "date_created": "2015-01-19 23:06:22.299721+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/148393", 
            "date_created": "2015-01-20 00:59:00.591655+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by John Griffith (<email address hidden>) on branch: master\nReview: https://review.openstack.org/148393\nReason: reworking a better approach", 
            "date_created": "2015-01-20 23:31:22.402425+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/148747", 
            "date_created": "2015-01-20 23:36:26.628007+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/148747\nCommitted: https://git.openstack.org/cgit/openstack/cinder/commit/?id=eb7bb3e08c1b8e1008ba447b842b635821e2097e\nSubmitter: Jenkins\nBranch:    master\n\ncommit eb7bb3e08c1b8e1008ba447b842b635821e2097e\nAuthor: John Griffith <email address hidden>\nDate:   Tue Jan 20 16:31:57 2015 -0700\n\n    Enable use of an /etc/cinder/lvm.conf file\n    \n    During tempest and Rally runs we've noticed occasional\n    LVM command hangs (lvs, vgs and pvs), we've also gathered\n    enough data to show that we see very slow response times from\n    these commands almost all of the time.\n    \n    It turns out that this seems to be an issue with us scanning\n    all devices during LVM operations, including devices that may\n    be Cinder Volumes that are attaching and detaching from the system.\n    \n    Inspecting a run instrumented with strace shows a number of LVM\n    commands timing out due to the device being scanned being removed\n    during scan, and the LVM command in turn waiting until it times out\n    on the scan that's in process.\n    \n    This patch just adds the ability to setup a lvm.conf file in\n    /etc/cinder.  The Cinder LVM code will now specifically set\n    the LVM_SYSTEM_DIR environment variable to that directory for\n    each of the LVM scan commands in brick/local_dev/lvm.py.\n    If the system doesn't have the file, we use the empty string\n    which tells LVM to use it's defaults.  This only affects LVM\n    commands in Cinder, the idea is to ensure we don't impact any\n    other LVM operations on the node outside of Cinder and that we\n    behave as we always have in the case of no lvm.conf file being\n    setup in /etc/cinder.  The presence of the file is auto-detected\n    on brick/localdev/lvm init.\n    \n    We'll update the OpenStack Devstack deployment scripts to put this\n    together and fix things up there first. Until that's done and until\n    we auto-generate the conf (or document it well), this will be a\n    *partial* bugfix.\n    \n    I considered adding a default lvm.conf file to cinder/etc/<sample>\n    that would be copied in on install, but decided against this to\n    avoid any possible issues with compatability issues between\n    platforms or versions.\n    \n    To use, just copy the /etc/lvm/lvm.conf file to /etc/cinder and\n    modify the filter as appropriate, for example:\n      To use loopback device only:\n        filter = [ \"a/loop/\", \"r/.*/\" ]\n      If you have a physical drive like /dev/sdb1\n        filter = [ \"a/dev/sdb1/\", \"r/.*/\" ]\n    \n    Finally, this patch also goes through and cleans up our cmd\n    variables in brick/localdev/lvm.  We had a mix of using a\n    cmd array, and strings; this causes inconsistencies and makes\n    it difficult to extend or modify commands.  Switch everything to\n    using an array and use extend to provide the correct prefix.\n    \n    Need to update docs to include a recommendation to create an\n    /etc/cinder/lvm.conf file and set device filters appropriately.\n    Doc-Impact\n    Partial-Bug: #1373513\n    \n    Change-Id: Ia2289197a6d7fcfc910ee3de48e0a2173881a1e6\n", 
            "date_created": "2015-02-06 04:52:13.570023+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Made some progress with the LVM.filter,  we're still seeing the LVM calls that take greater than 60 seconds in Kibana fairly often.  Taking a closer look at those however it seems that we're typically only seeing the \"too long\" on one call in each run, and that's always one of the lvs calls (I think anyway).\n\nI also haven't seen a full on hang in a couple weeks so that's good, even though this is still not good; we're not causing failures in the gate like we were with it.  I need to take a look at updating the ERQ so we don't come up with this bug for other failures but maybe we can actually get this nailed down.\n\nOne interesting observation is that in a bunch of these I looked at today the only entry for an LVM command being > 60 seconds was an LVS as mentioned, and it looks like there may be a pattern in the sequence when it happens.  \n\nMarking the bug as in progress again", 
            "date_created": "2015-02-14 15:18:03.239741+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/155981", 
            "date_created": "2015-02-14 15:48:45.671914+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Spent some time digging through logs, it seems that there's one specific case where we hit this long running LVS and VGS command.  It doesn't appear that the deactivate conf change has any relevance here at all.\n\nMost of the logs I've looked at from the last few days have only two long running commands (over 60 seconds); one for an lvs and one for a vgs right after that.  In addition, if you look at timings for the other calls they're all coming in around half a second.\n\nLooks like there's something we're doing in the test process, or maybe it's an interaction with another LVM related command that's causing the remaining issues.\n\nAlso notice that we don't seem to be hitting Test failures for this any longer since the lvm.conf change landed.  ERQ is still associating failures due to this being in the logs for almost every run, but the root failure on most of these seems to be unrelated to this issue.", 
            "date_created": "2015-02-16 01:02:19.154058+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "One more update...  Tried instrumenting this locally with strace, but unfortunately when I do that and the issue is encountered I loose network connectivity and everything just barfs.\n\nBUT, what i have noticed is ti's always one test that seems to have the issue:\ntempest.scenario.test_encrypted_cinder_volumes.TestEncryptedCinderVolumes.test_encrypted_cinder_volumes_luks\n\nLooking closer at that now.", 
            "date_created": "2015-02-16 17:35:14.771928+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Change abandoned by John Griffith (<email address hidden>) on branch: master\nReview: https://review.openstack.org/155981\nReason: This doesn't seem to have any impact at all, I would say that we should pick either using the conf designator like Dirk does here or the /etc/cinder/lvm.conf file and make things consistent.\n\nUsing the designator here is probably more correct than what I did with the Conf file, only catch is I couldn't get the designator to work, I'll try it again after I get this hang sorted out.", 
            "date_created": "2015-02-16 17:42:56.295251+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Caught an strace by calling \"strace lvs\" external when the system was hung up: \nhttps://gist.github.com/j-griffith/5448723bada6de44de6c", 
            "date_created": "2015-02-16 19:07:00.816857+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "I've added Nova to the projects here because currently we're at a stale mate where there seems to be a single case during unrescue that tirggers this.  Patc is proposed but looks like it won't be accepted, want to make sure we link this and keep it tracked although it is different than the original bug posted by Vish, I believe it's related.", 
            "date_created": "2015-03-10 15:24:13.149146+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Related bug with more detail: https://bugs.launchpad.net/nova/+bug/1423654", 
            "date_created": "2015-03-10 15:25:05.480776+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Change abandoned by John Griffith (<email address hidden>) on branch: master\nReview: https://review.openstack.org/159713\nReason: I give up, no hard feelings but honestly this is silly.", 
            "date_created": "2015-03-10 21:01:40.758778+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "As I remember the lvm2-metad agent is not enabled on Ubuntu by default,\nI wonder is it it able to help , can it be enabled on devstack side ?", 
            "date_created": "2015-03-13 10:21:29.964837+00:00", 
            "author": "https://api.launchpad.net/1.0/~afazekas"
        }, 
        {
            "content": "Seeing this everywhere now, even without the migration test.  Working on a few things around session disconnects, still need to look at what Eric has posted.  metad might be an interesting thing to investigate too.\n\nNote, you can simply run \"tox -eall -- volume\" and hit this error pretty much 100% of the time.  Search your c-vol logs for \"returned: 0 in 1\" and you'll see several LVM commands that take greater than 100 seconds.", 
            "date_created": "2015-03-14 20:35:42.618215+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "FWIW, doing some deeper looking here:\n1. skipping the detach during rescue negative reduces the occurences, but doesn't eliminate them.  End up with two instances of long running/hung lvm commands\n\n2. skipping the tempest/api/compute/servers/test_server_rescue_negative.py entirely\n     We're down to a single case of the long running /hung LVM calls\n\n3. skipping the tempest/scenario/test_encrypted_cinder_volumes.py\n      We eliminate the problem altogether (at least 4 out of 4 runs so far)\n\nI'm certainly not saying you should remove all of these tests, but I am saying that there's something specific to the behaviors in these tests that causes the LVM to try and query devices that don't exist any longer.  \n\nThe overwhelming feedback from Nova folks is \"we don't do anything to paths here\", I still say there's data that says otherwise.  At the same time I don't have concrete answers that pass the Nova core test, so I  think we'll just live with this scenario in the gate.", 
            "date_created": "2015-03-16 15:51:36.965467+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Turns out this can be addressed by making our filter a global_filter in lvm.conf.\n\nThe issue that we're running into is that dev files can come and go on the system, and there are cases where an LVS or VGS call will try and open a dev/sdX device that no longer exists when trying to query it for LVM data to see if it's an LVM disk.  I added a filter a while back via /etc/cinder/lvm.conf, but it turns out that doesn't do us any good for the device open/query routines on lvs and vgs.  For those you need to use the global_filter which tells LVM to not even bother with devices other than those specified by said filter.\n\nThere's also a possibility that we can just skip the filters and use lvmetad which is not enabled by default on Ubuntu.  Afazekas mentioned something about this earlier today but I wasn't sure what he was referring to until I read some man pages and it turns out that does a lot of the same things as filters only a bit smarter and more dynamically.  I would like some investigation done with that approach though because it also changes the activate/deactivate behavior so I want to make sure we fully understand it and test it thoroughly before adding it.", 
            "date_created": "2015-03-17 03:39:26.857432+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }, 
        {
            "content": "Fix committed via filters in devstack here: 4bf861c76c220a98a3b3165eea5448411d000f3a", 
            "date_created": "2015-03-20 16:04:40.247615+00:00", 
            "author": "https://api.launchpad.net/1.0/~john-griffith"
        }
    ], 
    "closed": "2015-03-17 03:33:00.753739+00:00"
}
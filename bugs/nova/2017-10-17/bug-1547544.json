{
    "status": "Won't Fix", 
    "last_updated": "2016-03-11 11:28:48.836206+00:00", 
    "description": "Setup: \n\nSingle controller[48 GB RAM, 16vCPU, 120GB Disk]\n3 Network Nodes\n100 ESX hypervisors distributed in 10 nova-compute nodes\n\nTest:\n\n1. Create /16 network\n2. Heat template which which will launch 100 instances on network created step 1\n3. Create 10 stack back2back so that we reach 1000 instances without waiting for previous stack to complete\n\nObservation:\n\nstack creations are failing while nova run_periodic_tasks at different places like _heal_instance_info_cache,  _sync_scheduler_instance_info, _update_available_resource etc\n\nHave attached sample heat template, heat logs, nova compute log from one of the host.\n\n\nLogs:\n\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py\", line 271, in inner\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     return f(*args, **kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/opt/stack/nova/nova/compute/resource_tracker.py\", line 553, in _update_available_resource\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     context, self.host, self.nodename)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_versionedobjects/base.py\", line 174, in wrapper\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     args, kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/opt/stack/nova/nova/conductor/rpcapi.py\", line 240, in object_class_action_versions\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     args=args, kwargs=kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     retry=self.retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     timeout=timeout, retry=retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 465, in send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     retry=retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 454, in _send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     result = self._waiter.wait(msg_id, timeout)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 337, in wait\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     message = self.waiters.get(msg_id, timeout=timeout)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 239, in get\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     'to message ID %s' % msg_id)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager MessagingTimeout: Timed out waiting for a reply to message ID a87a7f358a0948efa3ab5beb0c8f45e7\n--\n\n\nstack@esx-compute-9:/opt/stack/nova$ git log -1\ncommit d51c5670d8d26e989d92eb29658eed8113034c0f\nMerge: 4fade90 30d5d80\nAuthor: Jenkins <email address hidden>\nDate:   Thu Feb 18 17:56:32 2016 +0000\n\n    Merge \"reset task_state after select_destinations failed.\"\nstack@esx-compute-9:/opt/stack/nova$", 
    "tags": [
        "vmware"
    ], 
    "importance": "Undecided", 
    "heat": 18, 
    "link": "https://bugs.launchpad.net/nova/+bug/1547544", 
    "owner": "None", 
    "id": 1547544, 
    "index": 7313, 
    "openned": "2016-02-19 14:48:58.080652+00:00", 
    "created": "2016-02-19 14:48:58.080652+00:00", 
    "title": "heat: MessagingTimeout: Timed out waiting for a reply to message ID", 
    "comments": [
        {
            "content": "Setup: \n\nSingle controller[48 GB RAM, 16vCPU, 120GB Disk]\n3 Network Nodes\n100 ESX hypervisors distributed in 10 nova-compute nodes\n\nTest:\n\n1. Create /16 network\n2. Heat template which which will launch 100 instances on network created step 1\n3. Create 10 stack back2back so that we reach 1000 instances without waiting for previous stack to complete\n\nObservation:\n\nstack creations are failing while nova run_periodic_tasks at different places like _heal_instance_info_cache,  _sync_scheduler_instance_info, _update_available_resource etc\n\nHave attached sample heat template, heat logs, nova compute log from one of the host.\n\n\nLogs:\n\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_concurrency/lockutils.py\", line 271, in inner\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     return f(*args, **kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/opt/stack/nova/nova/compute/resource_tracker.py\", line 553, in _update_available_resource\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     context, self.host, self.nodename)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_versionedobjects/base.py\", line 174, in wrapper\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     args, kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/opt/stack/nova/nova/conductor/rpcapi.py\", line 240, in object_class_action_versions\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     args=args, kwargs=kwargs)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     retry=self.retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     timeout=timeout, retry=retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 465, in send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     retry=retry)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 454, in _send\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     result = self._waiter.wait(msg_id, timeout)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 337, in wait\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     message = self.waiters.get(msg_id, timeout=timeout)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager   File \"/usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 239, in get\n2016-02-19 04:21:54.691 TRACE nova.compute.manager     'to message ID %s' % msg_id)\n2016-02-19 04:21:54.691 TRACE nova.compute.manager MessagingTimeout: Timed out waiting for a reply to message ID a87a7f358a0948efa3ab5beb0c8f45e7\n--\n\n\nstack@esx-compute-9:/opt/stack/nova$ git log -1\ncommit d51c5670d8d26e989d92eb29658eed8113034c0f\nMerge: 4fade90 30d5d80\nAuthor: Jenkins <email address hidden>\nDate:   Thu Feb 18 17:56:32 2016 +0000\n\n    Merge \"reset task_state after select_destinations failed.\"\nstack@esx-compute-9:/opt/stack/nova$", 
            "date_created": "2016-02-19 14:48:58.080652+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "", 
            "date_created": "2016-02-19 15:12:27.241789+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "", 
            "date_created": "2016-02-19 15:23:37.819858+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "I realistically expect that you have just overloaded the system so these requests are taking too long. dstat info during the run would be useful to figure that out.", 
            "date_created": "2016-02-19 20:13:49.118072+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "", 
            "date_created": "2016-02-20 04:04:22.334114+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "Sean, Not able to makeout much from dstat logs. Have attached it to bug", 
            "date_created": "2016-02-20 04:05:06.760764+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "Hi Davanum,\n\nPlz let me know what info you need?. Thanks", 
            "date_created": "2016-02-21 04:49:20.570286+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "FYI had tweaked below parameters before running test:\n\nneutron.conf\nmax_overflow = 64\nmax_pool_size = 64\n\nnova.conf\nexecutor_thread_pool_size = 1200\nrpc_response_timeout = 1200", 
            "date_created": "2016-02-22 05:21:38.551615+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "From looking at the dstat output, the node in question is above load avg of 11 for nearly 2 hours, about an hour into it is where your error happens.\n\nRealistically, that's just too much work being asked of the node. We have found in the gate that once you get sustained load average over 10 things start to break down. There is no bug fix for this, it's just a fallout of our architecture. \n\nMarking as won't fix, as I don't think there is anything actionable here. If you have performance improvements in your environment that make this better, that's great. However there are bounds in which the nova compute worker just does fail over, and there is not much to be done about it. ", 
            "date_created": "2016-02-22 12:24:44.454025+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Hi Sean,\n\nController in my system is running with 16vCPU, 48GB RAM and max load average was 11 or 12. I guess this load is OK for machine running with 16vCPU?\n\nThanks,\nPrashant", 
            "date_created": "2016-02-23 08:38:47.224218+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "Hi Sean,\n\nIf system is running with 16vCPU, load average of 11 or 12 should be normal load. With 100 ESX compute, expecting 10VM's/ESX after test completion. I think its not too much ask for setup.\n\nThanks", 
            "date_created": "2016-03-11 10:38:45.025274+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }, 
        {
            "content": "Just to add more info:\n\nCorrection. We have controller with 32 vCPU with 2 cores each i.e 64 cores total & 48GB RAM.\n\nstack@controller:~$ cat /proc/cpuinfo  | grep processor| wc -l\n32\nstack@controller:~$ \n\nstack@controller:~$ cat /proc/cpuinfo  | grep cores\ncpu cores       : 2\ncpu cores       : 2\ncpu cores       : 2\n.\n.\n\nstack@controller:~$ cat /proc/cpuinfo  | grep cores | wc -l\n32\nstack@controller:~$ \n\nFor 64 cores system load average of 16 means its 25% of total capacity. 25% of utilization shouldnt be breaking system?\n\nThanks,\nPrashant", 
            "date_created": "2016-03-11 11:28:47.459663+00:00", 
            "author": "https://api.launchpad.net/1.0/~prashantshetty"
        }
    ], 
    "closed": "2016-02-22 12:24:52.953924+00:00"
}
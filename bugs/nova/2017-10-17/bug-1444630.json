{
    "status": "Fix Released", 
    "last_updated": "2016-04-18 19:22:44.635846+00:00", 
    "description": "This is a follow on to bug 1293480 and related to bug 1408176 and bug 1443186.\n\nThere can be a race when rebooting a compute host where libvirt is shutting down guest VMs and sending STOPPED lifecycle events up to nova compute which then tries to stop them via the stop API, which sometimes works and sometimes doesn't - the compute service can go down with a vm_state of ACTIVE and task_state of powering-off which isn't resolve on host reboot.\n\nSometimes the stop API completes and the instance is stopped with power_state=4 (shutdown) in the nova database.  When the host comes back up and libvirt restarts, it starts up the guest VMs which sends the STARTED lifecycle event and nova handles that but because the vm_state in the nova database is STOPPED and the power_state is 1 (running) from the hypervisor, nova things it started up unexpectedly and stops it:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py?id=2015.1.0rc1#n6145\n\nSo nova shuts the running guest down.\n\nActually the block in:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py?id=2015.1.0rc1#n6145\n\nconflicts with the statement in power_state.py:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/power_state.py?id=2015.1.0rc1#n19\n\n\"The hypervisor is always considered the authority on the status\nof a particular VM, and the power_state in the DB should be viewed as a\nsnapshot of the VMs's state in the (recent) past.\"\n\nAnyway, that's a different issue but the point is when nova-compute is shutting down it should stop accepting lifecycle events from the hypervisor (virt driver code) since it can't really reliably act on them anyway - we can leave any sync up that needs to happen in init_host() in the compute manager.", 
    "tags": [
        "compute", 
        "in-stable-juno", 
        "libvirt"
    ], 
    "importance": "Medium", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1444630", 
    "owner": "https://api.launchpad.net/1.0/~mriedem", 
    "id": 1444630, 
    "index": 4222, 
    "openned": "2015-04-15 18:08:40.643272+00:00", 
    "created": "2015-04-15 18:08:40.643272+00:00", 
    "title": "nova-compute should stop handling virt lifecycle events when it's shutting down", 
    "comments": [
        {
            "content": "This is a follow on to bug 1293480 and related to bug 1408176 and bug 1443186.\n\nThere can be a race when rebooting a compute host where libvirt is shutting down guest VMs and sending STOPPED lifecycle events up to nova compute which then tries to stop them via the stop API, which sometimes works and sometimes doesn't - the compute service can go down with a vm_state of ACTIVE and task_state of powering-off which isn't resolve on host reboot.\n\nSometimes the stop API completes and the instance is stopped with power_state=4 (shutdown) in the nova database.  When the host comes back up and libvirt restarts, it starts up the guest VMs which sends the STARTED lifecycle event and nova handles that but because the vm_state in the nova database is STOPPED and the power_state is 1 (running) from the hypervisor, nova things it started up unexpectedly and stops it:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py?id=2015.1.0rc1#n6145\n\nSo nova shuts the running guest down.\n\nActually the block in:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py?id=2015.1.0rc1#n6145\n\nconflicts with the statement in power_state.py:\n\nhttp://git.openstack.org/cgit/openstack/nova/tree/nova/compute/power_state.py?id=2015.1.0rc1#n19\n\n\"The hypervisor is always considered the authority on the status\nof a particular VM, and the power_state in the DB should be viewed as a\nsnapshot of the VMs's state in the (recent) past.\"\n\nAnyway, that's a different issue but the point is when nova-compute is shutting down it should stop accepting lifecycle events from the hypervisor (virt driver code) since it can't really reliably act on them anyway - we can leave any sync up that needs to happen in init_host() in the compute manager.", 
            "date_created": "2015-04-15 18:08:40.643272+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Attaching some logs sent from someone with IBM that recreated this on Juno with a debug patch (https://review.openstack.org/#/c/169782/) for logging:\n\nHi, I finished another round of testing, this time all the VMs were in SHUTOFF state after hypervisor reboot (), here are the key time points in the log file:\n13:41:47 Triggered hypervisor reboot, \"Emitting event\" arrived\n13:45:33 Nova compute server started after hypervisor started up\n13:46:25 Finished VM state sync up\nFor more details please check the attached log file: compute_kvm_reboot.2.log.zip\nThanks!\n\n\n================================ Before host reboot: =============================\n================= on kvm001 node Before KVM Reboot ===================\n[root@hkg02kvm001ccz023 \u02dc]# date\nWed Apr 15 13:39:32 UTC 2015\n[root@hkg02kvm001ccz023 \u02dc]# virsh list\nId    Name                           State\n----------------------------------------------------\n3     instance-000000a2              running\n4     instance-00000058              running\n\n\n================= on controller node Before KVM Reboot ===================\n[root@hkg02ops001ccz023 \u02dc]# date\nWed Apr 15 13:39:52 UTC 2015\n[root@hkg02ops001ccz023 \u02dc]# nova list\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| ID                                   | Name  | Status  | Task State | Power State | Networks                              |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| e53dcdcd-1e19-4a89-8648-1373b4e29e6a | zy001 | ACTIVE | -          | Running     | Shared-Custom-Network1=192.168.100.18 |\n| 3bcdec02-bb42-4eb7-bfca-eca1686f735b | zy002 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.19 |\n| e0638150-6ef0-4e98-884d-fb4cfda140a3 | zy004 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.21 |\n| 793cd8ba-fcb2-4e42-8b83-fcb8bdf519e6 | zy005 | ACTIVE | -          | Running     | Shared-Custom-Network1=192.168.100.25 |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n\n\n================================ After host reboot: =============================\n================= on kvm001 After KVM Reboot ===================\n[root@hkg02kvm001ccz023 \u02dc]# date\nWed Apr 15 13:47:46 UTC 2015\n[root@hkg02kvm001ccz023 \u02dc]# virsh list\nId    Name                           State\n----------------------------------------------------\n\n[root@hkg02kvm001ccz023 \u02dc]# nova list\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| ID                                   | Name  | Status  | Task State | Power State | Networks                              |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| e53dcdcd-1e19-4a89-8648-1373b4e29e6a | zy001 | SHUTOFF | -          | Shutdown   | Shared-Custom-Network1=192.168.100.18 |\n| 3bcdec02-bb42-4eb7-bfca-eca1686f735b | zy002 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.19 |\n| e0638150-6ef0-4e98-884d-fb4cfda140a3 | zy004 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.21 |\n| 793cd8ba-fcb2-4e42-8b83-fcb8bdf519e6 | zy005 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.25 |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+", 
            "date_created": "2015-04-15 18:44:33.797401+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Attaching some logs sent from someone with IBM that recreated this on Juno with a debug patch (https://review.openstack.org/#/c/169782/) for logging:\n\nHi, I finished another round of testing, this time all the VMs were in SHUTOFF state after hypervisor reboot (), here are the key time points in the log file:\n13:41:47 Triggered hypervisor reboot, \"Emitting event\" arrived\n13:45:33 Nova compute server started after hypervisor started up\n13:46:25 Finished VM state sync up\nFor more details please check the attached log file: compute_kvm_reboot.2.log.zip\nThanks!\n\n\n================================ Before host reboot: =============================\n================= on kvm001 node Before KVM Reboot ===================\n[root@hkg02kvm001ccz023 \u02dc]# date\nWed Apr 15 13:39:32 UTC 2015\n[root@hkg02kvm001ccz023 \u02dc]# virsh list\nId    Name                           State\n----------------------------------------------------\n3     instance-000000a2              running\n4     instance-00000058              running\n\n\n================= on controller node Before KVM Reboot ===================\n[root@hkg02ops001ccz023 \u02dc]# date\nWed Apr 15 13:39:52 UTC 2015\n[root@hkg02ops001ccz023 \u02dc]# nova list\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| ID                                   | Name  | Status  | Task State | Power State | Networks                              |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| e53dcdcd-1e19-4a89-8648-1373b4e29e6a | zy001 | ACTIVE | -          | Running     | Shared-Custom-Network1=192.168.100.18 |\n| 3bcdec02-bb42-4eb7-bfca-eca1686f735b | zy002 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.19 |\n| e0638150-6ef0-4e98-884d-fb4cfda140a3 | zy004 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.21 |\n| 793cd8ba-fcb2-4e42-8b83-fcb8bdf519e6 | zy005 | ACTIVE | -          | Running     | Shared-Custom-Network1=192.168.100.25 |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n\n\n================================ After host reboot: =============================\n================= on kvm001 After KVM Reboot ===================\n[root@hkg02kvm001ccz023 \u02dc]# date\nWed Apr 15 13:47:46 UTC 2015\n[root@hkg02kvm001ccz023 \u02dc]# virsh list\nId    Name                           State\n----------------------------------------------------\n\n[root@hkg02kvm001ccz023 \u02dc]# nova list\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| ID                                   | Name  | Status  | Task State | Power State | Networks                              |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+\n| e53dcdcd-1e19-4a89-8648-1373b4e29e6a | zy001 | SHUTOFF | -          | Shutdown   | Shared-Custom-Network1=192.168.100.18 |\n| 3bcdec02-bb42-4eb7-bfca-eca1686f735b | zy002 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.19 |\n| e0638150-6ef0-4e98-884d-fb4cfda140a3 | zy004 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.21 |\n| 793cd8ba-fcb2-4e42-8b83-fcb8bdf519e6 | zy005 | SHUTOFF | -          | Shutdown    | Shared-Custom-Network1=192.168.100.25 |\n+--------------------------------------+-------+---------+------------+-------------+---------------------------------------+", 
            "date_created": "2015-04-15 18:44:41.048138+00:00", 
            "author": "https://api.launchpad.net/1.0/~mriedem"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/174069", 
            "date_created": "2015-04-15 20:00:14.107904+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/174069\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0\nSubmitter: Jenkins\nBranch:    master\n\ncommit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n", 
            "date_created": "2015-04-16 09:26:31.925387+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/kilo\nReview: https://review.openstack.org/174477", 
            "date_created": "2015-04-16 17:33:01.461496+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/174477\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=b19764d2c6a8160102a806c1d6811c4182a8bac8\nSubmitter: Jenkins\nBranch:    stable/kilo\n\ncommit b19764d2c6a8160102a806c1d6811c4182a8bac8\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n", 
            "date_created": "2015-04-21 11:17:43.758184+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/159275\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=d09785b97a282e8538642f6f8bcdd8491197ed74\nSubmitter: Jenkins\nBranch:    master\n\ncommit d09785b97a282e8538642f6f8bcdd8491197ed74\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Feb 25 14:13:45 2015 -0800\n\n    Add config option to disable handling virt lifecycle events\n    \n    Historically the _sync_power_states periodic task has had the potential\n    for race conditions and several changes have been made to try and\n    tighten up this code:\n    \n    cc5388bbe81aba635fb757e202d860aeed98f3e8\n    aa1792eb4c1d10e9a192142ce7e20d37871d916a\n    baabab45e0ae0e9e35872cae77eb04bdb5ee0545\n    bd8329b34098436d18441a8129f3f20af53c2b91\n    \n    The handle_lifecycle_events method which gets power state change events\n    from the compute driver (currently only implemented by the libvirt\n    driver) and calls _sync_instance_power_state - the same method that the\n    _sync_power_states periodic task uses, except the periodic task at least\n    locks when it's running - expands the scope for race problems in the\n    compute manager so cloud providers should be able to turn it off. It is\n    also known to have races with reboot where rebooted instances are\n    automatically shutdown because of delayed lifecycle events that the\n    instance is stopped even though it's running.\n    \n    This is consistent with the view that Nova should manage it's own state\n    and not rely on external events telling it what to do about state\n    changes. For example, in _sync_instance_power_state, if the Nova\n    database thinks an instance is stopped but the hypervisor says it's\n    running, the compute manager issues a force-stop on the instance.\n    \n    Also, although not documented (at least from what I can find), Nova has\n    historically held a stance that it does not support out-of-band\n    discovery and management of instances, so allowing external events to\n    change state somewhat contradicts that stance and should be at least a\n    configurable deployment option.\n    \n    DocImpact: New config option \"handle_virt_lifecycle_events\" in the\n               DEFAULT group of nova.conf. By default the value is True\n               so there is no upgrade impact or change in functionality.\n    \n    Related-Bug: #1293480\n    Partial-Bug: #1443186\n    Partial-Bug: #1444630\n    \n    Change-Id: I26a1bc70939fb40dc38e9c5c43bf58ed1378bcc7\n", 
            "date_created": "2015-04-29 12:39:29.586873+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/179284", 
            "date_created": "2015-04-30 23:18:59.165665+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/179284\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=5228d4e418734164ffa5ccd91d2865d9cc659c00\nSubmitter: Jenkins\nBranch:    master\n\ncommit 906ab9d6522b3559b4ad36d40dec3af20397f223\nAuthor: He Jie Xu <email address hidden>\nDate:   Thu Apr 16 07:09:34 2015 +0800\n\n    Update rpc version aliases for kilo\n    \n    Update all of the rpc client API classes to include a version alias\n    for the latest version implemented in Kilo.  This alias is needed when\n    doing rolling upgrades from Kilo to Liberty.  With this in place, you can\n    ensure all services only send messages that both Kilo and Liberty will\n    understand.\n    \n    Closes-Bug: #1444745\n    \n    Conflicts:\n    \tnova/conductor/rpcapi.py\n    \n    NOTE(alex_xu): The conflict is due to there are some logs already added\n    into the master.\n    \n    Change-Id: I2952aec9aae747639aa519af55fb5fa25b8f3ab4\n    (cherry picked from commit 78a8b5802ca148dcf37c5651f75f2126d261266e)\n\ncommit f191a2147a21c7e50926b288768a96900cf4c629\nAuthor: Hans Lindgren <email address hidden>\nDate:   Fri Apr 24 13:10:39 2015 +0200\n\n    Add security group calls missing from latest compute rpc api version bump\n    \n    The recent compute rpc api version bump missed out on the security group\n    related calls that are part of the api.\n    \n    One possible reason is that both compute and security group client side\n    rpc api:s share a single target, which is of little value and only cause\n    mistakes like this.\n    \n    This change eliminates future problems like this by combining them into\n    one to get a 1:1 relationship between client and server api:s.\n    \n    Change-Id: I9207592a87fab862c04d210450cbac47af6a3fd7\n    Closes-Bug: #1448075\n    (cherry picked from commit bebd00b117c68097203adc2e56e972d74254fc59)\n\ncommit a2872a9262985bd0ee2c6df4f7593947e0516406\nAuthor: Dan Smith <email address hidden>\nDate:   Wed Apr 22 09:02:03 2015 -0700\n\n    Fix migrate_flavor_data() to catch instances with no instance_extra rows\n    \n    The way the query was being performed previously, we would not see any\n    instances that didn't have a row in instance_extra. This could happen if\n    an instance hasn't been touched for several releases, or if the data\n    set is old.\n    \n    The fix is a simple change to use outerjoin instead of join. This patch\n    includes a test that ensures that instances with no instance_extra rows\n    are included in the migration. If we query an instance without such a\n    row, we create it before doing a save on the instance.\n    \n    Closes-Bug: #1447132\n    Change-Id: I2620a8a4338f5c493350f26cdba3e41f3cb28de7\n    (cherry picked from commit 92714accc49e85579f406de10ef8b3b510277037)\n\ncommit e3a7b83834d1ae2064094e9613df75e3b07d77cd\nAuthor: OpenStack Proposal Bot <email address hidden>\nDate:   Thu Apr 23 02:18:41 2015 +0000\n\n    Updated from global requirements\n    \n    Change-Id: I5d4acd36329fe2dccb5772fed3ec55b442597150\n\ncommit 8c9b5e620eef3233677b64cd234ed2551e6aa182\nAuthor: Divya <email address hidden>\nDate:   Tue Apr 21 08:26:29 2015 +0200\n\n    Control create/delete flavor api permissions using policy.json\n    \n    The permissions of create/delete flavor api is currently broken\n    and expects the user to be always an admin, instead of controlling\n    the permissions by the rules defined in the nova policy.json.\n    \n    Change-Id: Ide3c9ec2fa674b4fe3ea9d935cd4f7848914b82e\n    Closes-Bug: 1445335\n    (cherry picked from commit ced60b1d1b1608dc8229741b207a95498bc0b212)\n\ncommit bf79742d26ae66886bcdc55eeaf27e1d7ce24be5\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Tue Apr 14 16:28:57 2015 +0100\n\n    Fix handling of pci_requests in consume_from_instance.\n    \n    Properly retrieve requests from pci_requests in consume_from_instance.\n    Without this the call to numa_fit_instance_to_host will fail because\n    it expects the request list.\n    And change the order in which apply_requests and numa_fit_instance_to_host\n    are called. Calling apply_requests first will remove devices from pools\n    and  may make numa_fit_instance_to_host fail.\n    \n    Change-Id: I41cf4e8e5c1dea5f91e5261a8f5e88f46c7994ef\n    Closes-bug: #1444021\n    (cherry picked from commit 0913e799e9ce3138235f5ea6f80159f468ad2aaa)\n\ncommit c2d7060b480608d9773340f51d6496fadf97b667\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Thu Apr 16 17:10:06 2015 +0100\n\n    Use list of requests in InstancePCIRequests.obj_from_db.\n    \n    InstancePCIRequests.obj_from_db assumes it's called with with a dict\n    of values from instances_extra table, but in some cases it's called\n    with just the value of pci_requests column.\n    This changes obj_from_db to be used with just the value of pci_requests column.\n    \n    Change-Id: I7bed733c845c365081719a70b8a2f0cc9a58370c\n    Closes-bug: #1445040\n    (cherry picked from commit a074d7b4465b45730a5171e024c5c39a66a9c927)\n\ncommit 7a609f153808f7cee1edbbb36accc292fa8df0d0\nAuthor: Przemyslaw Czesnowicz <email address hidden>\nDate:   Tue Apr 7 16:31:05 2015 +0100\n\n    Add numa_node field to PciDevicePool\n    \n    Without this field, PciDevicePool.from_dict will treat numa_node key in\n    the dict as a tag, which in turn means that the scheduler client will\n    drop it when converting stats to objects before reporting.\n    \n    Converting it back to dicts on the scheduler side thus will not have\n    access to the numa_node information which would cause any requests that\n    will look for the exact match between the device and instance NUMA nodes\n    in the NUMATopologyFilter to fail.\n    \n    Closes-Bug: #1441169\n    (cherry picked from commit 7db1ebc66c59205f78829d1e9cd10dcc1201d798)\n    \n    Conflicts:\n    \tnova/tests/unit/objects/test_objects.py\n    \n    Change-Id: I7381f909620e8e787178c0be9a362f8d3eb9ff7d\n\ncommit 880a356e40d327c0af4ce94b5a08fe0cd6fcab5d\nAuthor: Nikola Dipanov <email address hidden>\nDate:   Tue Apr 7 20:53:32 2015 +0100\n\n    scheduler: re-calculate NUMA on consume_from_instance\n    \n    This patch narrows down the race window between the filter running and\n    the consumption of resources from the instance after the host has been\n    chosen.\n    \n    It does so by re-calculating the fitted NUMA topology just before consuming it\n    from the chosen host. Thus we avoid any locking, but also make sure that\n    the host_state is kept as up to date as possible for concurrent\n    requests, as there is no opportunity for switching threads inside a\n    consume_from_instance.\n    \n    Several things worth noting:\n      * Scheduler being lock free (and thus racy) does not really affect\n      resources other than PCI and NUMA topology this badly - this is due\n      to complexity of said resources. In order for scheduler decesions to not\n      be based on basically guessing, in case of those two we will likely need\n      to introduce either locking or special heuristics.\n    \n      * There is a lot of repeated code between the 'consume_from_instance'\n      method and the actual filters. This situation should really be fixed but\n      is out of scope for this bug fix (which is about preventing valid\n      requests failing because of races in the scheduler).\n    \n    Change-Id: If0c7ad20506c9dddf4dec1eb64c9d6dd4fb75633\n    Closes-bug: #1438238\n    (cherry picked from commit d6b3156a6c89ddff9b149452df34c4b32c50b6c3)\n\ncommit a4e9a146c3993f5775501716a21632f34a63a3ad\nAuthor: Rajesh Tailor <email address hidden>\nDate:   Wed Apr 15 06:59:04 2015 -0700\n\n    Fix kwargs['migration'] KeyError in @errors_out_migration decorator\n    \n    @errors_out_migration decorator is used in the compute manager on\n    resize_instance and finish_resize methods of ComputeManager class.\n    It is decorated via @utils.expects_func_args('migration') to check\n    'migration' is a parameter to the decorator method, however, that\n    only ensures there is a migration argument, not that it's in args or\n    kwargs (either is fine for what expects_func_args checks).\n    The errors_out_migration decorator can get a KeyError when checking\n    kwargs['migration'] and fails to set the migration status to 'error'.\n    \n    This fixes the KeyError in the decorator by normalizing the args/kwargs\n    list into a single dict that we can pull the migration from.\n    \n    Change-Id: I774ac9b749b21085f4fbcafa4965a78d68eec9c7\n    Closes-Bug: 1444300\n    (cherry picked from commit 3add7923fc16c050d4cfaef98a87886c6b6a589c)\n\ncommit 389368bcfe498323b369f68682babb92a5b0ca54\nAuthor: Gary Kotton <email address hidden>\nDate:   Wed Apr 15 05:14:42 2015 -0700\n\n    Resource tracker: unable to restart nova compute\n    \n    The resource tracker calculates its used resources. In certain cases\n    of failed migrations and an instance being deleted the resource tracker\n    causes an exception in nova compute. If this situation arises then nova\n    compute may not even be able to restart.\n    \n    Change-Id: I4a154e0cae3b8e22bd59ed05ba708e07eed8dea7\n    Closes-bug: #1444439\n    (cherry picked from commit ee7a7446cc6947a6bacacb6cb514934cc22e5782)\n\ncommit bd6a40fecde943a3ded0124481a12c27dbb167de\nAuthor: Andreas Jaeger <email address hidden>\nDate:   Mon Apr 20 11:01:22 2015 +0200\n\n    Release Import of Translations from Transifex\n    \n    Manual import of Translations from Transifex. This change also removes\n    all po files that are less than 66 per cent translated since such\n    partially translated files will not help users.\n    \n    This updates also recreates all pot (translation source files) to\n    reflect the state of the repository.\n    \n    This change needs to be done manually since the automatic import does\n    not handle the proposed branches and we need to sync with latest\n    translations.\n    \n    Change-Id: I0e9ef00182a2229602d23b8a67a02f0be62ee239\n\ncommit 8ebd515aa94ed399074a3b55bd36fd8cd579a499\nAuthor: Matt Riedemann <email address hidden>\nDate:   Thu Apr 16 11:08:50 2015 -0700\n\n    Use kwargs from compute v4 proxy change_instance_metadata\n    \n    The args were passed to the compute manager method in the wrong order.\n    We noticed this in the gate with KeyError: 'uuid' in the logs because of\n    the LOG.debug statement in change_instance_metadata. Just use kwargs\n    like rpcapi would normally.\n    \n    There isn't a unit test for this since the v4 proxy code goes away in\n    liberty, this is for getting it into stable/kilo.\n    \n    Closes-Bug: #1444728\n    \n    Change-Id: Ic988f48d99e626ee5773c97904e09dbf00c5414a\n    (cherry picked from commit e55f746ea8590cce7c2b07a023197f369251a7ef)\n\ncommit b19764d2c6a8160102a806c1d6811c4182a8bac8\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n\ncommit 75e9de5d572578520c217b540aa2a40726f137f0\nAuthor: Roman Podoliaka <email address hidden>\nDate:   Wed Mar 4 17:27:06 2015 +0200\n\n    Forbid booting of QCOW2 images with virtual_size > root_gb\n    \n    Currently, it's possible to boot an instance from a QCOW2 image,\n    which has virtual_size bigger than one allowed by the given flavor\n    (root_gb).\n    \n    The issue is caused by two different problems in the code:\n    \n    1) typo in get_disk_size() has made it always return None and\n       effectively disabled verify_base_size() checks\n    \n    2) Rbd image backend skips the verify_base_size() step for\n       'cached' images (the one with base files), so it is possible to\n       boot an instance using a larger flavor once and then use smaller\n       flavors to boot the same image, even if allowed root_gb size is\n       smaller than the image virtual size\n    \n    Closes-Bug: #1429093\n    \n    Change-Id: I383130e5f8cc288f4b428ed43fe4d3aba7169473\n    (cherry picked from commit c1f9ed27af64e6893d9d0153a964df5aba99b8f0)\n\ncommit 33ba90240a2ad3165274d9e54ceb156273404c9a\nAuthor: Matt Riedemann <email address hidden>\nDate:   Mon Apr 13 14:47:20 2015 -0700\n\n    Pass migrate_data to pre_live_migration\n    \n    Commit ebfa09fa197a1d88d1b3ab1f308232c3df7dc009 added an RPC proxy but\n    as part of that was passing migrate_data=None for pre_live_migration\n    which breaks live block migration when not using shared storage.\n    \n    Closes-Bug: #984996\n    \n    Change-Id: I2a83f1fb0e4468f9a6c67a188af725c3406139d1\n    (cherry picked from commit 4e515ec2269a1c3187ee9ffad3a6be059ec74b0b)\n\ncommit dea6116723f22632c2e478e00bb0aafcd2febdc9\nAuthor: Timofey Durakov <email address hidden>\nDate:   Fri Apr 10 19:38:33 2015 +0300\n\n    Fixed order of arguments during execution live_migrate()\n    \n    order of arguments that passed to\n    ComputeManager.live_,migration() differs in ComputeManager and\n    _ComputeV4Proxy classes\n    \n    Change-Id: I23c25d219e9cdd0673ae6a12250219680fb7bda9\n    Closes-Bug:#1442656\n    (cherry picked from commit ba521fa53711774e0718808fe333aca676de57ae)\n\ncommit 22d7547c6b62fb9dabd861e4941edd34eedabfc6\nAuthor: Doug Hellmann <email address hidden>\nDate:   Wed Apr 15 19:58:17 2015 +0000\n\n    update .gitreview for stable/kilo\n    \n    Change-Id: I6356513ac42b79402dbde8ee5e75cbbd1aee7eef\n\ncommit 68d6f924037f3b931add2ce5d0d433913e720ca6\nAuthor: Ken'ichi Ohmichi <email address hidden>\nDate:   Wed Apr 15 03:13:43 2015 +0000\n\n    Add min/max of API microversions to version API\n    \n    As nova-spec api-microversions, versions API needs to expose minimum\n    and maximum microversions to version API, because clients need to\n    know available microversions through the API. That is very important\n    for the interoperability.\n    This patch adds these versions as the nova-spec mentioned.\n    \n    Note:\n      As v2(not v2.1) API change manner, we have added new extensions if\n      changing API. However, this patch doesn't add a new extension even\n      if adding new parameters \"version\" and \"min_version\" because version\n      API is independent from both v2 and v2.1 APIs.\n    \n    Change-Id: Id464a07d624d0e228fe0aa66a04c8e51f292ba0c\n    Closes-Bug: #1443375\n    (cherry picked from commit 1830870718fe7472b47037f3331cfe59b5bdda07)\n    (cherry picked from commit 853671e912c6ad9a4605acad2575417911875cdd)\n\ncommit d82d492c67db546d01addc5fead9708760fb6abd\nAuthor: Sabari Kumar Murugesan <email address hidden>\nDate:   Mon Apr 6 20:36:48 2015 -0700\n\n    VMware: Fix attribute error in resize\n    \n    The class DatastorePath was recently removed from ds_util as it's\n    available in oslo.vmware. One of the reference was missed during\n    the refactor.\n    \n    Change-Id: Idc5825c304a99e83cbf36e93751148d6f995131a\n    Closes-Bug: #1440968\n    (cherry picked from commit ab4a5a5300179a79f7a67688f0e9f3fc280c0efa)\n\ncommit 3cff2c673c6cdf487c2a1eb2a5c6c89c6de80d11\nAuthor: jichenjc <email address hidden>\nDate:   Fri Mar 20 08:36:37 2015 +0800\n\n    Release bdm constraint source and dest type\n    \n    https://bugs.launchpad.net/nova/+bug/1377958 fixed a problem\n    that source_type: image, destination_type: local is not\n    supported for boot instance, exception should be raised to\n    reject the param otherwise it will lead to instance become\n    ERROR state.\n    \n    However the fix introduced a problem on nova client\n    https://bugs.launchpad.net/python-novaclient/+bug/1418484\n    The fix of the bug leads to following command become invalid\n    \n    nova boot test-vm --flavor m1.medium --image centos-vm-32\n    --nic net-id=c3f40e33-d535-4217-916b-1450b8cd3987 --block-device\n    id=26b7b917-2794-452a-95e5-2efb2ca6e32d,bus=sata,source=volume,bootindex=1\n    \n    So we need to release the original constraint to allow\n    the above special case pass the validation check then\n    we can revert the nova client exception\n    (https://review.openstack.org/#/c/165932/)\n    \n    This patch checks the boot_index and whether image param is\n    given after we found the bdm has source_type: image,\n    destination_type: local, if this is the special case, then\n    no exception will be raised.\n    \n    Closes-Bug: #1433609\n    \n    Change-Id: If43faae95169bc3864449a8364975f5c887aac14\n    (cherry picked from commit cadbcc440a2fcfb8532f38111999a06557fbafc2)\n\ncommit 97145ba175291d6522afa079860c72220e43024a\nAuthor: Dan Smith <email address hidden>\nDate:   Fri Apr 10 07:10:52 2015 -0700\n\n    Fix check_can_live_migrate_destination() in ComputeV4Proxy\n    \n    There was a mismatch in the V4 proxy in the call signatures of this\n    function. This was missed because the \"destination\" parameter is passed\n    in the rpcapi as the host to contact, which is consumed by the rpc\n    layer and not passed. Since it was not called one of the standard\n    names (either host if to be not passed, or host_param if to be passed),\n    this was missed.\n    \n    Change-Id: Idf2160934dade650ed02b672f3b64cb26247f8e6\n    Closes-Bug: #1442602\n    (cherry picked from commit 0c08f7f2ef070f7c6172d7742f9789e0a8bda91a)\n", 
            "date_created": "2015-05-06 13:45:02.261463+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Fix proposed to branch: stable/juno\nReview: https://review.openstack.org/192244", 
            "date_created": "2015-06-16 15:08:50.094093+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Reviewed:  https://review.openstack.org/192244\nCommitted: https://git.openstack.org/cgit/openstack/nova/commit/?id=7bc4be781564c6b9e7a519aecea84ddbee6bd935\nSubmitter: Jenkins\nBranch:    stable/juno\n\ncommit 7bc4be781564c6b9e7a519aecea84ddbee6bd935\nAuthor: Matt Riedemann <email address hidden>\nDate:   Wed Apr 15 11:51:26 2015 -0700\n\n    compute: stop handling virt lifecycle events in cleanup_host()\n    \n    When rebooting a compute host, guest VMs can be getting shutdown\n    automatically by the hypervisor and the virt driver is sending events to\n    the compute manager to handle them. If the compute service is still up\n    while this happens it will try to call the stop API to power off the\n    instance and update the database to show the instance as stopped.\n    \n    When the compute service comes back up and events come in from the virt\n    driver that the guest VMs are running, nova will see that the vm_state\n    on the instance in the nova database is STOPPED and shut down the\n    instance by calling the stop API (basically ignoring what the virt\n    driver / hypervisor tells nova is the state of the guest VM).\n    \n    Alternatively, if the compute service shuts down after changing the\n    intance task_state to 'powering-off' but before the stop API cast is\n    complete, the instance can be in a strange vm_state/task_state\n    combination that requires the admin to manually reset the task_state to\n    recover the instance.\n    \n    Let's just try to avoid some of this mess by disconnecting the event\n    handling when the compute service is shutting down like we do for\n    neutron VIF plugging events. There could still be races here if the\n    compute service is shutting down after the hypervisor (e.g. libvirtd),\n    but this is at least a best attempt to do the mitigate the potential\n    damage.\n    \n    Closes-Bug: #1444630\n    Related-Bug: #1293480\n    Related-Bug: #1408176\n    \n    Conflicts:\n    \tnova/compute/manager.py\n    \tnova/tests/unit/compute/test_compute_mgr.py\n    \n    Change-Id: I1a321371dff7933cdd11d31d9f9c2a2f850fd8d9\n    (cherry picked from commit d1fb8d0fbdd6cb95c43b02f754409f1c728e8cd0)\n", 
            "date_created": "2015-06-18 01:51:15.195229+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Libvirt event threads are not stopped during stopping of nova-compute service. That'w why during restart nova-compute with SIGHUP signal we can see traceback:\n\n2015-11-30 10:03:06.013 INFO nova.service [-] Starting compute node (version 13.0.0)\n2015-11-30 10:03:06.013 DEBUG nova.virt.libvirt.host [-] Starting native event thread from (pid=17505) _init_events /opt/stack/nova/nova/virt/libvirt/host.py:452\n2015-11-30 10:03:06.014 DEBUG nova.virt.libvirt.host [-] Starting green dispatch thread from (pid=17505) _init_events /opt/stack/nova/nova/virt/libvirt/host.py:458\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py\", line 115, in wait\n    listener.cb(fileno)\n  File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n    result = function(*args, **kwargs)\n  File \"/opt/stack/nova/nova/utils.py\", line 1158, in context_wrapper\n    return func(*args, **kwargs)\n  File \"/opt/stack/nova/nova/virt/libvirt/host.py\", line 248, in _dispatch_thread\n    self._dispatch_events()\n  File \"/opt/stack/nova/nova/virt/libvirt/host.py\", line 353, in _dispatch_events\n    assert _c\nAssertionError\nRemoving descriptor: 9\n\nStarted threads should be stopped during stopping of nova-compute service", 
            "date_created": "2015-12-17 14:56:08.160127+00:00", 
            "author": "https://api.launchpad.net/1.0/~mhorban"
        }, 
        {
            "content": "Fix proposed to branch: master\nReview: https://review.openstack.org/259066", 
            "date_created": "2015-12-17 15:10:33.860801+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }, 
        {
            "content": "Change abandoned by Matt Riedemann (<email address hidden>) on branch: master\nReview: https://review.openstack.org/259066\nReason: Please open a new bug for tracking this rather than re-opening something that was already marked as fixed.", 
            "date_created": "2016-04-18 19:21:46.886304+00:00", 
            "author": "https://api.launchpad.net/1.0/~hudson-openstack"
        }
    ], 
    "closed": "2016-04-18 19:21:57.009235+00:00"
}
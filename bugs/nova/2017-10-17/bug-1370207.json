{
    "status": "Invalid", 
    "last_updated": "2015-12-14 14:12:10.174151+00:00", 
    "description": "This is for nova 2014.1.2.\n\nHere, nova DB is the shared resources between nova-scheduler and nova-compute. Nova-scheduler checks DB to see if hv node can meet the provision requirement, nova-compute is the actual process to modify DB to reduce the free_ram_mb.\n\nFor example, current available RAM on hv is 56G, with ram_allocation_ration=1.0. Within a minute, 3 vm provision requests are coming to scheduler, each asking for 24G RAM.\n \n     t1: scheduler gets a request for vm1, assign vm1 to hv\n     t2: scheduler gets a request for vm2, assign vm2 to hv\n     t3: vm1 is created, nova-compute updates nova DB with RAM=32G\n     t4: scheduler gets a request for vm3, assign vm3 to hv\n     t5: vm2 is created, nova-compute updates nova DB with RAM=8G\n     t6: vm3 is created, nova-compute updates nova DB with RAM=-16G\n\nIn the end, we have a negative RAM with ram_allocation_ratio=1.0.", 
    "tags": [
        "scheduler"
    ], 
    "importance": "Low", 
    "heat": 10, 
    "link": "https://bugs.launchpad.net/nova/+bug/1370207", 
    "owner": "None", 
    "id": 1370207, 
    "index": 1384, 
    "openned": "2014-09-16 19:08:18.500681+00:00", 
    "created": "2014-09-16 19:08:18.500681+00:00", 
    "title": "race condition between nova scheduler and nova compute", 
    "comments": [
        {
            "content": "This is for nova 2014.1.2.\n\nHere, nova DB is the shared resources between nova-scheduler and nova-compute. Nova-scheduler checks DB to see if hv node can meet the provision requirement, nova-compute is the actual process to modify DB to reduce the free_ram_mb.\n\nFor example, current available RAM on hv is 56G, with ram_allocation_ration=1.0. Within a minute, 3 vm provision requests are coming to scheduler, each asking for 24G RAM.\n \n     t1: scheduler gets a request for vm1, assign vm1 to hv\n     t2: scheduler gets a request for vm2, assign vm2 to hv\n     t3: vm1 is created, nova-compute updates nova DB with RAM=32G\n     t4: scheduler gets a request for vm3, assign vm3 to hv\n     t5: vm2 is created, nova-compute updates nova DB with RAM=8G\n     t6: vm3 is created, nova-compute updates nova DB with RAM=-16G\n\nIn the end, we have a negative RAM with ram_allocation_ratio=1.0.", 
            "date_created": "2014-09-16 19:08:18.500681+00:00", 
            "author": "https://api.launchpad.net/1.0/~weidu"
        }, 
        {
            "content": "I'm surprised that nova-compute actually started the last one and didn't run out of memory on the environment. Any idea how that happened?", 
            "date_created": "2014-09-16 20:26:53.186092+00:00", 
            "author": "https://api.launchpad.net/1.0/~sdague"
        }, 
        {
            "content": "Hi Sean, thanks for your quick response. Does nova-compute check for resource availability before actually creating a domain? I did see nova compute updates resources through ResourceTracker::_update_usage_from_instance function in https://github.com/openstack/nova/blob/stable/icehouse/nova/compute/resource_tracker.py.  Thanks.", 
            "date_created": "2014-09-17 00:30:23.971892+00:00", 
            "author": "https://api.launchpad.net/1.0/~weidu"
        }, 
        {
            "content": "There seems to be a method in the scheduler that virtually consumes resources on cache when a vm is assigned. Does that not prevent the scheduler from assigning vm3 in step t4?", 
            "date_created": "2015-06-18 21:03:07.303631+00:00", 
            "author": "https://api.launchpad.net/1.0/~mingy"
        }, 
        {
            "content": "I was trying to replicate the bug but wasn't successful in reproducing it. Could you still check if you are able to reproduce the bug?", 
            "date_created": "2015-11-02 22:00:48.976334+00:00", 
            "author": "https://api.launchpad.net/1.0/~sumant-murke"
        }, 
        {
            "content": "Currently the following happens:\n\n1. scheduler selects a destination host based on host state obtained from the DB\n\n2. request sent to that host\n\n3. host checks resource availability locally - see claims code in resource tracker\n3.a if enough resource boot VM and update state in the DB\n\nThe consume method in the filter scheduler is only used for multiple boot requests - e.g. the scheduler can be asked to find places to fit 10 similar VMs. Without this they would all be put in the same place. The filter scheduler updates the host state from the DB for each request.\n\nThe above behavior is basically \"as designed\" an not a bug as such. But you can argue that it is not appropriate for a constrained set of resources.", 
            "date_created": "2015-12-14 08:40:01.437319+00:00", 
            "author": "https://api.launchpad.net/1.0/~pmurray"
        }, 
        {
            "content": "This seems to be by design i.e. Scheduler can get out of sync, and we have the claim-and-retry mechanism in place so request for vm3 would fail and trigger a reschedule. ", 
            "date_created": "2015-12-14 14:11:59.919264+00:00", 
            "author": "https://api.launchpad.net/1.0/~ndipanov"
        }
    ], 
    "closed": "2015-12-14 14:12:08.404854+00:00"
}